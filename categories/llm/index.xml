<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM on Linote</title>
    <link>http://yangchnet.github.io/Dessert/categories/llm/</link>
    <description>Recent content in LLM on Linote</description>
    <image>
      <url>http://yangchnet.github.io/Dessert/papermod-cover.png</url>
      <link>http://yangchnet.github.io/Dessert/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 04 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://yangchnet.github.io/Dessert/categories/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>拥抱大模型（三）：OutputParser，格式化输出</title>
      <link>http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%89outputparser%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%89outputparser%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA/</guid>
      <description>大模型可以回答你的任何问题，但有时我们需要将大模型的回复进行格式化解析以便进行后续的处理，此时就需要我们使用一些特殊的技巧提示大模型：你应该如此如此，这般这般返回一个简单的例子（文心一言）：可以看到，我们在prompt中告诉大模型，你应该以如下json格式返回，大模型按照我们的要求，切实返回了我们要求的json格式。这样，我们就可以把大模型的输出进行解析，大模型的输出，不再是无法解析的数据。langchain为我们提供了一系列工具来为prompt添加输出格式指令，解析输出，重试机制等等。 使用LangChain工具 PydanticOutputParser(json输出解析) from pydantic import BaseModel, Field from langchain.output_parsers import PydanticOutputParser, OutputFixingParser class FlowerDescription(BaseModel): title: str = Field(description=&amp;#34;这本书的标题&amp;#34;) author: int = Field(description=&amp;#34;这本书的作者&amp;#34;) words: str = Field(description=&amp;#34;这本书的字数&amp;#34;) description: str = Field(description=&amp;#34;这本书的主要情节&amp;#34;) # 定义输出解析器 output_parser = PydanticOutputParser(pydantic_object=FlowerDescription) # 获取输出格式指示 format_instructions = output_parser.get_format_instructions() print(format_instructions) The output should be formatted as a JSON instance that conforms to the JSON schema below. As an example, for the schema {&amp;#34;properties&amp;#34;: {&amp;#34;foo&amp;#34;: {&amp;#34;title&amp;#34;: &amp;#34;Foo&amp;#34;, &amp;#34;description&amp;#34;: &amp;#34;a list of strings&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;array&amp;#34;, &amp;#34;items&amp;#34;: {&amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;}}}, &amp;#34;required&amp;#34;: [&amp;#34;foo&amp;#34;]}} the object {&amp;#34;foo&amp;#34;: [&amp;#34;bar&amp;#34;, &amp;#34;baz&amp;#34;]} is a well-formatted instance of the schema.</description>
    </item>
    
    <item>
      <title>拥抱大模型（二）：Prompt，给大模型有用的提示</title>
      <link>http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%8Cprompt%E7%BB%99%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E7%94%A8%E7%9A%84%E6%8F%90%E7%A4%BA/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%8Cprompt%E7%BB%99%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E7%94%A8%E7%9A%84%E6%8F%90%E7%A4%BA/</guid>
      <description>极客时间《LangChain实战课》学习笔记
 构建prompt的原则 原则（吴恩达版）
 写出清晰而具体的提示 给模型思考的时间  原则（OpenAI版）
 写清晰的指示 给模型提供参考（也就是示例） 将复杂任务拆分成子任务 给 GPT 时间思考 使用外部工具 反复迭代问题   prompt的基本结构 一个prompt主要包含四块内容：
 instruction（指令）：告诉大模型要做什么，一个常见且有效的例子是，告诉大模型“你是一个XX专家” context（上下文）：充当模型的额外知识来源，这些知识可以从矢量数据库中得来或通过其他方式拉入 prompt input （提示输入）：具体的问题或大模型做的具体事情 output indicator（标记要生成的文本的开始）：用一个明显的提示词让大模型开始回答，这一部分不是必须的  使用langchain构建prompt
from langchain import PromptTemplate template = &amp;#34;&amp;#34;&amp;#34;\ 你是业务咨询顾问。 你给一个销售{product}的电商公司，起一个好的名字？ &amp;#34;&amp;#34;&amp;#34; prompt = PromptTemplate.from_template(template) print(prompt.format(product=&amp;#34;鲜花&amp;#34;)) prompt = PromptTemplate( input_variables=[&amp;#34;product&amp;#34;, &amp;#34;market&amp;#34;], template=&amp;#34;你是业务咨询顾问。对于一个面向{market}市场的，专注于销售{product}的公司，你会推荐哪个名字？&amp;#34; ) print(prompt.format(product=&amp;#34;鲜花&amp;#34;, market=&amp;#34;高端&amp;#34;)) 二者效果相同构建chat prompt对于像ChatGPT这种聊天模型，langchain提供了ChatPromptTemplate，其中有多种角色类型:
import openai openai.ChatCompletion.create( model=&amp;#34;gpt-3.5-turbo&amp;#34;, messages=[ {&amp;#34;role&amp;#34;: &amp;#34;system&amp;#34;, &amp;#34;content&amp;#34;: &amp;#34;You are a helpful assistant.</description>
    </item>
    
    <item>
      <title>拥抱大模型（五）：Memory，让大模型拥有记忆</title>
      <link>http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%94memory%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%94memory%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86/</guid>
      <description>要让大模型有记忆也很简单，你只需要把之前的对话传递给他就行，在ConversationChain中，就使用了这一技巧：
from langchain import OpenAI from langchain.chains import ConversationChain # 初始化大语言模型 llm = OpenAI( temperature=0.5, model_name=&amp;quot;text-davinci-003&amp;quot; ) # 初始化对话链 conv_chain = ConversationChain(llm=llm) # 打印对话的模板 print(conv_chain.prompt.template) 来看下模板：
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI: 可以看到，在模板里又一个history字段，这里存储了之前对话的上下文信息。当有了 {history} 参数，以及 Human 和 AI 这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理。 langchain中提供的记忆机制 ConversationBufferMemory 在 LangChain 中，通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。下面，我们就在对话链中引入 ConversationBufferMemory。</description>
    </item>
    
    <item>
      <title>拥抱大模型（四）：Chain，像人类一样思考</title>
      <link>http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%9Bchain%E5%83%8F%E4%BA%BA%E7%B1%BB%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/</link>
      <pubDate>Sun, 04 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%9Bchain%E5%83%8F%E4%BA%BA%E7%B1%BB%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/</guid>
      <description>什么叫Chain，从字面意思理解，Chain是一个链，我们可以通过Chain来链接LangChain的各个组件和功能-模型之间彼此链接，或模型与其他组件链接。这种将多个组件相互链接，组合成一个链的想法简单但很强大。它简化了复杂应用程序的实现，并使之更加模块化，能够创建出单一的、连贯的应用程序，从而使调试、维护和改进应用程序变得容易。
我们可以简单的把Chain理解为通过设计好的一些链路去调用大模型，从而获取我们想要的结果。下面是一个例子：
 首先我们让大模型扮演产品经理，给出小说推荐网站的产品设计。  有了产品设计后，由架构师进行初步的架构设计  现在架构设计也有了，来个程序员写SQL：  那，上面的这种promot链，我们用langchain怎么实现呢？ 使用langchain实现 Sequential Chain 首先，导入所有需要的库
# 设置OpenAI API密钥 import os os.environ[&amp;#34;OPENAI_API_KEY&amp;#34;] = &amp;#39;你的OpenAI API Key&amp;#39; from langchain.llms import OpenAI from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain.chains import SequentialChain 添加第一个LLMChain，生成小说网站的产品设计。LLMChain可以看作是链条的一个环节
# 这是第一个LLMChain，用于生成鲜花的介绍，输入为花的名称和种类 llm = OpenAI(temperature=.7) template = &amp;#34;&amp;#34;&amp;#34; 你是一个产品经理，请你对{product}给出一个设计想法。&amp;#34;&amp;#34;&amp;#34; prompt_template = PromptTemplate(input_variables=[&amp;#34;product&amp;#34;], template=template) introduction_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=&amp;#34;introduction&amp;#34;) 添加第二个LLMChain，根据产品设计生成软件架构
# 这是第二个LLMChain，用于根据鲜花的介绍写出鲜花的评论 llm = OpenAI(temperature=.7) template = &amp;#34;&amp;#34;&amp;#34; 你是一位软件架构师。给定一个{product}的设计，你需要给出产品软件架构说明和技术栈选型。 产品设计: {introduction} 软件架构说明:&amp;#34;&amp;#34;&amp;#34; prompt_template = PromptTemplate(input_variables=[&amp;#34;product&amp;#34;, &amp;#34;introduction&amp;#34;], template=template) review_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=&amp;#34;framwork&amp;#34;) 第三个Chain，根据产品架构生成SQL语句</description>
    </item>
    
  </channel>
</rss>
