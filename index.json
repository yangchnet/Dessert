[{"content":"一切行动有规划 作为计算机专业的学生，我们要考的科目很多，遍历下来有政治、英语、高等数学、线性代数、概率论与数理统计、408计算机专业综合（数据结构、计算机组成原理、操作系统、计算机网络），如果没有一个详细的规划，不知道如何安排时间，就会感觉一团乱麻，总是感觉厚此薄彼或是毫无头绪。因此，我们需要一个合理的时间安排。首先是一个总的时间安排，即复习的进度安排，比如，要在9月之前完成一轮复习、11月之前完成二轮复习等。接下来，要根据进度安排，适当安排自己的每一天，这里所说的适当是指：按照你的能力和复习进度给自己一个在能力范围之内的目标值，并努力去完成它，但同时我们要给自己设定一个最低目标以鞭策自己。政治中说，我们要坚持底线思维，立足最低点，争取最大值，就是这个道理。\n不浪费一分一秒 很明显，考研时间非常之紧张，我们一般是从3月左右开始复习，而此时的复习显然不能很快进入状态。因此，很多人的复习实际上是从暑假才真正开始，前面只能算是预热。从暑假开始，我们就要进入一个紧张的状态，每天基本只有三件事：看书、吃饭、睡觉。我告诉大家不要浪费一分一秒，不是让大家废寝忘食，不顾身体，每天做一个莫得感情的刷题机器，而是让大家珍惜学习时间中的每一分，每一秒。也就是说，你可以有适当的休息时间，暑假也可以适当回家玩玩，但是当你坐到书桌旁，就必须心无旁骛，专心学习。\n一切目标为考研 考研的时候，你可能觉得有很多事想做，但是你要考虑到，做这件事对你的考研初试成绩提高有没有意义。这里之所以强调初试，是因为很多人觉得，这件事对复试很好，我可以提前做。其实不然，说句不好听的，还不一定能进复试呢。牢记自己的目标，将考研优先级提到最高，在考研期间，只有那些能提高你成绩的事做的才有意义，其他的事，先放一放罢。延迟享受。\n 适当休息 大家都说考研是一场持久战，事实如此。每天从早上到晚上坐在书桌旁，刚开始可能不觉得有什么，然后你会开始屁股疼，这时候一个坐垫就可以解决问题，再然后你会感到骨头仿佛生锈，脖子不舒服，这时候你需要一本颈复康指南，最后你会发现视力下降、发量减少，这时候你需要一瓶霸王。每周我们应该为自己安排一个休息时间，比如周日或周六，而且我建议，这个休息时间应放在上午，而不是下午或晚上。放在下午的话，我们可能休息的有点懒散，干脆晚上也不去了，这样就把晚上也浪费掉了，晚上的话，大家可能又玩的太嗨，导致第二天起不来，而放在上午，一是可以睡个懒觉，二是当你学习了一个下午加一个晚上后，晚上回到寝室，依然还跟以前的状态一样，感觉上就像学了一整天，可以充分保持自己的状态。坚持锻炼，身体是革命的本钱，没有良好的身体素质做支撑，如何能承受每天14个小时，全年无休的学习.\n做题要做实 每做一题，应该做一个深入的思考，并拿笔记录下来，这并不是在浪费你的刷题时间，而是在节省你将来的复习时间。比如，我做完一题，我会给他评一个分数，后期还会指出其所考的知识点。之所以后期才指出知识点，是因为前期我对知识点的掌握还不全面不到位。我对题目的评分规则如下：（大家都知道，在Linux系统中有权限管理，比如一个文件，它对于文件所有者的权限可能是rwx,代表可读可写可执行，其中r为读，用数字1表示，w为写，用数字3表示，x为执行，用数字5表示，那么此文件对于另一个用户的权限为5，我们很容易就可推算出这个用户对这个文件只具有读和执行权限，不具有写权限。）[“第一次不会做”，“好题”，“难题”，“综合题”], 其中“第一次不会做”对应1，“好题”对应2，“难题”对应4，“综合题”对应8，这样的话，如果我对一个题目的评分为6（可以写作$=6或任何其他标志），那么就可以很容易得知，这个题比较好，并且比较难，但是我第一次可以做出来。当然，这四个评价完全可以换成任何其他评价，只要你觉得有用就好。知识点我会用[]来标注，比如某个题考了二维概率密度，那么就用红笔标注[二维概率密度]。你可以用你独特的方法来标注你做过的题目，来让自己以后的复习更加具有针对性。但是不要忘记，这些东西只是帮你记录一些东西，你需要自己动脑想一想，将这个题的出题目的，答题技巧，相似考题等都过一遍。最重要的还是自己的思考过程，不要被这些花里胡哨的东西迷花了眼。\n知识要总结 学过的知识，要趁热打铁，对其进行有针对性的总结，将知识点，自己的理解等都记下来。不过不可能会说，那些知识点辅导书上都已经有了，我为什么要再写一遍，这不是浪费我时间嘛。我想告诉你的是，书上的是面向所有考生的，而你写下来，只是面对你自己。现在的东西都讲究个性化，我们也要给自己一个个性化定制的知识系统。总结不只是简单的把知识点抄下来，其中还包含这你对知识点的理解，甚至你还可以加上这个知识点的经典例题。不过，我见过很多人，知识点总结的很好，可惜到最后都没看，诸君要避免这个问题，不然不如省下时间背几个英语单词。\n做错题要订正并牢记 考研复习，前期做加法，即不断增加自己的知识，后期做减法，即减少自己不会的，查漏补缺。那么我们怎么知道哪里有漏洞，哪里需要补呢？显然，错题可以帮我们快速定位，就好像一个错误的输出可以帮我们定位一个bug。题目做错，不一定是因为此处未掌握好，还有可能是题目太难、粗心做错，但无论是哪种问题，错题都是我们快速找到漏洞的最好办法。对待错题，我们最好有一个专门的错题本（找一个好点的本子，这样你丢也不舍得丢，写起来也舒服），将自己的错题摘抄到上面并写下错误原因与正确答案。后期错题本将派上大用场。\n不畏难 不畏难，指不怕难题，不怕难学的知识点，任何知识点只要出现，就说明它完全在我们可理解的范围之内。不要让一个基本的知识点成为你的绊脚石，那就太蠢了。不会的可以向别人虚心请教，可以和同学或朋友约好，每周找个时间专门讨论这个星期遇到的硬骨头。\n不浮躁 考研是十分枯燥乏味的，终日埋头苦学，没人说话，也没人管你，你需要调节自己的心理，不要心浮气躁。考研的时候大家一般都不带手机，有的人会带个耳机，带个MP3听个音乐。远离手机，从你做起。\n不妄自菲薄 不管是考研前期、中期还是后期，总会听到一些“大神”的战绩。比如考研初，听说哪个哪个辅导书都二刷了，考研后期，听说哪个哪个数学真题做了好几个150。我想说，这些和你没关系，辅导书二刷的未必坚持到考试，可能中间就业签三方走人了（真实经历，当时听说辅导书二刷给我整蒙了，后来听说签三方又蒙了，真可谓是：懵逼树上懵逼果，懵逼果落砸死我），考150的大神有可能在吹牛，可能最后考试只考了100不到，可能还有的真大神，平时很厉害，一问啥都会，但是一到考试可能又不行了。说这些是想告诉大家，按照自己的计划，完成自己的预定目标就可以了。九阳真经中有一句话送给大家：他强任他强，清风拂山岗，他横任他横，明月照大江。\n不自我膨胀 有的时候，可能是给某个同学解答了问题、可能是今天的英语阅读全对了、可能是今天的数学考了140，总之就是膨胀了。这时候一定要告诉自己，天外有天，人外有人，不能膨胀，一定是同学太菜、英语走了狗屎运、数学太简单，想办法打击一下自己。不能膨胀，但也要保持自信，我就是图书馆最靓的仔。\n Q\u0026amp;A Q:专业课如何复习？（408）\nA：专业课一般从9月开始复习，前面可以稍微看看，但不要作为重点。专业课的特点是知识点多，需要在理解的基础上进行记忆。因此我们一定要打牢基础，首先将书本知识过一遍，在书本知识全部理解的基础上，再做辅导书上的习题。辅导书需要看很多遍，所以我们做的时候可以在其他地方比如草稿纸上写下答案，并将错题记录下来。专业课要想考的好，一定要多看几轮，复习的全面一点，408考4门课，每一门课都有很多知识点，搞不定就考到哪个知识点，我们只能尽量复习的全面。后期也要刷真题，最好多刷几遍。\n Q：怎样才能看到一个学校的报录比之类的？\nA：每个学校可能不大一样，你可以去该校的研究生招生网看一看，或者去他们的考研群找一些学长问问。\n Q：英语数学之类看的视频从哪找?\nA：可以从百度云、B站、公众号等处，甚至你还可以去Pornhub看考研视频。。。。\n Q: 一定要报补习班或者跟着某个老师的教学视频学吗？\nA：不一定，因人而异，有的人喜欢看视频，觉得自己看书太慢，有的人看视频犯困，只喜欢自己慢慢啃（比如我）。补习班这个嘛，咱也没上过，咱也不知道。\n Any problems?\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%9D%82%E8%AE%B0/%E8%80%83%E7%A0%94/","summary":"一切行动有规划 作为计算机专业的学生，我们要考的科目很多，遍历下来有政治、英语、高等数学、线性代数、概率论与数理统计、408计算机专业综合（数据结构、计算机组成原理、操作系统、计算机网络），如果没有一个详细的规划，不知道如何安排时间，就会感觉一团乱麻，总是感觉厚此薄彼或是毫无头绪。因此，我们需要一个合理的时间安排。首先是一个总的时间安排，即复习的进度安排，比如，要在9月之前完成一轮复习、11月之前完成二轮复习等。接下来，要根据进度安排，适当安排自己的每一天，这里所说的适当是指：按照你的能力和复习进度给自己一个在能力范围之内的目标值，并努力去完成它，但同时我们要给自己设定一个最低目标以鞭策自己。政治中说，我们要坚持底线思维，立足最低点，争取最大值，就是这个道理。\n不浪费一分一秒 很明显，考研时间非常之紧张，我们一般是从3月左右开始复习，而此时的复习显然不能很快进入状态。因此，很多人的复习实际上是从暑假才真正开始，前面只能算是预热。从暑假开始，我们就要进入一个紧张的状态，每天基本只有三件事：看书、吃饭、睡觉。我告诉大家不要浪费一分一秒，不是让大家废寝忘食，不顾身体，每天做一个莫得感情的刷题机器，而是让大家珍惜学习时间中的每一分，每一秒。也就是说，你可以有适当的休息时间，暑假也可以适当回家玩玩，但是当你坐到书桌旁，就必须心无旁骛，专心学习。\n一切目标为考研 考研的时候，你可能觉得有很多事想做，但是你要考虑到，做这件事对你的考研初试成绩提高有没有意义。这里之所以强调初试，是因为很多人觉得，这件事对复试很好，我可以提前做。其实不然，说句不好听的，还不一定能进复试呢。牢记自己的目标，将考研优先级提到最高，在考研期间，只有那些能提高你成绩的事做的才有意义，其他的事，先放一放罢。延迟享受。\n 适当休息 大家都说考研是一场持久战，事实如此。每天从早上到晚上坐在书桌旁，刚开始可能不觉得有什么，然后你会开始屁股疼，这时候一个坐垫就可以解决问题，再然后你会感到骨头仿佛生锈，脖子不舒服，这时候你需要一本颈复康指南，最后你会发现视力下降、发量减少，这时候你需要一瓶霸王。每周我们应该为自己安排一个休息时间，比如周日或周六，而且我建议，这个休息时间应放在上午，而不是下午或晚上。放在下午的话，我们可能休息的有点懒散，干脆晚上也不去了，这样就把晚上也浪费掉了，晚上的话，大家可能又玩的太嗨，导致第二天起不来，而放在上午，一是可以睡个懒觉，二是当你学习了一个下午加一个晚上后，晚上回到寝室，依然还跟以前的状态一样，感觉上就像学了一整天，可以充分保持自己的状态。坚持锻炼，身体是革命的本钱，没有良好的身体素质做支撑，如何能承受每天14个小时，全年无休的学习.\n做题要做实 每做一题，应该做一个深入的思考，并拿笔记录下来，这并不是在浪费你的刷题时间，而是在节省你将来的复习时间。比如，我做完一题，我会给他评一个分数，后期还会指出其所考的知识点。之所以后期才指出知识点，是因为前期我对知识点的掌握还不全面不到位。我对题目的评分规则如下：（大家都知道，在Linux系统中有权限管理，比如一个文件，它对于文件所有者的权限可能是rwx,代表可读可写可执行，其中r为读，用数字1表示，w为写，用数字3表示，x为执行，用数字5表示，那么此文件对于另一个用户的权限为5，我们很容易就可推算出这个用户对这个文件只具有读和执行权限，不具有写权限。）[“第一次不会做”，“好题”，“难题”，“综合题”], 其中“第一次不会做”对应1，“好题”对应2，“难题”对应4，“综合题”对应8，这样的话，如果我对一个题目的评分为6（可以写作$=6或任何其他标志），那么就可以很容易得知，这个题比较好，并且比较难，但是我第一次可以做出来。当然，这四个评价完全可以换成任何其他评价，只要你觉得有用就好。知识点我会用[]来标注，比如某个题考了二维概率密度，那么就用红笔标注[二维概率密度]。你可以用你独特的方法来标注你做过的题目，来让自己以后的复习更加具有针对性。但是不要忘记，这些东西只是帮你记录一些东西，你需要自己动脑想一想，将这个题的出题目的，答题技巧，相似考题等都过一遍。最重要的还是自己的思考过程，不要被这些花里胡哨的东西迷花了眼。\n知识要总结 学过的知识，要趁热打铁，对其进行有针对性的总结，将知识点，自己的理解等都记下来。不过不可能会说，那些知识点辅导书上都已经有了，我为什么要再写一遍，这不是浪费我时间嘛。我想告诉你的是，书上的是面向所有考生的，而你写下来，只是面对你自己。现在的东西都讲究个性化，我们也要给自己一个个性化定制的知识系统。总结不只是简单的把知识点抄下来，其中还包含这你对知识点的理解，甚至你还可以加上这个知识点的经典例题。不过，我见过很多人，知识点总结的很好，可惜到最后都没看，诸君要避免这个问题，不然不如省下时间背几个英语单词。\n做错题要订正并牢记 考研复习，前期做加法，即不断增加自己的知识，后期做减法，即减少自己不会的，查漏补缺。那么我们怎么知道哪里有漏洞，哪里需要补呢？显然，错题可以帮我们快速定位，就好像一个错误的输出可以帮我们定位一个bug。题目做错，不一定是因为此处未掌握好，还有可能是题目太难、粗心做错，但无论是哪种问题，错题都是我们快速找到漏洞的最好办法。对待错题，我们最好有一个专门的错题本（找一个好点的本子，这样你丢也不舍得丢，写起来也舒服），将自己的错题摘抄到上面并写下错误原因与正确答案。后期错题本将派上大用场。\n不畏难 不畏难，指不怕难题，不怕难学的知识点，任何知识点只要出现，就说明它完全在我们可理解的范围之内。不要让一个基本的知识点成为你的绊脚石，那就太蠢了。不会的可以向别人虚心请教，可以和同学或朋友约好，每周找个时间专门讨论这个星期遇到的硬骨头。\n不浮躁 考研是十分枯燥乏味的，终日埋头苦学，没人说话，也没人管你，你需要调节自己的心理，不要心浮气躁。考研的时候大家一般都不带手机，有的人会带个耳机，带个MP3听个音乐。远离手机，从你做起。\n不妄自菲薄 不管是考研前期、中期还是后期，总会听到一些“大神”的战绩。比如考研初，听说哪个哪个辅导书都二刷了，考研后期，听说哪个哪个数学真题做了好几个150。我想说，这些和你没关系，辅导书二刷的未必坚持到考试，可能中间就业签三方走人了（真实经历，当时听说辅导书二刷给我整蒙了，后来听说签三方又蒙了，真可谓是：懵逼树上懵逼果，懵逼果落砸死我），考150的大神有可能在吹牛，可能最后考试只考了100不到，可能还有的真大神，平时很厉害，一问啥都会，但是一到考试可能又不行了。说这些是想告诉大家，按照自己的计划，完成自己的预定目标就可以了。九阳真经中有一句话送给大家：他强任他强，清风拂山岗，他横任他横，明月照大江。\n不自我膨胀 有的时候，可能是给某个同学解答了问题、可能是今天的英语阅读全对了、可能是今天的数学考了140，总之就是膨胀了。这时候一定要告诉自己，天外有天，人外有人，不能膨胀，一定是同学太菜、英语走了狗屎运、数学太简单，想办法打击一下自己。不能膨胀，但也要保持自信，我就是图书馆最靓的仔。\n Q\u0026amp;A Q:专业课如何复习？（408）\nA：专业课一般从9月开始复习，前面可以稍微看看，但不要作为重点。专业课的特点是知识点多，需要在理解的基础上进行记忆。因此我们一定要打牢基础，首先将书本知识过一遍，在书本知识全部理解的基础上，再做辅导书上的习题。辅导书需要看很多遍，所以我们做的时候可以在其他地方比如草稿纸上写下答案，并将错题记录下来。专业课要想考的好，一定要多看几轮，复习的全面一点，408考4门课，每一门课都有很多知识点，搞不定就考到哪个知识点，我们只能尽量复习的全面。后期也要刷真题，最好多刷几遍。\n Q：怎样才能看到一个学校的报录比之类的？\nA：每个学校可能不大一样，你可以去该校的研究生招生网看一看，或者去他们的考研群找一些学长问问。\n Q：英语数学之类看的视频从哪找?\nA：可以从百度云、B站、公众号等处，甚至你还可以去Pornhub看考研视频。。。。\n Q: 一定要报补习班或者跟着某个老师的教学视频学吗？\nA：不一定，因人而异，有的人喜欢看视频，觉得自己看书太慢，有的人看视频犯困，只喜欢自己慢慢啃（比如我）。补习班这个嘛，咱也没上过，咱也不知道。\n Any problems?","title":"考研"},{"content":" 本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic   推荐阅读：slice和数组的区别 切片append规则\n ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/runtime%E7%AF%87%E4%BA%94slice/","summary":" 本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic   推荐阅读：slice和数组的区别 切片append规则\n ","title":"runtime篇五：slice"},{"content":" 本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic  1. defer是什么 defer，是一种特殊的机制，在调用普通函数或方法前加上关键字defer，就完成了defer所需要的语法。当defer语句被执行时，跟在defer后面的函数会被延迟执行。直到包含该defer语句的函数执行完毕时，defer后的函数才会被执行，多个defer的执行顺序与声明顺序相反。\n对于defer的使用及需要注意的地方，可参考defer用法。这里不再讨论。\n在golang runtime中，defer被描述为一个结构体：\ntype _defer struct { started bool // 是否开始执行defer函数 \theap bool // 是否分配在堆上  openDefer bool // 是否经过开放编码（open-coded）的优化 \tsp uintptr // 调用defer时的栈指针 stack pointer \tpc uintptr // 调用defer函数时的pc值 \tfn func() // defer关键字传入的函数， 当使用open-coded defer时可为空 \t_panic *_panic // defer函数中的_panic链表 \tlink *_defer // 在goroutine中的下一个defer，可指向堆或栈  // 如果openDefer为true，则下面的字段将记录具有open-code defer的栈帧和相关的函数。  // 上面的sp将为帧的sp，pc将为defer调用的地址。 \tfd unsafe.Pointer // funcdata for the function associated with the frame \tvarp uintptr // value of varp for the stack frame  framepc uintptr // 当前栈帧的pc } 一个_defer结构体是defer调用的一环，一个函数中的多个defer被组织为链表的形式。defer有的分配在栈上，有的分配在堆上，但逻辑上他们都是属于栈的，因此在进行访问不需要加写屏障。\n可以看到， _defer中保存了很多字段，主要可分为三类，一是指示defer自身状态的标志位，如started,heap等；二是保存defer上下文，如sp, pc等；三是有关开放编码的一些字段。\n1.1 open-coded defer 在_defer中，有一个字段openDefer指示是否这个defer经过开放编码的优化，那么，什么是open code defer?\n对于一个defer函数来说，将其放入defer链表调用与直接调用是存在性能差异的，例如直接调用的耗时可能在6ns左右，而从defer链表中调用的耗时在35ns左右[1]，其中的主要原因在于，将函数放入defer链表或将其取出执行时，需要对上下文环境做保存和重做。因此，在go1.14中对其进行了优化，对于满足一定条件的defer，会进行open-coded优化。\n例如，对于如下代码：\ndefer f1(a) if cond { defer f2(b) } body... 将被编译为：\n// 设置标识位 deferBits |= 1\u0026lt;\u0026lt;0 tmpF1 = f1 tmpA = a if cond { deferBits |= 1\u0026lt;\u0026lt;1 tmpF2 = f2 tmpB = b } body... exit: // 在退出时检查标记位，以判断某个defer函数是否需要被执行 if deferBits \u0026amp; 1\u0026lt;\u0026lt;1 != 0 { deferBits \u0026amp;^= 1\u0026lt;\u0026lt;1 tmpF2(tmpB) } if deferBits \u0026amp; 1\u0026lt;\u0026lt;0 != 0 { deferBits \u0026amp;^= 1\u0026lt;\u0026lt;0 tmpF1(tmpA) } 这里不再将defer简单的放入defer链表了事，而是将其添加到了函数的底部，并通过标志位进行检查。\n1.2 newdefer // 每个p维护一个defer池  // Allocate a Defer, usually using per-P pool. // Each defer must be released with freedefer. The defer is not // added to any defer chain yet. func newdefer() *_defer { var d *_defer mp := acquirem() // 获取当前g的m, m.locks++ \tpp := mp.p.ptr() // m上的p \tif len(pp.deferpool) == 0 \u0026amp;\u0026amp; sched.deferpool != nil { // p的deferpool为空，但sched的deferpool不为空 \tlock(\u0026amp;sched.deferlock) // 加锁 \tfor len(pp.deferpool) \u0026lt; cap(pp.deferpool)/2 \u0026amp;\u0026amp; sched.deferpool != nil { // p的deferpool长度小于其容量的一半，且sched的deferpool不为空 \td := sched.deferpool // 从sched.deferpool取出一个_defer结构体 \tsched.deferpool = d.link // 将取出的_defer的下一个_defer重新链接到sched.deferpool \td.link = nil // 断链 \tpp.deferpool = append(pp.deferpool, d) // 取出的这个_defer加入到p的deferpool中 \t} unlock(\u0026amp;sched.deferlock) // 解锁 \t} if n := len(pp.deferpool); n \u0026gt; 0 { // 从p的deferpool中取出一个_defer \td = pp.deferpool[n-1] pp.deferpool[n-1] = nil pp.deferpool = pp.deferpool[:n-1] } releasem(mp) // m.locks-- \tmp, pp = nil, nil if d == nil { // 如果从sched的deferpool和p的deferpool中都没有取到现成的_defer，只要新构建一个 \t// Allocate new defer. \td = new(_defer) } d.heap = true // 默认defer分配在堆上 \treturn d } 从newdefer函数中中可以看出，sched和p上均有deferpool，里面均保存了若干空的_defer对象以便复用，在想要创建一个新的_defer时，如果p的deferpool为空，会尝试从sched的deferpool中去取，然后放在p的deferpool中。如果两个地方都没有空_defer对象，那就只要新建一个了，而新建的这个_defer对象，默认分配在堆上。\n既然在newdefer时会从sched.deferpool中取，那么在释放_defer，响应的应该也会将用过的空_defer放入sched或p的deferpool中去。\n1.3 freedefer func freedefer(d *_defer) { d.link = nil // 断链 \t// After this point we can copy the stack.  if d._panic != nil { freedeferpanic() } if d.fn != nil { freedeferfn() } if !d.heap { // _defer不在堆上，不需要单独进行释放 \treturn } mp := acquirem() // 获取当前g的m \tpp := mp.p.ptr() // 获取m的p \tif len(pp.deferpool) == cap(pp.deferpool) { // p的deferpool满了 \t// Transfer half of local cache to the central cache. \t// 将一半的_defer从p.deferpool移动到sched.deferpool中 \tvar first, last *_defer // 这个循环取p.deferpool中的一半_defer组成一个链表，first、last分别为其第一、最后一个元素 \tfor len(pp.deferpool) \u0026gt; cap(pp.deferpool)/2 { n := len(pp.deferpool) d := pp.deferpool[n-1] // d是p.deferpool中的最后一个_defer \tpp.deferpool[n-1] = nil pp.deferpool = pp.deferpool[:n-1] // p.deferpool长度-1 \tif first == nil { first = d } else { last.link = d } last = d } lock(\u0026amp;sched.deferlock) // 加锁 \tlast.link = sched.deferpool // 头插 \tsched.deferpool = first unlock(\u0026amp;sched.deferlock) // 释放锁 \t} *d = _defer{} // 清空内部字段  pp.deferpool = append(pp.deferpool, d) // 直接放入p.deferpool  releasem(mp) mp, pp = nil, nil } 当一个_defer要被释放时，会尝试将其放入当前g所属p的deferpool中去，如果p的deferpool满了，则将p的deferpool中的一半元素放到sched的deferpool中，然后再将_defer放到p.deferpool\n2. 调用defer 从_defer的结构我们可以看到，_defer可能会分配到栈上，也可能会分配在堆上。\n2.1 栈上分配 对于大部分场景来说，在不开启open-coded defer的情况下会使用栈上分配。\n对于如下代码：\npackage main import \u0026#34;fmt\u0026#34; func main() { defer fmt.Println(1) defer fmt.Println(2) defer fmt.Println(3) defer func() { fmt.Println(4) }() } 对其进行编译:\ngo tool compile -S -N main.go # -N参数禁止编译时优化，如果不加-N，会使用open-coded defer ... 0x01e2 00482 (main.go:9) LEAQ \u0026#34;\u0026#34;.main.func1·f(SB), CX 0x01e9 00489 (main.go:9) MOVQ CX, \u0026#34;\u0026#34;..autotmp_12+112(SP) 0x01ee 00494 (main.go:9) LEAQ \u0026#34;\u0026#34;..autotmp_12+88(SP), AX 0x01f3 00499 (main.go:9) CALL runtime.deferprocStack(SB) ... 0x01e2处，将\u0026quot;\u0026quot;.main.func1·f(SB)函数加载到CX，接下来将CX中的内容放置在\u0026quot;\u0026quot;..autotmp_12+112(SP)。下一步将\u0026quot;\u0026quot;..autotmp_12+88(SP)作为runtime.deferprocStack的参数。\n这里可以简单分析一下，从\u0026quot;\u0026quot;..autotmp_12+88(SP)到\u0026quot;\u0026quot;..autotmp_12+112(SP)，这中间有24个字节的宽度，而_defer的前几个字段如下：\ntype _defer struct { started bool // 1字节 \theap bool // 1字节  openDefer bool // 1字节 \tsp uintptr // 8字节 \tpc uintptr // 8字节 \tfn func() // defer关键字传入的函数， 当使用open-coded defer时可为空 \t... } 在fn之前共有1+1+1+8+8=19个字节的宽度，但实际上在进行内存分配时，在3个bool型字段后，需要添加5字节进行内存对齐，因此从_defer的开始地址到fn的地址，这中间的宽度就变成了1+1+1+5+8+8=24。\n因此我们可以知道这几行汇编的前三行，是将函数赋值到_defer的fn字段上。\n知道了这些，我们再来看runtime.deferprocStack`：\nfunc deferprocStack(d *_defer) { gp := getg() // 获取当前g \tif gp.m.curg != gp { // go code on the system stack can\u0026#39;t defer \tthrow(\u0026#34;defer on system stack\u0026#34;) } // fn已被赋值过，其他的字段在这里赋值 \td.started = false d.heap = false d.openDefer = false d.sp = getcallersp() d.pc = getcallerpc() d.framepc = 0 d.varp = 0 *(*uintptr)(unsafe.Pointer(\u0026amp;d._panic)) = 0 *(*uintptr)(unsafe.Pointer(\u0026amp;d.fd)) = 0 *(*uintptr)(unsafe.Pointer(\u0026amp;d.link)) = uintptr(unsafe.Pointer(gp._defer)) *(*uintptr)(unsafe.Pointer(\u0026amp;gp._defer)) = uintptr(unsafe.Pointer(d)) return0() } 将_defer分配在栈上，并将其放入g的defer链表中。\n2.2 堆上分配 堆上分配是默认的兜底方案。\n编译如下代码：\npackage main import \u0026#34;fmt\u0026#34; func main() { for i := 0; i \u0026lt; 10; i++ { defer func(i int) { fmt.Println(i) }(i) } } go tool compile -S -N main.go 可看到其调用了runtime.deferproc:\nfunc deferproc(fn func()) { gp := getg() // 获取g \tif gp.m.curg != gp { // go code on the system stack can\u0026#39;t defer \tthrow(\u0026#34;defer on system stack\u0026#34;) } d := newdefer() // 获取一个_defer \tif d._panic != nil { throw(\u0026#34;deferproc: d.panic != nil after newdefer\u0026#34;) } d.link = gp._defer // 将_defer以头插法插入到g的_defer链表中 \tgp._defer = d d.fn = fn // 设置_defer函数 \td.pc = getcallerpc() // 保存上下文环境 \td.sp = getcallersp() return0() } 相对于栈上分配，在堆上分配设计到newdefer的调用，而通过上文的分析我们可知，在调用newdefer时，会涉及到sched、p以及对m的加锁，因此性能上不如栈上分配。且，栈上分配不需要写屏障。\n3. 执行defer package main import \u0026#34;fmt\u0026#34; func main() { defer fmt.Println(1) defer fmt.Println(2) defer fmt.Println(3) defer func() { fmt.Println(4) }() } 查看以上代码的汇编代码时，可看到：\n0x0200 00512 (main.go:12) CALL runtime.deferreturn(SB) 0x0205 00517 (main.go:12) MOVQ 440(SP), BP 0x020d 00525 (main.go:12) ADDQ $448, SP 0x0214 00532 (main.go:12) RET 0x0215 00533 (main.go:9) CALL runtime.deferreturn(SB) 0x021a 00538 (main.go:9) MOVQ 440(SP), BP 0x0222 00546 (main.go:9) ADDQ $448, SP 0x0229 00553 (main.go:9) RET 0x022a 00554 (main.go:8) CALL runtime.deferreturn(SB) 0x022f 00559 (main.go:8) MOVQ 440(SP), BP 0x0237 00567 (main.go:8) ADDQ $448, SP 0x023e 00574 (main.go:8) RET 0x023f 00575 (main.go:8) NOP 0x0240 00576 (main.go:7) CALL runtime.deferreturn(SB) 0x0245 00581 (main.go:7) MOVQ 440(SP), BP 0x024d 00589 (main.go:7) ADDQ $448, SP 0x0254 00596 (main.go:7) RET 0x0255 00597 (main.go:6) CALL runtime.deferreturn(SB) 可以看到会defer的调用是按照其定义的顺序反向调用的。\n在调用了defer的函数中，编译器会自动在函数尾部插入对runtime.deferreturn的调用。\nfunc deferreturn() { gp := getg() // 获取当前g \tfor { d := gp._defer // defer链表的第一个defer，也是最后一个被定义的defer \tif d == nil { return } sp := getcallersp() if d.sp != sp { return } if d.openDefer { // 如果开启了open-coded defer \tdone := runOpenDeferFrame(gp, d) if !done { throw(\u0026#34;unfinished open-coded defers in deferreturn\u0026#34;) } gp._defer = d.link //// 删除链表的第一个元素 \tfreedefer(d) // If this frame uses open defers, then this \t// must be the only defer record for the \t// frame, so we can just return. \treturn } fn := d.fn // 取出defer函数 \td.fn = nil gp._defer = d.link // 删除链表的第一个元素 \tfreedefer(d) // 释放_defer \tfn() // 调用defer函数 \t} } 4. defer的三种处理机制 经过以上的分析，我们可以总结一下，defer的三种处理机制：\n open-coded defer 栈上分配的defer 堆上分配的defer  其执行效率为：open-coded defer \u0026gt; 栈上分配的defer \u0026gt; 堆上分配的defer\n4.1 处理机制的选择   在defer语句出现在了循环语句里，或者无法执行更高阶的编译器优化时，亦或者同一个函数中使用了过多的defer时，会使用堆上分配[2]  满足以下三种情况时[3]，会使用open-coded defer\n 没有禁用编译器优化，即没有设置 -gcflags \u0026ldquo;-N\u0026rdquo;； 函数内 defer 的数量不超过 8 个，且返回语句与延迟语句个数的乘积不超过 15； defer 不是在循环语句中。    其他大部分情况下，会使用栈上分配\n  END\nReferences https://github.com/golang/proposal/blob/master/design/34481-opencoded-defers.md\nhttps://cloud.tencent.com/developer/article/1596802\nhttps://draveness.me/golang/docs/part2-foundation/ch05-keyword/golang-defer/#535-%e5%bc%80%e6%94%be%e7%bc%96%e7%a0%81\nhttps://juejin.cn/post/6844904078569373710\nhttps://xargin.com/go-1-13-defer-change/\nhttps://juejin.cn/post/6975686540601245709\nhttps://zhuanlan.zhihu.com/p/401339057\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/runtime%E7%AF%87%E4%B8%89defer/","summary":"本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic  1. defer是什么 defer，是一种特殊的机制，在调用普通函数或方法前加上关键字defer，就完成了defer所需要的语法。当defer语句被执行时，跟在defer后面的函数会被延迟执行。直到包含该defer语句的函数执行完毕时，defer后的函数才会被执行，多个defer的执行顺序与声明顺序相反。\n对于defer的使用及需要注意的地方，可参考defer用法。这里不再讨论。\n在golang runtime中，defer被描述为一个结构体：\ntype _defer struct { started bool // 是否开始执行defer函数 \theap bool // 是否分配在堆上  openDefer bool // 是否经过开放编码（open-coded）的优化 \tsp uintptr // 调用defer时的栈指针 stack pointer \tpc uintptr // 调用defer函数时的pc值 \tfn func() // defer关键字传入的函数， 当使用open-coded defer时可为空 \t_panic *_panic // defer函数中的_panic链表 \tlink *_defer // 在goroutine中的下一个defer，可指向堆或栈  // 如果openDefer为true，则下面的字段将记录具有open-code defer的栈帧和相关的函数。  // 上面的sp将为帧的sp，pc将为defer调用的地址。 \tfd unsafe.","title":"runtime篇三：defer"},{"content":" 本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic  1. panic的底层结构 panic在runtime中的底层表示是runtime._panic结构体。\ntype _panic struct { argp unsafe.Pointer // 指向defer调用时参数的指针 \targ any // panic参数 \tlink *_panic // 连接到更早的_panic \tpc uintptr // 程序计数器 \tsp unsafe.Pointer // 栈指针 \trecovered bool // 当前panic是否被recover恢复 \taborted bool // 当前panic是否被中止 \tgoexit bool // 是否调用了runtime.Goexit } 类似于_defer，panic也被组织成链表结构，多个panic通过link字段连接成一个链表。\n在_panic结构体中，pc、sp、goexit三个字段是为了修复runtime.Goexit带来的问题引入的[1].\n2. 调用panic 在函数中调用panic时，底层会调用runtime.gopanic，其源码如下：\nfunc gopanic(e any) { gp := getg() // 获取当前g  // ...  // 此处省略部分代码  var p _panic p.arg = e // panic参数 \tp.link = gp._panic // 头插 \tgp._panic = (*_panic)(noescape(unsafe.Pointer(\u0026amp;p))) // 省略defer调用部分  // ran out of deferred calls - old-school panic now \t// Because it is unsafe to call arbitrary user code after freezing \t// the world, we call preprintpanics to invoke all necessary Error \t// and String methods to prepare the panic strings before startpanic. \tpreprintpanics(gp._panic) fatalpanic(gp._panic) // should not return \t*(*int)(nil) = 0 // not reached } 先看panic主干流程，首先获取当前发生了panic的g，然后新建了一个_panic，将其字段赋值后，以头插的形式插入到g的_panic链表中，在函数的最后，调用了runtime.fatalpanic，这个函数实现了无法被恢复的程序崩溃：\nfunc fatalpanic(msgs *_panic) { pc := getcallerpc() sp := getcallersp() gp := getg() var docrash bool // Switch to the system stack to avoid any stack growth, which \t// may make things worse if the runtime is in a bad state.  // 切换到系统栈以避免用户栈增长 \tsystemstack(func() { // startpanic_m在应该打印panic信息时返回true \tif startpanic_m() \u0026amp;\u0026amp; msgs != nil { // \tatomic.Xadd(\u0026amp;runningPanicDefers, -1) printpanics(msgs) // 打印panic信息 \t} docrash = dopanic_m(gp, pc, sp) }) if docrash { crash() } systemstack(func() { exit(2) }) *(*int)(nil) = 0 // not reached } runtime.fatalpanic最后调用exit(2)终止程序，返回值为2.\n3. 在有defer调用时panic 上面介绍的情况是在函数运行时没有设置defer调用，然后直接panic，现在来看具有defer调用的函数发生panic时会怎样。\n回顾runtime篇三：defer我们知道，程序的defer调用以_defer链表的形式存储在g中。\n先大致看下源码:\nfunc gopanic(e any) { gp := getg() // 获取当前g  // 省略部分代码  var p _panic p.arg = e // panic参数 \tp.link = gp._panic // 头插 \tgp._panic = (*_panic)(noescape(unsafe.Pointer(\u0026amp;p))) // 将当前这个panic赋值给当前defer  atomic.Xadd(\u0026amp;runningPanicDefers, 1) // By calculating getcallerpc/getcallersp here, we avoid scanning the \t// gopanic frame (stack scanning is slow...) \taddOneOpenDeferFrame(gp, getcallerpc(), unsafe.Pointer(getcallersp())) // 这里添加了一个open-code defer  // 检查g中是否还存在defer调用 \tfor { d := gp._defer // 尝试获取_defer \tif d == nil { // 如果没有设置_defer，则直接跳出 \tbreak } // 如果defer被更早的panic或Goexit启动了（或者在程序到达这里之前，又触发了一个新的panic）， \t// 则将当前defer移出defer链表，先前的panic将不再执行，但确保先前的Goexit继续执行  if d.started { // defer已经被启动了 \tif d._panic != nil { // defer函数中也存在panic \td._panic.aborted = true // 终止defer的panic \t} d._panic = nil if !d.openDefer { // 未使用开放编码 \td.fn = nil gp._defer = d.link // 继续检查下一个defer \tfreedefer(d) continue } } // Mark defer as started, but keep on list, so that traceback \t// can find and update the defer\u0026#39;s argument frame if stack growth \t// or a garbage collection happens before executing d.fn. \td.started = true // 将defer标记为启动  // Record the panic that is running the defer. \t// If there is a new panic during the deferred call, that panic \t// will find d in the list and will mark d._panic (this panic) aborted. \td._panic = (*_panic)(noescape(unsafe.Pointer(\u0026amp;p))) // 将当前panic赋值給defer  done := true if d.openDefer { done = runOpenDeferFrame(gp, d) if done \u0026amp;\u0026amp; !d._panic.recovered { addOneOpenDeferFrame(gp, 0, nil) } } else { p.argp = unsafe.Pointer(getargp()) d.fn() // 调用defer函数 \t} p.argp = nil // Deferred function did not panic. Remove d. \tif gp._defer != d { throw(\u0026#34;bad defer entry in panic\u0026#34;) } d._panic = nil // trigger shrinkage to test stack copy. See stack_test.go:TestStackPanic \t//GC()  pc := d.pc sp := unsafe.Pointer(d.sp) // must be pointer so it gets adjusted during stack copy \tif done { // 如果完成了defer函数 \td.fn = nil gp._defer = d.link freedefer(d) } if p.recovered { // 如果panic被recover，则继续执行下一个panic \t// 省略recover部分 \t} } preprintpanics(gp._panic) fatalpanic(gp._panic) // should not return \t*(*int)(nil) = 0 // not reached } 有点复杂，结合具体程序来看这段代码：\n1│ package main 2│ 3│ func main() { 4│ defer func() { 5│ panic(\u0026#34;2\u0026#34;) 6│ }() 7│ panic(\u0026#34;1\u0026#34;) 8│ } 对于这个程序，我们来分析它的运行过程，首先程序在运行到第4行时，会将这个defer放入g的_defer链表中，这个defer的fn字段指向func(){panic(\u0026quot;2\u0026quot;)}。然后程序继续执行，来到第7行，在这里调用了runtime.gopanic函数。\n 新建了一个_panic结构体，并将其插入到g的_panic链表头部，这里称为panic1 在g上又新增了一个open-coded defer，现在g上有两个defer了，第一个为我们调用defer产生的（暂称为mydefer），第二个为open-coded defer，是runtime添加的(暂称openDefer) 当g上还有defer时，取出第一个defer，这里为mydefer  mydefer没有在运行 标记mydefer为运行状态，将panic1放入mydefer的_panic字段 检查mydefer不是open-coded defer，调用_defer.fn()     这里暂停一下，我们需要明确此时g中_defer和_panic的状态，在调用_defer.fn()之前，g中有两个defer，分别为mydefer、openDefer，且mydefer.link = openDefer：有一个panic，为panic1。且，mydefer._panic = panic1.\n 继续，这里调用的_defer.fn()为func(){panic(\u0026quot;2\u0026quot;)}，在defer函数中再一次调用了panic，注意这里进行了栈帧的切换，当前的panic变成了panic2。这次调用panic的执行过程为：\n 新建一个新建了一个_panic结构体，并将其插入到g的_panic链表头部，这里称为panic2 这回不再增加新的open-coded defer 当g上还有defer时，取出第一个defer，这里为mydefer  mydefer在运行  mydefer._panic不为空，将其标记为aborted，即把panic1标记为aborted mydefer不是open-coded defer，将mydefer.fn设为空，将mydefer从g._defer链表中取出 重新检查g._defer中是否还存在defer       再次暂停，此时g上只剩下一个_defer：openDefer\n 继续：\n g上还有openDefer存在 openDefer不在运行，将其标记为运行，将panic2赋值到openDefer._panic上 执行openDefer 完成openDefer后，free it 检查是否有recover调用 调用fatalpanic使程序崩溃  分析完毕。\n4. recover 编译器在将关键字recover转换成runtime.gorecover:\nfunc gorecover(argp uintptr) any { gp := getg() p := gp._panic if p != nil \u0026amp;\u0026amp; !p.goexit \u0026amp;\u0026amp; !p.recovered \u0026amp;\u0026amp; argp == uintptr(p.argp) { p.recovered = true return p.arg } return nil } 这个函数很简单，先是获取g，然后再获取g._panic的第一个元素，然后将其recovered标志设为true。\n让我们先结合具体程序来简单看下recover流程：\n1 │ package main 2 │ 3 │ func main() { 4 │ defer func() { 5 │ if r := recover(); r != nil { 6 │ println(r) 7 │ } 8 │ }() 9 │ panic(\u0026#34;1\u0026#34;) 10│ } 首先程序会执行到第9行，然后一个panic1将会被添加到g._panic链表上；然后在runtime.gopanic中会添加一个openDefer, 然后调用defer.fn，会执行到recover，根据runtime.gorecover，会将g._panic的第一个元素取出，然后将其设置为可recover。\n现在，g中有了两个_defer（mydefer.link = openDefer），一个_panic（panic1），且mydefer被设置recovered = true。我们可以开始分析recover是怎么执行的了：\n而对recover的处理，还要来看runtime.gopanic:\n1 │func gopanic(e any) { 2 │ ... 3 │ for { 4 │ d := gp._defer // panic退出程序前，要执行defer 5 │ if d == nil { 6 │ break 7 │ } 8 │ 9 │ ... 10│ 11│ if p.recovered { // 如果panic被recover，则继续执行下一个panic 12│ gp._panic = p.link 13│ if gp._panic != nil \u0026amp;\u0026amp; gp._panic.goexit \u0026amp;\u0026amp; gp._panic.aborted { 14│ // A normal recover would bypass/abort the Goexit. Instead, 15│ // we return to the processing loop of the Goexit. 16│ gp.sigcode0 = uintptr(gp._panic.sp) 17│ gp.sigcode1 = uintptr(gp._panic.pc) 18│ mcall(recovery) 19│ throw(\u0026#34;bypassed recovery failed\u0026#34;) // mcall should not return 20│ } 21│ atomic.Xadd(\u0026amp;runningPanicDefers, -1) 22│ 23│ // After a recover, remove any remaining non-started, 24│ // open-coded defer entries, since the corresponding defers 25│ // will be executed normally (inline). Any such entry will 26│ // become stale once we run the corresponding defers inline 27│ // and exit the associated stack frame. We only remove up to 28│ // the first started (in-progress) open defer entry, not 29│ // including the current frame, since any higher entries will 30│ // be from a higher panic in progress, and will still be 31│ // needed. 32│ d := gp._defer 33│ var prev *_defer 34│ if !done { 35│ // Skip our current frame, if not done. It is 36│ // needed to complete any remaining defers in 37│ // deferreturn() 38│ prev = d 39│ d = d.link 40│ } 41│ for d != nil { // 这里去除了已经开始的open defer 42│ // 暂时省略 43│ } 44│ 45│ gp._panic = p.link 46│ // Aborted panics are marked but remain on the g.panic list. 47│ // Remove them from the list. 48│ for gp._panic != nil \u0026amp;\u0026amp; gp._panic.aborted { 49│ gp._panic = gp._panic.link 50│ } 51│ if gp._panic == nil { // must be done with signal 52│ gp.sig = 0 53│ } 54│ // Pass information about recovering frame to recovery. 55│ gp.sigcode0 = uintptr(sp) 56│ gp.sigcode1 = pc 57│ mcall(recovery) 58│ throw(\u0026#34;recovery failed\u0026#34;) // mcall should not return 59│ } 60│ } 61│ 62│ // ... 63│} 当程序开始进行recover时，首先在13行会做一个if判断。正常recover是会绕过Goexit的，所以为了解决这个，添加了这个判断，这样就可以保证Goexit也会被recover住，这里是通过从runtime._panic中取出了程序计数器pc和栈指针sp并且调用runtime.recovery函数触发goroutine的调度，调度之前会准备好 sp、pc 以及函数的返回值。\n对于我们的程序来说，并未调用Goexit，因此这里会跳过，然后在32～43行，由于done为true，这里d将会被赋值为mydefer，然后来到45行，将defer1从g._panic链表中取出，然后将余下的被标记为aborted的_panic删除，这里没有。\n55、56两行设置g的sigcode0、sigcode1指针，用于跳转，然后57行mcall(recovery)。\nmcall是一个汇编实现的函数，其函数原型为：func mcall(fn func(*g))，其主要功能是切换到g0的栈，然后调用fn(g)，fn(g)将不会返回，并且触发g的重新调度。\n这里的fn就是recovery，来看：\nfunc recovery(gp *g) { // Info about defer passed in G struct. \tsp := gp.sigcode0 pc := gp.sigcode1 // d\u0026#39;s arguments need to be in the stack. \tif sp != 0 \u0026amp;\u0026amp; (sp \u0026lt; gp.stack.lo || gp.stack.hi \u0026lt; sp) { print(\u0026#34;recover: \u0026#34;, hex(sp), \u0026#34; not in [\u0026#34;, hex(gp.stack.lo), \u0026#34;, \u0026#34;, hex(gp.stack.hi), \u0026#34;]\\n\u0026#34;) throw(\u0026#34;bad recovery\u0026#34;) } // Make the deferproc for this d return again, \t// this time returning 1. The calling function will \t// jump to the standard return epilogue. \tgp.sched.sp = sp gp.sched.pc = pc gp.sched.lr = 0 gp.sched.ret = 1 gogo(\u0026amp;gp.sched) } 没什么特别的魔法，就是重新设置了g的一些指针，然后对其重新进行调度。\n这样就完成了panic的恢复。\n5. 场景分析 Q1 为什么recover必须放在defer里面 A1 不放到defer里面，没机会运行啊。。。\nQ2 为什么如下使用方法不会恢复：\nfunc main() { defer recover() panic(\u0026#34;1\u0026#34;) } A2 在调用recover()函数时，会有如下if条件：\nif p != nil \u0026amp;\u0026amp; !p.goexit \u0026amp;\u0026amp; !p.recovered \u0026amp;\u0026amp; argp == uintptr(p.argp) { p.recovered = true return p.arg } 这里p != nil \u0026amp;\u0026amp; !p.goexit \u0026amp;\u0026amp; !p.recovered会满足，而argp和uintptr(p.argp)并不相等，argp是runtime.gopinic报告的参数指针，p.argp是最顶层 defer 函数调用的参数指针，二者并不相等。\nQ3 下面这段代码将输出什么？为什么？\nfunc main() { defer func() { // topdefer \tfmt.Println(recover()) }() defer panic(3) // defer3 \tdefer panic(2) // defer2 \tdefer panic(1) // defer1 \tpanic(0) } A3 将输出3.\n分析：在runtime.gopanic中，有如下代码：\nif d.started { if d._panic != nil { d._panic.aborted = true } d._panic = nil if !d.openDefer { d.fn = nil gp._defer = d.link freedefer(d) continue } } 当我们执行到panic(0)后，将返回执行defer1，这时defer1被设置为started，panic(0)被设置为aborted，然后defer1被释放； 紧接着执行defer2，defer2被设置为started，panic(1)被设置为aborted，然后defer2被释放； 紧接着执行defer3，defer3被设置为started，panic(2)被设置为aborted，然后defer3被释放； 最后执行到topdefer，又因如下代码：\nfor gp._panic != nil \u0026amp;\u0026amp; gp._panic.aborted { gp._panic = gp._panic.link } 被标记为aborted的panic将被忽略，因此只剩下了panic(3)。\n这样，最后输出的值就是3。\nQ4 为什么recover不能捕获不同goroutine的panic A4 查看runtime.gorecover源码：\nfunc gorecover(argp uintptr) any { gp := getg() p := gp._panic if p != nil \u0026amp;\u0026amp; !p.goexit \u0026amp;\u0026amp; !p.recovered \u0026amp;\u0026amp; argp == uintptr(p.argp) { p.recovered = true return p.arg } return nil } 这个函数获取了当前的g，并为其第一个_panic设置recover，跟其他g没有关系\nQ5 为什么子goroutine的panic不被recover会造成整个程序的崩溃 A5 查看runtime.fatalpanic:\nfunc fatalpanic(msgs *_panic) { // ...  systemstack(func() { exit(2) }) *(*int)(nil) = 0 // not reached } 其在执行exit(2)时，是在systemstack上执行的，因此整个程序都会退出。\nEND\nReferences https://gfw.go101.org/article/panic-and-recover-more.html\nhttps://golang.design/under-the-hood/zh-cn/part1basic/ch03lang/panic/\nhttps://www.purewhite.io/2019/11/28/runtime-hacking-translate/\nhttps://zhuanlan.zhihu.com/p/346514343\nhttps://draveness.me/golang/docs/part2-foundation/ch05-keyword/golang-panic-recover/\nhttps://xiaomi-info.github.io/2020/01/20/go-trample-panic-recover/\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/runtime%E7%AF%87%E5%9B%9Bpanic/","summary":"本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic  1. panic的底层结构 panic在runtime中的底层表示是runtime._panic结构体。\ntype _panic struct { argp unsafe.Pointer // 指向defer调用时参数的指针 \targ any // panic参数 \tlink *_panic // 连接到更早的_panic \tpc uintptr // 程序计数器 \tsp unsafe.Pointer // 栈指针 \trecovered bool // 当前panic是否被recover恢复 \taborted bool // 当前panic是否被中止 \tgoexit bool // 是否调用了runtime.Goexit } 类似于_defer，panic也被组织成链表结构，多个panic通过link字段连接成一个链表。\n在_panic结构体中，pc、sp、goexit三个字段是为了修复runtime.Goexit带来的问题引入的[1].\n2. 调用panic 在函数中调用panic时，底层会调用runtime.gopanic，其源码如下：\nfunc gopanic(e any) { gp := getg() // 获取当前g  // .","title":"runtime篇四：panic"},{"content":" 在使用top命令查看系统状态时，会出现一堆参数，这些参数分别代表什么意思呢？\n 第一行 首先打印了当前时间21:11:55，然后是系统已经启动了多久up 4 days, 8:45，我已经4天8小时45分钟没关机了。。\n然后当前有3个用户登录3 users，load average是1.47, 1.33, 1.14\nload average load average后面有三个值，分别代表过去 1 分钟，5 分钟，15 分钟在这个节点上的 load average.\nLoad Average 是一种 CPU 资源需求的度量。\n举例来说，对于一个单个 CPU 的系统，如果在 1 分钟的时间里，处理器上始终有一个进程在运行，同时操作系统的进程可运行队列中始终都有 9 个进程在等待获取 CPU 资源。那么对于这 1 分钟的时间来说，系统的\u0026quot;load average\u0026quot;就是 1+9=10，这个定义对绝大部分的 Unix 系统都适用。\n对于 Linux 来说，如果只考虑 CPU 的资源，Load Averag 等于单位时间内正在运行的进程加上可运行队列的进程，这个定义也是成立的。\n对于load average的理解有以下三点：\n  不论计算机 CPU 是空闲还是满负载，Load Average 都是 Linux 进程调度器中可运行队列（Running Queue）里的一段时间的平均进程数目。\n  计算机上的 CPU 还有空闲的情况下，CPU Usage 可以直接反映到\u0026quot;load average\u0026quot;上，什么是 CPU 还有空闲呢？具体来说就是可运行队列中的进程数目小于 CPU 个数，这种情况下，单位时间进程 CPU Usage 相加的平均值应该就是\u0026quot;load average\u0026quot;的值。\n  计算机上的 CPU 满负载的情况下，计算机上的 CPU 已经是满负载了，同时还有更多的进程在排队需要 CPU 资源。这时\u0026quot;load average\u0026quot;就不能和 CPU Usage 等同了。\n  比如对于单个 CPU 的系统，CPU Usage 最大只是有 100%，也就 1 个 CPU；而\u0026quot;load average\u0026quot;的值可以远远大于 1，因为\u0026quot;load average\u0026quot;看的是操作系统中可运行队列中进程的个数。\n要注意的是，平均负载统计了这两种情况的进程：\n  第一种是 Linux 进程调度器中可运行队列（Running Queue）一段时间（1 分钟，5 分钟，15 分钟）的进程平均数。\n  第二种是 Linux 进程调度器中休眠队列（Sleeping Queue）里的一段时间的 TASK_UNINTERRUPTIBLE 状态下的进程平均数。所以，最后的公式就是：Load Average= 可运行队列进程平均数 + 休眠队列中不可打断的进程平均数\n  因此，Load Average= 可运行队列进程平均数 + 休眠队列中不可打断的进程平均数\n第二行 第二行主要记录了当前系统中的进程数量和其状态信息。\n例如当前我的系统中有373个进程totol 373，但只有一个进程正在运行1 running，365个在sleep365 sleep，0个stoped0 stoped，7个为僵尸进程7 zombie\nzombie 状态下 子进程结束时父进程没有调用wait()/waitpid()等待子进程结束，那么就会产生僵尸进程。\n原因是子进程结束时并没有真正退出，而是留下一个僵尸进程的数据结构在系统进程表中，等待父进程清理，如果父进程已经退出则会由init进程接替父进程进行处理（收尸）。之所以要留下一个僵尸进程zombie是因为父进程可能会需要子进程的的某些结果。\n由此可见，如果父进程不作为并且又不退出，就会有大量的僵尸进程，每个僵尸进程会占用进程表的一个位置（slot），如果僵尸进程太多会导致系统无法创建新的进程，因为进程表的容量是有限的。所以当zombie这个指标太大时需要引起我们的注意。\n杀死僵尸进程的方法不是直接对其进行kill，而是去找他的父进程，对其附进程进行kill -9，毕竟，僵尸是杀不死的嘛。。\n第三行 第三行比较复杂，值比较多，先来总体看看：\n us: user 表示用户态的CPU时间比例 sy: system 表示内核态的CPU时间比例 wa: iowait 表示处于IO等待的CPU时间比例 ni: nice 表示运行低优先级进程的CPU时间比例 id: idle 表示空闲CPU时间比例 hi: hard interrupt 表示处理硬中断的CPU时间比例 si: soft interrupt 表示处理软中断的CPU时间比例 st: steal 表示当前系统运行在虚拟机中的时候，被其他虚拟机占用的CPU时间比例。  解释一下:\n对于下图，横向为时间轴，上半部分代表系统用户态，下半部分代表系统内核态，假设系统只有一个CPU\n假设一个用户程序开始运行了，那么就对应着第一个\u0026quot;us\u0026quot;框，\u0026ldquo;us\u0026quot;是\u0026quot;user\u0026quot;的缩写，代表 Linux 的用户态 CPU Usage。普通用户程序代码中，只要不是调用系统调用（System Call），这些代码的指令消耗的 CPU 就都属于\u0026quot;us\u0026rdquo;。\n当这个用户程序代码中调用了系统调用，比如说 read() 去读取一个文件，这时候这个用户进程就会从用户态切换到内核态。\n内核态 read() 系统调用在读到真正 disk 上的文件前，就会进行一些文件系统层的操作。那么这些代码指令的消耗就属于\u0026quot;sy\u0026quot;，这里就对应上面图里的第二个框。\u0026ldquo;sy\u0026quot;是 \u0026ldquo;system\u0026quot;的缩写，代表内核态 CPU 使用。\n接下来，这个 read() 系统调用会向 Linux 的 Block Layer 发出一个 I/O Request，触发一个真正的磁盘读取操作。\n这时候，这个进程一般会被置为 TASK_UNINTERRUPTIBLE。而 Linux 会把这段时间标示成\u0026quot;wa\u0026rdquo;，对应图中的第三个框。\u0026ldquo;wa\u0026quot;是\u0026quot;iowait\u0026quot;的缩写，代表等待 I/O 的时间，这里的 I/O 是指 Disk I/O。\n紧接着，当磁盘返回数据时，进程在内核态拿到数据，这里仍旧是内核态的 CPU 使用中的\u0026quot;sy\u0026rdquo;，也就是图中的第四个框。\n然后，进程再从内核态切换回用户态，在用户态得到文件数据，这里进程又回到用户态的 CPU 使用，\u0026ldquo;us\u0026rdquo;，对应图中第五个框。\n好，这里我们假设一下，这个用户进程在读取数据之后，没事可做就休眠了。并且我们可以进一步假设，这时在这个 CPU 上也没有其他需要运行的进程了，那么系统就会进入\u0026quot;id\u0026quot;这个步骤，也就是第六个框。\u0026ldquo;id\u0026quot;是\u0026quot;idle\u0026quot;的缩写，代表系统处于空闲状态。\n如果这时这台机器在网络收到一个网络数据包，网卡就会发出一个中断（interrupt）。相应地，CPU 会响应中断，然后进入中断服务程序。\n这时，CPU 就会进入\u0026quot;hi\u0026rdquo;，也就是第七个框。\u0026ldquo;hi\u0026quot;是\u0026quot;hardware irq\u0026quot;的缩写，代表 CPU 处理硬中断的开销。由于我们的中断服务处理需要关闭中断，所以这个硬中断的时间不能太长。\n但是，发生中断后的工作是必须要完成的，如果这些工作比较耗时那怎么办呢？Linux 中有一个软中断的概念（softirq），它可以完成这些耗时比较长的工作。\n你可以这样理解这个软中断，从网卡收到数据包的大部分工作，都是通过软中断来处理的。那么，CPU 就会进入到第八个框，\u0026ldquo;si\u0026rdquo;。这里\u0026quot;si\u0026quot;是\u0026quot;softirq\u0026quot;的缩写，代表 CPU 处理软中断的开销。\n这里你要注意，无论是\u0026quot;hi\u0026quot;还是\u0026quot;si\u0026rdquo;，它们的 CPU 时间都不会计入进程的 CPU 时间。这是因为本身它们在处理的时候就不属于任何一个进程。\n除此之外，还有ni，是\u0026quot;nice\u0026quot;的缩写，这里表示如果进程的 nice 值是正值（1-19），代表优先级比较低的进程运行时所占用的 CPU。\n还有st，st是\u0026quot;steal\u0026quot;的缩写，是在虚拟机里用的一个 CPU 使用类型，表示有多少时间是被同一个宿主机上的其他虚拟机抢走的。\n第四、五行 第4、5行显示的是系统内存使用，单位KiB。totol 表示总内存，free 表示没使用过的内容，used是已经使用的内存。buff表示用于读写磁盘缓存的内存，cache表示用于读写文件缓存的内存,avail表示可用的应用内存。\nSwap： total表示能用的swap总量， free表示剩余，used表示已经使用的，avail表示可用的交换区大小。\n其他 从第6行开始，表示的是具体的进程状态：\n PID 进程ID USER 进程所有者的用户名，例如root PR进程调度优先级 NI进程nice值（优先级），越小的值代表越高的优先级 VIRT 进程使用的虚拟内存 RES 进程使用的物理内存（不包括共享内存） SHR 进程使用的共享内存 CPU 进程使用的CPU占比 MEM 进程使用的内存占比 TIME 进程启动后到现在所用的全部CPU时间 COMMAND 进程的启动命令（默认只显示二进制，top -c能够显示命令行和启动参数）  From https://www.cnblogs.com/makelu/p/11169270.html\nhttps://time.geekbang.org/column/article/311054\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/top%E5%91%BD%E4%BB%A4/","summary":"在使用top命令查看系统状态时，会出现一堆参数，这些参数分别代表什么意思呢？\n 第一行 首先打印了当前时间21:11:55，然后是系统已经启动了多久up 4 days, 8:45，我已经4天8小时45分钟没关机了。。\n然后当前有3个用户登录3 users，load average是1.47, 1.33, 1.14\nload average load average后面有三个值，分别代表过去 1 分钟，5 分钟，15 分钟在这个节点上的 load average.\nLoad Average 是一种 CPU 资源需求的度量。\n举例来说，对于一个单个 CPU 的系统，如果在 1 分钟的时间里，处理器上始终有一个进程在运行，同时操作系统的进程可运行队列中始终都有 9 个进程在等待获取 CPU 资源。那么对于这 1 分钟的时间来说，系统的\u0026quot;load average\u0026quot;就是 1+9=10，这个定义对绝大部分的 Unix 系统都适用。\n对于 Linux 来说，如果只考虑 CPU 的资源，Load Averag 等于单位时间内正在运行的进程加上可运行队列的进程，这个定义也是成立的。\n对于load average的理解有以下三点：\n  不论计算机 CPU 是空闲还是满负载，Load Average 都是 Linux 进程调度器中可运行队列（Running Queue）里的一段时间的平均进程数目。\n  计算机上的 CPU 还有空闲的情况下，CPU Usage 可以直接反映到\u0026quot;load average\u0026quot;上，什么是 CPU 还有空闲呢？具体来说就是可运行队列中的进程数目小于 CPU 个数，这种情况下，单位时间进程 CPU Usage 相加的平均值应该就是\u0026quot;load average\u0026quot;的值。","title":"top命令"},{"content":"容器具有自己的Network Namespace.\neht0是这个Network Namespace里的网络接口。而宿主机上也有自己的 eth0，宿主机上的 eth0 对应着真正的物理网卡，可以和外面通讯\n要让容器 Network Namespace 中的数据包最终发送到物理网卡上，需要以下两步：\n 将数据包从容器的 Network Namespace 发送到 Host Network Namespace 上 数据包从宿主机的eth0发送出去  要想让容器从自己的Network Namespace连接到Host Namespace，一般来说就只有两类设备接口，一是veth，另外是macvlan/ipvlan.\nveth是一个虚拟的网络设备，一般是成对建立，而且这对设备是相互连接的。当每个设备在不同的 Network Namespaces 的时候，Namespace 之间就可以用这对 veth 设备来进行网络通讯了。\n到这里，解决了第一步，下一步需要将数据包从宿主机的eth0发送出去。\nDocker 程序在节点上安装完之后，就会自动建立了一个 docker0 的 bridge interface。所以我们只需要把第一步中建立的 veth_host 这个设备，接入到 docker0 这个 bridge 上。\n容器和docker0组成了一个子网，docker0上的IP就是这个子网的网关IP。\n然后docekr0通过nat或route的方式，经过eth0将数据包向外发送。\nReference https://time.geekbang.org/column/article/323325\nhttp://icyfenix.cn/immutable-infrastructure/network/linux-vnet.html\nhttps://morven.life/posts/networking-4-docker-sigle-host/\nhttps://network.51cto.com/article/708901.html\n","permalink":"http://yangchnet.github.io/Dessert/posts/cloud/%E6%A6%82%E8%A7%88%E5%AE%B9%E5%99%A8%E7%AF%87%E4%BA%8C%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/","summary":"容器具有自己的Network Namespace.\neht0是这个Network Namespace里的网络接口。而宿主机上也有自己的 eth0，宿主机上的 eth0 对应着真正的物理网卡，可以和外面通讯\n要让容器 Network Namespace 中的数据包最终发送到物理网卡上，需要以下两步：\n 将数据包从容器的 Network Namespace 发送到 Host Network Namespace 上 数据包从宿主机的eth0发送出去  要想让容器从自己的Network Namespace连接到Host Namespace，一般来说就只有两类设备接口，一是veth，另外是macvlan/ipvlan.\nveth是一个虚拟的网络设备，一般是成对建立，而且这对设备是相互连接的。当每个设备在不同的 Network Namespaces 的时候，Namespace 之间就可以用这对 veth 设备来进行网络通讯了。\n到这里，解决了第一步，下一步需要将数据包从宿主机的eth0发送出去。\nDocker 程序在节点上安装完之后，就会自动建立了一个 docker0 的 bridge interface。所以我们只需要把第一步中建立的 veth_host 这个设备，接入到 docker0 这个 bridge 上。\n容器和docker0组成了一个子网，docker0上的IP就是这个子网的网关IP。\n然后docekr0通过nat或route的方式，经过eth0将数据包向外发送。\nReference https://time.geekbang.org/column/article/323325\nhttp://icyfenix.cn/immutable-infrastructure/network/linux-vnet.html\nhttps://morven.life/posts/networking-4-docker-sigle-host/\nhttps://network.51cto.com/article/708901.html","title":"概览容器篇二：容器网络"},{"content":" 本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic  1. chan的结构 一个channel长这样：\ntype hchan struct { qcount uint // total data in the queue \tdataqsiz uint // size of the circular queue \tbuf unsafe.Pointer // points to an array of dataqsiz elements \telemsize uint16 // chan中元素大小 \tclosed uint32 // 是否关闭 \telemtype *_type // element type \tsendx uint // send index \trecvx uint // receive index \trecvq waitq // list of recv waiters \tsendq waitq // list of send waiters  lock mutex } channel的字段中，主要可以分为三部分：\n第一部分是标识channel自身的一些状态和性质，如hchan.closed标识chan是否关闭，hchan.elemsize标识chan中元素的大小、hchan.elemtype标识chan中元素类型；\n第二部分是标识底层循环数组的状态的字段，如hchan.qcount标识当前数组中元素数量、hchan.dataqsiz标识循环数组的大小、hchan.buf是指向底层数组的指针、hchan.sendx标识待发送元素的下标、hchan.recvx标识待接收元素的下标；\n第三部分是存储正等待当前chan的goroutine，如hchan.recvq存储等待接收的goroutine，hchan.sendq存储等待发送的gotoutine\n最后是一个锁，保证了并发安全。\n1.1 如何构造一个chan 通过汇编可以看到，在构造chan时，调用的是runtime.makechan函数，函数如下：\ntype chantype struct { typ _type elem *_type dir uintptr } func makechan(t *chantype, size int) *hchan { elem := t.elem // ...  // 省略部分检查代码  mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem \u0026gt; maxAlloc-hchanSize || size \u0026lt; 0 { panic(plainError(\u0026#34;makechan: size out of range\u0026#34;)) } // Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers. \t// buf points into the same allocation, elemtype is persistent. \t// SudoG\u0026#39;s are referenced from their owning thread so they can\u0026#39;t be collected. \t// TODO(dvyukov,rlh): Rethink when collector can move allocated objects. \tvar c *hchan switch { case mem == 0: // Queue or element size is zero. \tc = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. \tc.buf = c.raceaddr() case elem.ptrdata == 0: // Elements do not contain pointers. \t// Allocate hchan and buf in one call. \tc = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // Elements contain pointers. \tc = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(\u0026amp;c.lock, lockRankHchan) if debugChan { print(\u0026#34;makechan: chan=\u0026#34;, c, \u0026#34;; elemsize=\u0026#34;, elem.size, \u0026#34;; dataqsiz=\u0026#34;, size, \u0026#34;\\n\u0026#34;) } return c } 首先对mem进行了计算，mem是chan中元素所占的空间\nruntime.makechan函数中，受先是一些检查，然后根据chan的长度和elem的类型，会有不同的构造策略：\n 如果是构造一个无缓冲chan，即参数size为0，那么默认分配hchanSize=96字节空间并强制转换为*hchan类型，96字节是一个hchan的大小。然后 如果chan的elem不包含指针，则分配hchanSize+mem空间给chan，并将hchanSize之后的所有空间分配给hchan.buf。 如果chan的elel包含指针，那么直接new一个chan，并为hchan.buf分配meme大小的空间  最后将hchan.elemsize、hchan.elemtype、hchan.dataqsiz进行赋值。\n返回一个hchan的指针，这保证了我们在对channel进行传递时不会进行复制。\n2. chan的发送与接收 2.1 chan的发送 chan发送时调用了runtime.chansend1:\nfunc chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { if c == nil { // 如果chan为nil \tif !block { // 如果不可阻塞 \treturn false // 发送失败 \t} gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) // 可阻塞，goroutine挂起 \tthrow(\u0026#34;unreachable\u0026#34;) } // 省略部分代码  if !block \u0026amp;\u0026amp; c.closed == 0 \u0026amp;\u0026amp; full(c) { // 不可阻塞，chan未关闭且已满，则发送失败 \treturn false } var t0 int64 if blockprofilerate \u0026gt; 0 { t0 = cputicks() } lock(\u0026amp;c.lock) // 加锁，并发安全  if c.closed != 0 { // 如果chan已经被关闭了 \tunlock(\u0026amp;c.lock) // 解锁 \tpanic(plainError(\u0026#34;send on closed channel\u0026#34;)) // 向已关闭的chan发送会panic \t} // 如果接收队列中存在goroutine，则不经过hchan.buf，而直接复制到接收端缓冲区 \tif sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send \t// directly to the receiver, bypassing the channel buffer (if any). \tsend(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true } // chan还没满 \tif c.qcount \u0026lt; c.dataqsiz { // Space is available in the channel buffer. Enqueue the element to send. \tqp := chanbuf(c, c.sendx) // 计算存放元素的内存地址 \tif raceenabled { racenotify(c, c.sendx, nil) } typedmemmove(c.elemtype, qp, ep) // 将元素复制到buf \tc.sendx++ // 待发送下标+1 \tif c.sendx == c.dataqsiz { // 循环数组 \tc.sendx = 0 } c.qcount++ // 总元素数量+1 \tunlock(\u0026amp;c.lock) // 解锁 \treturn true // 发送成功 \t} // chan满了  if !block { // chan满了，且要求不可阻塞，则直接失败 \tunlock(\u0026amp;c.lock) return false } // 可以阻塞  // 获取当前goroutine指针 \tgp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) // 当前goroutine进入chan的待发送队列  atomic.Store8(\u0026amp;gp.parkingOnChan, 1) // 挂起当前goroutine \tgopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) KeepAlive(ep) // 当前goroutine被唤醒了 \tif mysg != gp.waiting { throw(\u0026#34;G waiting list is corrupted\u0026#34;) } gp.waiting = nil gp.activeStackChans = false closed := !mysg.success gp.param = nil if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) if closed { // 被唤醒后发现chan关闭了 \tif c.closed == 0 { throw(\u0026#34;chansend: spurious wakeup\u0026#34;) } panic(plainError(\u0026#34;send on closed channel\u0026#34;)) } return true } 上述代码的主要逻辑是：\n 检查会发送失败的情况 加锁  再次检查会发送失败的情况，失败则解锁返回失败 如果有goroutine在等着接收，直接复制给它，解锁返回成功 chan的buf没满，复制到buf中，解锁返回成功 如果chan的buf满了  不可以阻塞，解锁返回失败 可以阻塞，让goroutine进入等待队列并挂起，等待唤醒      唤醒后，不会再进行发送或复制等，因为一个goroutine如果是从等待队列被唤醒，则是直接从发送goroutine将消息复制过来，然后才唤醒，因此可以直接结束。\n如果在发送时发现有等待接收的goroutine，会调用runtime.send将元素复制给等待的goroutine，runtime.send如下：\nfunc send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) { if raceenabled { if c.dataqsiz == 0 { racesync(c, sg) } else { // Pretend we go through the buffer, even though \t// we copy directly. Note that we need to increment \t// the head/tail locations only when raceenabled. \tracenotify(c, c.recvx, nil) racenotify(c, c.recvx, sg) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz \t} } // sg.elem 指向接收到的值存放的位置，如 val \u0026lt;- ch，指的就是 \u0026amp;val \tif sg.elem != nil { sendDirect(c.elemtype, sg, ep) sg.elem = nil } gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) sg.success = true if sg.releasetime != 0 { sg.releasetime = cputicks() } goready(gp, skip+1) } 2.2 chan的接收 chan接收时调用的函数为runtime.chanrecv:\nfunc chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { if debugChan { print(\u0026#34;chanrecv: chan=\u0026#34;, c, \u0026#34;\\n\u0026#34;) } if c == nil { // chan为nil \tif !block { // 且不可阻塞 \treturn\t// 直接返回失败 \t} gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) // 可阻塞，gotoutine挂起 \tthrow(\u0026#34;unreachable\u0026#34;) } // Fast path: check for failed non-blocking operation without acquiring the lock. \tif !block \u0026amp;\u0026amp; empty(c) { // 不可阻塞且chan为空 \tif atomic.Load(\u0026amp;c.closed) == 0 { // chan没关闭 \treturn } // The channel is irreversibly closed. Re-check whether the channel has any pending data \t// to receive, which could have arrived between the empty and closed checks above. \t// Sequential consistency is also required here, when racing with such a send. \tif empty(c) { // chan被关闭了，但其中可能还有未取出的值 \t// The channel is irreversibly closed and empty. \tif raceenabled { raceacquire(c.raceaddr()) } if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } var t0 int64 if blockprofilerate \u0026gt; 0 { t0 = cputicks() } lock(\u0026amp;c.lock) // 上锁  if c.closed != 0 {\t// 再次检查chan是否被关闭 \tif c.qcount == 0 { if raceenabled { raceacquire(c.raceaddr()) } unlock(\u0026amp;c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } // The channel has been closed, but the channel\u0026#39;s buffer have data. \t} else { // Just found waiting sender with not closed. \tif sg := c.sendq.dequeue(); sg != nil { // 有个chan等着发 \t// Found a waiting sender. If buffer is size 0, receive value \t// directly from sender. Otherwise, receive from head of queue \t// and add sender\u0026#39;s value to the tail of the queue (both map to \t// the same buffer slot because the queue is full). \trecv(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true, true } } if c.qcount \u0026gt; 0 { // 当前chan中有值 \t// Receive directly from queue \tqp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) } if ep != nil { typedmemmove(c.elemtype, ep, qp)\t// 将消息复制给接收者 \t} typedmemclr(c.elemtype, qp)\t// 清理发送者内存空间 \tc.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(\u0026amp;c.lock) return true, true } if !block {\t// 当前chan中没有值，又不能阻塞，只好失败 \tunlock(\u0026amp;c.lock) return false, false } // 可以阻塞，先坐等一会 \tgp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg \t// on gp.waiting where copystack can find it. \tmysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil c.recvq.enqueue(mysg) atomic.Store8(\u0026amp;gp.parkingOnChan, 1) gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) // 被唤醒了 \tif mysg != gp.waiting { throw(\u0026#34;G waiting list is corrupted\u0026#34;) } gp.waiting = nil gp.activeStackChans = false if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } success := mysg.success gp.param = nil mysg.c = nil releaseSudog(mysg) return true, success } 总体来看，和发送的过程很相似，主要是检查chan是否关闭，是否可阻塞，是否有发送者等待，chan中是否有值，然后做出响应的处理。\n在使用chan进行接收时，有时可以使用ok来标识是否真的从chan中接收到了值，go底层通过不同的函数做到这一点：\n// a := \u0026lt;-chan func chanrecv1(c *hchan, elem unsafe.Pointer) { chanrecv(c, elem, true) } // a, ok := \u0026lt;-chan func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) { _, received = chanrecv(c, elem, true) return } 3. chan的关闭 关闭chan时，调用的是runtime.closechan:\nfunc closechan(c *hchan) { if c == nil { // 不能关闭一个nilchan \tpanic(plainError(\u0026#34;close of nil channel\u0026#34;)) } lock(\u0026amp;c.lock) if c.closed != 0 { // 不能重复关闭chan \tunlock(\u0026amp;c.lock) panic(plainError(\u0026#34;close of closed channel\u0026#34;)) } if raceenabled { callerpc := getcallerpc() racewritepc(c.raceaddr(), callerpc, abi.FuncPCABIInternal(closechan)) racerelease(c.raceaddr()) } c.closed = 1 // 设置chan状态为关闭  var glist gList // 处理所有等待读取的goroutine \tfor { sg := c.recvq.dequeue() if sg == nil { // 当recvq中没有goroutine时跳出 \tbreak } if sg.elem != nil { // sg.elem不为空说明接受者未忽略接收值 \ttypedmemclr(c.elemtype, sg.elem) // 给等待读取的设置零值 \tsg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } // 把goroutine取出来 \tgp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) // 将goroutine推入glist \t} // 所有想要发送的goroutine，对不起，你们panic吧 \tfor { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) // 将goroutine推入glist \t} unlock(\u0026amp;c.lock) // 遍历gList，把其中的所有goroutine唤醒 \tfor !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } } 对于chan的关闭，主要有两点：\n 对于待接收的chan，如果chan缓冲区中没有值了，则返回其一个零值，如果缓冲区还有值，则把值交给它 对于待发送的chan，对不起了宝贝们，你们都给爷爷panic吧  References https://golang.design/go-questions/channel/struct/\nhttps://codeburst.io/diving-deep-into-the-golang-channels-549fd4ed21a8\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/runtime%E7%AF%87%E4%BA%8C%E9%80%9A%E9%81%93/","summary":"本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic  1. chan的结构 一个channel长这样：\ntype hchan struct { qcount uint // total data in the queue \tdataqsiz uint // size of the circular queue \tbuf unsafe.Pointer // points to an array of dataqsiz elements \telemsize uint16 // chan中元素大小 \tclosed uint32 // 是否关闭 \telemtype *_type // element type \tsendx uint // send index \trecvx uint // receive index \trecvq waitq // list of recv waiters \tsendq waitq // list of send waiters  lock mutex } channel的字段中，主要可以分为三部分：","title":"runtime篇二：通道"},{"content":" 本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic  1. 接口的内部结构 type iface struct { tab *itab data unsafe.Pointer } 一个接口是一个iface结构体，其中包含一个itab指针和一个unsafe.Pointer。\n概念上讲一个接口的值,接口值,由两个部分组成,一个具体的类型和那个类型的值。它们被称为接口的动态类型和动态值。\n一个itab可以表示一个接口的类型和赋给这个接口的实体类型，即为接口的动态类型，而data所指向的unsafe.Pointer则指向接口的动态值。\n近距离来看itab：\ntype itab struct { inter *interfacetype _type *_type hash uint32 // copy of _type.hash. Used for type switches. \t_ [4]byte fun [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter. } 其中inter字段描述了接口的类型，_type字段描述了实体类型，hash字段是类型哈希，用于类型匹配，fun字段放置和接口方法对应的具体数据类型的方法地址。\n再来看interfacetype\ntype interfacetype struct { typ _type pkgpath name mhdr []imethod } 其中typ和itab中的_type为同一个值，pkgpath则存储了接口的包名，mhdr则表示接口所定义的函数列表。\n这样来看，一个接口主要有两个部分构成：第一是对于接口本身的描述，包括接口的包名iface.itab.inter.pkgpath、接口的函数列表iface.itab.inter.mhdr，接口的hash值iface.itab.hash。第二部分是对于实现接口的实体的描述，包括实体的类型iface.itab._type，实体的值iface.data。\n可以将itab的值输出看看：\ntype iface struct { tab *itab data unsafe.Pointer } type itab struct { inter *interfacetype _type uintptr hash uint32 _ [4]byte myfunc [1]uintptr } type interfacetype struct { mytype uintptr pkgpath string mhdr []uintptr } type Person interface { Walk() Say(words []string) string } type Student struct { name string age int } func (s Student) Walk() { return } func (s Student) Say(words []string) string { return strings.Join(words, \u0026#34; \u0026#34;) } func main() { var p = Person(Student{ name: \u0026#34;lichang\u0026#34;, age: 18, }) // 查看iface的结构 \tp_iface := *(*iface)(unsafe.Pointer(\u0026amp;p)) fmt.Println(p_iface.tab) // 查看接口的动态值 \ts := (*Student)(unsafe.Pointer(p_iface.data)) fmt.Println(*s) } 输出：\n\u0026amp;{0x489a80 4773888 3558907866 [0 0 0 0] [4234176]} {lichang 18} 而对于一个空接口来说，其并不是一个iface，而是一个eface:\ntype eface struct { _type *_type data unsafe.Pointer } 一个空接口，其没有函数列表，用于描述空接口的结构体只有两个字段，一是_type，表示实体类型，二是data，指向动态值。\n2. 接口的类型转换 2.1 结构体到接口类型的转换 对于如下代码：\ntype Person interface { Walk() Say(words []string) string } type Student struct { name string age int } func (s Student) Walk() { return } func (s Student) Say(words []string) string { return strings.Join(words, \u0026#34; \u0026#34;) } func main() { var p = Person(Student{ name: \u0026#34;lichang\u0026#34;, age: 18, }) fmt.Println(p) } 通过查看汇编代码可知，其在进行Student-\u0026gt;Person的转换时，调用了runtime.convT，来看一下这个函数：\nfunc convT(t *_type, v unsafe.Pointer) unsafe.Pointer { // ... 部分条件检查  x := mallocgc(t.size, t, true) typedmemmove(t, x, v) return x } func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer func typedmemmove(typ *_type, dst, src unsafe.Pointer) convT函数接收一个*_type：t, 一个unsafe.Pointer：v，将类型为t的v指向的值转换为一个可以作为iface结构体第二个字段的值。\n真正工作的只有两行，第一行分配一个类型为t的新的内存空间，并为其赋零值，第二行将v指向的值复制到x，最后返回。\n对于如下代码：\nvar _ Person = (*Student)(nil) 可以检查Student是否实现了Person接口，其底层同样是调用的runtime.convT()，如果Student未实现Person接口，则在typedmemmove会发生panic\n主要汇编代码如下：\n0x0027 00039 (main.go:28) LEAQ go.string.\u0026#34;lichang\u0026#34;(SB), CX\t# 将字符串从内存装载到CX中 0x002e 00046 (main.go:28) MOVQ CX, \u0026#34;\u0026#34;..autotmp_6+56(SP)\t# CX中的值被mov到\u0026#34;\u0026#34;..autotmp_6+56(SP) 0x0033 00051 (main.go:28) MOVQ $7, \u0026#34;\u0026#34;..autotmp_6+64(SP)\t# 字面量7被mov到\u0026#34;\u0026#34;..autotmp_6+64(SP)，这里可能是要做内存对齐 0x003c 00060 (main.go:29) MOVQ $18, \u0026#34;\u0026#34;..autotmp_6+72(SP)\t# 字面量18被mov到\u0026#34;\u0026#34;..autotmp_6+72(SP) 0x0045 00069 (main.go:27) LEAQ type.\u0026#34;\u0026#34;.Student(SB), AX\t# 组装好的Student被装载到AX 0x004c 00076 (main.go:27) LEAQ \u0026#34;\u0026#34;..autotmp_6+56(SP), BX\t# \u0026#34;\u0026#34;..autotmp_6+56(SP)被装载到BX 0x0051 00081 (main.go:27) PCDATA $1, $0 0x0051 00081 (main.go:27) CALL runtime.convT(SB)\t# 调用runtime.convT构造itab 0x0056 00086 (main.go:32) MOVUPS X15, \u0026#34;\u0026#34;..autotmp_11+40(SP) 0x005c 00092 (main.go:32) MOVQ go.itab.\u0026#34;\u0026#34;.Student,\u0026#34;\u0026#34;.Person+8(SB), CX # 构造好的itab被装载到CX 0x0063 00099 (main.go:32) MOVQ CX, \u0026#34;\u0026#34;..autotmp_11+40(SP)\t# CX中的值，即itab被mov到\u0026#34;\u0026#34;..autotmp_11+40(SP) 0x0068 00104 (main.go:32) MOVQ AX, \u0026#34;\u0026#34;..autotmp_11+48(SP)\t# AX中的值，即Student字面量被装载到\u0026#34;\u0026#34;..autotmp_11+48(SP) 2.2 接口类型之间的转换 type Person interface { Walk() Say(words []string) string } type Walker interface { Walk() } type Student struct { name string age int } func (s Student) Walk() { return } func (s Student) Say(words []string) string { return strings.Join(words, \u0026#34; \u0026#34;) } func main() { var p = Person(Student{ name: \u0026#34;lichang\u0026#34;, age: 18, }) var w Walker = p fmt.Println(w) } 编译后查看汇编代码可知，在进行Person接口到Student接口的转换时调用了runtime.convI2I:\nfunc convI2I(dst *interfacetype, src *itab) *itab { if src == nil { return nil } if src.inter == dst { return src } return getitab(dst, src._type, false) } runtime.convI2I函数将src itab中的inter转换到dst类型，并返回一个新的itab，首先检查src不为空，然后判断src的inter是否与dst相等，最后调用了runtime.getitab\n来看runtime.getitab\nfunc getitab(inter *interfacetype, typ *_type, canfail bool) *itab { // ...  var m *itab // 首先会从已经存在的表中查找，如果找到了可以直接结束，否则进行下一步。  // 这里使用原子操作保证在这之前对itabTable的写操作结束。 \tt := (*itabTableType)(atomic.Loadp(unsafe.Pointer(\u0026amp;itabTable))) if m = t.find(inter, typ); m != nil { goto finish } // 如果没找到，加锁继续找 \tlock(\u0026amp;itabLock) if m = itabTable.find(inter, typ); m != nil { unlock(\u0026amp;itabLock) goto finish } // 还没找到，搞个新的 \tm = (*itab)(persistentalloc(unsafe.Sizeof(itab{})+uintptr(len(inter.mhdr)-1)*goarch.PtrSize, 0, \u0026amp;memstats.other_sys)) m.inter = inter m._type = typ m.hash = 0 m.init() itabAdd(m) unlock(\u0026amp;itabLock) finish: if m.fun[0] != 0 { return m } if canfail { return nil } panic(\u0026amp;TypeAssertionError{concrete: typ, asserted: \u0026amp;inter.typ, missingMethod: m.init()}) } 这里一开始在构造itab时并没有直接构造，而是去一个runtime.itabTableType结构体中去查找这个itab是否存在，runtime.itabTableType的定义如下：\ntype itabTableType struct { size uintptr // length of entries array. Always a power of 2. \tcount uintptr // current number of filled entries. \tentries [itabInitSize]*itab // really [size] large } 用于查找itab的函数如下：\nfunc (t *itabTableType) find(inter *interfacetype, typ *_type) *itab { // Implemented using quadratic probing. \t// Probe sequence is h(i) = h0 + i*(i+1)/2 mod 2^k. \t// We\u0026#39;re guaranteed to hit all table entries using this probe sequence. \tmask := t.size - 1 h := itabHashFunc(inter, typ) \u0026amp; mask for i := uintptr(1); ; i++ { p := (**itab)(add(unsafe.Pointer(\u0026amp;t.entries), h*goarch.PtrSize)) // Use atomic read here so if we see m != nil, we also see \t// the initializations of the fields of m. \t// m := *p \tm := (*itab)(atomic.Loadp(unsafe.Pointer(p))) if m == nil { return nil } if m.inter == inter \u0026amp;\u0026amp; m._type == typ { return m } h += i h \u0026amp;= mask } } 在完成这些步骤之后，一个新的itab就构造完成了，而由于在进行转换时实现接口的实体并没有变化，只是接口类型发生了变化，因此我们只需要将iface.itab重新赋值为我们需要的itab即可。\n2.3 空接口的转换 func main() { var x int = 1 var ix interface{} = x fmt.Println(ix) } 这里调用的是runtime.convT64(SB)函数：\nfunc convT64(val uint64) (x unsafe.Pointer) { if val \u0026lt; uint64(len(staticuint64s)) { x = unsafe.Pointer(\u0026amp;staticuint64s[val]) } else { x = mallocgc(8, uint64Type, false) *(*uint64)(x) = val } return } 这里首先检查其值是否小于len(staticuint64s)，如果是的话，就不需要再去进行内存分配，而是直接到数组中取，算是进行了一步优化。否则调用mallocgc为其分配一个新的内存空间，然后将其底层值赋值为val。\n这里如果是将一个字符串转换为空接口类型，则调用的是runtime.convTstring:\nfunc convTstring(val string) (x unsafe.Pointer) { if val == \u0026#34;\u0026#34; { x = unsafe.Pointer(\u0026amp;zeroVal[0]) } else { x = mallocgc(unsafe.Sizeof(val), stringType, true) *(*string)(x) = val } return } 逻辑相差不大。\n在runtime包中，对于某些特殊的类型做了优化，可直接调用相应的函数进行实体类型到空接口类型的转化，这些被调用的函数有：\nfunc convT16(val uint16) unsafe.Pointer func convT32(val uint32) unsafe.Pointer func convT64(val uint64) unsafe.Pointer func convTstring(val string) unsafe.Pointer func convTslice(val []uint8) unsafe.Pointer 3. 类型断言 3.1 空接口断言 对如下golang代码进行编译：\ntype User struct { name string } func main() { var x interface{} = \u0026amp;User{name: \u0026#34;lichang\u0026#34;} i, ok := x.(int) if !ok { return } else { fmt.Println(i) } } 使用如下命令编译\ngo tool compile -S -N -l main.go # -N禁止优化，-l禁止内联 # 构造接口 0x0026 00038 (main.go:11) MOVUPS X15, \u0026#34;\u0026#34;..autotmp_8+96(SP) 0x002c 00044 (main.go:11) LEAQ \u0026#34;\u0026#34;..autotmp_8+96(SP), CX 0x0031 00049 (main.go:11) MOVQ CX, \u0026#34;\u0026#34;..autotmp_7+48(SP) 0x0036 00054 (main.go:11) TESTB AL, (CX) 0x0038 00056 (main.go:11) MOVQ $7, \u0026#34;\u0026#34;..autotmp_8+104(SP) 0x0041 00065 (main.go:11) LEAQ go.string.\u0026#34;lichang\u0026#34;(SB), DX 0x0048 00072 (main.go:11) MOVQ DX, \u0026#34;\u0026#34;..autotmp_8+96(SP) 0x004d 00077 (main.go:11) MOVQ CX, \u0026#34;\u0026#34;..autotmp_3+56(SP) 0x0052 00082 (main.go:11) LEAQ type.*\u0026#34;\u0026#34;.User(SB), DX 0x0059 00089 (main.go:11) MOVQ DX, \u0026#34;\u0026#34;.x+80(SP) # \u0026#34;\u0026#34;.x+80(SP)是eface._type 0x005e 00094 (main.go:11) MOVQ CX, \u0026#34;\u0026#34;.x+88(SP)\t# \u0026#34;\u0026#34;.x+88(SP)是eface.data # 接口断言 0x0063 00099 (main.go:13) MOVQ \u0026#34;\u0026#34;.x+80(SP), CX\t# 将eface._type移动到CX 0x0068 00104 (main.go:13) MOVQ \u0026#34;\u0026#34;.x+88(SP), DX\t# 将eface._data移动到DX 0x006d 00109 (main.go:13) LEAQ type.int(SB), BX\t# 加载int的_type到BX 0x0074 00116 (main.go:13) CMPQ CX, BX\t# 对比CX与BX，即eface._type与int._type 0x0077 00119 (main.go:13) JEQ 123\t# 如果_type对比成功，则跳转到123 0x0079 00121 (main.go:13) JMP 133\t# 对比不成功，跳转133 0x007b 00123 (main.go:13) MOVQ (DX), CX\t# 将eface.data暂存到CX 0x007e 00126 (main.go:13) MOVL $1, AX 0x0083 00131 (main.go:13) JMP 139 0x0085 00133 (main.go:13) XORL CX, CX\t# 类型断言失败，清空寄存器 0x0087 00135 (main.go:13) XORL AX, AX\t# 清空AX寄存器 0x0089 00137 (main.go:13) JMP 139 0x008b 00139 (main.go:13) MOVQ CX, \u0026#34;\u0026#34;..autotmp_4+40(SP) 0x0090 00144 (main.go:13) MOVB AL, \u0026#34;\u0026#34;..autotmp_5+31(SP) 0x0094 00148 (main.go:13) MOVQ \u0026#34;\u0026#34;..autotmp_4+40(SP), CX 0x0099 00153 (main.go:13) MOVQ CX, \u0026#34;\u0026#34;.i+32(SP)\t# 断言结果保存在\u0026#34;\u0026#34;.i+32(SP) 0x009e 00158 (main.go:13) MOVBLZX \u0026#34;\u0026#34;..autotmp_5+31(SP), CX 0x00a3 00163 (main.go:13) MOVB CL, \u0026#34;\u0026#34;.ok+30(SP)\t# ok保存在\u0026#34;\u0026#34;.ok+30(SP) 从以上汇编代码可以看出，空接口在进行类型断言时，会将eface._type取出与断言类型的_type进行对比，如果对比成功，则将其eface.data取出使用，否则断言失败。\n3.2 非空接口断言 3.2.1 断言为接口 对如下代码进行编译：\ntype Person interface { Walk() Say(words []string) string } type Walker interface { Walk() } type Student struct { name string age int } func (s Student) Walk() { return } func (s Student) Say(words []string) string { return strings.Join(words, \u0026#34; \u0026#34;) } func main() { var p = Person(Student{ name: \u0026#34;lichang\u0026#34;, age: 18, }) w, ok := p.(Walker) if !ok { return } else { fmt.Println(w) } } go tool compile -S -N -l main.go 可得部分汇编代码如下：\n# iface构建 0x0026 00038 (main.go:32) MOVQ $0, \u0026#34;\u0026#34;..autotmp_3+176(SP) 0x0032 00050 (main.go:31) MOVUPS X15, \u0026#34;\u0026#34;..autotmp_3+184(SP) 0x003b 00059 (main.go:32) LEAQ go.string.\u0026#34;lichang\u0026#34;(SB), CX 0x0042 00066 (main.go:32) MOVQ CX, \u0026#34;\u0026#34;..autotmp_3+176(SP) 0x004a 00074 (main.go:32) MOVQ $7, \u0026#34;\u0026#34;..autotmp_3+184(SP) 0x0056 00086 (main.go:33) MOVQ $18, \u0026#34;\u0026#34;..autotmp_3+192(SP) 0x0062 00098 (main.go:31) LEAQ type.\u0026#34;\u0026#34;.Student(SB), AX 0x0069 00105 (main.go:31) LEAQ \u0026#34;\u0026#34;..autotmp_3+176(SP), BX 0x0071 00113 (main.go:31) PCDATA $1, $0 0x0071 00113 (main.go:31) CALL runtime.convT(SB) 0x0076 00118 (main.go:31) MOVQ AX, \u0026#34;\u0026#34;..autotmp_7+56(SP) 0x007b 00123 (main.go:31) LEAQ go.itab.\u0026#34;\u0026#34;.Student,\u0026#34;\u0026#34;.Person(SB), CX 0x0082 00130 (main.go:31) MOVQ CX, \u0026#34;\u0026#34;.p+88(SP)\t# iface.itab在\u0026#34;\u0026#34;.p+88(SP)处 0x0087 00135 (main.go:31) MOVQ AX, \u0026#34;\u0026#34;.p+96(SP)\t# iface.data在\u0026#34;\u0026#34;.p+96(SP)处 # 类型断言 0x008c 00140 (main.go:36) MOVUPS X15, \u0026#34;\u0026#34;..autotmp_4+120(SP) 0x0092 00146 (main.go:36) MOVQ \u0026#34;\u0026#34;.p+88(SP), BX\t# iface.itab被转移到BX 0x0097 00151 (main.go:36) MOVQ \u0026#34;\u0026#34;.p+96(SP), CX\t# iface.data被转移到CX 0x009c 00156 (main.go:36) LEAQ type.\u0026#34;\u0026#34;.Walker(SB), AX\t# Walker._type被加载到AX 0x00a3 00163 (main.go:36) CALL runtime.assertI2I2(SB)\t# 调用runtime.assertI2I2 0x00a8 00168 (main.go:36) MOVQ AX, \u0026#34;\u0026#34;..autotmp_4+120(SP)\t# runtime.assertI2I2的返回值存储在\u0026#34;\u0026#34;..autotmp_4+120(SP) 0x00ad 00173 (main.go:36) MOVQ BX, \u0026#34;\u0026#34;..autotmp_4+128(SP) 0x00b5 00181 (main.go:36) TESTQ AX, AX 0x00b8 00184 (main.go:36) SETNE \u0026#34;\u0026#34;..autotmp_5+47(SP) 0x00bd 00189 (main.go:36) MOVQ \u0026#34;\u0026#34;..autotmp_4+128(SP), CX 0x00c5 00197 (main.go:36) MOVQ \u0026#34;\u0026#34;..autotmp_4+120(SP), DX 0x00ca 00202 (main.go:36) MOVQ DX, \u0026#34;\u0026#34;.w+72(SP)\t# 断言接口的itab在\u0026#34;\u0026#34;.w+72(SP) 0x00cf 00207 (main.go:36) MOVQ CX, \u0026#34;\u0026#34;.w+80(SP)\t# 断言接口的data在\u0026#34;\u0026#34;.w+80(SP) 0x00d4 00212 (main.go:36) MOVBLZX \u0026#34;\u0026#34;..autotmp_5+47(SP), CX 0x00d9 00217 (main.go:36) MOVB CL, \u0026#34;\u0026#34;.ok+46(SP)\t# ok被存储在\u0026#34;\u0026#34;.ok+46(SP) runtime.assertI2I2函数如下：\nfunc assertI2I2(inter *interfacetype, i iface) (r iface) { tab := i.tab if tab == nil { return } if tab.inter != inter { tab = getitab(inter, tab._type, true) if tab == nil { return } } r.tab = tab r.data = i.data return } 该函数接受一个一个interfacetype和一个iface，检查iface.itab.inter是否与传入的interfacetype相同，若相同，则直接将原来的iface复制一个新的返回，否则构建一个具有传入的interfacetype的新的itab，并赋值给新的iface;iface.data直接复制到新的iface。\n3.2.2 断言为实体对象 对如下代码进行编译：\ntype Person interface { Walk() Say(words []string) string } type Student struct { name string age int } func (s Student) Walk() { return } func (s Student) Say(words []string) string { return strings.Join(words, \u0026#34; \u0026#34;) } func main() { var p = Person(Student{ name: \u0026#34;lichang\u0026#34;, age: 18, }) w, ok := p.(Student) if !ok { return } else { fmt.Println(w) } } go tool compile -S -N -l main.go 有关类型断言的汇编代码如下：\n# 接口构造 0x0035 00053 (main.go:27) MOVUPS X15, \u0026#34;\u0026#34;..autotmp_8+224(SP) 0x003e 00062 (main.go:28) LEAQ go.string.\u0026#34;lichang\u0026#34;(SB), CX 0x0045 00069 (main.go:28) MOVQ CX, \u0026#34;\u0026#34;..autotmp_8+216(SP) 0x004d 00077 (main.go:28) MOVQ $7, \u0026#34;\u0026#34;..autotmp_8+224(SP) 0x0059 00089 (main.go:29) MOVQ $18, \u0026#34;\u0026#34;..autotmp_8+232(SP) 0x0065 00101 (main.go:27) MOVQ CX, \u0026#34;\u0026#34;..autotmp_12+240(SP) 0x006d 00109 (main.go:27) MOVQ $7, \u0026#34;\u0026#34;..autotmp_12+248(SP) 0x0079 00121 (main.go:27) MOVQ $18, \u0026#34;\u0026#34;..autotmp_12+256(SP) 0x0085 00133 (main.go:27) LEAQ go.itab.\u0026#34;\u0026#34;.Student,\u0026#34;\u0026#34;.Person(SB), CX 0x008c 00140 (main.go:27) MOVQ CX, \u0026#34;\u0026#34;.p+88(SP)\t# iface.itab 0x0091 00145 (main.go:27) LEAQ \u0026#34;\u0026#34;..autotmp_12+240(SP), CX 0x0099 00153 (main.go:27) MOVQ CX, \u0026#34;\u0026#34;.p+96(SP)\t# iface.data # 类型断言 0x009e 00158 (main.go:32) MOVQ $0, \u0026#34;\u0026#34;..autotmp_8+216(SP) 0x00aa 00170 (main.go:32) MOVUPS X15, \u0026#34;\u0026#34;..autotmp_8+224(SP) 0x00b3 00179 (main.go:32) MOVQ \u0026#34;\u0026#34;.p+96(SP), CX\t# iface.data被装载到CX 0x00b8 00184 (main.go:32) MOVQ \u0026#34;\u0026#34;.p+88(SP), DX\t# iface.itab被装载到DX 0x00bd 00189 (main.go:32) LEAQ go.itab.\u0026#34;\u0026#34;.Student,\u0026#34;\u0026#34;.Person(SB), SI # 重新构造一个itab，存储在SI 0x00c4 00196 (main.go:32) CMPQ DX, SI # 对比新构造的itab和老的itab 0x00c7 00199 (main.go:32) JEQ 203 0x00c9 00201 (main.go:32) JMP 221 0x00cb 00203 (main.go:32) MOVQ (CX), DX\t# CX(iface.data)指向的值0-7个字节mov到DX 0x00ce 00206 (main.go:32) MOVQ 8(CX), SI\t# 第8-15个字节mov到SI 0x00d2 00210 (main.go:32) MOVQ 16(CX), CX\t# 第16-24个字节mov到CX 0x00d6 00214 (main.go:32) MOVL $1, AX 0x00db 00219 (main.go:32) JMP 231 0x00dd 00221 (main.go:32) XORL SI, SI\t# 断言失败执行 0x00df 00223 (main.go:32) XORL CX, CX 0x00e1 00225 (main.go:32) XORL AX, AX 0x00e3 00227 (main.go:32) XORL DX, DX 0x00e5 00229 (main.go:32) JMP 231 0x00e7 00231 (main.go:32) MOVQ DX, \u0026#34;\u0026#34;..autotmp_8+216(SP) 0x00ef 00239 (main.go:32) MOVQ SI, \u0026#34;\u0026#34;..autotmp_8+224(SP) 0x00f7 00247 (main.go:32) MOVQ CX, \u0026#34;\u0026#34;..autotmp_8+232(SP) 0x00ff 00255 (main.go:32) MOVB AL, \u0026#34;\u0026#34;..autotmp_9+47(SP) 0x0103 00259 (main.go:32) MOVQ \u0026#34;\u0026#34;..autotmp_8+216(SP), CX 0x010b 00267 (main.go:32) MOVQ \u0026#34;\u0026#34;..autotmp_8+224(SP), DX 0x0113 00275 (main.go:32) MOVQ \u0026#34;\u0026#34;..autotmp_8+232(SP), SI 0x011b 00283 (main.go:32) MOVQ CX, \u0026#34;\u0026#34;.s+168(SP)\t# 构造Student结构体 0x0123 00291 (main.go:32) MOVQ DX, \u0026#34;\u0026#34;.s+176(SP) 0x012b 00299 (main.go:32) MOVQ SI, \u0026#34;\u0026#34;.s+184(SP) 0x0133 00307 (main.go:32) MOVBLZX \u0026#34;\u0026#34;..autotmp_9+47(SP), CX 0x0138 00312 (main.go:32) MOVB CL, \u0026#34;\u0026#34;.ok+46(SP)\t# ok存储在\u0026#34;\u0026#34;.ok+46(SP) 在进行接口到实体对象的断言时，编译器会尝试构建一个断言对象对应的itab，然后和原接口的itab进行对比，如果相同，则断言可成功，否则失败。\n断言成功后，将iface.data解析，重新组装成断言对象。断言失败则清空寄存器，返回一个断言对象的零值\nReferences Go Questions\ninterface的类型断言是如何实现\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/runtime%E7%AF%87%E4%B8%80%E6%8E%A5%E5%8F%A3/","summary":"本系列代码基于golang1.19\n  runtime篇一：接口 runtime篇二：通道 runtime篇三：defer runtime篇四：panic  1. 接口的内部结构 type iface struct { tab *itab data unsafe.Pointer } 一个接口是一个iface结构体，其中包含一个itab指针和一个unsafe.Pointer。\n概念上讲一个接口的值,接口值,由两个部分组成,一个具体的类型和那个类型的值。它们被称为接口的动态类型和动态值。\n一个itab可以表示一个接口的类型和赋给这个接口的实体类型，即为接口的动态类型，而data所指向的unsafe.Pointer则指向接口的动态值。\n近距离来看itab：\ntype itab struct { inter *interfacetype _type *_type hash uint32 // copy of _type.hash. Used for type switches. \t_ [4]byte fun [1]uintptr // variable sized. fun[0]==0 means _type does not implement inter. } 其中inter字段描述了接口的类型，_type字段描述了实体类型，hash字段是类型哈希，用于类型匹配，fun字段放置和接口方法对应的具体数据类型的方法地址。\n再来看interfacetype\ntype interfacetype struct { typ _type pkgpath name mhdr []imethod } 其中typ和itab中的_type为同一个值，pkgpath则存储了接口的包名，mhdr则表示接口所定义的函数列表。","title":"runtime篇一：接口"},{"content":"- 第一阶段 接受你的存在 0-1分 - 第二阶段 习惯你的存在 1-2分 - 第三阶段 依赖你的存在 2-3分 0.7以上就可以表白，2分以上可结婚\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%9D%82%E8%AE%B0/%E6%81%8B%E7%88%B1%E4%B8%89%E5%88%86%E7%90%86%E8%AE%BA/","summary":"- 第一阶段 接受你的存在 0-1分 - 第二阶段 习惯你的存在 1-2分 - 第三阶段 依赖你的存在 2-3分 0.7以上就可以表白，2分以上可结婚","title":"恋爱三分理论"},{"content":"1. TLS在哪些地方被使用 TLS(Transport Layer Security),是为计算机网络中提供安全通信的密码学协议。\n HTTPS = HTTP +　TLS SMTPS = SMTPS + TLS FTPS = FTP + TLS \u0026hellip;  2. TLS给我们带来了什么  认证  TLS检查通信双方的身份 借助于非对称加密，TLS保证我们访问的是“真网站”，不是假冒的。   加密  TLS 通过使用对称加密算法对其进行加密来保护交换的数据免受未经授权的访问。   校验  TLS 通过检查消息验证码来识别传输过程中的任何数据更改    3. TLS通信的工作的基本流程 通常，TLS包含2个过程，或者说2个协议。\n Handshake Protocol 在这个阶段，客户端和服务端：  协商协议版本 选择加密算法 通过非对称加密算法验证对方身份呢 建立一个共享的对称加密密钥以应用于接下来的通信   Record Protocol 在这个阶段：  所有发出的信息都被上个阶段商定的对称密钥加密 信息被发送到对面 接收方验证信息是否受到篡改 如果未被篡改，信息将被解密    4. 为什么TLS要同时使用对称加密和非对称加密 很明显，对称加密不能保证通信的安全性，通信双方共享同一个密钥，他们没法进行彼此验证，并且很难保证密钥不被泄露。\n那么为什么不都使用非对称加密？非对称加密远远慢于对称加密，大概是100～10000量级，因此，只使用非对称加密也是不可取的。\n5. 对称加密 对称加密的流程大概是这样：\nAlice有一个文本想要发送给Bob，但不想任何其他人读取到它。\n因此她使用之前已经共享过的密钥对文本进行加密，然后她通过公共互联网将加密文本发送给Bob\n接收到加密文本后，Bob使用相同的密钥解密。\n加密和解密使用的是相同的密钥，存在某种意义上的“对称”，因此我们称为“对成加密”\n现在有一个黑客Harry，他可以从网络中截取到Alice和Bob之间的通信信息。通信信息是加密的，Harry没法读取它。\n但，Harry可以对其进行更改！\n5.1 位翻转攻击 Bit-flipping attack 位翻转攻击的工作方式类似如下所述：\nAlice发送消息到银行，想要转账給某人100元，这个消息被密钥加密后通过网络发送给银行。\nHarry捕获了加密信息，他不能进行解密，但他可以翻转其中的某些位，然后再将修改过的信息发送给银行。\n这样银行在解密消息后，得到的是与原来不同的消息内容，转账100元变成了转账900元。\n显然这非常危险，我们需要某种方法来确保消息在传输过程中没有被改变。\n5.2 加密认证 Authenticated Encryption (AE) 一种检查消息未被篡改的方式是使用加密认证。这种方式并不是简单的加密，而是需要对消息进行认证。\n第一步是加密。\nAlice的源文本通过一个对称加密算法被加密，如AES-256-GCM或CHACHA20.\n这个加密算法需要一个共享的密钥和一个随机值（nonce），或是一个初始化向量initialization vector (IV)作为输入，返回加密消息。\n第二步是认证。\n加密消息、密钥、随机nonce值作为MAC算法的输入，例如，如果你用的是AES-256-GCM可以使用GMAC，如果用的是CHACHA20可以使用POLY1305.\n这些MAC算法类似一个hash加密函数，其输出为一个MAC，message authentication code.\n这个MAC值将被标记到加密消息上，然后整个消息将被发送到Bob。因此，我们有时称这个MAC为认证标签（authentication tag）。\n添加附加数据（Associated Data (AD)）\n在TLS1.3版本中，除了加密消息，我们还希望认证一些额外的数据，比如地址、端口、协议版本，或是序列号。该信息是未加密的，通信双方都知道\n因此AD同样是MAC算法的数据，正因如此，整个过程被称为Authenticated Encryption with Associated Data，简写为AEAD.\n解密和MAC验证\n现在来看Bob如何检查加密消息是否被篡改。\n这是一个简单的逆过程，首先我们将标签从加密消息中取下。\n然后加密消息将和密钥，nonce一起被再次输入到MAC算法中，这里注意需要使用与Alice相同的nonce值。通常在发送给接收者之前，随机数被填充到加密消息中。\n附加消息（AD）同样被添加到MAC算法中，MAC算法输出另一个hash值。\n然后Bob可以简单的对比两个MAC值，如果是相同的，代表消息未被篡改，他可以安心的解密消息并处理，否则消息是被篡改过的，应予以丢弃。\n5.3 交换密钥 上面描述的过程存在一个问题：Alice和Bob如何在不将密钥泄露的情况下共享同一个密钥。\n答案是：他们需要使用非对称加密技术或公钥加密来达到这个目的。例如Diffie-Hellman Ephemeral,Elliptic-Curve Diffie-Hellman Ephemeral.\n5.3.1 Diffie-Hellman 密钥交换 首先，Alice和Bob协商确认两个数字：基数g，和模数p。这两个数字是众所周知的（任何人都可以知道）。\n然后他们都为自己选择一个私有的数字，例如Alice是a,Bob是b.\n然后Alice计算出她的公钥并将其发送给Bob。\nA = (g^a) mod p 同样的，Bob计算他的公钥并发送给Alice\nB = (g^b) mod p Alice收到了Bob的公钥B，Bob收到了Alice的公钥A。\n重点来了！\nAlice进行如下计算：\nS = (B^a) mod p Bob进行如下计算：\nS = (A^b) mod p 他们得到了同样的数字S.\n证明如下：\n(B^a) mod p = (g^b)^a mod p = ( g^(b*a) ) mod p (A^b) mod p = (g^a)^b mod p = ( g^(a*b) ) mod p 因此Alice和Bob在不泄露密钥的情况下拥有了同样的密钥S.\n5.3.2 密钥导出函数 Key Derivation Function - KDF 不同的加密算法可能需要不同长度的密钥，因此要想获取密钥，Alice和Bob必须将S输入相同的密钥导出函数KDF，然后输出将是所需长度的共享密钥。\n在TLS1.3中，使用基于HMAC的密钥导出函数，称为HKDF\n通常，KDF需要以下输入：\n input key material (IKM),这里IKM为S 所需密钥长度 一个加密hash函数，例如HMAC-SHA256 可选的一些信息 可选的盐值  5.3.3 陷门函数 Trapdoor function 现在让我们回到Diffie-Hellman密钥交换\n我们知道p, g, A, B是众所周知的，因此Harry，同样可以访问这些数字。\n我们可能会问：这种密钥分享机制安全吗？给定p, g, A, B，Harry是否可以计算出a, b, S\n幸运的是，如果我们选定较好的p, g, a, b，这些函数将成为陷门函数。\n例如：\n 选择一个2048位的素数为p 选择 g 作为原根模 p 选择256位的随机值作为a，b  陷门函数的名字来源于其可轻易的从一个方向计算出来，但从另一个方向很难计算。就像陷阱，进去很容易，但出来很困难。在这种情况下：\n 给定p, g, a，很容易计算出A 但给定p, g, A，很难计算出a  我们可以以O(log(a))的时间复杂度计算出A，这是众所周知的 Modular exponentiation problem\n但计算出a非常困难，这是一个离散对数问题，在现有的计算能力下需要很长时间才能计算出。\n因此我们至少目前是安全的，或在下一代量子计算机到来之前。\n但目前来看，一个很长时间才能解决的问题，意味着无解。\n5.3.4 静态或临时密钥？ 如果Alice和Bob的每次通信都使用相同的密钥a和b,这样Harry就可以从他们第一次会话就开始计算a.\n虽然计算需要花费很长时间，但经过N次会话，Harry可能会得到正确的a。那么他就可以计算出正确的密钥A，因此他可以解密所有他记录的信息。\n如何解决？\n答案是使用临时密钥，在每次会话中使用不同的密钥。这样及时Harry已经计算出第一次会话的密钥a，但他依然不能将其用于其他会话。\n这在TLS中被称为完美前向保密\n现在我们可以理解什么是Diffie-Hellman Ephemeral，只是Diffie-Hellman算法加上临时，短期有效的key。\n6. 椭圆曲线密码学 椭圆曲线密码学（或 ECC）是一种非对称密码学方法，其算法相似，但使用了不同的陷门函数。\n该陷门函数基于椭圆曲线的代数结构。这就是这个名字的原因。\n椭圆曲线密码学的一个重点是：它可以更小的密钥提供相同的安全等级。在与RSA的比较中经常可以看到它。\n美国国家安全局 (NSA) 过去使用 ECC 384 位密钥保护其最高机密，该密钥提供与 RSA-7680 位密钥相同的安全级别。\n然而，椭圆曲线密码学更容易成为量子计算攻击的目标。与破解 RSA 相比，Shor 的算法可以在具有更少量子资源的假设量子计算机上破解 ECC。\n7. 非对称加密 现在让我们回到非对称加密算法，这种令人惊叹的技术得到了非常广泛的应用。\n我们已经使用Diffie-Hellman Ephemeral 和 Elliptic-Curve Diffie-Hellman Ephemeral 解释了其中一个应用：对称密钥的交换，\n事实上，RSA算法在过去也被用于密钥交换，但从TLS1.3起由于各种攻击和不具有前向保密能力其已被移除，\n非对称加密还被用于保密系统。下面是一些非对称加密算法：\n RSA with optimal asymmetric encryption padding (RSA-OAEP). RSA with public key cryptography standard 1 (RSA-PKCS1) with the latest version 2.2 Elgamal Encryption algorithm  最后，非对称加密的一个重要特性就是数字签名，TLS被广泛用于身份验证。\n一些在TLS使用的流行的数字签名算法有：\n RSA with Probabilitic Signature Scheme. Elliptic-Curve Digital Signature Algorithm. Edwards-Curve Digital Signature Algorithm.  下面我们会介绍数字证书，但先让我们来看一下非对称加密系统是如何工作的。\n7.1 非对称加密 类似对称加密，Alice有一个源文本需要发送给Bob。\n但这次，他们之间没有共享的密钥。Alice使用Bob的公钥加密消息，并发送密文給Bob。\nBob受到密文后，用他的私钥解密密文。\n虽然公钥和私钥完全不同，但他们之间仍存在某些陷门函数的联系，就像我们之前在Diffie-Hellman中看到的那样。\n这个机制这样运行：密钥成对出现，只有密钥对中的私钥可以解密被公钥加密的密文。\n因此，即使Harry具有Alice发送的密文和Bob的公钥，他依然无法解密消息。\n7.2 公钥共享 公钥共享非常简单，Bob直接通过公共网络发送他的公钥给Alice而不用担心密钥可以解密任何消息。\n这个密钥是公开的，因此任何人都可以用它进行加密消息，但只有Bob可以读取加密消息，即使他们从未交流过。\n7.3 中间人攻击 我们已经知道Harry不能通过Bob的公钥解密消息，但他依然可以影响公钥交换，例如使用自己的公钥替换Bob的公钥。\n现在当ALice受到密钥，她依然认为这个Bob的公钥，但事实上是Harry的。因此当Alice使用该公钥加密消息，Harry就可以用他的私钥解密消息。\n原因在于密钥只是一个数字，并没有身份标识其所有者。\n面对这种问题，我们该怎么做？我们应该将身份信息添加到密钥中，这就是数字证书的做法。\n7.4 数字证书 Bob将他的密钥加入到证书中，证书中包含他的姓名或其他身份信息。数字证书就像现实世界的签证。\n当我们怎么确认Bob确实是证书的所有者？如何组织Harry伪造一个具有Bob的身份信息却包含Harry密钥的证书。\n就像现实世界，签证必须由权威机构在一系列身份认证后签发，在数字世界，数字证书也必须由某些证书颁发机构认可并签发。\n这些证书颁发结构(certificat authority CA)和签证颁发结构被第三方认可，他们帮助我们阻止证书被伪造。\n证书签名\n证书签名过程如下：\n Bob具有一对公私钥 第一步，他构造一个证书签名请求（certificat signing request CSR），这个CSR包含他的公钥和一些身份信息，例如他的姓名，组织和邮箱。 第二步，他使用他的私钥对CSR进行签名，并且将其发送给CA CA将验证Bob的身份，其可要求Bob提供更多证明身份的信息。 然后证书颁发机构使用证书中Bob的公钥校验Bob的签名，这将确认Bob确实拥有与证书中公钥相对应的私钥。 认证全部通过，CA将使用他们的私钥对证书进行签名，并将其发送回Bob  证书分享过程： 现在Bob将把他的证书分享给Alice，其中包含他的公钥，而不再是只发送他的公钥\n收到证书后，Alice可以很容易通过CA的公钥来验证Bob的证书。\n因此，Harry不能将其中Bob的公钥换成他的公钥，因为Harry并没有CA的私钥用于重新对伪造的证书进行签名。\n这其中的重点在于我们都信任CA，如果CA不可信，将其私钥泄露给Harry，那么我们将面临严重的安全问题。\n7.5 CA-信任链 事实上，CA具有链式的结构。\n最高层是根CA，他们自己签发自己的证书，同样签发他们子机构的证书，也就是中间CA。\n中间CA又可以签发其他中间CA的证书，或者他们可以签发end-entity certificat，也即叶子证书。\n每个证书都将引用他们上层的证书，一直到root。\n操作系统和浏览器存储了一系列已被信任的CA结构，通过这种方式，可有效的验证所有证书的真实性。\n7.6 数字证书 现在来看数字证书到底是如何工作的。\n要对一个文档进行签名：\n 签名者首先需要对文档进行hash 然后hash值将被用签名者的私钥进行加密 加密结果叫数字签名digital signature 数字签名将被附加到原始文档中。  这是签名过程，那么如何验证签名是正确的。\n只需做一个反向操作：\n 首先将数字签名从文档中取出 使用signer的公钥进行解密得到hash值 然后使用同样的算法对原始文档进行hash 对比两个hash值 如果相同，则数字签名正确  8. TLS1.3 握手协议 现在我们有了上面的这些知识，可以来近距离看看TLS握手协议。\n8.1 完整握手 TLS1.3 完整握手从一个客户端发送给服务端的hello消息开始。事实上这个消息包含很多信息，但这里只列举一些较为重要的：\n 首先是客户端支持的协议版本号 然后是支持的 AEAD 对称密码套件列表。在这种情况下，有 2 个选项：AES-256-GCM 或 CHACHA20-POLY1305 接着是受支持的密钥交换组列表。例如，此客户端支持有限域 Diffie-Hellman Ephemeral 和 Elliptic-Curve Diffie-Hellman Ephemeral。 这就是为什么客户端还共享其 2 个公钥，1 个用于 Diffie-Hellman，另一个用于 Elliptic-Curve Diffie-Hellman。这样，无论选择何种算法，服务器都能够计算共享密钥。 客户端在此消息中发送的最后一个字段是它支持的签名算法列表。这是为了让服务器选择它应该使用哪种算法来签署整个握手。我们稍后会看到它是如何工作的。  收到客户端的hello消息后，服务端同样返回一个hello消息，其中包含：\n 选择的协议版本： TLS1.3 选择的密码套件：AES-256-GCM 选择的密钥交换方法：Diffie-Hellman Ephemeral 所选方法的服务器公钥 下一个字段是对客户端证书的请求，它是可选的，只有在服务器想要通过其证书对客户端进行身份验证时才会发送。 通常在 HTTPS 网站上，只有服务器端需要将其证书发送给客户端。这将在此消息的下一个字段中发送。    下一个字段是证书验证，实际上是到此为止的整个握手的签名。 以下是它的生成方式：从握手开始到证书请求的整个数据称为握手上下文。 我们将此上下文与服务器的证书连接，对其进行哈希处理，并使用客户端支持的一种签名算法使用服务器的私钥对哈希值进行签名。\n  以类似的方式，通过连接握手上下文、证书和证书来生成服务器完成，验证、散列它，并将散列值放入所选密码套件的 MAC 算法。 结果是整个握手的 MAC。\n  此处的服务器证书、证书验证和服务器完成称为身份验证消息，因为它们用于对服务器进行身份验证。 借助整个握手的签名和 MAC，TLS 1.3 可以安全地抵御多种类型的中间人降级攻击。\n现在客户端收到服务端的hello消息后，会以root权限验证服务端的证书，并检查整个握手过程的签名和MAC，确保没有被篡改。\n如果一切正常，则客户端发送其完成消息，其中包含整个握手的 MAC，并且可以选择客户端的证书和证书验证以防服务器请求。\n这就是整个TLS握手的流程。\n8.2 带 PSK 恢复的简短握手 为了提高性能，客户端和服务器并不总是经过这个完整的握手。 有时，他们通过使用预共享密钥恢复来执行简短的握手。\n思路是：在上一次握手之后，客户端和服务端已经相互认识，不需要再次认证。\n因此服务器可能会向客户端发送一个或多个会话票据，该票据可以用作下次握手时的预共享密钥（pre-shared key PSK）身份。 它与票证有效期以及其他一些信息一起使用。\n这样在下一次握手，客户端只需要发送一个简单的hello消息：\n 从上次握手中获得的 PSK 身份（或票证）列表 一种 PSK 密钥交换模式，可以是仅 PSK，也可以是带有 Diffie-Hellman 的 PSK。 如果使用具有 Diffie-Hellman 模式的 PSK，则客户端还需要共享其 Diffie-Hellman 公钥。 这将提供完美的前向保密，并允许服务器在需要时回退到完全握手。  服务收到客户端的hello消息后，将返回一个hello消息：\n 选定的预共享密钥身份 服务器的可选 Diffie-Hellman 公钥 和服务器完成就像在完整的握手中一样。  最后客户端发回它的 Finish，这就是 PSK 恢复的结束。\n如您所见，在这个简短的握手中，客户端和服务器之间没有证书身份验证。\n这也为零往返时间 (0-RTT) 数据提供了机会，这意味着客户端无需等待握手完成即可将其第一个应用程序数据发送到服务器。\n8.3 零往返 （0-RTT）握手 在 0-RTT 中，客户端将应用程序数据与客户端 hello 消息一起发送。 此数据使用从票证列表中的第一个 PSK 派生的密钥进行加密。\n它还增加了 1 个字段：早期数据指示，告诉服务器有早期应用程序数据正在发送。\n如果服务器接受了这个 0-RTT 请求，它将像正常的 PSK 恢复一样向服务器发送 hello，并且还可以选择一些应用程序数据。\n客户端将以包含 MAC 的消息和早期数据结束指示符结束。 这就是 TLS 1.3 中 0 往返时间的工作原理。\n它的优点是将延迟减少 1 个往返时间。 但缺点是打开了重放攻击的潜在威胁。 这意味着，黑客可以多次复制并发送相同的加密 0-RTT 请求到服务器。 为避免这种情况，服务器应用程序必须以一种对重复请求具有弹性的方式实现。\n9. TLS1.2 VS TLS1.3 在文章结束之前，来做一个TLS1.3和TLS1.2的对比\n  TLS 1.3 具有更安全的密钥交换机制，其中易受攻击的 RSA 和其他静态密钥交换方法被移除，只留下短暂的 Diffie-Hellman 或 Elliptic-Curve Diffie-Hellman，因此实现了完美的前向保密。\n  TLS 1.3 握手比 TLS 1.2 至少快 1 次往返。\n  TLS 1.3 中的对称加密更加安全，因为 AEAD 密码套件是强制性的，并且它还从列表中删除了一些弱算法，例如块密码模式 (CBC)、RC4 或三重 DES。\n  TLS 1.3 中的密码套件也更简单，因为它只包含 AEAD 算法和哈希算法。密钥交换和签名算法被移到单独的字段中。在 TLS 1.2 中，它们被合并到密码套件中。这使得推荐密码套件的数量变得太大，如果我没记错的话，TLS 1.2 中有 37 个选项。而在 TLS 1.3 中，只有 5 个。\n  接下来，TLS 1.3 还给了我们更强的签名，因为它对整个握手进行签名，而不仅仅是像 TLS 1.2 中那样覆盖其中的一部分。\n  最后但并非最不重要的一点是，椭圆曲线密码学在 TLS 1.3 中得到了极大的关注，并添加了一些更好的曲线算法，例如 Edward-curve 数字签名算法，它在不牺牲安全性的情况下速度更快。\n  END\nOrigin Article https://dev.to/techschoolguru/a-complete-overview-of-ssl-tls-and-its-cryptographic-system-36pd\n","permalink":"http://yangchnet.github.io/Dessert/posts/net/%E6%A6%82%E8%BF%B0tls/","summary":"1. TLS在哪些地方被使用 TLS(Transport Layer Security),是为计算机网络中提供安全通信的密码学协议。\n HTTPS = HTTP +　TLS SMTPS = SMTPS + TLS FTPS = FTP + TLS \u0026hellip;  2. TLS给我们带来了什么  认证  TLS检查通信双方的身份 借助于非对称加密，TLS保证我们访问的是“真网站”，不是假冒的。   加密  TLS 通过使用对称加密算法对其进行加密来保护交换的数据免受未经授权的访问。   校验  TLS 通过检查消息验证码来识别传输过程中的任何数据更改    3. TLS通信的工作的基本流程 通常，TLS包含2个过程，或者说2个协议。\n Handshake Protocol 在这个阶段，客户端和服务端：  协商协议版本 选择加密算法 通过非对称加密算法验证对方身份呢 建立一个共享的对称加密密钥以应用于接下来的通信   Record Protocol 在这个阶段：  所有发出的信息都被上个阶段商定的对称密钥加密 信息被发送到对面 接收方验证信息是否受到篡改 如果未被篡改，信息将被解密    4.","title":"详解TLS（译）"},{"content":" 极客时间《消息队列高手课》笔记\n  概览消息队列篇〇：为什么需要消息队列 概览消息队列篇一：不同的mq的消息模型 概览消息队列篇二：如何确保消息不丢失 概览消息队列篇三：重复消息的处理  1. 异步处理 对于一个秒杀系统，需要解决的核心问题是，如何利用有限的服务器资源，尽可能多地处理短时间内的海量请求。\n处理一个秒杀请求包含了很多步骤：如风险控制、库存锁定、生成订单、短信通知、更新通知数据等。如果没有任何优化，正常的处理流程是：App 将请求发送给网关，依次调用上述 5 个流程，然后将结果返回给 APP。\n但上述5个步骤中，只需要风险控制和库存锁定这两步就可以决定秒杀是否成功，对于后续的生成订单、短信通知和更新统计数据等步骤，并不一定要在秒杀请求中处理完成。\n所以当服务端完成前面 2 个步骤，确定本次请求的秒杀结果后，就可以马上给用户返回响应，然后把请求的数据放入消息队列中，由消息队列异步地进行后续的操作。\n处理一个秒杀请求，从 5 个步骤减少为 2 个步骤，这样不仅响应速度更快，并且在秒杀期间，我们可以把大量的服务器资源用来处理秒杀请求。秒杀结束后再把资源用于处理后面的步骤，充分利用有限的服务器资源处理更多的秒杀请求。\n可以看到，在这个场景中，消息队列被用于实现服务的异步处理。\n这样做的好处是：\n 可以更快地返回结果； 减少等待，自然实现了步骤之间的并发，提升系统总体的性能。  2. 流量控制 在用消息队列实现了部分工作的异步处理后，我们还需要考虑如何避免过多的请求压垮我们的秒杀系统。\n一个设计健壮的程序有自我保护的能力，也就是说，它应该可以在海量的请求下，还能在自身能力范围内尽可能多地处理请求，拒绝处理不了的请求并且保证自身运行正常。\n我们可以使用消息队列隔离网关和后端服务，以达到流量控制和保护后端服务的目的。\n加入消息队列后，整个秒杀流程变为：\n 网关在收到请求后，将请求放入请求消息队列； 后端服务从请求消息队列中获取 APP 请求，完成后续秒杀处理过程，然后返回结果。  这种设计的优点是：能根据下游的处理能力自动调节流量，达到“削峰填谷”的作用。但这样做同样是有代价的：\n 增加了系统调用链环节，导致总体的响应时延变长。 上下游系统都要将同步调用改为异步消息，增加了系统的复杂度。  3. 服务解耦 对于一个电商系统来说，当一个新订单创建时：\n 支付系统需要发起支付流程； 风控系统需要审核订单的合法性； 客服系统需要给用户发短信告知用户； 经营分析系统需要更新统计数据； ……  这些订单下游的系统都需要实时获得订单数据。随着业务不断发展，这些订单下游系统不断的增加，不断变化，并且每个系统可能只需要订单数据的一个子集，负责订单服务的开发团队不得不花费很大的精力，应对不断增加变化的下游系统，不停地修改调试订单系统与这些下游系统的接口。任何一个下游系统接口变更，都需要订单模块重新进行一次上线，对于一个电商的核心服务来说，这几乎是不可接受的。\n所有的电商都选择用消息队列来解决类似的系统耦合过于紧密的问题。引入消息队列后，订单服务在订单变化时发送一条消息到消息队列的一个主题 Order 中，所有下游系统都订阅主题 Order，这样每个下游系统都可以获得一份实时完整的订单数据。无论增加、减少下游系统或是下游系统需求如何变化，订单服务都无需做任何更改，实现了订单服务与下游服务的解耦。\n其他  作为发布 / 订阅系统实现一个微服务级系统间的观察者模式； 连接流计算任务和数据； 用于将消息广播给大量接收者。  消息队列带来的一些问题  引入消息队列带来的延迟问题； 增加了系统的复杂度； 可能产生数据不一致的问题。  ","permalink":"http://yangchnet.github.io/Dessert/posts/mq/%E6%A6%82%E8%A7%88%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%AF%87%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","summary":" 极客时间《消息队列高手课》笔记\n  概览消息队列篇〇：为什么需要消息队列 概览消息队列篇一：不同的mq的消息模型 概览消息队列篇二：如何确保消息不丢失 概览消息队列篇三：重复消息的处理  1. 异步处理 对于一个秒杀系统，需要解决的核心问题是，如何利用有限的服务器资源，尽可能多地处理短时间内的海量请求。\n处理一个秒杀请求包含了很多步骤：如风险控制、库存锁定、生成订单、短信通知、更新通知数据等。如果没有任何优化，正常的处理流程是：App 将请求发送给网关，依次调用上述 5 个流程，然后将结果返回给 APP。\n但上述5个步骤中，只需要风险控制和库存锁定这两步就可以决定秒杀是否成功，对于后续的生成订单、短信通知和更新统计数据等步骤，并不一定要在秒杀请求中处理完成。\n所以当服务端完成前面 2 个步骤，确定本次请求的秒杀结果后，就可以马上给用户返回响应，然后把请求的数据放入消息队列中，由消息队列异步地进行后续的操作。\n处理一个秒杀请求，从 5 个步骤减少为 2 个步骤，这样不仅响应速度更快，并且在秒杀期间，我们可以把大量的服务器资源用来处理秒杀请求。秒杀结束后再把资源用于处理后面的步骤，充分利用有限的服务器资源处理更多的秒杀请求。\n可以看到，在这个场景中，消息队列被用于实现服务的异步处理。\n这样做的好处是：\n 可以更快地返回结果； 减少等待，自然实现了步骤之间的并发，提升系统总体的性能。  2. 流量控制 在用消息队列实现了部分工作的异步处理后，我们还需要考虑如何避免过多的请求压垮我们的秒杀系统。\n一个设计健壮的程序有自我保护的能力，也就是说，它应该可以在海量的请求下，还能在自身能力范围内尽可能多地处理请求，拒绝处理不了的请求并且保证自身运行正常。\n我们可以使用消息队列隔离网关和后端服务，以达到流量控制和保护后端服务的目的。\n加入消息队列后，整个秒杀流程变为：\n 网关在收到请求后，将请求放入请求消息队列； 后端服务从请求消息队列中获取 APP 请求，完成后续秒杀处理过程，然后返回结果。  这种设计的优点是：能根据下游的处理能力自动调节流量，达到“削峰填谷”的作用。但这样做同样是有代价的：\n 增加了系统调用链环节，导致总体的响应时延变长。 上下游系统都要将同步调用改为异步消息，增加了系统的复杂度。  3. 服务解耦 对于一个电商系统来说，当一个新订单创建时：\n 支付系统需要发起支付流程； 风控系统需要审核订单的合法性； 客服系统需要给用户发短信告知用户； 经营分析系统需要更新统计数据； ……  这些订单下游的系统都需要实时获得订单数据。随着业务不断发展，这些订单下游系统不断的增加，不断变化，并且每个系统可能只需要订单数据的一个子集，负责订单服务的开发团队不得不花费很大的精力，应对不断增加变化的下游系统，不停地修改调试订单系统与这些下游系统的接口。任何一个下游系统接口变更，都需要订单模块重新进行一次上线，对于一个电商的核心服务来说，这几乎是不可接受的。\n所有的电商都选择用消息队列来解决类似的系统耦合过于紧密的问题。引入消息队列后，订单服务在订单变化时发送一条消息到消息队列的一个主题 Order 中，所有下游系统都订阅主题 Order，这样每个下游系统都可以获得一份实时完整的订单数据。无论增加、减少下游系统或是下游系统需求如何变化，订单服务都无需做任何更改，实现了订单服务与下游服务的解耦。\n其他  作为发布 / 订阅系统实现一个微服务级系统间的观察者模式； 连接流计算任务和数据； 用于将消息广播给大量接收者。  消息队列带来的一些问题  引入消息队列带来的延迟问题； 增加了系统的复杂度； 可能产生数据不一致的问题。  ","title":"概览消息队列篇〇：为什么需要消息队列"},{"content":" 极客时间《消息队列高手课》笔记\n  概览消息队列篇〇：为什么需要消息队列 概览消息队列篇一：不同的mq的消息模型 概览消息队列篇二：如何确保消息不丢失 概览消息队列篇三：重复消息的处理  消息队列本身可以保证消息不重复吗 在 MQTT 协议中，给出了三种传递消息时能够提供的服务质量标准，这三种服务质量从低到高依次是：\n At most once: 至多一次。消息在传递时，最多会被送达一次。换一个说法就是，没什么消息可靠性保证，允许丢消息。一般都是一些对消息可靠性要求不太高的监控场景使用，比如每分钟上报一次机房温度数据，可以接受数据少量丢失。 At least once: 至少一次。消息在传递时，至少会被送达一次。也就是说，不允许丢消息，但是允许有少量重复消息出现。 Exactly once：恰好一次。消息在传递时，只会被送达一次，不允许丢失也不允许重复，这个是最高的等级。  这个服务质量标准不仅适用于 MQTT，对所有的消息队列都是适用的。我们现在常用的绝大部分消息队列提供的服务质量都是 At least once，包括 RocketMQ、RabbitMQ 和 Kafka 都是这样。\n也就是说，消息队列很难保证消息不重复\n用幂等性解决重复消息问题 一般解决重复消息的办法是，在消费端，让我们消费消息的操作具备幂等性。\n一个幂等操作的特点是，其任意多次执行所产生的影响均与一次执行的影响相同。\n一个幂等的方法，使用同样的参数，对它进行多次调用和一次调用，对系统产生的影响是一样的。所以，对于幂等的方法，不用担心重复执行会对系统造成任何改变。\n如果我们系统消费消息的业务逻辑具备幂等性，那就不用担心消息重复的问题了，因为同一条消息，消费一次和消费多次对系统的影响是完全一样的。也就可以认为，消费多次等于消费一次。从对系统的影响结果来说：At least once + 幂等消费 = Exactly once。\n常用设计幂等操作的方法\n 利用数据库的唯一约束实现幂等  对于一个转账操作，我们可以设置对于每一个转账单只能进行一次转账操作，这样除第一次操作外其他重复操作都会失败。\n基于这个思路，不光是可以使用关系型数据库，只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统都可以用于实现幂等，比如，还可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。\n为更新的数据设置前置条件  如果满足条件就更新数据，否则拒绝更新数据，在更新数据的时候，同时变更前置条件中需要判断的数据。这样，重复执行这个操作时，由于第一次更新数据的时候已经变更了前置条件中需要判断的数据，不满足前置条件，则不会重复执行更新数据操作。\n记录并检查操作  记录并检查操作，也称为“Token 机制或者 GUID（全局唯一 ID）机制”，实现的思路特别简单：在执行数据更新操作之前，先检查一下是否执行过这个更新操作。\n具体的实现方法是，在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。\n","permalink":"http://yangchnet.github.io/Dessert/posts/mq/%E6%A6%82%E8%A7%88%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%AF%87%E4%B8%89%E9%87%8D%E5%A4%8D%E6%B6%88%E6%81%AF%E7%9A%84%E5%A4%84%E7%90%86/","summary":"极客时间《消息队列高手课》笔记\n  概览消息队列篇〇：为什么需要消息队列 概览消息队列篇一：不同的mq的消息模型 概览消息队列篇二：如何确保消息不丢失 概览消息队列篇三：重复消息的处理  消息队列本身可以保证消息不重复吗 在 MQTT 协议中，给出了三种传递消息时能够提供的服务质量标准，这三种服务质量从低到高依次是：\n At most once: 至多一次。消息在传递时，最多会被送达一次。换一个说法就是，没什么消息可靠性保证，允许丢消息。一般都是一些对消息可靠性要求不太高的监控场景使用，比如每分钟上报一次机房温度数据，可以接受数据少量丢失。 At least once: 至少一次。消息在传递时，至少会被送达一次。也就是说，不允许丢消息，但是允许有少量重复消息出现。 Exactly once：恰好一次。消息在传递时，只会被送达一次，不允许丢失也不允许重复，这个是最高的等级。  这个服务质量标准不仅适用于 MQTT，对所有的消息队列都是适用的。我们现在常用的绝大部分消息队列提供的服务质量都是 At least once，包括 RocketMQ、RabbitMQ 和 Kafka 都是这样。\n也就是说，消息队列很难保证消息不重复\n用幂等性解决重复消息问题 一般解决重复消息的办法是，在消费端，让我们消费消息的操作具备幂等性。\n一个幂等操作的特点是，其任意多次执行所产生的影响均与一次执行的影响相同。\n一个幂等的方法，使用同样的参数，对它进行多次调用和一次调用，对系统产生的影响是一样的。所以，对于幂等的方法，不用担心重复执行会对系统造成任何改变。\n如果我们系统消费消息的业务逻辑具备幂等性，那就不用担心消息重复的问题了，因为同一条消息，消费一次和消费多次对系统的影响是完全一样的。也就可以认为，消费多次等于消费一次。从对系统的影响结果来说：At least once + 幂等消费 = Exactly once。\n常用设计幂等操作的方法\n 利用数据库的唯一约束实现幂等  对于一个转账操作，我们可以设置对于每一个转账单只能进行一次转账操作，这样除第一次操作外其他重复操作都会失败。\n基于这个思路，不光是可以使用关系型数据库，只要是支持类似“INSERT IF NOT EXIST”语义的存储类系统都可以用于实现幂等，比如，还可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。\n为更新的数据设置前置条件  如果满足条件就更新数据，否则拒绝更新数据，在更新数据的时候，同时变更前置条件中需要判断的数据。这样，重复执行这个操作时，由于第一次更新数据的时候已经变更了前置条件中需要判断的数据，不满足前置条件，则不会重复执行更新数据操作。\n记录并检查操作  记录并检查操作，也称为“Token 机制或者 GUID（全局唯一 ID）机制”，实现的思路特别简单：在执行数据更新操作之前，先检查一下是否执行过这个更新操作。\n具体的实现方法是，在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。","title":"概览消息队列篇三：重复消息的处理"},{"content":" 极客时间《消息队列高手课》笔记\n  概览消息队列篇〇：为什么需要消息队列 概览消息队列篇一：不同的mq的消息模型 概览消息队列篇二：如何确保消息不丢失 概览消息队列篇三：重复消息的处理  检测消息丢失的方法 可以使用类似分布式链路追踪系统来追踪每一条信息。\n还可以利用消息队列的有序性来验证是否有消息丢失。\n在 Producer 端，我们给每个发出的消息附加一个连续递增的序号，然后在 Consumer 端来检查这个序号的连续性\n如果没有消息丢失，Consumer 收到消息的序号必然是连续递增的，或者说收到的消息，其中的序号必然是上一条消息的序号 +1。如果检测到序号不连续，那就是丢消息了。\n大多数消息队列的客户端都支持拦截器机制，你可以利用这个拦截器机制，在 Producer 发送消息之前的拦截器中将序号注入到消息中，在 Consumer 收到消息的拦截器中检测序号的连续性，这样实现的好处是消息检测的代码不会侵入到你的业务代码中，待你的系统稳定后，也方便将这部分检测的逻辑关闭或者删除。\n确保消息可靠传递 一条消息从生产到消费完成这个过程，可以划分三个阶段，\n 生产阶段: 在这个阶段，从消息在 Producer 创建出来，经过网络传输发送到 Broker(mq服务端)端。 存储阶段: 在这个阶段，消息在 Broker 端存储，如果是集群，消息会在这个阶段被复制到其他的副本上。 消费阶段: 在这个阶段，Consumer 从 Broker 上拉取消息，经过网络传输发送到 Consumer 上。  下面就如何确保消息可靠传递分别对三个阶段进行分析\n 生产阶段  在生产阶段，消息队列通过最常用的请求确认机制，来保证消息的可靠传递：当你的代码调用发消息方法时，消息队列的客户端会把消息发送到 Broker，Broker 收到消息后，会给客户端返回一个确认响应，表明消息已经收到了。客户端收到响应后，完成了一次正常消息的发送。\n只要 Producer 收到了 Broker 的确认响应，就可以保证消息在生产阶段不会丢失。有些消息队列在长时间没收到发送确认响应后，会自动重试，如果重试再失败，就会以返回值或者异常的方式告知用户。\n存储阶段  在存储阶段正常情况下，只要 Broker 在正常运行，就不会出现丢失消息的问题，但是如果 Broker 出现了故障，比如进程死掉了或者服务器宕机了，还是可能会丢失消息的。\n如果对消息的可靠性要求非常高，可以通过配置 Broker 参数来避免因为宕机丢消息。\n对于单个节点的 Broker，需要配置 Broker 参数，在收到消息后，将消息写入磁盘后再给 Producer 返回确认响应，这样即使发生宕机，由于消息已经被写入磁盘，就不会丢失消息，恢复后还可以继续消费。例如，在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘。\n如果是 Broker 是由多个节点组成的集群，需要将 Broker 集群配置成：至少将消息发送到 2 个以上的节点，再给客户端回复发送确认响应。这样当某个 Broker 宕机时，其他的 Broker 可以替代宕机的 Broker，也不会发生消息丢失。\n消费者阶段  消费阶段采用和生产阶段类似的确认机制来保证消息的可靠传递，客户端从 Broker 拉取消息后，执行用户的消费业务逻辑，成功后，才会给 Broker 发送消费确认响应。如果 Broker 没有收到消费确认响应，下次拉消息的时候还会返回同一条消息，确保消息不会在网络传输过程中丢失，也不会因为客户端在执行消费逻辑中出错导致丢失。\n在编写消费代码时需要注意的是，不要在收到消息后就立即发送消费确认，而是应该在执行完所有消费业务逻辑之后，再发送消费确认。\n","permalink":"http://yangchnet.github.io/Dessert/posts/mq/%E6%A6%82%E8%A7%88%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%AF%87%E4%BA%8C%E5%A6%82%E4%BD%95%E7%A1%AE%E4%BF%9D%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1/","summary":"极客时间《消息队列高手课》笔记\n  概览消息队列篇〇：为什么需要消息队列 概览消息队列篇一：不同的mq的消息模型 概览消息队列篇二：如何确保消息不丢失 概览消息队列篇三：重复消息的处理  检测消息丢失的方法 可以使用类似分布式链路追踪系统来追踪每一条信息。\n还可以利用消息队列的有序性来验证是否有消息丢失。\n在 Producer 端，我们给每个发出的消息附加一个连续递增的序号，然后在 Consumer 端来检查这个序号的连续性\n如果没有消息丢失，Consumer 收到消息的序号必然是连续递增的，或者说收到的消息，其中的序号必然是上一条消息的序号 +1。如果检测到序号不连续，那就是丢消息了。\n大多数消息队列的客户端都支持拦截器机制，你可以利用这个拦截器机制，在 Producer 发送消息之前的拦截器中将序号注入到消息中，在 Consumer 收到消息的拦截器中检测序号的连续性，这样实现的好处是消息检测的代码不会侵入到你的业务代码中，待你的系统稳定后，也方便将这部分检测的逻辑关闭或者删除。\n确保消息可靠传递 一条消息从生产到消费完成这个过程，可以划分三个阶段，\n 生产阶段: 在这个阶段，从消息在 Producer 创建出来，经过网络传输发送到 Broker(mq服务端)端。 存储阶段: 在这个阶段，消息在 Broker 端存储，如果是集群，消息会在这个阶段被复制到其他的副本上。 消费阶段: 在这个阶段，Consumer 从 Broker 上拉取消息，经过网络传输发送到 Consumer 上。  下面就如何确保消息可靠传递分别对三个阶段进行分析\n 生产阶段  在生产阶段，消息队列通过最常用的请求确认机制，来保证消息的可靠传递：当你的代码调用发消息方法时，消息队列的客户端会把消息发送到 Broker，Broker 收到消息后，会给客户端返回一个确认响应，表明消息已经收到了。客户端收到响应后，完成了一次正常消息的发送。\n只要 Producer 收到了 Broker 的确认响应，就可以保证消息在生产阶段不会丢失。有些消息队列在长时间没收到发送确认响应后，会自动重试，如果重试再失败，就会以返回值或者异常的方式告知用户。\n存储阶段  在存储阶段正常情况下，只要 Broker 在正常运行，就不会出现丢失消息的问题，但是如果 Broker 出现了故障，比如进程死掉了或者服务器宕机了，还是可能会丢失消息的。\n如果对消息的可靠性要求非常高，可以通过配置 Broker 参数来避免因为宕机丢消息。\n对于单个节点的 Broker，需要配置 Broker 参数，在收到消息后，将消息写入磁盘后再给 Producer 返回确认响应，这样即使发生宕机，由于消息已经被写入磁盘，就不会丢失消息，恢复后还可以继续消费。例如，在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘。","title":"概览消息队列篇二：如何确保消息不丢失"},{"content":" 极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  什么是RDB 所谓RDB，即Redis DataBase。是对Redis做的一个内存快照，即将内存中的数据在某一个时刻的状态记录。\n对 Redis 来说，它实现类似照片记录效果的方式，就是把某一时刻的状态以文件的形式写到磁盘上，也就是快照。这样一来，即使宕机，快照文件也不会丢失，数据的可靠性也就得到了保证。这个快照文件就称为 RDB 文件。\n和 AOF 相比，RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我们可以直接把 RDB 文件读入内存，很快地完成恢复。\n給哪些内存数据做快照 給哪些内存数据做快照，这关系到快照的执行效率。\nRedis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中。这样做的好处是，一次性记录了所有数据，一个都不少。\n但给内存的全量数据做快照，把它们全部写入磁盘也会花费很多时间。而且，全量数据越多，RDB 文件就越大，往磁盘上写数据的时间开销就越大。\n这里我们要做一个“灵魂之问”：RDB文件的生成会阻塞主线程吗？\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。\n save：在主线程中执行，会导致阻塞； bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。  因此我们可以使用bgsave命令来执行全量快照，这既提供了数据的可靠性保证，也避免了对 Redis 的性能影响。\n快照时数据能修改吗 这个问题非常重要，这是因为，如果数据能被修改，那就意味着 Redis 还能正常处理写操作。否则，所有写操作都得等到快照完了才能执行，性能一下子就降低了。\n如果我们使用save命令，显然是不能修改的，以为主线程已经被阻塞掉了。\n如果使用bgsave命令，看起来数据还是可以修改的，因为主线程没有被阻塞。但避免阻塞和正常处理写操作并不是一回事。此时，主线程的确没有阻塞，可以正常接收请求，但是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。\n为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。\n简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。\n此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本（键值对 C’）。然后，主线程在这个数据副本上进行修改。同时，bgsave 子进程可以继续把原来的数据（键值对 C）写入 RDB 文件。\n这既保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。\n可以连拍吗 对于快照来说，所谓“连拍”就是指连续地做快照。这样一来，快照的间隔时间变得很短，即使某一时刻发生宕机了，因为上一时刻快照刚执行，丢失的数据也不会太多。\n但是，如果频繁地执行全量快照，也会带来两方面的开销。\n  频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。\n  bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了\n  那么，如何防止在上一次快照之后修改的数据丢失呢？\n可以使用增量快照，所谓增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。\n但这样又带来另外一个问题：做增量快照需要记住哪些数据被修改了，这又是一笔不小的内存开销。\n对比之前的AOF日志可以发现，快照的恢复速度较快，但频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销。\nRedis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。\n这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF 日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。\n如下图所示，T1 和 T2 时刻的修改，用 AOF 日志记录，等到第二次做全量快照时，就可以清空 AOF 日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。\n关于AOF和RDB的选择  数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择； 如果允许分钟级别的数据丢失，可以只使用 RDB； 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。  ","permalink":"http://yangchnet.github.io/Dessert/posts/cache/%E6%A6%82%E8%A7%88redis%E7%AF%87%E4%B8%89rdb%E5%BF%AB%E7%85%A7/","summary":"极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  什么是RDB 所谓RDB，即Redis DataBase。是对Redis做的一个内存快照，即将内存中的数据在某一个时刻的状态记录。\n对 Redis 来说，它实现类似照片记录效果的方式，就是把某一时刻的状态以文件的形式写到磁盘上，也就是快照。这样一来，即使宕机，快照文件也不会丢失，数据的可靠性也就得到了保证。这个快照文件就称为 RDB 文件。\n和 AOF 相比，RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我们可以直接把 RDB 文件读入内存，很快地完成恢复。\n給哪些内存数据做快照 給哪些内存数据做快照，这关系到快照的执行效率。\nRedis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中。这样做的好处是，一次性记录了所有数据，一个都不少。\n但给内存的全量数据做快照，把它们全部写入磁盘也会花费很多时间。而且，全量数据越多，RDB 文件就越大，往磁盘上写数据的时间开销就越大。\n这里我们要做一个“灵魂之问”：RDB文件的生成会阻塞主线程吗？\nRedis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。\n save：在主线程中执行，会导致阻塞； bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。  因此我们可以使用bgsave命令来执行全量快照，这既提供了数据的可靠性保证，也避免了对 Redis 的性能影响。\n快照时数据能修改吗 这个问题非常重要，这是因为，如果数据能被修改，那就意味着 Redis 还能正常处理写操作。否则，所有写操作都得等到快照完了才能执行，性能一下子就降低了。\n如果我们使用save命令，显然是不能修改的，以为主线程已经被阻塞掉了。\n如果使用bgsave命令，看起来数据还是可以修改的，因为主线程没有被阻塞。但避免阻塞和正常处理写操作并不是一回事。此时，主线程的确没有阻塞，可以正常接收请求，但是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。\n为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。\n简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。\n此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本（键值对 C’）。然后，主线程在这个数据副本上进行修改。同时，bgsave 子进程可以继续把原来的数据（键值对 C）写入 RDB 文件。","title":"概览Redis篇三：RDB快照"},{"content":" 极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  什么是哨兵，哨兵有什么用 哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。\n有了哨兵，我们就可以在主库故障的情况下快速实现主从库自动切换。\n哨兵机制的基本流程  监控  哨兵在运行时，会周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程。\n选主  主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。\n通知  在选出新的主库后，哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。\n哨兵如何判断主库是否下线  主观下线  哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态。如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”\n如果检测的是从库，那么，哨兵简单地把它标记为“主观下线”就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断。\n但是，如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可能存在这么一个情况：那就是哨兵误判了，其实主库并没有故障。可是，一旦启动了主从切换，后续的选主和通知操作都会带来额外的计算和通信开销。\n因此我们要特别注意误判的情况。\n为了减少误判，可以采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。\n客观下线  当大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”，这个叫法也是表明主库下线成为一个客观事实了。\n如何选择新主库 简单来说，哨兵选择新主库的过程可以描述为“筛选+打分”。筛选是说，我们在多个从库中，先按照一定的筛选条件，把不符合条件的从库去掉。而打分意思是按照一定的规则，给剩下的从库逐个打分，将得分最高的从库选为新主库，\n那么筛选和打分的标准是怎样的？\n在筛选时，除了要检查从库的当前在线状态，还要判断它之前的网络连接状态。使用配置项 down-after-milliseconds * 10。其中，down-after-milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。\n筛掉了不适合做主库的从库后，需要对剩下的从库进行打分。分别按照一下三个规则进行三轮：\n  优先级最高的从库得分高。 用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级。\n  和旧主库同步程度最接近的从库得分高 如果在所有从库中，有从库的 slave_repl_offset 最接近 master_repl_offset，那么它的得分就最高，可以作为新主库。\n  ID 号小的从库得分高 Redis 在选主库时，有一个默认的规定：在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库。\n  在三轮打分过程中，如果有一个从库在某轮得分最高，那么其直接成为主库，不用再进行打分。\n基于 pub/sub 机制的哨兵集群组成 哨兵首先需要知道主库的ip和端口，可以使用如下命令配置：\nsentinel monitor \u0026lt;master-name\u0026gt; \u0026lt;ip\u0026gt; \u0026lt;redis-port\u0026gt; \u0026lt;quorum\u0026gt; 哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布/订阅机制。\n哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端口。\n在主从集群中，主库上有一个名为__sentinel__:hello的频道，不同哨兵就是通过它来相互发现，实现互相通信的。\n哨兵除了彼此之间建立起连接形成集群外，还需要和从库建立连接。这是因为，在哨兵的监控任务中，它需要对主从库都进行心跳判断，而且在主从库切换完成后，它还需要通知从库，让它们和新主库进行同步。\n哨兵如何知道从库的IP地址和端口？\n这是由哨兵向主库发送 INFO 命令来完成的。哨兵给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。\n基于 pub/sub 机制的客户端事件通知 在进行主从切换时以及完成主从切换后，哨兵需要监控整个集群的状态，还需要通知客户端新的主库信息。这也是通过pub/sub机制完成的。每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。\n客户端在订阅了相应的频道后，就可以获取不同的事件消息，从而进行必要响应。\n哪个哨兵执行主从切换？ 确定由哪个哨兵执行主从切换的过程，和主库“客观下线”的判断过程类似，也是一个“投票仲裁”的过程。\n任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down-by-addr 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。\n一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的。例如，现在有 5 个哨兵，quorum 配置的是 3，那么，一个哨兵需要 3 张赞成票，就可以标记主库为“客观下线”了。这 3 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。\n此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。\n在投票过程中，任何一个想成为 Leader 的哨兵，要满足两个条件：第一，拿到半数以上的赞成票；第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值。\n需要注意的是，如果哨兵集群只有 2 个实例，此时，一个哨兵要想成为 Leader，必须获得 2 票，而不是 1 票。所以，如果有个哨兵挂掉了，那么，此时的集群是无法进行主从库切换的。因此，通常我们至少会配置 3 个哨兵实例。\n","permalink":"http://yangchnet.github.io/Dessert/posts/cache/%E6%A6%82%E8%A7%88redis%E7%AF%87%E4%BA%94%E5%93%A8%E5%85%B5/","summary":"极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  什么是哨兵，哨兵有什么用 哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。\n有了哨兵，我们就可以在主库故障的情况下快速实现主从库自动切换。\n哨兵机制的基本流程  监控  哨兵在运行时，会周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程。\n选主  主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。\n通知  在选出新的主库后，哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。\n哨兵如何判断主库是否下线  主观下线  哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态。如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”\n如果检测的是从库，那么，哨兵简单地把它标记为“主观下线”就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断。\n但是，如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可能存在这么一个情况：那就是哨兵误判了，其实主库并没有故障。可是，一旦启动了主从切换，后续的选主和通知操作都会带来额外的计算和通信开销。\n因此我们要特别注意误判的情况。\n为了减少误判，可以采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。\n客观下线  当大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”，这个叫法也是表明主库下线成为一个客观事实了。\n如何选择新主库 简单来说，哨兵选择新主库的过程可以描述为“筛选+打分”。筛选是说，我们在多个从库中，先按照一定的筛选条件，把不符合条件的从库去掉。而打分意思是按照一定的规则，给剩下的从库逐个打分，将得分最高的从库选为新主库，\n那么筛选和打分的标准是怎样的？\n在筛选时，除了要检查从库的当前在线状态，还要判断它之前的网络连接状态。使用配置项 down-after-milliseconds * 10。其中，down-after-milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。\n筛掉了不适合做主库的从库后，需要对剩下的从库进行打分。分别按照一下三个规则进行三轮：\n  优先级最高的从库得分高。 用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级。","title":"概览Redis篇五：哨兵"},{"content":" 极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  什么是切片集群，有什么用 切片集群，也叫分片集群，就是指启动多个 Redis 实例组成一个集群，然后按照一定的规则，把收到的数据划分成多份，每一份用一个实例来保存。\n当Redis保存大量数据时，其在进行RDB持久化时需要fork一个子进程，而fork子进程的用时和Redis的数据量是正相关的，而 fork 在执行时会阻塞主线程。数据量越大，fork 操作造成的主线程阻塞的时间越长。因此如果Redis保存了大量数据，会导致Redis响应变慢。\n数据切片后，在多个实例之间如何分布 从 3.0 开始，官方提供了一个名为 Redis Cluster 的方案，用于实现切片集群。Redis Cluster 方案中就规定了数据和实例的对应规则。\nRedis Cluster 方案采用哈希槽Hash Slot，来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。\n具体的映射过程分为两大步：首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值；然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。\n在部署 Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N 个。当然，也可以使用cluster meet命令手动建立实例间的连接，形成集群，再使用cluster addslots 命令，指定每个实例上的哈希槽个数。在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。\n客户端如何定位数据 Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。\n客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端。客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。\n但实例与Slot的对应关系并不是一成不变的：\n 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽； 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。  客户端无法主动感知这些变化，但Redis Cluster 方案提供了一种重定向机制，所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。\n当客户端把一个键值对的操作请求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。\nGET hello:key (error) MOVED 13320 172.16.19.5:6379  Redis Cluster不采用把key直接映射到实例的方式，而采用哈希槽的方式原因：\n1、整个集群存储key的数量是无法预估的，key的数量非常多时，直接记录每个key对应的实例映射关系，这个映射表会非常庞大，这个映射表无论是存储在服务端还是客户端都占用了非常大的内存空间。\n2、Redis Cluster采用无中心化的模式（无proxy，客户端与服务端直连），客户端在某个节点访问一个key，如果这个key不在这个节点上，这个节点需要有纠正客户端路由到正确节点的能力（MOVED响应），这就需要节点之间互相交换路由表，每个节点拥有整个集群完整的路由关系。如果存储的都是key与实例的对应关系，节点之间交换信息也会变得非常庞大，消耗过多的网络资源，而且就算交换完成，相当于每个节点都需要额外存储其他节点的路由表，内存占用过大造成资源浪费。\n3、当集群在扩容、缩容、数据均衡时，节点之间会发生数据迁移，迁移时需要修改每个key的映射关系，维护成本高。\n4、而在中间增加一层哈希槽，可以把数据和节点解耦，key通过Hash计算，只需要关心映射到了哪个哈希槽，然后再通过哈希槽和节点的映射表找到节点，相当于消耗了很少的CPU资源，不但让数据分布更均匀，还可以让这个映射表变得很小，利于客户端和服务端保存，节点之间交换信息时也变得轻量。\n5、当集群在扩容、缩容、数据均衡时，节点之间的操作例如数据迁移，都以哈希槽为基本单位进行操作，简化了节点扩容、缩容的难度，便于集群的维护和管理。\n","permalink":"http://yangchnet.github.io/Dessert/posts/cache/%E6%A6%82%E8%A7%88redis%E7%AF%87%E5%85%AD%E5%88%87%E7%89%87%E9%9B%86%E7%BE%A4/","summary":"极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  什么是切片集群，有什么用 切片集群，也叫分片集群，就是指启动多个 Redis 实例组成一个集群，然后按照一定的规则，把收到的数据划分成多份，每一份用一个实例来保存。\n当Redis保存大量数据时，其在进行RDB持久化时需要fork一个子进程，而fork子进程的用时和Redis的数据量是正相关的，而 fork 在执行时会阻塞主线程。数据量越大，fork 操作造成的主线程阻塞的时间越长。因此如果Redis保存了大量数据，会导致Redis响应变慢。\n数据切片后，在多个实例之间如何分布 从 3.0 开始，官方提供了一个名为 Redis Cluster 的方案，用于实现切片集群。Redis Cluster 方案中就规定了数据和实例的对应规则。\nRedis Cluster 方案采用哈希槽Hash Slot，来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。\n具体的映射过程分为两大步：首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值；然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。\n在部署 Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例上的槽个数为 16384/N 个。当然，也可以使用cluster meet命令手动建立实例间的连接，形成集群，再使用cluster addslots 命令，指定每个实例上的哈希槽个数。在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。","title":"概览Redis篇六：切片集群"},{"content":" 极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  Redis的高可靠性 Redis 具有高可靠性，又是什么意思呢？其实，这里有两层含义：一是数据尽量少丢失，二是服务尽量少中断。\nAOF 和 RDB 保证了前者，而对于后者，Redis 的做法就是增加副本冗余量，将一份数据同时保存在多个实例上。即使有一个实例出现了故障，需要过一段时间才能恢复，其他实例也可以对外提供服务，不会影响业务使用。\nRedis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。\n 读操作：主库、从库都可以接收； 写操作：首先到主库执行，然后，主库将写操作同步给从库。  为什么要读写分离\n如果在上图中，不管是主库还是从库，都能接收客户端的写操作，那么，一个直接的问题就是：如果客户端对同一个数据（例如 k1）前后修改了三次，每一次的修改请求都发送到不同的实例上，在不同的实例上执行，那么，这个数据在这三个实例上的副本就不一致了（分别是 v1、v2 和 v3）。在读取这个数据的时候，就可能读取到旧的值。\n当然我们可以使这个数据在三个实例上保持一致，但这涉及到加锁，实例间协商等一系列操作，会带来巨额的开销。\n而主从库模式一旦采用了读写分离，所有数据的修改只会在主库上进行，不用协调三个实例。主库有了最新的数据后，会同步给从库，这样，主从库的数据就是一致的。\n主从库如何进行第一次同步 当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。\n例如，现在有实例 1（ip：172.16.19.3）和实例 2（ip：172.16.19.5），我们在实例 2 上执行以下这个命令后，实例 2 就变成了实例 1 的从库，并从实例 1 上复制数据：\nreplicaof 172.16.19.3 6379 主从库间数据第一次同步有三个阶段：   主从库间建立连接、协商同步。主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。\n 具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID 和复制进度 offset 两个参数.\n runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。\noffset，此时设为 -1，表示第一次复制。\n   主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。 这里有个地方需要注意，FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库。\n   主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。这个过程依赖于内存快照生成的 RDB 文件。\n 具体来说，主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。 在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则，Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件中。为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作。\n   主库把第二阶段执行过程中新收到的写命令，再发送给从库\n 当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。\n   经过这样三步，主从库就实现了同步。\n主从级联 一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成 RDB 文件和传输 RDB 文件。\n如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步。fork 这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。此外，传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力。\n我们可以通过主 - 从 - 从模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上。\n简单来说，我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系。也就是说，大哥的小弟也收了小小弟了。\nreplicaof 所选从库的IP 6379 一旦主从库完成了全量复制，它们之间就会一直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这个过程也称为基于长连接的命令传播，可以避免频繁建立连接的开销。\n网络中断的风险 主从之间要保持长连接，以便数据交流，并节省频繁建立连接的开销。\n但如果网络断连，主从库之间就无法进行命令传播了，从库的数据自然也就没办法和主库保持一致了，客户端就可能从从库读到旧数据。\n在 Redis 2.8 之前，如果主从库在命令传播时出现了网络闪断，那么，从库就会和主库重新进行一次全量复制，开销非常大。\n从 Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步。\n增量复制时，为了保持主从之间的同步，需要使用repl_backlog_buffer这个缓冲区。\n当主从库断连后，主库会把断连期间收到的写操作命令，写入 repl_backlog_buffer 这个缓冲区。\nrepl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。\n刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是 master_repl_offset。主库接收的新写操作越多，这个值就会越大。\n同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 slave_repl_offset 也在不断增加。正常情况下，这两个偏移量基本相等。\n主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距。\n在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset 会大于 slave_repl_offset。此时，主库只用把 master_repl_offset 和 slave_repl_offset 之间的命令操作同步给从库就行。\n关于repl_backlog_buffer 这里存在一个问题：repl_backlog_buffer是一个环形缓冲区，如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从不一致。\n因此，我们要想办法避免这一情况，一般而言，我们可以调整 repl_backlog_size 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size的最终值。\n这样一来，增量复制时主从库的数据不一致风险就降低了。不过，如果并发请求量非常大，连两倍的缓冲空间都存不下新操作请求的话，此时，主从库数据仍然可能不一致。\n针对这种情况，一方面，你可以根据 Redis 所在服务器的内存资源再适当增加 repl_backlog_size 值，比如说设置成缓冲空间大小的 4 倍，另一方面，你可以考虑使用切片集群来分担单个主库的请求压力。\n另外需要注意的是：一个从库如果和主库断连时间过长，造成它在主库repl_backlog_buffer的slave_repl_offset位置上的数据已经被覆盖掉了，此时从库和主库间将进行全量复制。\n为什么主从同步不适用AOF 1、RDB文件内容是经过压缩的二进制数据（不同数据类型数据做了针对性优化），文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量同步的成本最低。\n2、假设要使用AOF做全量同步，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。\n replication buffer Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互，客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的：Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互。所以主从在增量同步时，从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer\n再延伸一下，既然有这个内存buffer存在，那么这个buffer有没有限制呢？如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个buffer就会持续增长，消耗大量的内存资源，甚至OOM。所以Redis提供了client-output-buffer-limit参数限制这个buffer的大小，如果超过限制，主库会强制断开这个client的连接，也就是说从库处理慢导致主库内存buffer的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。\n","permalink":"http://yangchnet.github.io/Dessert/posts/cache/%E6%A6%82%E8%A7%88redis%E7%AF%87%E5%9B%9B%E4%B8%BB%E4%BB%8E/","summary":"极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  Redis的高可靠性 Redis 具有高可靠性，又是什么意思呢？其实，这里有两层含义：一是数据尽量少丢失，二是服务尽量少中断。\nAOF 和 RDB 保证了前者，而对于后者，Redis 的做法就是增加副本冗余量，将一份数据同时保存在多个实例上。即使有一个实例出现了故障，需要过一段时间才能恢复，其他实例也可以对外提供服务，不会影响业务使用。\nRedis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。\n 读操作：主库、从库都可以接收； 写操作：首先到主库执行，然后，主库将写操作同步给从库。  为什么要读写分离\n如果在上图中，不管是主库还是从库，都能接收客户端的写操作，那么，一个直接的问题就是：如果客户端对同一个数据（例如 k1）前后修改了三次，每一次的修改请求都发送到不同的实例上，在不同的实例上执行，那么，这个数据在这三个实例上的副本就不一致了（分别是 v1、v2 和 v3）。在读取这个数据的时候，就可能读取到旧的值。\n当然我们可以使这个数据在三个实例上保持一致，但这涉及到加锁，实例间协商等一系列操作，会带来巨额的开销。\n而主从库模式一旦采用了读写分离，所有数据的修改只会在主库上进行，不用协调三个实例。主库有了最新的数据后，会同步给从库，这样，主从库的数据就是一致的。\n主从库如何进行第一次同步 当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 replicaof（Redis 5.0 之前使用 slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。\n例如，现在有实例 1（ip：172.16.19.3）和实例 2（ip：172.16.19.5），我们在实例 2 上执行以下这个命令后，实例 2 就变成了实例 1 的从库，并从实例 1 上复制数据：\nreplicaof 172.16.19.3 6379 主从库间数据第一次同步有三个阶段：   主从库间建立连接、协商同步。主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。\n 具体来说，从库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 runID 和复制进度 offset 两个参数.\n runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为“？”。","title":"概览Redis篇四：主从"},{"content":"8086结构 x86架构中经典的处理器8086的大体结构如下：\n其寻址范围为1M\n为了暂存数据，8086 处理器内部有 8 个 16 位的通用寄存器，也就是刚才说的 CPU 内部的数据单元，分别是 AX、BX、CX、DX、SP、BP、SI、DI。这些寄存器主要用于在计算过程中暂存数据。这些寄存器比较灵活，其中 AX、BX、CX、DX 可以分成两个 8 位的寄存器来使用，分别是 AH、AL、BH、BL、CH、CL、DH、DL，其中 H 就是 High（高位），L 就是 Low（低位）的意思。\n控制单元：\nIP 寄存器就是指令指针寄存器（Instruction Pointer Register），指向代码段中下一条指令的位置。CPU 会根据它来不断地将指令从内存的代码段中，加载到 CPU 的指令队列中，然后交给运算单元去执行。\n如果需要切换进程，每个进程都分代码段和数据段，为了指向不同进程的地址空间，有四个 16 位的段寄存器，分别是 CS、DS、SS、ES。\n其中，CS 就是代码段寄存器（Code Segment Register），通过它可以找到代码在内存中的位置；DS 是数据段的寄存器，通过它可以找到数据在内存中的位置。SS 是栈寄存器（Stack Register）。栈是程序运行中一个特殊的数据结构，数据的存取只能从一端进行，秉承后进先出的原则，push 就是入栈，pop 就是出栈。\n存储起始地址的CS和DS都是16位的；存储偏移量的IP寄存器和通用寄存器都是16位的；但8086地址总线是20位的。如何从16位的寄存器寻址到20位的地址？\n方法是：起始地址×16+偏移量，也就是把 CS 和 DS 中的值左移 4 位，变成 20 位的，加上 16 位的偏移量，这样就可以得到最终 20 位（1M）的数据地址。\n32位处理器 32位处理器必须保持和原来处理器的兼容。\n首先，通用寄存器有扩展，可以将 8 个 16 位的扩展到 8 个 32 位的，但是依然可以保留 16 位的和 8 位的使用方式。其中，指向下一条指令的指令指针寄存器 IP，就会扩展成 32 位的，同样也兼容 16 位的。\n而段寄存器仍然是16位，但其含义不再是段的起始地址。段的起始地址放在内存的某个地方。这个地方是一个表格，表格中的一项一项是段描述符（Segment Descriptor）。这里面才是真正的段的起始地址。而段寄存器里面保存的是在这个表格中的哪一项，称为选择子（Selector）。\n这样，将一个从段寄存器直接拿到的段起始地址，就变成了先间接地从段寄存器找到表格中的一项，再从表格中的一项中拿到段起始地址。这样段起始地址就会很灵活了。当然为了快速拿到段起始地址，段寄存器会从内存中拿到 CPU 的描述符高速缓存器中。\n实模式和保护模式 段寄存器的含义改变后，就与原来的的模式不兼容了。处理这个问题的办法是为处理器定义两种模式，当系统刚刚启动时，CPU处于实模式，这时候和原来的模式是兼容的。当需要更多内存时，切换到保护模式，就能用32位CPU更强大的能力。\n实模式直接获取到段的起始地址，保护模式先读取寄存器选择子，再从选择子中获取实际的段的起始地址\n 切换到保护模式需要做以下三件事:  启用分段, 辅助进程管理 启动分页, 辅助内存管理 打开其他地址线    ","permalink":"http://yangchnet.github.io/Dessert/posts/os/%E5%AE%9E%E6%A8%A1%E5%BC%8F%E5%92%8C%E4%BF%9D%E6%8A%A4%E6%A8%A1%E5%BC%8F/","summary":"8086结构 x86架构中经典的处理器8086的大体结构如下：\n其寻址范围为1M\n为了暂存数据，8086 处理器内部有 8 个 16 位的通用寄存器，也就是刚才说的 CPU 内部的数据单元，分别是 AX、BX、CX、DX、SP、BP、SI、DI。这些寄存器主要用于在计算过程中暂存数据。这些寄存器比较灵活，其中 AX、BX、CX、DX 可以分成两个 8 位的寄存器来使用，分别是 AH、AL、BH、BL、CH、CL、DH、DL，其中 H 就是 High（高位），L 就是 Low（低位）的意思。\n控制单元：\nIP 寄存器就是指令指针寄存器（Instruction Pointer Register），指向代码段中下一条指令的位置。CPU 会根据它来不断地将指令从内存的代码段中，加载到 CPU 的指令队列中，然后交给运算单元去执行。\n如果需要切换进程，每个进程都分代码段和数据段，为了指向不同进程的地址空间，有四个 16 位的段寄存器，分别是 CS、DS、SS、ES。\n其中，CS 就是代码段寄存器（Code Segment Register），通过它可以找到代码在内存中的位置；DS 是数据段的寄存器，通过它可以找到数据在内存中的位置。SS 是栈寄存器（Stack Register）。栈是程序运行中一个特殊的数据结构，数据的存取只能从一端进行，秉承后进先出的原则，push 就是入栈，pop 就是出栈。\n存储起始地址的CS和DS都是16位的；存储偏移量的IP寄存器和通用寄存器都是16位的；但8086地址总线是20位的。如何从16位的寄存器寻址到20位的地址？\n方法是：起始地址×16+偏移量，也就是把 CS 和 DS 中的值左移 4 位，变成 20 位的，加上 16 位的偏移量，这样就可以得到最终 20 位（1M）的数据地址。\n32位处理器 32位处理器必须保持和原来处理器的兼容。\n首先，通用寄存器有扩展，可以将 8 个 16 位的扩展到 8 个 32 位的，但是依然可以保留 16 位的和 8 位的使用方式。其中，指向下一条指令的指令指针寄存器 IP，就会扩展成 32 位的，同样也兼容 16 位的。","title":"实模式和保护模式"},{"content":"一个UDP客户可以创建一个套接口并发送一个数据报給一个服务器，然后立即用同一个套接口发送另一个数据报給另一个服务器。同样，一个UDP服务器可以用同一个UDP套接口从5个不同的客户一连串接收5个数据报。\nUDP可以是全双工的\n TCP连接断开时，主动要求断开的一方会存在TIME_WAIT状态，此状态需要持续60s时间（Linux），在此状态期间，连接未完全断开，依然占用一个socket连接。\n存在TIME_WAIT状态的理由：\n  实现终止TCP全双工连接的可靠性 如果最后一个ACK丢失，服务器将重发最终的FIN，因此客户必须维护状态信息以允许它重发最终的ACK。\n  允许老的重复分节在网络中消逝。 如果某个连接被关闭后，在以后的某个时刻又重新建立起相同的IP地址可端口之间的TCP连接。后一个连接称为前一个连接的化身（incarnation），因为它们的IP地址和端口号都相同，TCP必须防止来自某个连接的老重复分组在连接终止后再现，从而被误解成属于同一连接的化身。 也就是说，某一个Socket对失效后，在网络中还有针对这个Socket对的分组的情况下又建立了一个一样的Socket对，为了防止网络中针对上一个socket对的分组被误认为是当前连接的分组，必须存在一个时间间隔，使得网络中针对上一个socket对的分组失效。\n   TCP连接耗尽： 对于客户端来说，其端口耗尽后，就不能再建立连接。 对于服务端来说，其使用socket对server_ip.server_port:client_ip.client_port来标识一个socket连接，对于某个特定服务来说，其server_ip和server_port是不变的，每次有一个连接进来，服务端都会fork出一个自身的子进程来对连接进行服务。这样，可服务的连接数就由client_ip和client_port两个共同决定，client_ip最多有$2^32$个，client_port最多有$2^16$个，因此理论上最多可服务连接数为$2^48$个。但每一个socket连接都需要消耗一个文件描述符，因此最大连接数还收到文件描述符数目的限制。除此之外，socket连接还会占用内存等资源，这些资源也限制了最大连接数。\n Unix系统有保留端口的概念，它是小于1024的任何端口。这些端口只能分配給超级用户进程的套接口，所有众所周知的端口（0-1023）都为保留端口，因此分配这些端口的服务器启动时必须具有超级用户的特权。\n TCP套接口编程  socket函数   为了执行网络I/O，一个进程必须做的第一件事就是调用socket函数，指定期望的通信协议类型。\n #include \u0026lt;sys/socket.h\u0026gt; int socket(int family, int type, int protocol); 其中family指明协议族，type是某个常值，参数protocol一般设为0，除非用在原始套接口上。\n对于family，其是以下常值之一：\n   族 解释     AF_INET IPv4协议   AF_INET6 IPv6协议   AF_LOCAL Unix域协议   AF_ROUTE 路由套接口   AF_KEY 密钥套接口    对于type，其是以下常值之一：\n   类型 解释     SOCK_STREAM 字节流套接口   SOCK_DGRAM 数据报套接口   SOCKE_RAW 原始套接口    socket函数在成功时返回一个小的非负整数值，它与文件描述字类似，我们将其称为套接口描述字(socket descriptor)，简称套接字(socketfd)。注意，套接字并没有指定本地协议地址或远程协议地址。\nbind函数   函数bind給套接字分配一个本地协议地址，对于网际协议，协议地址是32位IPv4地址或128位IPv6与16位的TCP或UDP端口号的组合。\n #include\u0026lt;sys/socket.h\u0026gt; int bind(int sockefd, const struct sockeaddr *myaddr, sockelen_t addrlen); 第一个参数是套接口描述字，第二个参数是一个指向特定于协议的地址结构的指针，第三个参数是该地址结构的长度。\n对于TCP，调用函数bind可以指定一个端口号，指定一个IP地址，可以两者都指定，也可以一个也不指定。\nlisten函数  函数listen仅被TCP服务器调用，它做两件事情：\n 当函数socket创建一个套接口时，它被假设为一个主动接口，也就是说，它是一个将调用connect发起连接的客户套接口，函数listen将未连接的套接口转换成被动套接口，指示内核应接受指向此套接口的连接请求。调用函数listen导致套接口从CLOSED状态转换到LISTEN状态 函数的第二个参数规定了内核为此套接口排队的最大连接个数  #include\u0026lt;sys/socket.h\u0026gt; int listen(int sockefd, int backlog); // return: 0,success; -1,failed accept函数  accept由TCP服务器调用，从已完成连接队列头返回下一个已完成连接。若已完成连接队列为空，则进程睡眠。\n#include\u0026lt;sys/socket.h\u0026gt; int accept(int sockefd, struct sockaddr *cliaddr, socklen_t *addrlen); // return: 非负描述字，OK；-1，出错 参数cliaddr和addrlen用来返回连接对方进程（客户）的协议地址。\n如果函数accept执行成功，则返回值是由内核自动生成的一个全新描述字，代表与客户的TCP连接。\n当我们讨论函数accept时，常把它的第一个参数称为监听套接口(linstening socket)描述字(由函数socket生成的描述字，用作函数bind和函数listen的第一个参数)，把它的返回值称为已连接套接口(connected socket)描述字。\n将这两个套接口区分开是很重要的，一个给定的服务常常是只生成一个监听套接口且一直存在，直到该服务关闭。内核为每个被接受的客户连接创建了一个已连接套接口(也就是说内核已为它完成TCP的三路握手过程)。当服务器完成某客户的服务时，关闭已连接套接口。\nconnect函数  TCP客户用connect函数来建立一个与TCP服务器的连接。\n#include\u0026lt;sys/socket.h\u0026gt; int connect(int sockefd, const struct sockeaddr *servaddr, socklen_t addrlen); // return: 0,success; -1,failed sockefd是由socket函数返回的套接口描述字，第二，第三个函数分别是一个指向套接口地址结构的指针和该结构的大小。\n如果是TCP套接口的话，函数connect激发TCP的三路握手过程，且仅在连接建立成功或出错时才返回。\nclose函数  close用来关闭套接口，终止TCP连接\n#include\u0026lt;unistd.h\u0026gt; int close(int sockefd); // return: 0,ok; -1,failed ","permalink":"http://yangchnet.github.io/Dessert/posts/net/socket%E9%9A%8F%E7%AC%94/","summary":"一个UDP客户可以创建一个套接口并发送一个数据报給一个服务器，然后立即用同一个套接口发送另一个数据报給另一个服务器。同样，一个UDP服务器可以用同一个UDP套接口从5个不同的客户一连串接收5个数据报。\nUDP可以是全双工的\n TCP连接断开时，主动要求断开的一方会存在TIME_WAIT状态，此状态需要持续60s时间（Linux），在此状态期间，连接未完全断开，依然占用一个socket连接。\n存在TIME_WAIT状态的理由：\n  实现终止TCP全双工连接的可靠性 如果最后一个ACK丢失，服务器将重发最终的FIN，因此客户必须维护状态信息以允许它重发最终的ACK。\n  允许老的重复分节在网络中消逝。 如果某个连接被关闭后，在以后的某个时刻又重新建立起相同的IP地址可端口之间的TCP连接。后一个连接称为前一个连接的化身（incarnation），因为它们的IP地址和端口号都相同，TCP必须防止来自某个连接的老重复分组在连接终止后再现，从而被误解成属于同一连接的化身。 也就是说，某一个Socket对失效后，在网络中还有针对这个Socket对的分组的情况下又建立了一个一样的Socket对，为了防止网络中针对上一个socket对的分组被误认为是当前连接的分组，必须存在一个时间间隔，使得网络中针对上一个socket对的分组失效。\n   TCP连接耗尽： 对于客户端来说，其端口耗尽后，就不能再建立连接。 对于服务端来说，其使用socket对server_ip.server_port:client_ip.client_port来标识一个socket连接，对于某个特定服务来说，其server_ip和server_port是不变的，每次有一个连接进来，服务端都会fork出一个自身的子进程来对连接进行服务。这样，可服务的连接数就由client_ip和client_port两个共同决定，client_ip最多有$2^32$个，client_port最多有$2^16$个，因此理论上最多可服务连接数为$2^48$个。但每一个socket连接都需要消耗一个文件描述符，因此最大连接数还收到文件描述符数目的限制。除此之外，socket连接还会占用内存等资源，这些资源也限制了最大连接数。\n Unix系统有保留端口的概念，它是小于1024的任何端口。这些端口只能分配給超级用户进程的套接口，所有众所周知的端口（0-1023）都为保留端口，因此分配这些端口的服务器启动时必须具有超级用户的特权。\n TCP套接口编程  socket函数   为了执行网络I/O，一个进程必须做的第一件事就是调用socket函数，指定期望的通信协议类型。\n #include \u0026lt;sys/socket.h\u0026gt; int socket(int family, int type, int protocol); 其中family指明协议族，type是某个常值，参数protocol一般设为0，除非用在原始套接口上。\n对于family，其是以下常值之一：\n   族 解释     AF_INET IPv4协议   AF_INET6 IPv6协议   AF_LOCAL Unix域协议   AF_ROUTE 路由套接口   AF_KEY 密钥套接口    对于type，其是以下常值之一：","title":"SOCKET随笔"},{"content":" 极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  AOF: Append Only File\nAOF日志的WAL 数据库中的WAL是Write-Ahead Logging, 为先写日志后执行。而Redis中的WAL为Write Ahead Log,先执行后写日志。\n为了避免额外的开销，Redis在向AOF里面记录日志时，不回去对这些命令进行语法检查，因此，如果先记录日志再执行命令的话，日志中有可能记录了错误的命令，Redis在使用日志恢复数据时，就可能会出错。\nAOF日志中记录了什么内容 有如下命令：\nset testkey testvalue 则AOF日志内容为：\n*3 $3 set $7 testkey $9 testvalue 其中“*3”表示当前命令有3个部分，每部分都由“$+数字”开头，后面紧跟具体的命令、键或值。\nAOF的潜在风险 AOF使用先执行，再写日志的方式，这样可以避免记录错误的命令，同时不会阻塞当前的写操作。\n但这样也带来了一些潜在风险：\n 刚执行完一个命令，没来得及记日志就宕机了，这个命令和相应的数据就有丢失的风险。 AOF避免了对当前命令的阻塞，但AOF写日志是在主线程中执行的，可能会给下一个操作带来阻塞风险。  这就需要我们对AOF写日志的实际进行把控。\nAOF的三种写回策略   Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；\n  Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；\n  No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。\n  针对避免主线程阻塞和减少数据丢失问题，这三种写回策略都无法做到两全其美。原因如下：\n “同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能； 虽然“操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了； “每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。  总结一下：\n 想要获得高性能，就选择 No 策略； 如果想要得到高可靠性保证，就选择 Always 策略； 如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择 Everysec 策略。  AOF重写 当系统运行一段时间后，AOF日志会越来越大，这会带来一些问题：\n  文件系统本身对文件大小有限制，无法保存过大的文件；\n  如果文件太大，之后再往里面追加命令记录的话，效率也会变低；\n  如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。\n  因此，我们希望AOF可以“压缩一下”，这就需要用到AOF重写机制\n简单来说，AOF 重写机制就是在重写时，Redis 根据数据库的现状创建一个新的 AOF 文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入\n比如我们在系统运行过程中对一个count值进行了多次累加，最终值为n，原来的AOF日志了记录了其从0变为n的所有语句，经过AOF重写后，针对该count值，只需要记录一条set count n就可以了。\nAOF重写会阻塞吗 和 AOF 日志由主线程写回不同，重写过程是由后台子进程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。\n重写的过程为：一个拷贝，两处复制\n“一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。\n两处复制指，因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。\n总结来说，每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。\n问题2则   AOF 日志重写的时候，是由 bgrewriteaof 子进程来完成的，不用主线程参与，我们今天说的非阻塞也是指子进程的执行不阻塞主线程。但是，你觉得，这个重写过程有没有其他潜在的阻塞风险呢？如果有的话，会在哪里阻塞？\n  AOF 重写也有一个重写日志，为什么它不共享使用 AOF 本身的日志呢？\n  答： 问题1，Redis采用fork子进程重写AOF文件时，潜在的阻塞风险包括：fork子进程 和 AOF重写过程中父进程产生写入的场景，下面依次介绍。\na、fork子进程，fork这个瞬间一定是会阻塞主线程的（注意，fork时并不会一次性拷贝所有内存数据给子进程，老师文章写的是拷贝所有内存数据给子进程，我个人认为是有歧义的），fork采用操作系统提供的写实复制(Copy On Write)机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题，但fork子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork阻塞时间越久。拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。那什么时候父子进程才会真正内存分离呢？“写实复制”顾名思义，就是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险，就是下面介绍的场景。\nb、fork出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行AOF重写，把内存中的所有数据写入到AOF文件中。但是此时父进程依旧是会有流量写入的，如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。另外，如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。\n问题2，AOF重写不复用AOF本身的日志，一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。所以Redis AOF重写一个新文件，重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可。\n","permalink":"http://yangchnet.github.io/Dessert/posts/cache/%E6%A6%82%E8%A7%88redis%E7%AF%87%E4%BA%8Caof%E6%97%A5%E5%BF%97/","summary":"极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  AOF: Append Only File\nAOF日志的WAL 数据库中的WAL是Write-Ahead Logging, 为先写日志后执行。而Redis中的WAL为Write Ahead Log,先执行后写日志。\n为了避免额外的开销，Redis在向AOF里面记录日志时，不回去对这些命令进行语法检查，因此，如果先记录日志再执行命令的话，日志中有可能记录了错误的命令，Redis在使用日志恢复数据时，就可能会出错。\nAOF日志中记录了什么内容 有如下命令：\nset testkey testvalue 则AOF日志内容为：\n*3 $3 set $7 testkey $9 testvalue 其中“*3”表示当前命令有3个部分，每部分都由“$+数字”开头，后面紧跟具体的命令、键或值。\nAOF的潜在风险 AOF使用先执行，再写日志的方式，这样可以避免记录错误的命令，同时不会阻塞当前的写操作。\n但这样也带来了一些潜在风险：\n 刚执行完一个命令，没来得及记日志就宕机了，这个命令和相应的数据就有丢失的风险。 AOF避免了对当前命令的阻塞，但AOF写日志是在主线程中执行的，可能会给下一个操作带来阻塞风险。  这就需要我们对AOF写日志的实际进行把控。\nAOF的三种写回策略   Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；\n  Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；\n  No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。\n  针对避免主线程阻塞和减少数据丢失问题，这三种写回策略都无法做到两全其美。原因如下：\n “同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能； 虽然“操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了； “每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。  总结一下：","title":"概览Redis篇二：AOF日志"},{"content":"1. 分层的镜像 在我们启动一个容器之前，通常需要下载这个容器对应的镜像，以这个镜像为基础启动容器。镜像中包含了对应的程序的二进制文件与其所依赖的文件，程序在启动后看到的rootfs只是这个镜像中存在的文件。这样，我们就可以为容器中的进程提供一个干净的文件系统。\n创建一个镜像（image）的最简单方法是使用Dockerfile。\nFROMscratchCOPY hello /CMD [\u0026#34;/hello\u0026#34;]scratch是docker为我们提供的一个空镜像，我们可以在此基础上构建任何我们想要的镜像。\n在书写Dockerfile时，想必你听说过这么一句话，不要在Dockerfile中创建太多层.\n在Dockerfile中，每一个指令都会创建一个新的“层”，这里的层，指的是UnionFS中的一个文件目录。当我们创建了过多的层，会导致镜像体积变大，除此之外，Union FS 也会有最大层数限制。\n因此对于如下的Dockerfile文件写法，应尽量避免：\nFROMdebian:stretchRUN apt-get updateRUN apt-get install -y gcc libc6-dev make wgetRUN wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install可优化为如下写法\nFROMdebian:stretchRUN set -x; buildDeps=\u0026#39;gcc libc6-dev make wget\u0026#39; \\  \u0026amp;\u0026amp; apt-get update \\  \u0026amp;\u0026amp; apt-get install -y $buildDeps \\  \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; \\  \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\  \u0026amp;\u0026amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\  \u0026amp;\u0026amp; make -C /usr/src/redis \\  \u0026amp;\u0026amp; make -C /usr/src/redis install \\  \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\  \u0026amp;\u0026amp; rm redis.tar.gz \\  \u0026amp;\u0026amp; rm -r /usr/src/redis \\  \u0026amp;\u0026amp; apt-get purge -y --auto-remove $buildDeps这里的原理有点类似于Redis的AOF日志重写。\n为什么要将镜像分为很多层，而不是像系统镜像那样打包为一个ISO文件。\n不同的镜像，其所需的文件存在重复，如果对每个镜像都单独复制一份，会导致镜像过于臃肿，浪费磁盘空间，也会占用大量网络资源下载镜像文件。\n例如我们有100个服务都需要依赖于ubuntu18.04环境，如果我们需要为这些服务制作镜像，显然不可能真的将ubuntu18.04的所有文件复制100份，最好的办法是机器上只存在一份这样的文件，每个容器都来复用它，这就需要使用UnionFS。\n2. image分层的基础：UnionFS UnionFS称：联合挂载。其最主要的功能是将多个目录挂载在同一个目录下。\n前面说到，对于100个都需要Ubuntu18.04环境的服务，最好的办法是在机器上提供一份ubuntu18.04环境文件，所有的服务都来依赖这一份文件。\n但这就产生了另一个问题：如果某一个服务需要对ubuntu18.04中的某个文件进行修改，或是在这之上产生新的文件，那么其他同样依赖于这个环境的服务可能会被影响。\n这就需要用到UnionFS的特性。\n目前docker使用的UnionFS实现是OverlayFS.下面通过一个小实验来掌握overlay的基本特性：\n执行以下脚本\n#!/bin/bash  umount ./merged rm upper lower merged work -r mkdir upper lower merged work echo \u0026#34;I\u0026#39;m from lower!\u0026#34; \u0026gt; lower/in_lower.txt echo \u0026#34;I\u0026#39;m from upper!\u0026#34; \u0026gt; upper/in_upper.txt # `in_both` is in both directories echo \u0026#34;I\u0026#39;m from lower!\u0026#34; \u0026gt; lower/in_both.txt echo \u0026#34;I\u0026#39;m from upper!\u0026#34; \u0026gt; upper/in_both.txt sudo mount -t overlay overlay \\  -o lowerdir=./lower,upperdir=./upper,workdir=./work \\  ./merged OverlayFS 的一个 mount 命令牵涉到四类目录，分别是 lower，upper，merged 和 work，\n  \u0026ldquo;lower/\u0026quot;，也就是被 mount 两层目录中底下的这层（lowerdir）。在 OverlayFS 中，最底下这一层里的文件是不会被修改的，你可以认为它是只读的。OverlayFS 支持多个 lowerdir。\n  \u0026ldquo;upper/\u0026quot;，它是被 mount 两层目录中上面的这层 （upperdir）。在 OverlayFS 中，如果有文件的创建，修改，删除操作，那么都会在这一层反映出来，它是可读写的。\n  \u0026ldquo;merged\u0026rdquo; ，是挂载点（mount point）目录，也是用户看到的目录，用户的实际文件操作在这里进行。\n  \u0026ldquo;work/\u0026quot;，是一个存放临时文件的目录，OverlayFS 中如果有文件修改，就会在中间过程中临时存放文件到这里。并未表现在图中。\n  如果lower和upper中存在同名文件，那么不会显示lower中的文件。\n对于以下三种文件操作：\n  新建文件。这个文件将会出现在upper目录中。\n  删除文件。如果我们删除\u0026quot;in_upper.txt\u0026rdquo;，那么这个文件会在 upper/ 目录中消失。如果删除\u0026quot;in_lower.txt\u0026rdquo;, 在 lower/ 目录里的\u0026quot;in_lower.txt\u0026quot;文件不会有变化，只是在 upper/ 目录中增加了一个特殊文件来告诉 OverlayFS，\u0026ldquo;in_lower.txt\u0026rsquo;这个文件不能出现在 merged/ 里了，这就表示它已经被删除了。\n  修改文件。如果修改\u0026quot;in_lower.txt\u0026rdquo;，那么就会在 upper/ 目录中新建一个\u0026quot;in_lower.txt\u0026quot;文件，包含更新的内容，而在 lower/ 中的原来的实际文件\u0026quot;in_lower.txt\u0026quot;不会改变。\n  所有的更改都不会反映到lowerdir上，因此容器的镜像文件中各层正好作为 OverlayFS 的 lowerdir 的目录，然后加上一个空的 upperdir 一起挂载好后，就组成了容器的文件系统。\n同时，overlayfs还支持挂载多个lower目录，这样就实现了镜像的多个层。\n以下是一个正在运行的容器他的UnionFS挂载情况：\n\u0026#34;GraphDriver\u0026#34;: { \u0026#34;Data\u0026#34;: { \u0026#34;LowerDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/94809845e9decf34968235fe45bc9d66780a6c2f1929e6aeace43c62ae8a8473-init/diff:/var/lib/docker/overlay2/c95ef099ca18c9b33ff36907bb94c28f3c81a9758058b1698ae438fc88552ac7/diff:/var/lib/docker/overlay2/84cc0780b885789c335f30a784fde4355bfc11b4b9c538da0d6be665d0e87f74/diff:/var/lib/docker/overlay2/94ab5f531aa05c31918599de4a947b6af18488a5bd9f62186060aa32a0938677/diff:/var/lib/docker/overlay2/4154cfa0480d3be6d22ecd4b52225798d8c7d2f349602d36a821fd310318f19e/diff\u0026#34;, \u0026#34;MergedDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/94809845e9decf34968235fe45bc9d66780a6c2f1929e6aeace43c62ae8a8473/merged\u0026#34;, \u0026#34;UpperDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/94809845e9decf34968235fe45bc9d66780a6c2f1929e6aeace43c62ae8a8473/diff\u0026#34;, \u0026#34;WorkDir\u0026#34;: \u0026#34;/var/lib/docker/overlay2/94809845e9decf34968235fe45bc9d66780a6c2f1929e6aeace43c62ae8a8473/work\u0026#34; }, \u0026#34;Name\u0026#34;: \u0026#34;overlay2\u0026#34; }, 3. 镜像的组成 docker对镜像的管理是根据OCI标准来的，根据OCI标准，一个image可以分为以下四个部分：\n Image Index: image index是个json文件，用于使image能支持多个平台和多tag。 Image Manifest: manifest是一个json文件，这个文件包含了对filesystem layers和image config的描述。 Image Config: image config就是一个json文件，这个json文件包含了对这个image的描述。 FileSystem Layer: Filesystem Layer包含了文件系统的信息，即该image包含了哪些文件/目录，以及它们的属性和数据。  它们之间的关系如下：\n3.1 Media Type Media Type, 媒体类型。\n在OCI标准中，不同的文件对应不同的media type。其对应关系如下：\n   Media Type 说明     application/vnd.oci.descriptor.v1+json Content Descriptor 内容描述文件   application/vnd.oci.layout.header.v1+json OCI Layout 布局描述文件   application/vnd.oci.image.index.v1+json Image Index 高层次的镜像元信息文件   application/vnd.oci.image.manifest.v1+json Image Manifest 镜像元信息文件   application/vnd.oci.image.config.v1+json Image Config 镜像配置文件   application/vnd.oci.image.layer.v1.tar Image Layer 镜像层文件   application/vnd.oci.image.layer.v1.tar+gzip Image Layer 镜像层文件gzip压缩   application/vnd.oci.image.layer.nondistributable.v1.tar Image Layer 非内容寻址管理   application/vnd.oci.image.layer.nondistributable.v1.tar+gzip Image Layer, gzip压缩 非内容寻址管理    3.2 FileSystem Layer 每个filesystem layer都包含了在上一个layer上的改动情况，主要包含三方面的内容：\n  变化类型：是增加、修改还是删除了文件\n  文件类型：每个变化发生在哪种文件类型上\n  文件属性：文件的修改时间、用户ID、组ID、RWX权限等\n  根据我们上面所介绍的UnionFS，很容易理解。\n3.3 Image Config Config包含了对镜像文件的描述，它是一个json文件，示例如下：\n{ \u0026#34;created\u0026#34;: \u0026#34;2015-10-31T22:22:56.015925234Z\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Alyssa P. Hacker \u0026lt;alyspdev@example.com\u0026gt;\u0026#34;, \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;User\u0026#34;: \u0026#34;alice\u0026#34;, \u0026#34;ExposedPorts\u0026#34;: { \u0026#34;8080/tcp\u0026#34;: {} }, \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;FOO=oci_is_a\u0026#34;, \u0026#34;BAR=well_written_spec\u0026#34; ], ... }, \u0026#34;rootfs\u0026#34;: { \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:c6f988f4874bb0add23a778f753c65efe992244e148a1d2ec2a8b664fb66bbd1\u0026#34;, \u0026#34;sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34; }, \u0026#34;history\u0026#34;: [ ... ] } 其中大部分是可选参数，必须的参数有：\n architecture: CPU架构 os： 镜像在什么操作系统上运行 rootfs：指定了image所包含的filesystem layers，type的值必须是layers，diff_ids包含了layer的列表（顺序排列），每一个sha256就是每层layer对应tar包的sha256码  type： type的值必须是layers diff_ids: 每层layer包的哈希值的列表    其他都是可选参数，见名知意\n3.4 Image Manifest Image Manifest为特定架构和操作系统的单个容器镜像提供配置和一系列的layer。\nManifest 的三大主要目标：\n  内容可寻址（通过 hash 算法为镜像和它的组件生成唯一ID）\n  支持多种平台的架构镜像（由一个更上层的 manifest 说明包含的镜像 manifests 其具体平台的版本情况）\n  能被解析成为 OCI 运行时规范\n  示例如下：\n{ \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.config.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 7023, \u0026#34;digest\u0026#34;: \u0026#34;sha256:b5b2b2c507a0944348e0303114d8d93aaaa081732b86451d9bce1f432a537bc7\u0026#34; }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;size\u0026#34;: 32654, \u0026#34;digest\u0026#34;: \u0026#34;sha256:9834876dcfb05cb167a5c24953eba58c4ac89b1adf57f28f2f9d09af107ee8f0\u0026#34; }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;size\u0026#34;: 16724, \u0026#34;digest\u0026#34;: \u0026#34;sha256:3c3a4604a545cdc127456d94e421cd355bca5b528f4a9c1905b15da2eb4a4c6b\u0026#34; }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;size\u0026#34;: 73109, \u0026#34;digest\u0026#34;: \u0026#34;sha256:ec4b8955958665577945c89419d1af06b5f7636b4ac3da7f12184802ad867736\u0026#34; } ], \u0026#34;annotations\u0026#34;: { \u0026#34;com.example.key1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;com.example.key2\u0026#34;: \u0026#34;value2\u0026#34; } } 从上面的json文件中可以看出，manifest中包含对config的描述，包括其mediaType， size，digest等，这里的digest就是config文件的sha256值，可以看出manifest与config的一对一对应关系。而layer中则指明了这个manifest包含哪些层，manifest与layer是一对多关系。\n3.5 Image Index Image Index指向一个特定manifests的更高层manifest，用于支持多平台。index是可选的。\n示例如下：\n{ \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.index.v1+json\u0026#34;, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 7143, \u0026#34;digest\u0026#34;: \u0026#34;sha256:e692418e4cbaf90ca69d05a66403747baa33ee08806650b51fab815ad7fc331f\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;ppc64le\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;size\u0026#34;: 7682, \u0026#34;digest\u0026#34;: \u0026#34;sha256:5b0bcabd1ed22e9fb1310cf6c2dec7cdef19f0ad69efa1f392e94a4333501270\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } } ], \u0026#34;annotations\u0026#34;: { \u0026#34;com.example.key1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;com.example.key2\u0026#34;: \u0026#34;value2\u0026#34; } } index文件包含了对image中所有manifest的描述，相当于一个manifest列表，包括每个manifest的media type，文件大小，sha256码，支持的平台以及平台特殊的配置。\n比如ubuntu想让它的image支持amd64和arm64平台，于是它在两个平台上都编译好相应的包，然后将两个平台的layer都放到这个image的filesystem layers里面，然后写两个config文件和两个manifest文件，再加上这样一个描述不同平台manifest的index文件，就可以让这个image支持两个平台了，两个平台的用户可以使用同样的命令得到自己平台想要的那些layer。\n3.6 镜像layout 镜像的布局包含以下内容：\n blobs：内容寻址的块文件，目录必须存在，但是可以为空 oci-layout: 必须存在的json对象，必须包含imageLayoutVersion字段 index.json: 必须存在的JSON格式，文件中必须包含镜像Index的基本属性  可以查看hello-world镜像的内容：\n$ yay -S skopeo # 安装skopeo $ skopeo copy docker://hello-world oci:hello-world # 利用skopeo下载hello-world镜像 $ tree -L 3 hello-world hello-world ├── blobs │ └── sha256 │ ├── 2db29710123e3e53a794f2694094b9b4338aa9ee5c40b930cb8063a1be392c54 │ ├── 75ab15a4973c91d13d02b8346763142ad26095e155ca756c79ee3a4aa792991f │ └── 811f3caa888b1ee5310e2135cfd3fe36b42e233fe0d76d9798ebd324621238b9 ├── index.json └── oci-layout index.json\n{ \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:75ab15a4973c91d13d02b8346763142ad26095e155ca756c79ee3a4aa792991f\u0026#34;, \u0026#34;size\u0026#34;: 402 } ] } oci-layout\n{ \u0026#34;imageLayoutVersion\u0026#34;: \u0026#34;1.0.0\u0026#34; } 4. 从远程获取镜像的过程   docker发送image的名称+tag（或者digest）给registry服务器，服务器根据收到的image的名称+tag（或者digest），找到相应image的manifest，然后将manifest返回给docker\n  docker得到manifest后，读取里面image配置文件的digest(sha256)，这个sha256码就是image的ID\n  根据ID在本地找有没有存在同样ID的image，有的话就不用继续下载了\n  如果没有，那么会给registry服务器发请求（里面包含配置文件的sha256和media type），拿到image的配置文件（Image Config）\n  根据配置文件中的diff_ids（每个diffid对应一个layer tar包的sha256，tar包相当于layer的原始格式），在本地找对应的layer是否存在\n  如果layer不存在，则根据manifest里面layer的sha256和media type去服务器拿相应的layer（相当去拿压缩格式的包）。\n  拿到后进行解压，并检查解压后tar包的sha256能否和配置文件（Image Config）中的diff_id对的上，对不上说明有问题，下载失败\n  根据docker所用的后台文件系统类型，解压tar包并放到指定的目录\n  等所有的layer都下载完成后，整个image下载完成，就可以使用了\n  上面的过程涉及到两个接口：\nGET /v2/\u0026lt;name\u0026gt;/manifests/\u0026lt;reference\u0026gt; 其中为镜像名称，可能包含tag或digest。\n这个接口用于获取镜像的manifest。\nGET /v2/\u0026lt;name\u0026gt;/blobs/\u0026lt;digest\u0026gt; 这个接口用于获取镜像的layer\n5. 镜像在本地的存储  镜像默认在系统中的存储位置在/var/lib/docker\n  这里我们单独选择一个镜像golang:1.16.6来看\n 5.1 镜像的digest 镜像的digest即为镜像manifest文件的sha256值，当镜像内容发生变化，即其中的layer发生了变化，则其layer的sha256值必然发生变化，而相应的包含layer哈希值的manifest必然也会发生变化，这样就保证了digest能唯一对应一个镜像。\n5.2 repositories.json repositories.json中记录了和本地image相关的repository信息，主要是name和image id的对应关系，当image从registry上被pull下来后，就会更新该文件，其位置在/var/lib/docker/image/overlay2/repositories.json\n$ cat repositories.json | jq { \u0026#34;Repositories\u0026#34;: { \u0026#34;golang\u0026#34;: { \u0026#34;golang:1.16.6\u0026#34;: \u0026#34;sha256:028d102f774acfd5d9a17d60dc321add40bea5cc6f06e0a84fd3aca1bb4c2b12\u0026#34;, \u0026#34;golang@sha256:4544ae57fc735d7e415603d194d9fb09589b8ad7acd4d66e928eabfb1ed85ff1\u0026#34;: \u0026#34;sha256:028d102f774acfd5d9a17d60dc321add40bea5cc6f06e0a84fd3aca1bb4c2b12\u0026#34;, }, ... }, } 记住golang:1.16.6的digest为028d102f774acfd5d9a17d60dc321add40bea5cc6f06e0a84fd3aca1bb4c2b12，记住前三个字母就可以了028\n5.3 配置文件（image config） 在从服务器获取image时，会先获取manifest，然后从manifest中拿到config的hash，从而获取config，保存在/var/lib/docker/image/overlay2/imagedb/content/sha256，文件名就是image id\n$ cat 028d102f774acfd5d9a17d60dc321add40bea5cc6f06e0a84fd3aca1bb4c2b12 | jq { ... \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:afa3e488a0ee76983343f8aa759e4b7b898db65b715eb90abc81c181388374e3\u0026#34;, \u0026#34;sha256:4b0edb23340c111e75557748161eed3ca159584871569ce7ec9b659e1db201b4\u0026#34;, \u0026#34;sha256:5a9a65095453efb15a9b378a3c1e7699e8004744ecd6dd519bdbabd0ca9d2efc\u0026#34;, \u0026#34;sha256:ad83f0aa5c0abe35b7711f226037a5557292d479c21635b3522e1e5a41e3ce23\u0026#34;, \u0026#34;sha256:d1c59e37fbfc7294184d6fbe4ff8e1690d9119b6233f91af5ad0a4b36e45dff7\u0026#34;, \u0026#34;sha256:e46b2fd4e4eadc0ee107417c66e968f417e0759fd7d625ab6f0537ba02c1c868\u0026#34;, \u0026#34;sha256:9672a02ff8cffb302a4c5cef60f8b36cbbe9709a5ba78b7d0ce56db3219c5e51\u0026#34; ] } } 这里要注意的时，如果image存在多个层，即diff_ids数组长度大于一，则diff_ids[0]是UnionFS最底层，diff_ids[-1]是UnionFS最高层。\n5.4 layer的diff_id和digest的对应关系 layer的diff_id存在image的配置文件中，而layer的digest存在image的manifest中，他们的对应关系被存储在了/var/lib/docker/image/overlay2/distribution目录下：\n. ├── diffid-by-digest │ └── sha256 └── v2metadata-by-diffid └── sha256   diffid-by-digest： 存放digest到diffid的对应关系\n  v2metadata-by-diffid： 存放diffid到digest的对应关系\n  查看mysql最上层layer的digest:\n$ cd /var/lib/docker/image/overlay2/distribution/v2metadata-by-diffid/sha256 $ cat 9672a02ff8cffb302a4c5cef60f8b36cbbe9709a5ba78b7d0ce56db3219c5e51 | jq # sha256:967...是golang:1.16.6的最上层layer [ { \u0026#34;Digest\u0026#34;: \u0026#34;sha256:ff36ba4656980ea99a067c8a9b39f210a4da82badccaa2bae27317e711985668\u0026#34;, \u0026#34;SourceRepository\u0026#34;: \u0026#34;docker.io/library/golang\u0026#34;, \u0026#34;HMAC\u0026#34;: \u0026#34;\u0026#34; } ] 5.5 layer元数据 从image config中我们可以得到diff_ids，从而得到image各个层的sha256值，这时我们可以到/var/lib/docker/image/overlay2/layerdb/sha256去查看这些层，例如golang:1.16.6的最底层sha256为afa3e488a0ee76983343f8aa759e4b7b898db65b715eb90abc81c181388374e3：\n$ cat /var/lib/docker/image/overlay2/layerdb/sha256/afa3e488a0ee76983343f8aa759e4b7b898db65b715eb90abc81c181388374e3 $ tree . ├── cache-id ├── diff ├── size └── tar-split.json.gz 但在查看第二层的时候发现：\n$ cd 4b0edb23340c111e75557748161eed3ca159584871569ce7ec9b659e1db201b4 bash: cd: 4b0edb23340c111e75557748161eed3ca159584871569ce7ec9b659e1db201b4: 没有那个文件或目录 这是因为docker使用了chainID的方式去保存这些layer，简单来说就是chainID=sha256sum(H(chainID), diffid)，因此golang:1.16.6的第二层为：\n$ echo -n \u0026quot;sha256:afa3e488a0ee76983343f8aa759e4b7b898db65b715eb90abc81c181388374e3 sha256:4b0edb23340c111e75557748161eed3ca159584871569ce7ec9b659e1db201b4\u0026quot; | sha256sum - c21ff68b02e7caf277f5d356e8b323a95e8d3969dd1ab0d9f60e7c8b4a01c874 - 这时候查看c21ff68b02e7caf277f5d356e8b323a95e8d3969dd1ab0d9f60e7c8b4a01c874:\n$ cd c21ff68b02e7caf277f5d356e8b323a95e8d3969dd1ab0d9f60e7c8b4a01c874 $ tree . ├── cache-id ├── diff ├── parent ├── size └── tar-split.json.gz 以此类推，我们可以找到任意一层。\n在每一层中，一般包含5个文件：\n cache-id是docker下载layer的时候在本地生成的一个随机uuid，指向真正存放layer文件的地方 diff文件存放layer的diffid parent文件存放当前layer的父layer的diffid，而由于最底层layer没有parent，因此它没有这个文件 size当前layer的大小，单位是字节 tar-split.json.gz，layer压缩包的split文件，通过这个文件可以还原layer的tar包  打印出golang:1.16.6第二层的cache-id:\n$ cat cache-id 40a11a2b4f594bcccbe2cbb6cd3e6fa4a3c2f8427817f43bbade4331173611d2 5.6 layer数据 layer数据在/var/lib/docker/overlay2/\n通过我们刚才打印的golang:1.16.6第二层的cache-id，可以在这里找到golang:1.16.6第二层的文件：\n$ cd /var/lib/docker/overlay2/40a11a2b4f594bcccbe2cbb6cd3e6fa4a3c2f8427817f43bbade4331173611d2 $ tree . ├── committed ├── diff ├── link ├── lower └── work diff文件夹中就是这一层相对于下层修改的文件。\n5.7 manifest文件 manifest里面包含的内容就是对config和layer的sha256 + media type描述，目的就是为了下载config和layer，等image下载完成后，manifest的使命就完成了，里面的信息对于image的本地管理来说没什么用，因此本地并没与单独对manifest进行存储。\nReferences https://cloud.tencent.com/developer/article/1769020\nhttps://yeasy.gitbook.io/docker_practice/image/list\nhttps://www.jianshu.com/p/3ba255463047\nhttps://staight.github.io/2019/10/04/%E5%AE%B9%E5%99%A8%E5%AE%9E%E7%8E%B0-overlay2/\nhttps://www.jianshu.com/p/3826859a6d6e\nhttps://time.geekbang.org/column/article/318173\nhttps://jvns.ca/blog/2019/11/18/how-containers-work--overlayfs/\nhttps://vividcode.cc/oci-image-spec-introduction/\nhttps://segmentfault.com/a/1190000009309276\nhttps://github.com/opencontainers/image-spec\nhttps://github.com/containers/skopeo\nhttps://docs.docker.com/registry/spec/api/#pulling-an-image\n","permalink":"http://yangchnet.github.io/Dessert/posts/cloud/%E6%A6%82%E8%A7%88%E5%AE%B9%E5%99%A8%E7%AF%87%E4%B8%80%E9%95%9C%E5%83%8F/","summary":"1. 分层的镜像 在我们启动一个容器之前，通常需要下载这个容器对应的镜像，以这个镜像为基础启动容器。镜像中包含了对应的程序的二进制文件与其所依赖的文件，程序在启动后看到的rootfs只是这个镜像中存在的文件。这样，我们就可以为容器中的进程提供一个干净的文件系统。\n创建一个镜像（image）的最简单方法是使用Dockerfile。\nFROMscratchCOPY hello /CMD [\u0026#34;/hello\u0026#34;]scratch是docker为我们提供的一个空镜像，我们可以在此基础上构建任何我们想要的镜像。\n在书写Dockerfile时，想必你听说过这么一句话，不要在Dockerfile中创建太多层.\n在Dockerfile中，每一个指令都会创建一个新的“层”，这里的层，指的是UnionFS中的一个文件目录。当我们创建了过多的层，会导致镜像体积变大，除此之外，Union FS 也会有最大层数限制。\n因此对于如下的Dockerfile文件写法，应尽量避免：\nFROMdebian:stretchRUN apt-get updateRUN apt-get install -y gcc libc6-dev make wgetRUN wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install可优化为如下写法\nFROMdebian:stretchRUN set -x; buildDeps=\u0026#39;gcc libc6-dev make wget\u0026#39; \\  \u0026amp;\u0026amp; apt-get update \\  \u0026amp;\u0026amp; apt-get install -y $buildDeps \\  \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-5.0.3.tar.gz\u0026#34; \\  \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\  \u0026amp;\u0026amp; tar -xzf redis.","title":"概览容器篇一：镜像"},{"content":" 极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  Redis单线程模型 Redis 是单线程，主要是指 Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。\n为什么Redis采用单线程 通常的程序设计都采用多线程来提高性能，获得更快的响应速度。\n但采用多线程模型将不可避免地面对以下问题：\n 多个线程对同一资源的数据竞争 因解决数据竞争而导致的性能损耗 增加系统复杂度，降低系统代码的易调试性和可维护性  为啥Redis单线程还这么快 主要是两点：\n Redis大部分操作在内存上完成，并采用了高效的数据结构 Redis采用了多路复用机制  ","permalink":"http://yangchnet.github.io/Dessert/posts/cache/%E6%A6%82%E8%A7%88redis%E7%AF%87%E4%B8%80%E5%8D%95%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/","summary":" 极客时间《Redis 核心技术与实战》学习笔记\n  概览Redis篇一：单线程模型 概览Redis篇二：AOF日志 概览Redis篇三：RDB快照 概览Redis篇四：主从 概览Redis篇五：哨兵 概览Redis篇六：切片集群  Redis单线程模型 Redis 是单线程，主要是指 Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。\n为什么Redis采用单线程 通常的程序设计都采用多线程来提高性能，获得更快的响应速度。\n但采用多线程模型将不可避免地面对以下问题：\n 多个线程对同一资源的数据竞争 因解决数据竞争而导致的性能损耗 增加系统复杂度，降低系统代码的易调试性和可维护性  为啥Redis单线程还这么快 主要是两点：\n Redis大部分操作在内存上完成，并采用了高效的数据结构 Redis采用了多路复用机制  ","title":"概览Redis篇一：单线程模型"},{"content":"1. 什么是inotify Inotify API提供了一种监视文件系统的机制事件。 Inotify可用于监视单个文件或监视目录。当监视目录时，Inotify将返回目录本身的事件，以及内部的文件目录。\n简单的说，就是inotify可以为你监控文件系统的变化，在发生一些事件时通知你。\n2. 实验 需要安装inotify-tools包\n# arch系统 yay -S inotify-tools inotify-tools提供了两个命令：inotifywait和inotifywatch\n2.1 inotifywait 先来看inotifywait,它被用来\u0026quot;Wait for a particular event on a file or set of files.\u0026quot;, 也就是说等待在文件上的某些事件发生。使用inotifywait -h查看其支持的参数：\n-h show this help info --exclude \u0026lt;pattern\u0026gt; 排除匹配所给正则的所有事件 --excludei \u0026lt;pattern\u0026gt; 类似前一个命令但非敏感 --include \u0026lt;pattern\u0026gt; 排除除匹配正则之外的所有事件 --includei \u0026lt;pattern\u0026gt; 类似前一个命令但非敏感 -m|--monitor 在timeout之前保持监听，若不设置此标志，inotifywait将在一个事件后退出。 -d|--daemon 类似前一个命令但在后台运行，将日志输出到`--outfile`所指定的文件 -P|--no-dereference 不跟踪符号链接 -r|--recursive 递归的监听、 --fromfile \u0026lt;file\u0026gt; Read files to watch from \u0026lt;file\u0026gt; or `-' for stdin. -o|--outfile \u0026lt;file\u0026gt; 输出到\u0026lt;file\u0026gt;而不是标准输出 -s|--syslog 向syslog发送错误而不是Stderr -q|--quiet 只输出事件 -qq 啥也不输出 --format \u0026lt;fmt\u0026gt; 以特定格式输出 --no-newline 在格式化输出后不打印换行符 --timefmt \u0026lt;fmt\u0026gt; strftime-compatible format string for use with %T in --format string. -c|--csv 以csv格式输出 -t|--timeout \u0026lt;seconds\u0026gt; 当监听一个事件时，在\u0026lt;seconds\u0026gt;秒后超时 -e|--event \u0026lt;event1\u0026gt; 监听特定事件，若不设置，则监听所有事件 支持的event有：\nEvents: access file or directory contents were read modify file or directory contents were written attrib file or directory attributes changed close_write file or directory closed, after being opened in writable mode close_nowrite file or directory closed, after being opened in read-only mode close file or directory closed, regardless of read/write mode open file or directory opened moved_to file or directory moved to watched directory moved_from file or directory moved from watched directory move file or directory moved to or from watched directory move_self A watched file or directory was moved. create file or directory created within watched directory delete file or directory deleted within watched directory delete_self file or directory was deleted unmount file system containing file or directory unmounted 在终端1中：\nmkdir ~/inotify-demo \u0026amp;\u0026amp; cd inotify-demo # 建立测试文件夹 inotifywait -rmc -q . 在终端2中：\ncd ~/inotify-demo touch aa.txt 此时切换到终端1，可以看到事件输出：\n... ./,CREATE,aa.txt ./,OPEN,aa.txt ./,ATTRIB,aa.txt ./,\u0026#34;CLOSE_WRITE,CLOSE\u0026#34;,aa.txt ./,\u0026#34;OPEN,ISDIR\u0026#34;, ./,\u0026#34;CLOSE_NOWRITE,CLOSE,ISDIR\u0026#34;, ./,OPEN,aa.txt ./,\u0026#34;CLOSE_NOWRITE,CLOSE\u0026#34;,aa.txt 打开文件aa.txt，写入一行\ninotify-demo 可看到终端1中也已将事件打印出：\n... ./,CREATE,4913 ./,OPEN,4913 ./,ATTRIB,4913 ./,\u0026#34;CLOSE_WRITE,CLOSE\u0026#34;,4913 ./,DELETE,4913 ./,MOVED_FROM,aa.txt ./,CREATE,aa.txt ./,OPEN,aa.txt ./,MODIFY,aa.txt ./,MODIFY,aa.txt ./,ATTRIB,aa.txt ./,\u0026#34;CLOSE_WRITE,CLOSE\u0026#34;,aa.txt ./,ATTRIB,aa.txt ./,\u0026#34;OPEN,ISDIR\u0026#34;, ./,\u0026#34;CLOSE_NOWRITE,CLOSE,ISDIR\u0026#34;, 将所有的事件都打印出略显杂乱，这里我们只让其监听modify事件\ninotifywait -rmc -q -e modify . 创建文件bb.txt\ntouch bb.txt 可观察到，未有任何事件被打印。\n在bb.txt中写入一行inotify-demo\n可观察到事件打印：\n./,MODIFY,bb.txt ./,MODIFY,bb.txt 2.2 inotifywatch inotifywatch被用于收集文件系统使用信息\n其命令参数和inotifywait大致相同，可使用inotifywatch查看\n实验：\ncd ~/inotify-demo # 在30秒内监听crate,modify,delete三个事件，在30秒结束后打印统计信息 inotifywatch -v -e create -e modify -e delete -t 30 -r . 接下来30秒，创建了4个文件，修改了2个文件，删除了1个文件\n事件统计信息为:\nEstablishing watches... Setting up watch(es) on . OK, . is now being watched. Total of 1 watches. Finished establishing watches, now collecting statistics. Will listen for events for 30 seconds. total modify create delete filename 15 4 8 3 ./ TODO：统计信息与实际操作有所不符，目前不清楚为啥\n3. 使用golang调用inotify  https://github.com/fsnotify/fsnotify\n package main import ( \u0026#34;log\u0026#34; \u0026#34;github.com/fsnotify/fsnotify\u0026#34; ) func main() { watcher, err := fsnotify.NewWatcher() if err != nil { log.Fatal(err) } defer watcher.Close() done := make(chan bool) go func() { for { select { case event, ok := \u0026lt;-watcher.Events: if !ok { return } log.Println(\u0026#34;event:\u0026#34;, event) if event.Op\u0026amp;fsnotify.Write == fsnotify.Write { log.Println(\u0026#34;modified file:\u0026#34;, event.Name) } case err, ok := \u0026lt;-watcher.Errors: if !ok { return } log.Println(\u0026#34;error:\u0026#34;, err) } } }() err = watcher.Add(\u0026#34;/tmp/foo\u0026#34;) if err != nil { log.Fatal(err) } \u0026lt;-done } References https://juejin.cn/post/6988561056364757022\nhttps://www.infoq.cn/article/inotify-linux-file-system-event-monitoring\nhttps://www.cnblogs.com/centos2017/p/7896715.html\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/inotify/","summary":"1. 什么是inotify Inotify API提供了一种监视文件系统的机制事件。 Inotify可用于监视单个文件或监视目录。当监视目录时，Inotify将返回目录本身的事件，以及内部的文件目录。\n简单的说，就是inotify可以为你监控文件系统的变化，在发生一些事件时通知你。\n2. 实验 需要安装inotify-tools包\n# arch系统 yay -S inotify-tools inotify-tools提供了两个命令：inotifywait和inotifywatch\n2.1 inotifywait 先来看inotifywait,它被用来\u0026quot;Wait for a particular event on a file or set of files.\u0026quot;, 也就是说等待在文件上的某些事件发生。使用inotifywait -h查看其支持的参数：\n-h show this help info --exclude \u0026lt;pattern\u0026gt; 排除匹配所给正则的所有事件 --excludei \u0026lt;pattern\u0026gt; 类似前一个命令但非敏感 --include \u0026lt;pattern\u0026gt; 排除除匹配正则之外的所有事件 --includei \u0026lt;pattern\u0026gt; 类似前一个命令但非敏感 -m|--monitor 在timeout之前保持监听，若不设置此标志，inotifywait将在一个事件后退出。 -d|--daemon 类似前一个命令但在后台运行，将日志输出到`--outfile`所指定的文件 -P|--no-dereference 不跟踪符号链接 -r|--recursive 递归的监听、 --fromfile \u0026lt;file\u0026gt; Read files to watch from \u0026lt;file\u0026gt; or `-' for stdin. -o|--outfile \u0026lt;file\u0026gt; 输出到\u0026lt;file\u0026gt;而不是标准输出 -s|--syslog 向syslog发送错误而不是Stderr -q|--quiet 只输出事件 -qq 啥也不输出 --format \u0026lt;fmt\u0026gt; 以特定格式输出 --no-newline 在格式化输出后不打印换行符 --timefmt \u0026lt;fmt\u0026gt; strftime-compatible format string for use with %T in --format string.","title":"inotify"},{"content":" 极客时间《MySQL实战45讲》笔记\n MySQL的基本结构 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。\nServer 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。\n现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。\n从一条查询语句看各个模块的执行过程 对于如下语句：\nmysql\u0026gt; select * from T where ID=10； 连接器 第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：\nmysql -h$ip -P$port -u$user -p 这里要注意的一个问题是：一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n因为一个连接的权限实在用户名密码认证通过后，连接器到权限表中查出的，之后这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。\n查询缓存 连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。\n之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果在缓存中可以找到查询结果，可以直接返回结果，如果找不到，就会继续后面的执行阶段。\n但大多数情况下缓存往往弊大于利，因为查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。\n可以将query_cache_type参数设置为DEMAND,这样对于默认的SQL查询语句都不使用缓存。\n需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。\n分析器 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。\n分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。\n做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。\n优化器 经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。\n优化器会选择执行效率最高的方案。\n优化器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。\n执行器 MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。\n开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误。\n如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。\n执行器的执行流程如下：\n  调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中；\n  调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。\n  执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。\n  ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%A6%82%E8%A7%88mysql%E7%AF%87mysql%E6%9E%B6%E6%9E%84/","summary":"极客时间《MySQL实战45讲》笔记\n MySQL的基本结构 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。\nServer 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。\n现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。\n从一条查询语句看各个模块的执行过程 对于如下语句：\nmysql\u0026gt; select * from T where ID=10； 连接器 第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的：\nmysql -h$ip -P$port -u$user -p 这里要注意的一个问题是：一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n因为一个连接的权限实在用户名密码认证通过后，连接器到权限表中查出的，之后这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。\n查询缓存 连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。\n之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果在缓存中可以找到查询结果，可以直接返回结果，如果找不到，就会继续后面的执行阶段。\n但大多数情况下缓存往往弊大于利，因为查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。\n可以将query_cache_type参数设置为DEMAND,这样对于默认的SQL查询语句都不使用缓存。\n需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。\n分析器 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。\n分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。\n做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。\n优化器 经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。","title":"概览MySQL篇〇：MySQL架构"},{"content":" 极客时间《消息队列高手课》笔记\n  概览消息队列篇〇：为什么需要消息队列 概览消息队列篇一：不同的mq的消息模型 概览消息队列篇二：如何确保消息不丢失 概览消息队列篇三：重复消息的处理  1. 主题和队列 最基本的队列模型，是按照“队列”的数据结构来设计的，即先进先出。生产者发送消息（入队），消费者获取消息（出队）。\n当有多个生产者往同一个队列中发送消息，则这个队列中可以消费到的消息，就是这些生产者生产的所有消息的合集。消息的顺序就是这些生产者发送消息的自然顺序。如果有多个消费者接收同一个队列的消息，这些消费者之间实际上是竞争的关系，每个消费者只能收到队列中的一部分消息，也就是说任何一条消息只能被其中的一个消费者收到。\n这时候问题就出现了，如果一份消息需要被多个消费者消费，比如，对于一个订单，它需要被风控系统、支付系统等系统消费，显然上述的模型不能满足这个需求。这时候一个可行的解决方案是：为每个消费者创建一个单独的队列，让生产者发送多份。\n但显然这样会浪费较多的资源，同一个消息复制了多份。更重要的是，生产者必须知道有多少个消费者。为每个消费者单独发送一份消息，这实际上违背了消息队列“解耦”这个设计初衷。\n为了解决这个问题，演化出“发布-订阅”(pub-sub)模型。\n在发布-订阅模型中，消息的发送方称为发布者（Publisher），消息的接收方称为订阅者（Subscriber），服务端存放消息的容器称为主题（Topic）。发布者将消息发送到主题中，订阅者在接收消息之前需要先“订阅主题”。“订阅”在这里既是一个动作，同时还可以认为是主题在消费时的一个逻辑副本，每份订阅中，订阅者都可以接收到主题的所有消息。\n当发布-订阅模型中只有一个订阅者时，那它和队列模型就基本上是一样的了。也就是说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。\n2. 常见mq的消息模型 2.1 RabbitMQ的消息模型 RabbitMQ是少数仍坚持使用队列模型的产品之一。\n那么RabbitMQ是如何解决多个消费者的问题呢？\nRabbitMQ有一个Exchange模块，在 RabbitMQ 中，Exchange 位于生产者和队列之间，生产者并不关心将消息发送给哪个队列，而是将消息发送给 Exchange，由 Exchange 上配置的策略来决定将消息投递到哪些队列中。\n同一份消息如果需要被多个消费者来消费，需要配置 Exchange 将消息发送到多个队列，每个队列中都存放一份完整的消息数据，可以为一个消费者提供消费服务。\n2.2 RocketMQ的消息模型 RocketMQ 使用的消息模型是标准的发布-订阅模型\n但是，在 RocketMQ 也有队列（Queue）这个概念，并且队列在 RocketMQ 中是一个非常重要的概念，要了解队列在RocketMQ中的作用，我们首先要了解消息确认机制及其带来的问题\n为了确保消息不会在传递过程中由于网络或服务器故障丢失，消息队列一般采用“请求-确认”机制来确认消息的成功消费。消费者在成功消费一条消息，完成自己的业务逻辑后，会发送确认給消息队列。消息队列只有收到确认后，才认为一条消息被成功消费。\n这种机制很好地保证了消息传递过程中的可靠性，但是其带来另一个问题：在某一条消息被成功消费之前，下一条消息是不能被消费的，否则就会出现消息空洞，违背了有序性这个原则。\n也就是说，每个主题在任意时刻，至多只能有一个消费者实例在进行消费，那就没法通过水平扩展消费者的数量来提升消费端总体的消费性能。\n为了解决这个问题，RocketMQ在主题下增加了队列的概念，每个主题包含多个队列，通过多个队列来实现多实例并行生产和消费。\nRocketMQ 只在队列上保证消息的有序性，主题层面是无法保证消息的严格顺序的。\nRocketMQ 中，订阅者的概念是通过消费组（Consumer Group）来体现的。每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响，也就是说，一条消息被 Consumer Group1 消费过，也会再给 Consumer Group2 消费。\n同时，一个消费组中包含多个消费者，同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息。如果一条消息被消费者 Consumer1 消费了，那同组的其他消费者就不会再收到这条消息。\n在 Topic 的消费过程中，由于消息需要被不同的组进行多次消费，所以消费完的消息并不会立即被删除，这就需要 RocketMQ 为每个消费组在每个队列上维护一个消费位置（Consumer Offset），这个位置之前的消息都被消费过，之后的消息都没有被消费过，每成功消费一条消息，消费位置就加一。\n2.3 Kafka的消息模型 Kafka 的消息模型和 RocketMQ 是完全一样的，唯一的区别是，在 Kafka 中，队列这个概念的名称不一样，Kafka 中对应的名称是“分区（Partition）”，含义和功能是没有任何区别的。\n","permalink":"http://yangchnet.github.io/Dessert/posts/mq/%E6%A6%82%E8%A7%88%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%AF%87%E4%B8%80%E4%B8%8D%E5%90%8C%E7%9A%84mq%E7%9A%84%E6%B6%88%E6%81%AF%E6%A8%A1%E5%9E%8B/","summary":"极客时间《消息队列高手课》笔记\n  概览消息队列篇〇：为什么需要消息队列 概览消息队列篇一：不同的mq的消息模型 概览消息队列篇二：如何确保消息不丢失 概览消息队列篇三：重复消息的处理  1. 主题和队列 最基本的队列模型，是按照“队列”的数据结构来设计的，即先进先出。生产者发送消息（入队），消费者获取消息（出队）。\n当有多个生产者往同一个队列中发送消息，则这个队列中可以消费到的消息，就是这些生产者生产的所有消息的合集。消息的顺序就是这些生产者发送消息的自然顺序。如果有多个消费者接收同一个队列的消息，这些消费者之间实际上是竞争的关系，每个消费者只能收到队列中的一部分消息，也就是说任何一条消息只能被其中的一个消费者收到。\n这时候问题就出现了，如果一份消息需要被多个消费者消费，比如，对于一个订单，它需要被风控系统、支付系统等系统消费，显然上述的模型不能满足这个需求。这时候一个可行的解决方案是：为每个消费者创建一个单独的队列，让生产者发送多份。\n但显然这样会浪费较多的资源，同一个消息复制了多份。更重要的是，生产者必须知道有多少个消费者。为每个消费者单独发送一份消息，这实际上违背了消息队列“解耦”这个设计初衷。\n为了解决这个问题，演化出“发布-订阅”(pub-sub)模型。\n在发布-订阅模型中，消息的发送方称为发布者（Publisher），消息的接收方称为订阅者（Subscriber），服务端存放消息的容器称为主题（Topic）。发布者将消息发送到主题中，订阅者在接收消息之前需要先“订阅主题”。“订阅”在这里既是一个动作，同时还可以认为是主题在消费时的一个逻辑副本，每份订阅中，订阅者都可以接收到主题的所有消息。\n当发布-订阅模型中只有一个订阅者时，那它和队列模型就基本上是一样的了。也就是说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。\n2. 常见mq的消息模型 2.1 RabbitMQ的消息模型 RabbitMQ是少数仍坚持使用队列模型的产品之一。\n那么RabbitMQ是如何解决多个消费者的问题呢？\nRabbitMQ有一个Exchange模块，在 RabbitMQ 中，Exchange 位于生产者和队列之间，生产者并不关心将消息发送给哪个队列，而是将消息发送给 Exchange，由 Exchange 上配置的策略来决定将消息投递到哪些队列中。\n同一份消息如果需要被多个消费者来消费，需要配置 Exchange 将消息发送到多个队列，每个队列中都存放一份完整的消息数据，可以为一个消费者提供消费服务。\n2.2 RocketMQ的消息模型 RocketMQ 使用的消息模型是标准的发布-订阅模型\n但是，在 RocketMQ 也有队列（Queue）这个概念，并且队列在 RocketMQ 中是一个非常重要的概念，要了解队列在RocketMQ中的作用，我们首先要了解消息确认机制及其带来的问题\n为了确保消息不会在传递过程中由于网络或服务器故障丢失，消息队列一般采用“请求-确认”机制来确认消息的成功消费。消费者在成功消费一条消息，完成自己的业务逻辑后，会发送确认給消息队列。消息队列只有收到确认后，才认为一条消息被成功消费。\n这种机制很好地保证了消息传递过程中的可靠性，但是其带来另一个问题：在某一条消息被成功消费之前，下一条消息是不能被消费的，否则就会出现消息空洞，违背了有序性这个原则。\n也就是说，每个主题在任意时刻，至多只能有一个消费者实例在进行消费，那就没法通过水平扩展消费者的数量来提升消费端总体的消费性能。\n为了解决这个问题，RocketMQ在主题下增加了队列的概念，每个主题包含多个队列，通过多个队列来实现多实例并行生产和消费。\nRocketMQ 只在队列上保证消息的有序性，主题层面是无法保证消息的严格顺序的。\nRocketMQ 中，订阅者的概念是通过消费组（Consumer Group）来体现的。每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响，也就是说，一条消息被 Consumer Group1 消费过，也会再给 Consumer Group2 消费。\n同时，一个消费组中包含多个消费者，同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息。如果一条消息被消费者 Consumer1 消费了，那同组的其他消费者就不会再收到这条消息。\n在 Topic 的消费过程中，由于消息需要被不同的组进行多次消费，所以消费完的消息并不会立即被删除，这就需要 RocketMQ 为每个消费组在每个队列上维护一个消费位置（Consumer Offset），这个位置之前的消息都被消费过，之后的消息都没有被消费过，每成功消费一条消息，消费位置就加一。\n2.3 Kafka的消息模型 Kafka 的消息模型和 RocketMQ 是完全一样的，唯一的区别是，在 Kafka 中，队列这个概念的名称不一样，Kafka 中对应的名称是“分区（Partition）”，含义和功能是没有任何区别的。","title":"概览消息队列篇一：不同的mq的消息模型"},{"content":"1. Go汇编基础  这里只介绍本文会用到的语法\n 4个虚拟寄存器   FP: Frame pointer：伪FP寄存器对应函数的栈帧指针，一般用来访问函数的参数和返回值；golang语言中，函数的参数和返回值，函数中的局部变量，函数中调用子函数的参数和返回值都是存储在栈中的，我们把这一段栈内存称为栈帧（frame），伪FP寄存器对应栈帧的底部，但是伪FP只包括函数的参数和返回值这部分内存，其他部分由伪SP寄存器表示；注意golang中函数的返回值也是通过栈帧返回的，这也是golang函数可以有多个返回值的原因；\n  PC: Program counter：指令计数器，用于分支和跳转，它是汇编的IP寄存器的别名；\n  SB: Static base pointer：一般用于声明函数或者全局变量，对应代码区（text）内存段底部；可认为是内存的起源，所以符号foo(SB)就是名称foo作为内存中的一个地址。这种形式被用于命名全局函数和数据，如果将\u0026lt;\u0026gt;添加到名称中，如foo\u0026lt;\u0026gt;(SB)，则代表此标识符只在当前源文件中可见。可对名称添加偏移量，如foo+4(SB)指foo开头之后的四个字节。\n  SP: Stack pointer：指向当前栈帧的局部变量的开始位置，一般用来引用函数的局部变量，这里需要注意汇编中也有一个SP寄存器，它们的区别是：1.伪SP寄存器指向栈帧（不包括函数参数和返回值部分）的底部，真SP寄存器对应栈的顶部；所以伪SP寄存器一般用于寻址函数局部变量，真SP寄存器一般用于调用子函数时，寻址子函数的参数和返回值（后面会有具体示例演示）；2.当需要区分伪寄存器和真寄存器的时候只需要记住一点：伪寄存器一般需要一个标识符和偏移量为前缀，如果没有标识符前缀则是真寄存器。比如(SP)、+8(SP)没有标识符前缀为真SP寄存器，而a(SP)、b+8(SP)有标识符为前缀表示伪寄存器；\n  所有用户定义的符号都作为偏移量写入伪寄存器 FP（参数和局部变量）和 SB（全局变量）\n常量 Go汇编语言中常量以$美元符号为前缀。常量的类型有整数常量、浮点数常量、字符常量和字符串常量等几种类型。\n$1 // 十进制 $0xf4f8fcff // 十六进制 $1.5 // 浮点数 $'a' // 字符 $\u0026quot;abcd\u0026quot; DATA指令 DATA命令用于初始化包变量，DATA命令的语法如下：\nDATA symbol+offset(SB)/width, value 其中symbol为变量在汇编语言中对应的标识符，offset是符号开始地址的偏移量，width是要初始化内存的宽度大小，value是要初始化的值。其中当前包中Go语言定义的符号symbol，在汇编代码中对应·symbol，其中·中点符号为一个特殊的unicode符号；DATA命令示例如下\nDATA ·Id+0(SB)/1,$0x37 DATA ·Id+1(SB)/1,$0x25 这两条指令的含义是将全局变量Id赋值为16进制数0x2537，也就是十进制的9527； 我们也可以合并成一条指令\nGLOBL 用于将符号导出，例如将全局变量导出（所谓导出就是把汇编中的全局变量导出到go代码中声明的相同变量上，否则go代码中声明的变量感知不到汇编中变量的值的变化），其语法如下：\nGLOBL symbol(SB), width 其中symbol对应汇编中符号的名字，width为符号对应内存的大小；GLOBL命令示例如下： GLOBL ·Id, $8这条指令的含义是导出一个全局变量Id，其大小是8字节（byte）； 结合DATA和GLOBL指令，我们就可以初始化并导出一个全局变量.例如：\nGLOBL ·Id, $8 DATA ·Id+0(SB)/8,$0x12345 2. 基本数据类型 建立如下文件结构：\n├── main.go ├── go.mod └── pkg ├── pkg_amd64.s └── pkg.go int 文件内容如下:\n// pkg/pkg_amd64.s #include \u0026quot;textflag.h\u0026quot; // 使用NOPTR标志时必须导入此文件，此文件位置在：$GOROOT/src/runtime/textflag.h // var MyInt int = 1234 GLOBL ·MyInt(SB),NOPTR,$8 // 导出MyInt,NOPTR表明不包含指针 DATA ·MyInt+0(SB)/8,$1234 // 定义了一个8字节的数据，值为1234 // pkg/pkg.go  package pkg var MyInt int // main.go  package main import ( \u0026#34;simple-go/pkg\u0026#34; ) func main() { println(MyInt) } 运行main.go可得到MyInt的值为1234\nfloat // pkg/pkg_amd64.s #include \u0026quot;textflag.h\u0026quot; GLOBL ·MyFloat64(SB),NOPTR,$8 DATA ·MyFloat64+0(SB)/8,$0.01 // pkg.go  var MyFloat64 float64 string 首先需要了解一下字符串的内部标识\n// $GOROOT/src/runtime/string.go type stringStruct struct { str unsafe.Pointer len int } 因此，我们需要首先定义字符数据，再将字符串的str指向这个字符数据\n// pkg/pkg_amd64.s GLOBL NameData\u0026lt;\u0026gt;(SB),NOPTR,$8 DATA NameData\u0026lt;\u0026gt;(SB)/8,$\u0026quot;abc\u0026quot; // 定义一个非导出的数据NameData\u0026lt;\u0026gt;(SB),size为8字节 GLOBL ·MyStr0(SB),NOPTR,$16 DATA ·MyStr0+0(SB)/8,$NameData\u0026lt;\u0026gt;(SB) // MyStr的前8个字节内容为NameData\u0026lt;\u0026gt;(SB) DATA ·MyStr0+8(SB)/8,$3 // 后8个字节为3,代表字符串长度 // pkg/pkg.go  var MyString string bool // pkg/pkg_amd64.s GLOBL ·MyBool(SB),NOPTR,$1 DATA ·MyBool+0(SB)/1,$1 // pkg/pkg.go  var MyBool bool *int // 首先定义一个int变量 GLOBL IntData\u0026lt;\u0026gt;(SB),NOPTR,$8 DATA IntData\u0026lt;\u0026gt;(SB)/8,$9876 // 将指针指向int变量 GLOBL ·MyIntPtr(SB),NOPTR,$8 DATA ·MyIntPtr+0(SB)/8,$IntData\u0026lt;\u0026gt;(SB) var MyIntPtr *int 3. 复合数据类型 数组 // pkg/pkg_amd64.s // array [2]int = {12, 34} GLOBL ·MyArray(SB),NOPTR,$16 DATA ·MyArray+0(SB)/8,$12 DATA ·MyArray+8(SB)/8,$34 // pkg/pkg.go  var MyArray [2]int 切片 首先需要了解切片的内部结构\n// $GOROOT/src/runtime/slice.go type slice struct { array unsafe.Pointer len int cap int } 切片是截取数组的一部分得来的，因此要先定义一个数组，然后将切片的array指针指向这个数组的某个偏移\n// pkg/pkg_amd64.s // 定义三个string临时变量，作为切片元素 GLOBL str0\u0026lt;\u0026gt;(SB),NOPTR,$40 DATA str0\u0026lt;\u0026gt;(SB)/40,$\u0026quot;Thoughts in the Still of the Night\u0026quot; GLOBL str1\u0026lt;\u0026gt;(SB),NOPTR,$40 DATA str1\u0026lt;\u0026gt;(SB)/40,$\u0026quot;A pool of moonlight before the bed\u0026quot; GLOBL str2\u0026lt;\u0026gt;(SB),NOPTR,$8 DATA str2\u0026lt;\u0026gt;(SB)/8,$\u0026quot;libai\u0026quot; // 定义一个[3]string的数组，元素就是上面的三个string变量 GLOBL strarray\u0026lt;\u0026gt;(SB),NOPTR,$48 DATA strarray\u0026lt;\u0026gt;+0(SB)/8,$str0\u0026lt;\u0026gt;(SB) DATA strarray\u0026lt;\u0026gt;+8(SB)/8,$34 DATA strarray\u0026lt;\u0026gt;+16(SB)/8,$str1\u0026lt;\u0026gt;(SB) DATA strarray\u0026lt;\u0026gt;+24(SB)/8,$34 DATA strarray\u0026lt;\u0026gt;+32(SB)/8,$str2\u0026lt;\u0026gt;(SB) DATA strarray\u0026lt;\u0026gt;+40(SB)/8,$5 // var MySlice []string GLOBL ·MySlice(SB),NOPTR,$24 // 上面[3]string数组的首地址用来初始化切片的Data字段 DATA ·MySlice+0(SB)/8,$strarray\u0026lt;\u0026gt;(SB) DATA ·MySlice+8(SB)/8,$3 DATA ·MySlice+16(SB)/8,$4 上面的切片是截取了全部的数组元素，如果想要从第二个开始截取，可增加偏移：\nDATA ·MySlice+0(SB)/8,$strarray\u0026lt;\u0026gt;+16(SB) map/chan map/channel等类型并没有公开的内部结构，它们只是一种未知类型的指针，无法直接初始化。在汇编代码中我们只能为类似变量定义并进行0值初始化：\nvar m map[string]int var ch chan int GLOBL ·m(SB),$8 // var m map[string]int DATA ·m+0(SB)/8,$0 GLOBL ·ch(SB),$8 // var ch chan int DATA ·ch+0(SB)/8,$0 References Golang学习笔记-汇编\nmap的实现原理\nchan\nA Quick Guide to Go\u0026rsquo;s Assembler\nGo语言高级编程\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/go%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","summary":"1. Go汇编基础  这里只介绍本文会用到的语法\n 4个虚拟寄存器   FP: Frame pointer：伪FP寄存器对应函数的栈帧指针，一般用来访问函数的参数和返回值；golang语言中，函数的参数和返回值，函数中的局部变量，函数中调用子函数的参数和返回值都是存储在栈中的，我们把这一段栈内存称为栈帧（frame），伪FP寄存器对应栈帧的底部，但是伪FP只包括函数的参数和返回值这部分内存，其他部分由伪SP寄存器表示；注意golang中函数的返回值也是通过栈帧返回的，这也是golang函数可以有多个返回值的原因；\n  PC: Program counter：指令计数器，用于分支和跳转，它是汇编的IP寄存器的别名；\n  SB: Static base pointer：一般用于声明函数或者全局变量，对应代码区（text）内存段底部；可认为是内存的起源，所以符号foo(SB)就是名称foo作为内存中的一个地址。这种形式被用于命名全局函数和数据，如果将\u0026lt;\u0026gt;添加到名称中，如foo\u0026lt;\u0026gt;(SB)，则代表此标识符只在当前源文件中可见。可对名称添加偏移量，如foo+4(SB)指foo开头之后的四个字节。\n  SP: Stack pointer：指向当前栈帧的局部变量的开始位置，一般用来引用函数的局部变量，这里需要注意汇编中也有一个SP寄存器，它们的区别是：1.伪SP寄存器指向栈帧（不包括函数参数和返回值部分）的底部，真SP寄存器对应栈的顶部；所以伪SP寄存器一般用于寻址函数局部变量，真SP寄存器一般用于调用子函数时，寻址子函数的参数和返回值（后面会有具体示例演示）；2.当需要区分伪寄存器和真寄存器的时候只需要记住一点：伪寄存器一般需要一个标识符和偏移量为前缀，如果没有标识符前缀则是真寄存器。比如(SP)、+8(SP)没有标识符前缀为真SP寄存器，而a(SP)、b+8(SP)有标识符为前缀表示伪寄存器；\n  所有用户定义的符号都作为偏移量写入伪寄存器 FP（参数和局部变量）和 SB（全局变量）\n常量 Go汇编语言中常量以$美元符号为前缀。常量的类型有整数常量、浮点数常量、字符常量和字符串常量等几种类型。\n$1 // 十进制 $0xf4f8fcff // 十六进制 $1.5 // 浮点数 $'a' // 字符 $\u0026quot;abcd\u0026quot; DATA指令 DATA命令用于初始化包变量，DATA命令的语法如下：\nDATA symbol+offset(SB)/width, value 其中symbol为变量在汇编语言中对应的标识符，offset是符号开始地址的偏移量，width是要初始化内存的宽度大小，value是要初始化的值。其中当前包中Go语言定义的符号symbol，在汇编代码中对应·symbol，其中·中点符号为一个特殊的unicode符号；DATA命令示例如下\nDATA ·Id+0(SB)/1,$0x37 DATA ·Id+1(SB)/1,$0x25 这两条指令的含义是将全局变量Id赋值为16进制数0x2537，也就是十进制的9527； 我们也可以合并成一条指令\nGLOBL 用于将符号导出，例如将全局变量导出（所谓导出就是把汇编中的全局变量导出到go代码中声明的相同变量上，否则go代码中声明的变量感知不到汇编中变量的值的变化），其语法如下：\nGLOBL symbol(SB), width 其中symbol对应汇编中符号的名字，width为符号对应内存的大小；GLOBL命令示例如下： GLOBL ·Id, $8这条指令的含义是导出一个全局变量Id，其大小是8字节（byte）； 结合DATA和GLOBL指令，我们就可以初始化并导出一个全局变量.例如：\nGLOBL ·Id, $8 DATA ·Id+0(SB)/8,$0x12345 2.","title":"Go汇编之定义基本数据类型"},{"content":"1. Cgroup概览 cgroup是Linux内核提供的一种按层次组织进程，并对进程资源按层次进行分配和限制的机制。\ncgroup 主要由两部分组成——core和controller。core主要负责分层组织进程。 controller负责为属于当前cgroup的进程分配和限制资源.\n多个cgroup以树形结构组织，系统中每个进程都属于一个cgroup，一个进程中的所有线程都属于同一个cgroup。\ncontroller可以在cgroup上有选择的开启，开启后的controller将影响这个cgroup内的所有进程。\n使用如下命令挂载cgroupv2\nmount -t cgroup2 none $MOUNT_POINT # MOUNT_POINT 是任意你想要挂载到的位置 直接在$MOUNT_POINT创建一个文件夹即可创建一个cgroup\nmkdir $MOUNT_POINT/$GROUP_NAME 每个cgroup内都有一个cgroup.procs接口文件，其中逐行列出了属于当前cgroup的所有进程的PID。需要注意的是，PID可能重复出现且无序。\n若想将某个进程移动到一个cgroup中，只需将其PID写入cgroup.procs文件,进程中的所有线程也会迁移到该cgroup中。fork出的子进程依然属于这个cgroup。\n若要删除一个cgroup，需要注意一点：这个cgroup内需要没有任何子进程且仅与僵尸进程相关联，且没有子cgroup.满足了上述条件后，将其作为一个空目录删除即可,使用rm -rf无法对cgroup目录进行删除。\nrmdir $MOUNT_POINT/$GROUP_NAME /proc/$PID/cgroup中包含一个进程所属的cgroup。\n$ cat /proc/self/cgroup # self表示当前shell进程 0::/user.slice/user-1000.slice/session-2.scope 如果进程成为僵尸进程并且随后删除了与之关联的 cgroup，则将“（已删除）”附加到路径中：\n$ cat /proc/842/cgroup 0::/test-cgroup/test-cgroup-nested (deleted) CgroupV2还支持线程模式\n每个非根 cgroup 都有一个cgroup.events文件，其中包含populated字段，指示 cgroup 的子层次结构中是否有实时进程。 如果 cgroup 及其后代中没有实时进程，则其值为 0； 否则为1.\n例如：考虑如下cgroup结构，括号内数字代表cgroup内进程数：\nA(4) - B(0) - C(1) \\ D(0) 则A、B 和 C 的populated字段将为 1，而 D 为 0。在 C 中的一个进程退出后，B 和 C 的populated字段将翻转为“0”，文件修改事件将在两个cgroup的cgroup.events中出现。\n每个 cgroup 都有一个“cgroup.controllers”文件，其中列出了 cgroup 可启用的所有控制器：\n$ cat cgroup.controllers cpu io memory 默认情况下不启用任何控制器。可以通过写入cgroup.subtree_control文件来启用和禁用控制器：\n$ echo \u0026#34;+cpu +memory -io\u0026#34; \u0026gt; cgroup.subtree_control # 开启cpu、memory 控制器， 关闭io控制器 资源是自上而下分布的，只有当资源从父级分发给它时，cgroup 才能进一步分发资源。父cgroup可以限制子cgroup中所包含的controller。只有包含在父cgroup的cgroup.subtree_control中的controller才能分配給子cgroup。\n进程只能分配到根cgroup和叶子cgroup。\n2. core接口文件  cgroup.type\n 存在于非根cgroup中的可读写单值文件。指示当前cgroup的类型。 其可能是以下值： domain: 当前cgroup是一个正常的domain cgroup\ndomain threaded: 一个线程域 cgroup，用作线程子树的根。\ndomain invalid: 处在非法状态的cgroup，不能包含实时进程或controller，但允许成为一个threadedcgroup.\nthreaded: 一个线程化的 cgroup，它是线程化子树的成员。\n可直接向此文件写入threaded使当前cgroup成为一个线程cgroup\n cgroup.procs\n 按行列出当前cgroup中包含的进程PID. PID无序，且可能重复。可直接向此文件写入某个PID以实现将进程迁移进当前cgroup。\n cgroup.threads\n 同cgroup.procs,但其中列出的数字为线程TID\n cgroup.controllers\n 列出当前cgroup中启用的controller\n cgroup.subtree_control\n 指出子cgroup可启用的controller。可通过以下命令启用或关闭某个controller\n$ echo \u0026#34;+cpu +memory -io\u0026#34; \u0026gt; cgroup.subtree_control # 开启cpu、memory 控制器， 关闭io控制器  cgroup.events\n 只读文件.populated指示cgroup中是否存在存活进程，frozen指示 cgroup是否被冻结\n cgroup.max.descendants\n 允许的最大后代cgroup数量。\n cgroup.max.depth\n 最大cgroup树深度。\n cgroup.stat\n 只读文件。\nnr_descendants： 可见后代cgroup的总数 nr_dying_descendants: 待死亡的后代 cgroup 总数。 一个 cgroup 在被用户删除后会死掉。 在完全销毁之前，cgroup 将保持在死亡状态一段时间（可能取决于系统负载）。\n进程不能加入待死亡的cgroup. 待死亡的 cgroup 可以消耗不超过限制的系统资源，这些资源在 cgroup 删除时处于活动状态。\n cgroup.freeze\n 存在于非根cgroup。允许的值为1或0，默认为0.\n当值为1时，代表冻结这个cgroup。冻结一个cgroup，其中的所有进程将不再运行，直到这个cgroup解冻。对于冻结的cgroup，其子cgroup也将冻结。\n cgroup.kill\n 只写文件，唯一允许值为1.向这个文件写入1将导致这个cgroup以及所有子cgroup被kill。这将导致当前cgroup中的所有进程都被SIGKILL信号杀死。\n3. cpu controller  cpu.stat\n 只读键值文件，在cpu controller开启时才会出现。\nusage_usec：占用cpu总时间。\nuser_usec：用户态占用时间。\nsystem_usec：内核态占用时间。\nnr_periods：周期计数。\nnr_throttled：周期内的限制计数。\nthrottled_usec：限制执行的时间。\n cpu.weight\n 只存在于非根cgroup，默认值100. 值域为[1, 10000]\n cpu.weight.nice\n 只存在于非根cgroup，默认值0, 值域为[-20, 19]\n cpu.max\n 其中包含空格分隔的两个值，$MAX $PERIOD，表示在每个$PERIOD时期内，可消耗的cpu为$MAX，当$MAX为max时，代表无限制。\n cpu.max.burst\n 默认值为0，burst值域为[0, $MAX]\n cpu.pressure\n 显示当前cgroup的cpu使用压力状态。详情参见：Documentation/accounting/psi.rst。psi是内核新加入的一种负载状态检测机制，可以目前可以针对cpu、memory、io的负载状态进行检测。通过设置，我们可以让psi在相关资源负载达到一定阈值的情况下给我们发送一个事件。用户态可以通过对文件事件的监控，实现针对相关负载作出相关相应行为的目的。\n cpu.uclamp.min\n uclamp提供了一种用户空间对于task util进行限制的机制，通过该机制用户空间可以将task util钳制在[util_min, util_max]范围内，而cpu util则由处于其运行队列上的task的uclamp值决定.\n这个文件表明了对cpu利用率的最小限制。其数字为百分比数字。\n cpu.uclamp.max\n 设置了cpu的最大利用率\n4. memory controller References   浅谈 Cgroups V2\n  详解Cgroup V2\n  The Linux Kernel · Control Group v2\n  ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/cgroupv2/","summary":"1. Cgroup概览 cgroup是Linux内核提供的一种按层次组织进程，并对进程资源按层次进行分配和限制的机制。\ncgroup 主要由两部分组成——core和controller。core主要负责分层组织进程。 controller负责为属于当前cgroup的进程分配和限制资源.\n多个cgroup以树形结构组织，系统中每个进程都属于一个cgroup，一个进程中的所有线程都属于同一个cgroup。\ncontroller可以在cgroup上有选择的开启，开启后的controller将影响这个cgroup内的所有进程。\n使用如下命令挂载cgroupv2\nmount -t cgroup2 none $MOUNT_POINT # MOUNT_POINT 是任意你想要挂载到的位置 直接在$MOUNT_POINT创建一个文件夹即可创建一个cgroup\nmkdir $MOUNT_POINT/$GROUP_NAME 每个cgroup内都有一个cgroup.procs接口文件，其中逐行列出了属于当前cgroup的所有进程的PID。需要注意的是，PID可能重复出现且无序。\n若想将某个进程移动到一个cgroup中，只需将其PID写入cgroup.procs文件,进程中的所有线程也会迁移到该cgroup中。fork出的子进程依然属于这个cgroup。\n若要删除一个cgroup，需要注意一点：这个cgroup内需要没有任何子进程且仅与僵尸进程相关联，且没有子cgroup.满足了上述条件后，将其作为一个空目录删除即可,使用rm -rf无法对cgroup目录进行删除。\nrmdir $MOUNT_POINT/$GROUP_NAME /proc/$PID/cgroup中包含一个进程所属的cgroup。\n$ cat /proc/self/cgroup # self表示当前shell进程 0::/user.slice/user-1000.slice/session-2.scope 如果进程成为僵尸进程并且随后删除了与之关联的 cgroup，则将“（已删除）”附加到路径中：\n$ cat /proc/842/cgroup 0::/test-cgroup/test-cgroup-nested (deleted) CgroupV2还支持线程模式\n每个非根 cgroup 都有一个cgroup.events文件，其中包含populated字段，指示 cgroup 的子层次结构中是否有实时进程。 如果 cgroup 及其后代中没有实时进程，则其值为 0； 否则为1.\n例如：考虑如下cgroup结构，括号内数字代表cgroup内进程数：\nA(4) - B(0) - C(1) \\ D(0) 则A、B 和 C 的populated字段将为 1，而 D 为 0。在 C 中的一个进程退出后，B 和 C 的populated字段将翻转为“0”，文件修改事件将在两个cgroup的cgroup.","title":"CgroupV2"},{"content":"1. Linux 中的信号 信号这个概念在很早期的 Unix 系统上就有。它一般会从 1 开始编号，通常来说，信号编号是 1 到 31，这个编号在所有的 Unix 系统上都是一样的。\n   取值 名称 解释 默认动作     1 SIGHUP 挂起    2 SIGINT 中断    3 SIGQUIT 退出    4 SIGILL 非法指令    5 SIGTRAP 断点或陷阱指令    6 SIGABRT abort发出的信号    7 SIGBUS 非法内存访问    8 SIGFPE 浮点异常    9 SIGKILL kill信号 不能被忽略、处理和阻塞   10 SIGUSR1 用户信号1    11 SIGSEGV 无效内存访问    12 SIGUSR2 用户信号2    13 SIGPIPE 管道破损，没有读端的管道写数据    14 SIGALRM alarm发出的信号    15 SIGTERM 终止信号    16 SIGSTKFLT 栈溢出    17 SIGCHLD 子进程退出 默认忽略   18 SIGCONT 进程继续    19 SIGSTOP 进程停止 不能被忽略、处理和阻塞   20 SIGTSTP 进程停止    21 SIGTTIN 进程停止，后台进程从终端读数据时    22 SIGTTOU 进程停止，后台进程想终端写数据时    23 SIGURG I/O有紧急数据到达当前进程 默认忽略   24 SIGXCPU 进程的CPU时间片到期    25 SIGXFSZ 文件大小的超出上限    26 SIGVTALRM 虚拟时钟超时    27 SIGPROF profile时钟超时    28 SIGWINCH 窗口大小改变 默认忽略   29 SIGIO I/O相关    30 SIGPWR 关机 默认忽略   31 SIGSYS 系统调用异常     用一句话来概括，信号（Signal）其实就是 Linux 进程收到的一个通知。这些通知产生的源头有很多种，通知的类型也有很多种。比如下面这几个典型的场景：\n 如果按下键盘“Ctrl+C”，当前运行的进程就会收到一个信号 SIGINT 而退出； 如果代码写得有问题，导致内存访问出错了，当前的进程就会收到另一个信号 SIGSEGV； 也可以通过命令 kill ，直接向一个进程发送一个信号，缺省情况下不指定信号的类型，那么这个信号就是 SIGTERM。也可以指定信号类型，比如命令 \u0026ldquo;kill -9 \u0026ldquo;, 这里的 9，就是编号为 9 的信号，SIGKILL 信号。  2. Linux内核对信号的处理是怎么样的 进程在收到信号后，就会去做相应的处理。对于每一个信号，进程对它的处理都有下面三个选择。\n 第一个选择是忽略（Ignore），就是对这个信号不做任何处理，但是有两个信号例外，对于 SIGKILL 和 SIGSTOP 这个两个信号，进程是不能忽略的。这是因为它们的主要作用是为 Linux kernel 和超级用户提供删除任意进程的特权。 第二个选择，就是捕获（Catch），这个是指让用户进程可以注册自己针对这个信号的 handler。对于捕获，SIGKILL 和 SIGSTOP 这两个信号也同样例外，这两个信号不能有用户自己的处理代码，只能执行系统的缺省行为。 还有一个选择是缺省行为（Default），Linux 为每个信号都定义了一个缺省的行为，可以在 Linux 系统中运行 man 7 signal来查看每个信号的缺省行为。  对于大部分的信号而言，应用程序不需要注册自己的 handler，使用系统缺省定义行为就可以了。\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/linux%E4%B8%AD%E7%9A%84%E4%BF%A1%E5%8F%B7/","summary":"1. Linux 中的信号 信号这个概念在很早期的 Unix 系统上就有。它一般会从 1 开始编号，通常来说，信号编号是 1 到 31，这个编号在所有的 Unix 系统上都是一样的。\n   取值 名称 解释 默认动作     1 SIGHUP 挂起    2 SIGINT 中断    3 SIGQUIT 退出    4 SIGILL 非法指令    5 SIGTRAP 断点或陷阱指令    6 SIGABRT abort发出的信号    7 SIGBUS 非法内存访问    8 SIGFPE 浮点异常    9 SIGKILL kill信号 不能被忽略、处理和阻塞   10 SIGUSR1 用户信号1    11 SIGSEGV 无效内存访问    12 SIGUSR2 用户信号2    13 SIGPIPE 管道破损，没有读端的管道写数据    14 SIGALRM alarm发出的信号    15 SIGTERM 终止信号    16 SIGSTKFLT 栈溢出    17 SIGCHLD 子进程退出 默认忽略   18 SIGCONT 进程继续    19 SIGSTOP 进程停止 不能被忽略、处理和阻塞   20 SIGTSTP 进程停止    21 SIGTTIN 进程停止，后台进程从终端读数据时    22 SIGTTOU 进程停止，后台进程想终端写数据时    23 SIGURG I/O有紧急数据到达当前进程 默认忽略   24 SIGXCPU 进程的CPU时间片到期    25 SIGXFSZ 文件大小的超出上限    26 SIGVTALRM 虚拟时钟超时    27 SIGPROF profile时钟超时    28 SIGWINCH 窗口大小改变 默认忽略   29 SIGIO I/O相关    30 SIGPWR 关机 默认忽略   31 SIGSYS 系统调用异常     用一句话来概括，信号（Signal）其实就是 Linux 进程收到的一个通知。这些通知产生的源头有很多种，通知的类型也有很多种。比如下面这几个典型的场景：","title":"Linux中的信号"},{"content":"proxy 见：https://yangchnet.github.io/Dessert/posts/env/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AEclash/\n系统更新 首先要换源\nsudo pacman-mirrors -i -c China -m rank 在弹出的窗口中选择你要切换的源。\n然后\nsudo pacman -Syyu 安装yay包管理\nsudo pacman -S yay vim 配置 见：https://yangchnet.github.io/Dessert/posts/linux/vim%E9%85%8D%E7%BD%AE/\n输入法配置 安装fcitx5（输入法框架）\nyay -S fcitx5-im 配置fcitx5的环境变量：\nvim ~/.pam_environment 内容为：\nGTK_IM_MODULE DEFAULT=fcitx QT_IM_MODULE DEFAULT=fcitx XMODIFIERS DEFAULT=\\@im=fcitx SDL_IM_MODULE DEFAULT=fcitx 安装fcitx5-rime（输入法引擎）\nyay -S fcitx5-rime 安装fcitx5-gtk\nyay -S fcitx5-gtk # 不装的话，部分软件可能会出现不能输入中文的情况 安装rime-cloverpinyin（输入方案）\nyay -S rime-cloverpinyin 如果出现问题可能还需要做下面这步：\nyay -S base-devel 创建并写入rime-cloverpinyin的输入方案：\nvim ~/.local/share/fcitx5/rime/default.custom.yaml 内容为：\npatch: \u0026#34;menu/page_size\u0026#34;: 5 schema_list: - schema: clover  可参考：https://github.com/fkxxyz/rime-cloverpinyin/wiki/linux\n 配置主题\n参考：https://github.com/hosxy/Fcitx5-Material-Color\n配置命令行工具 见：https://yangchnet.github.io/Dessert/posts/env/zsh%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/\n社交工具 微信\nyay -S deepin-wine-wechat 或（暂不可用）\nyay -S com.qq.weixin.spark QQ\nyay -S deepin-wine-tim 或\nyay -S linuxqq 或icalingua(自行在github搜索)\nVirtualbox sudo pacman -S virtualbox sudo pacman -S linux-headers # 安装对应自己内核版本的一个 sudo pacman -S virtualbox-host-dkms sudo modprobe vboxdrv Reference https://zhuanlan.zhihu.com/p/114296129?ivk_sa=1024320u\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/manjaro%E5%88%9D%E5%A7%8B%E5%8C%96/","summary":"proxy 见：https://yangchnet.github.io/Dessert/posts/env/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AEclash/\n系统更新 首先要换源\nsudo pacman-mirrors -i -c China -m rank 在弹出的窗口中选择你要切换的源。\n然后\nsudo pacman -Syyu 安装yay包管理\nsudo pacman -S yay vim 配置 见：https://yangchnet.github.io/Dessert/posts/linux/vim%E9%85%8D%E7%BD%AE/\n输入法配置 安装fcitx5（输入法框架）\nyay -S fcitx5-im 配置fcitx5的环境变量：\nvim ~/.pam_environment 内容为：\nGTK_IM_MODULE DEFAULT=fcitx QT_IM_MODULE DEFAULT=fcitx XMODIFIERS DEFAULT=\\@im=fcitx SDL_IM_MODULE DEFAULT=fcitx 安装fcitx5-rime（输入法引擎）\nyay -S fcitx5-rime 安装fcitx5-gtk\nyay -S fcitx5-gtk # 不装的话，部分软件可能会出现不能输入中文的情况 安装rime-cloverpinyin（输入方案）\nyay -S rime-cloverpinyin 如果出现问题可能还需要做下面这步：\nyay -S base-devel 创建并写入rime-cloverpinyin的输入方案：\nvim ~/.local/share/fcitx5/rime/default.custom.yaml 内容为：\npatch: \u0026#34;menu/page_size\u0026#34;: 5 schema_list: - schema: clover  可参考：https://github.","title":"manjaro初始化"},{"content":"# 编译环境FROMgolang:alpine as builder # 设置go环境变量ENV GO111MODULE=on \\  GOPROXY=https://goproxy.cn,direct# 工作目录WORKDIR/app# 将项目拷贝到docker中COPY . .# 拉取包，编译RUN go mod tidy \u0026amp;\u0026amp; CGO_ENABLED=0 GOOS=linux go build -a -ldflags \u0026#39;-extldflags \u0026#34;-static\u0026#34;\u0026#39; -o hello-app .# 运行环境FROMscratch# 设置时区COPY --from=builder /usr/share/zoneinfo/Asia/Shanghai /usr/share/zoneinfo/Asia/ShanghaiENV TZ Asia/ShanghaiWORKDIR/app# 将编译好的可执行文件从编译环境中拷贝到运行环境中COPY --from=builder /app/hello-app .# 启动ENTRYPOINT [\u0026#34;./hello-app\u0026#34;]# 端口EXPOSE10000","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E4%B8%80%E4%BB%BD%E5%A5%BD%E7%94%A8%E7%9A%84golang%E5%BA%94%E7%94%A8dockerfile%E6%A8%A1%E6%9D%BF/","summary":"# 编译环境FROMgolang:alpine as builder # 设置go环境变量ENV GO111MODULE=on \\  GOPROXY=https://goproxy.cn,direct# 工作目录WORKDIR/app# 将项目拷贝到docker中COPY . .# 拉取包，编译RUN go mod tidy \u0026amp;\u0026amp; CGO_ENABLED=0 GOOS=linux go build -a -ldflags \u0026#39;-extldflags \u0026#34;-static\u0026#34;\u0026#39; -o hello-app .# 运行环境FROMscratch# 设置时区COPY --from=builder /usr/share/zoneinfo/Asia/Shanghai /usr/share/zoneinfo/Asia/ShanghaiENV TZ Asia/ShanghaiWORKDIR/app# 将编译好的可执行文件从编译环境中拷贝到运行环境中COPY --from=builder /app/hello-app .# 启动ENTRYPOINT [\u0026#34;./hello-app\u0026#34;]# 端口EXPOSE10000","title":"一份好用的golang应用Dockerfile模板"},{"content":"1.概述 Linux 是多用户多任务操作系统，换句话说，Linux 系统支持多个用户在同一时间内登陆，不同用户可以执行不同的任务，并且互不影响。\n例如，某台 Linux 服务器上有 4 个用户，分别是 root、www、ftp 和 mysql，在同一时间内，root 用户可能在查看系统日志、管理维护系统；www 用户可能在修改自己的网页程序；ftp 用户可能在上传软件到服务器；mysql 用户可能在执行自己的 SQL 查询，每个用户互不干扰，有条不紊地进行着自己的工作。与此同时，每个用户之间不能越权访问，比如 www 用户不能执行 mysql 用户的 SQL 查询操作，ftp 用户也不能修改 www 用户的网页程序。\n不同用户具有不问的权限，毎个用户在权限允许的范围内完成不间的任务，Linux 正是通过这种权限的划分与管理，实现了多用户多任务的运行机制。\n因此，如果要使用 Linux 系统的资源，就必须向系统管理员申请一个账户，然后通过这个账户进入系统（账户和用户是一个概念）。通过建立不同属性的用户，一方面可以合理地利用和控制系统资源，另一方面也可以帮助用户组织文件，提供对用户文件的安全性保护。\n每个用户都有唯一的用户名和密码。在登录系统时，只有正确输入用户名和密码，才能进入系统和自己的主目录。\n用户组是具有相同特征用户的逻辑集合。简单的理解，有时我们需要让多个用户具有相同的权限，比如查看、修改某一个文件的权限，一种方法是分别对多个用户进行文件访问授权，如果有 10 个用户的话，就需要授权 10 次，那如果有 100、1000 甚至更多的用户呢？\n显然，这种方法不太合理。最好的方式是建立一个组，让这个组具有查看、修改此文件的权限，然后将所有需要访问此文件的用户放入这个组中。那么，所有用户就具有了和组一样的权限，这就是用户组。\n将用户分组是 Linux 系统中对用户进行管理及控制访问权限的一种手段，通过定义用户组，很多程序上简化了对用户的管理工作。\n2.用户和用户组的关系 用户和用户组的对应关系有以下 4 种：\n一对一：一个用户可以存在一个组中，是组中的唯一成员； 一对多：一个用户可以存在多个用户组中，此用户具有这多个组的共同权限； 多对一：多个用户可以存在一个组中，这些用户具有和组相同的权限； 多对多：多个用户可以存在多个组中，也就是以上 3 种关系的扩展。 用户和组之间的关系可用下图表示：图 1 Linux用户和用户组 3. UID和GID(用户ID和组ID) 登陆 Linux 系统时，虽然输入的是自己的用户名和密码，但其实 Linux 并不认识你的用户名称，它只认识用户名对应的 ID 号（也就是一串数字）。Linux 系统将所有用户的名称与 ID 的对应关系都存储在 /etc/passwd 文件中。\n说白了，用户名并无实际作用，仅是为了方便用户的记忆而已。\n要论证 \u0026ldquo;Linux系统不认识用户名\u0026rdquo; 也很简单，在前面章节，我们曾经在网络上下载过 \u0026ldquo;.tar.gz\u0026rdquo; 或 \u0026ldquo;.tar.bz2\u0026rdquo; 格式的文件，在解压缩之后的文件中，你会发现文件拥有者的属性显示的是一串数字，这很正常，就是因为系统只认识代表你身份的 ID，这串数字就是用户的 ID（UID）号。\nLinux 系统中，每个用户的 ID 细分为 2 种，分别是用户 ID（User ID，简称 UID）和组 ID（Group ID，简称 GID），这与文件有拥有者和拥有群组两种属性相对应（如下图所示）。\n每个文件都有自己的拥有者 ID 和群组 ID，当显示文件属性时，系统会根据 /etc/passwd 和 /etc/group 文件中的内容，分别找到 UID 和 GID 对应的用户名和群组名，然后显示出来。\n 使用cat命令，查看/etc/passwd文件，查看当前用户和root用户的UID和GID\n 4. /etc/passwd内容解释 Linux 系统中的 /etc/passwd 文件，是系统用户配置文件，存储了系统中所有用户的基本信息，并且所有用户都可以对此文件执行读操作。 可以看到，/etc/passwd 文件中的内容非常规律，每行记录对应一个用户。\n为什么Linux 系统中默认怎么会有这么多的用户？ 这些用户中的绝大多数是系统或服务正常运行所必需的用户，这种用户通常称为系统用户或伪用户。系统用户无法用来登录系统，但也不能删除，因为一旦删除，依赖这些用户运行的服务或程序就不能正常执行，会导致系统问题。\n不仅如此，每行用户信息都以 \u0026ldquo;：\u0026rdquo; 作为分隔符，划分为 7 个字段，每个字段所表示的含义如下： 用户名：密码：UID（用户ID）：GID（组ID）：描述性信息：主目录：默认Shell\n接下来，给大家逐个介绍这些字段。\n1). 用户名\n用户名，就是一串代表用户身份的字符串。\n前面讲过，用户名仅是为了方便用户记忆，Linux 系统是通过 UID 来识别用户身份，分配用户权限的。/etc/passwd 文件中就定义了用户名和 UID 之间的对应关系。\n2). 密码 \u0026ldquo;x\u0026rdquo; 表示此用户设有密码，但不是真正的密码，真正的密码保存在 /etc/shadow 文件中（下一节做详细介绍）。\n在早期的 UNIX 中，这里保存的就是真正的加密密码串，但由于所有程序都能读取此文件，非常容易造成用户数据被窃取。 虽然密码是加密的，但是采用暴力破解的方式也是能够进行破解的。\n因此，现在 Linux 系统把真正的加密密码串放置在 /etc/shadow 文件中，此文件只有 root 用户可以浏览和操作，这样就最大限度地保证了密码的安全。\n需要注意的是，虽然 \u0026ldquo;x\u0026rdquo; 并不表示真正的密码，但也不能删除，如果删除了 \u0026ldquo;x\u0026rdquo;，那么系统会认为这个用户没有密码，从而导致只输入用户名而不用输入密码就可以登陆（只能在使用无密码登录，远程是不可以的），除非特殊情况（如破解用户密码），这当然是不可行的。\n3). UID UID，也就是用户 ID。每个用户都有唯一的一个 UID，Linux 系统通过 UID 来识别不同的用户。\n实际上，UID 就是一个 0~65535 之间的数，不同范围的数字表示不同的用户身份，具体如表 1 所示。\n   UID范围 用户身份     0 超级用户。UID 为 0 就代表这个账号是管理员账号。在 Linux 中，如何把普通用户升级成管理员呢？只需把其他用户的 UID 修改为 0 就可以了，这一点和 Windows 是不同的。不过不建议建立多个管理员账号。   1-499 系统用户（伪用户）。也就是说，此范围的 UID 保留给系统使用。其中，1~99 用于系统自行创建的账号；100~499 分配给有系统账号需求的用户。其实，除了 0 之外，其他的 UID 并无不同，这里只是默认 500 以下的数字给系统作为保留账户，只是一个公认的习惯而已。   500-65535 普通用户。通常这些 UID 已经足够用户使用了。但不够用也没关系，2.6.x 内核之后的 Linux 系统已经可以支持 232 个 UID 了    4). GID 全称“Group ID”，简称“组ID”，表示用户初始组的组 ID 号。这里需要解释一下初始组和附加组的概念。\n初始组，指用户登陆时就拥有这个用户组的相关权限。每个用户的初始组只能有一个，通常就是将和此用户的用户名相同的组名作为该用户的初始组。比如说，我们手工添加用户 lamp，在建立用户 lamp 的同时，就会建立 lamp 组作为 lamp 用户的初始组。\n附加组，指用户可以加入多个其他的用户组，并拥有这些组的权限。每个用户只能有一个初始组，除初始组外，用户再加入其他的用户组，这些用户组就是这个用户的附加组。附加组可以有多个，而且用户可以有这些附加组的权限。\n举例来说，刚刚的 lamp 用户除属于初始组 lamp 外，我又把它加入了 users 组，那么 lamp 用户同时属于 lamp 组和 users 组，其中 lamp 是初始组，users 是附加组。\n当然，初始组和附加组的身份是可以修改的，但是我们在工作中不修改初始组，只修改附加组，因为修改了初始组有时会让管理员逻辑混乱。\n需要注意的是，在 /etc/passwd 文件的第四个字段中看到的 ID 是这个用户的初始组。\n5). 描述性信息 这个字段并没有什么重要的用途，只是用来解释这个用户的意义而已。\n6). 主目录 也就是用户登录后有操作权限的访问目录，通常称为用户的主目录。\n例如，root 超级管理员账户的主目录为 /root，普通用户的主目录为 /home/yourIDname，即在 /home/ 目录下建立和用户名相同的目录作为主目录，如 lamp 用户的主目录就是 /home/lamp/ 目录。\n7). 默认的Shell Shell 就是 Linux 的命令解释器，是用户和 Linux 内核之间沟通的桥梁。\n我们知道，用户登陆 Linux 系统后，通过使用 Linux 命令完成操作任务，但系统只认识类似 0101 的机器语言，这里就需要使用命令解释器。也就是说，Shell 命令解释器的功能就是将用户输入的命令转换成系统可以识别的机器语言。\n通常情况下，Linux 系统默认使用的命令解释器是 bash（/bin/bash），当然还有其他命令解释器，例如 sh、csh 等。\n5. /etc/shadow内容解释 /etc/shadow 文件，用于存储 Linux 系统中用户的密码信息，又称为“影子文件”。\n前面介绍了 /etc/passwd 文件，由于该文件允许所有用户读取，易导致用户密码泄露，因此 Linux 系统将用户的密码信息从 /etc/passwd 文件中分离出来，并单独放到了此文件中。\n/etc/shadow 文件只有 root 用户拥有读权限，其他用户没有任何权限，这样就保证了用户密码的安全性。 注意，如果这个文件的权限发生了改变，则需要注意是否是恶意攻击。\n介绍此文件之前，我们先打开看看，执行如下命令：\n[root@localhost ~]#vim /etc/shadow root: $6$9w5Td6lg $bgpsy3olsq9WwWvS5Sst2W3ZiJpuCGDY.4w4MRk3ob/i85fl38RH15wzVoom ff9isV1 PzdcXmixzhnMVhMxbvO:15775:0:99999:7::: bin:*:15513:0:99999:7::: daemon:*:15513:0:99999:7::: …省略部分输出… 同 /etc/passwd 文件一样，文件中每行代表一个用户，同样使用 \u0026ldquo;:\u0026rdquo; 作为分隔符，不同之处在于，每行用户信息被划分为 9 个字段。每个字段的含义如下： 用户名：加密密码：最后一次修改时间：最小修改时间间隔：密码有效期：密码需要变更前的警告天数：密码过期后的宽限时间：账号失效时间：保留字段\n接下来，给大家分别介绍这 9 个字段。\n1). 用户名 同 /etc/passwd 文件的用户名有相同的含义。\n2). 加密密码 这里保存的是真正加密的密码。目前 Linux 的密码采用的是 SHA512 散列加密算法，原来采用的是 MD5 或 DES 加密算法。SHA512 散列加密算法的加密等级更高，也更加安全。\n注意，这串密码产生的乱码不能手工修改，如果手工修改，系统将无法识别密码，导致密码失效。很多软件透过这个功能，在密码串前加上 \u0026ldquo;!\u0026quot;、\u0026quot;*\u0026rdquo; 或 \u0026ldquo;x\u0026rdquo; 使密码暂时失效。\n所有伪用户的密码都是 \u0026ldquo;!!\u0026rdquo; 或 \u0026ldquo;*\u0026quot;，代表没有密码是不能登录的。当然，新创建的用户如果不设定密码，那么它的密码项也是 \u0026ldquo;!!\u0026quot;，代表这个用户没有密码，不能登录。\n3). 最后一次修改时间 此字段表示最后一次修改密码的时间，可是，为什么 root 用户显示的是 15775 呢？\n这是因为，Linux 计算日期的时间是以 1970 年 1 月 1 日作为 1 不断累加得到的时间，到 1971 年 1 月 1 日，则为 366 天。这里显示 15775 天，也就是说，此 root 账号在 1970 年 1 月 1 日之后的第 15775 天修改的 root 用户密码。\n那么，到底 15775 代表的是哪一天呢？可以使用如下命令进行换算：\n[root@localhost ~]# date -d \u0026#34;1970-01-01 15775 days\u0026#34; 2013年03月11日 星期一 00:00:00 CST 可以看到，通过以上命令，即可将其换算为我们习惯的系统日期。\n4). 最小修改时间间隔 最小修改间隔时间，也就是说，该字段规定了从第 3 字段（最后一次修改密码的日期）起，多长时间之内不能修改密码。如果是 0，则密码可以随时修改；如果是 10，则代表密码修改后 10 天之内不能再次修改密码。\n此字段是为了针对某些人频繁更改账户密码而设计的。\n5). 密码有效期 经常变更密码是个好习惯，为了强制要求用户变更密码，这个字段可以指定距离第 3 字段（最后一次更改密码）多长时间内需要再次变更密码，否则该账户密码进行过期阶段。 该字段的默认值为 99999，也就是 273 年，可认为是永久生效。如果改为 90，则表示密码被修改 90 天之后必须再次修改，否则该用户即将过期。管理服务器时，通过这个字段强制用户定期修改密码。\n6). 密码需要变更前的警告天数 与第 5 字段相比较，当账户密码有效期快到时，系统会发出警告信息给此账户，提醒用户 \u0026ldquo;再过 n 天你的密码就要过期了，请尽快重新设置你的密码！\u0026quot;。\n该字段的默认值是 7，也就是说，距离密码有效期的第 7 天开始，每次登录系统都会向该账户发出 \u0026ldquo;修改密码\u0026rdquo; 的警告信息。\n7). 密码过期后的宽限天数 也称为“口令失效日”，简单理解就是，在密码过期后，用户如果还是没有修改密码，则在此字段规定的宽限天数内，用户还是可以登录系统的；如果过了宽限天数，系统将不再让此账户登陆，也不会提示账户过期，是完全禁用。\n比如说，此字段规定的宽限天数是 10，则代表密码过期 10 天后失效；如果是 0，则代表密码过期后立即失效；如果是 -1，则代表密码永远不会失效。\n8). 账号失效时间 同第 3 个字段一样，使用自 1970 年 1 月 1 日以来的总天数作为账户的失效时间。该字段表示，账号在此字段规定的时间之外，不论你的密码是否过期，都将无法使用！\n该字段通常被使用在具有收费服务的系统中。\n9). 保留 这个字段目前没有使用，等待新功能的加入。\n6. /etc/group解释 /ect/group 文件是用户组配置文件，即用户组的所有信息都存放在此文件中。\n此文件是记录组 ID（GID）和组名相对应的文件。前面讲过，etc/passwd 文件中每行用户信息的第四个字段记录的是用户的初始组 ID，那么，此 GID 的组名到底是什么呢？就要从 /etc/group 文件中查找。\n/etc/group 文件的内容可以通过 Vim 看到：\n[root@localhost ~]#vim /etc/group root:x:0: bin:x:1:bin,daemon daemon:x:2:bin,daemon …省略部分输出… lamp:x:502: 可以看到，此文件中每一行各代表一个用户组。在前面章节中，我们曾创建 lamp 用户，系统默认生成一个 lamp 用户组，在此可以看到，此用户组的 GID 为 502，目前它仅作为 lamp 用户的初始组。\n各用户组中，还是以 \u0026ldquo;：\u0026rdquo; 作为字段之间的分隔符，分为 4 个字段，每个字段对应的含义为： 组名：密码：GID：该用户组中的用户列表\n接下来，分别介绍各个字段具体的含义。\n1). 组名 也就是是用户组的名称，有字母或数字构成。同 /etc/passwd 中的用户名一样，组名也不能重复。\n2). 组密码 和 /etc/passwd 文件一样，这里的 \u0026ldquo;x\u0026rdquo; 仅仅是密码标识，真正加密后的组密码默认保存在 /etc/gshadow 文件中。\n不过，用户设置密码是为了验证用户的身份，那用户组设置密码是用来做什么的呢？用户组密码主要是用来指定组管理员的，由于系统中的账号可能会非常多，root 用户可能没有时间进行用户的组调整，这时可以给用户组指定组管理员，如果有用户需要加入或退出某用户组，可以由该组的组管理员替代 root 进行管理。但是这项功能目前很少使用，我们也很少设置组密码。如果需要赋予某用户调整某个用户组的权限，则可以使用 sudo 命令代替。\n3). 组ID (GID) 就是群组的 ID 号，Linux 系统就是通过 GID 来区分用户组的，同用户名一样，组名也只是为了便于管理员记忆。\n这里的组 GID 与 /etc/passwd 文件中第 4 个字段的 GID 相对应，实际上，/etc/passwd 文件中使用 GID 对应的群组名，就是通过此文件对应得到的。\n4). 组中的用户 此字段列出每个群组包含的所有用户。需要注意的是，如果该用户组是这个用户的初始组，则该用户不会写入这个字段，可以这么理解，该字段显示的用户都是这个用户组的附加用户。\n举个例子，lamp 组的组信息为 \u0026ldquo;lamp:x:502:\u0026quot;，可以看到，第四个字段没有写入 lamp 用户，因为 lamp 组是 lamp 用户的初始组。如果要查询这些用户的初始组，则需要先到 /etc/passwd 文件中查看 GID（第四个字段），然后到 /etc/group 文件中比对组名。\n每个用户都可以加入多个附加组，但是只能属于一个初始组。所以我们在实际工作中，如果需要把用户加入其他组，则需要以附加组的形式添加。例如，我们想让 lamp 也加入 root 这个群组，那么只需要在第一行的最后一个字段加入 lamp，即 root:x:0:lamp 就可以了。\n一般情况下，用户的初始组就是在建立用户的同时建立的和用户名相同的组。\n到此，我们已经学习了/etc/passwd、/etc/shadow、/etc/group，它们之间的关系可以这样理解，即先在 /etc/group 文件中查询用户组的 GID 和组名；然后在 /etc/passwd 文件中查找该 GID 是哪个用户的初始组，同时提取这个用户的用户名和 UID；最后通过 UID 到 /etc/shadow 文件中提取和这个用户相匹配的密码。\n7. /etc/gshadow解释 前面讲过，/etc/passwd 文件存储用户基本信息，同时考虑到账户的安全性，将用户的密码信息存放另一个文件 /etc/shadow 中。本节要将的 /etc/gshadow 文件也是如此，组用户信息存储在 /etc/group 文件中，而将组用户的密码信息存储在 /etc/gshadow 文件中。\n首先，我们借助 Vim 命令查看一下此文件中的内容：\n[root@localhost ~]#vim /etc/gshadow root::: bin:::bin, daemon daemon:::bin, daemon ...省略部分输出... lamp:!:: 文件中，每行代表一个组用户的密码信息，各行信息用 \u0026ldquo;:\u0026rdquo; 作为分隔符分为 4 个字段，每个字段的含义如下： 组名：加密密码：组管理员：组附加用户列表\n1). 组名 同 /etc/group 文件中的组名相对应。\n2). 组密码 对于大多数用户来说，通常不设置组密码，因此该字段常为空，但有时为 \u0026ldquo;!\u0026quot;，指的是该群组没有组密码，也不设有群组管理员。\n3). 组管理员 从系统管理员的角度来说，该文件最大的功能就是创建群组管理员。那么，什么是群组管理员呢？\n考虑到 Linux 系统中账号太多，而超级管理员 root 可能比较忙碌，因此当有用户想要加入某群组时，root 或许不能及时作出回应。这种情况下，如果有群组管理员，那么他就能将用户加入自己管理的群组中，也就免去麻烦 root 了。\n不过，由于目前有 sudo 之类的工具，因此群组管理员的这个功能已经很少使用了。\n4). 组中的附加用户 该字段显示这个用户组中有哪些附加用户，和 /etc/group 文件中附加组显示内容相同。\n8. 初始组和附加组 通过学习用户和群组我们知道，群组可以让多个用户具有相同的权限，同时也可以这样理解，一个用户可以所属多个群组，并同时拥有这些群组的权限，这就引出了初始组（有时也称主组）和附加组。\n/etc/passwd 文件中每个用户信息分为 7 个字段，其中第 4 字段（GID）指的就是每个用户所属的初始组，也就是说，当用户一登陆系统，立刻就会拥有这个群组的相关权限。\n举个例子，我们新建一个用户 lamp，并将其加入 users 群组中，执行命令如下：\n[root@localhost ~]# useradd lamp \u0026lt;--添加新用户 [root@localhost ~]# groupadd users \u0026lt;--添加新群组 [root@localhost ~]# usermod -G users lamp \u0026lt;--将用户lamp加入 users群组 [root@localhost ~]# grep \u0026#34;lamp\u0026#34; /etc/passwd /etc/group /etc/gshadow /etc/passwd:lamp:x:501:501::/home/lamp:/bin/bash /etc/group:users:x:100:lamp /etc/group:lamp:x:501: /etc/gshadow:users:::lamp /etc/gshadow:lamp:!:: useradd 和 groupadd 分别是添加用户和群组的命令，后续章节会做详细讲解。\n可以看到，在 etc/passwd 文件中，lamp 用户所属的 GID（群组 ID）为 501，通过搜索 /etc/group 文件得知，对应此 GID 的是 lamp 群组，也就是说，lamp 群组是 lamp 用户的初始组。\nlamp 群组是添加 lamp 用户时默认创建的群组，在 root 管理员使用 useradd 命令创建新用户时，若未明确指定该命令所属的初始组，useradd 命令会默认创建一个同用户名相同的群组，作为该用户的初始组。\n正因为 lamp 群组是 lamp 用户的初始组，该用户一登陆就会自动获取相应权限，因此不需要在 /etc/group 的第 4 个字段额外标注。\n但是，附加组就不一样了，从例子中可以看到，我们将 lamp 用户加入 users 群组中，由于 users 这个群组并不是 lamp 的初始组，因此必须要在 /etc/group 这个文件中找到 users 那一行，将 lamp 这个用户加入第 4 段中（群组包含的所有用户），这样 lamp 用户才算是真正加入到 users 这个群组中。\n在这个例子中，因为 lamp 用户同时属于 lamp 和users 两个群组，所在，在读取\\写入\\运行文件时，只要是 user 和 lamp 群组拥有的功能，lamp 用户都拥有。\n一个用户可以所属多个附加组，但只能有一个初始组。那么，如何知道某用户所属哪些群组呢？使用 groups 命令即可。\n例如，我们现在以 lamp 用户的身份登录系统，通过执行如下命令即可知晓当前用户所属的全部群组：\n[root@localhost ~]# groups lamp users 通过以上输出信息可以得知，lamp 用户同时属于 lamp 群组和 users 群组，而且，第一个出现的为用户的初始组，后面的都是附加组，所以 lamp 用户的初始组为 lamp 群组，附加组为 users 群组。\n9. useradd命令 Linux 系统中，可以使用 useradd 命令新建用户，此命令的基本格式如下：\n[root@localhost ~]#useradd [选项] 用户名    选项 含义     -u UID 手工指定用户的 UID，注意 UID 的范围（不要小于 500）。   -d 主目录 手工指定用户的主目录。主目录必须写绝对路径，而且如果需要手工指定主目录，则一定要注意权限；   -c 用户说明 手工指定/etc/passwd文件中各用户信息中第 5 个字段的描述性内容，可随意配置；   -g 组名 手工指定用户的初始组。一般以和用户名相同的组作为用户的初始组，在创建用户时会默认建立初始组。一旦手动指定，则系统将不会在创建此默认的初始组目录。   -G 组名 指定用户的附加组。我们把用户加入其他组，一般都使用附加组；   -s shell 手工指定用户的登录 Shell，默认是 /bin/bash；   -e 曰期 指定用户的失效曰期，格式为 \u0026ldquo;YYYY-MM-DD\u0026rdquo;。也就是 /etc/shadow 文件的第八个字段；   -o 允许创建的用户的 UID 相同。例如，执行 \u0026ldquo;useradd -u 0 -o usertest\u0026rdquo; 命令建立用户 usertest，它的 UID 和 root 用户的 UID 相同，都是 0；   -m 建立用户时强制建立用户的家目录。在建立系统用户时，该选项是默认的；   -r 创建系统用户，也就是 UID 在 1~499 之间，供系统程序使用的用户。由于系统用户主要用于运行系统所需服务的权限配置，因此系统用户的创建默认不会创建主目录。    其实，系统已经帮我们规定了非常多的默认值，在没有特殊要求下，无需使用任何选项即可成功创建用户。例如：\n[root@localhost ~]# useradd lamp 此行命令就表示创建 lamp 普通用户。\n10. passwd命令：修改用户密码 学习 useradd 命令我们知道，使用此命令创建新用户时，并没有设定用户密码，因此还无法用来登陆系统，本节就来学习 passwd 密码配置命令 。\npasswd 命令的基本格式如下：\n[root@localhost ~]#passwd [选项] 用户名 选项：\n -S：查询用户密码的状态，也就是 /etc/shadow 文件中此用户密码的内容。仅 root 用户可用； -l：暂时锁定用户，该选项会在 /etc/shadow 文件中指定用户的加密密码串前添加 \u0026ldquo;!\u0026quot;，使密码失效。仅 root 用户可用； -u：解锁用户，和 -l 选项相对应，也是只能 root 用户使用； \u0026ndash;stdin：可以将通过管道符输出的数据作为用户的密码。主要在批量添加用户时使用； -n 天数：设置该用户修改密码后，多长时间不能再次修改密码，也就是修改 /etc/shadow 文件中各行密码的第 4 个字段； -x 天数：设置该用户的密码有效期，对应 /etc/shadow 文件中各行密码的第 5 个字段； -w 天数：设置用户密码过期前的警告天数，对于 /etc/shadow 文件中各行密码的第 6 个字段； -i 日期：设置用户密码失效日期，对应 /etc/shadow 文件中各行密码的第 7 个字段。 例如，我们使用 root 账户修改 lamp 普通用户的密码，可以使用如下命令：  [root@localhost ~]#passwd lamp Changing password for user lamp. New password: \u0026lt;==直接输入新的口令，但屏幕不会有任何反应 BAD PASSWORD: it is WAY too short \u0026lt;==口令太简单或过短的错误！这里只是警告信息，输入的密码依旧能用 Retype new password: \u0026lt;==再次验证输入的密码，再输入一次即可 passwd: all authentication tokens updated successfully. \u0026lt;==提示修改密码成功 当然，也可以使用 passwd 命令修改当前系统已登录用户的密码，但要注意的是，需省略掉 \u0026ldquo;选项\u0026rdquo; 和 \u0026ldquo;用户名\u0026rdquo;。例如，我们登陆 lamp 用户，并使用 passwd 命令修改 lamp 的登陆密码，执行过程如下：\n[root@localhost ~]#passwd # passwd直接回车代表修改当前用户的密码 Changing password for user vbird2. Changing password for vbird2 (current) UNIX password: \u0026lt;==这里输入『原有的旧口令』 New password: \u0026lt;==这里输入新口令 BAD PASSWORD: it is WAY too short \u0026lt;==口令检验不通过，请再想个新口令 New password: \u0026lt;==这里再想个来输入吧 Retype new password: \u0026lt;==通过口令验证！所以重复这个口令的输入 passwd: all authentication tokens updated successfully. \u0026lt;==成功修改用户密码 注意，普通用户只能使用 passwd 命令修改自己的密码，而不能修改其他用户的密码。\n可以看到，与使用 root 账户修改普通用户的密码不同，普通用户修改自己的密码需要先输入自己的旧密码，只有旧密码输入正确才能输入新密码。不仅如此，此种修改方式对密码的复杂度有严格的要求，新密码太短、太简单，都会被系统检测出来并禁止用户使用。 很多Linux 发行版为了系统安装，都使用了 PAM 模块进行密码的检验，设置密码太短、与用户名相同、是常见字符串等，都会被 PAM 模块检查出来，从而禁止用户使用此类密码。\n而使用 root 用户，无论是修改普通用户的密码，还是修改自己的密码，都可以不遵守 PAM 模块设定的规则，就比如我刚刚给 lamp 用户设定的密码是 \u0026ldquo;123\u0026rdquo;，系统虽然会提示密码过短和过于简单，但依然可以设置成功。当然，在实际应用中，就算是 root 身份，在设定密码时也要严格遵守密码规范，因为只有好的密码规范才是服务器安全的基础。\n实例 【例 1】\n# 查看用户密码的状态 [root@localhost ~]# passwd -S lamp lamp PS 2013-01-06 0 99999 7 -1 (Password set, SHA512 crypt.) # 上面这行代码的意思依次是：用户名 密码 设定时间(2013*01-06) 密码修改间隔时间(0) 密码有效期(99999) 警告时间(7) 密码不失效(-1)，密码已使用 \u0026ldquo;-S\u0026quot;选项会显示出密码状态，这里的密码修改间隔时间、密码有效期、警告时间、密码宽限时间其实分别是 /etc/shadow 文件的第四、五、六、七个字段的内容。 当然，passwd 命令是可以通过命令选项修改这几个字段的值的，例如：\n# 修改 lamp的密码，使其具有 60 天变更、10 天密码失效 [root@localhost ~]# passwd -x 60 -i 10 lamp [root@localhost ~]# passwd -S lamp lamp PS 2013-01-06 0 60 7 10 (Password set, SHA512 crypt.) 【例 2】\n# 锁定 lamp 用户 [root@localhost ~]# passwd -l lamp Locking password for user lamp. passwd:Successg # 用\u0026#34;-S\u0026#34;选项査看状态，很清楚地提示密码已被锁定 [root@localhost ~]# passwd -S lamp lamp LK 2013-01-06 0 99999 7 -1 (Password locked.) [root@localhost ~]# grep \u0026#34;lamp\u0026#34; /etc/shadow lamp:!! $6$ZTq7o/9o $lj07iZ0bzW.D1zBa9CsY43d04onskUCzjwiFMNt8PX4GXJoHX9zA1S C9.i Yzh9LZA4fEM2lg92hM9w/p6NS50.:15711:0:99999:7::: # 可以看到，锁定其实就是在加密密码之前加入了\u0026#34;!!\u0026#34;，让密码失效而已 暂时锁定 lamp 用户后，此用户就不能登录系统了。那么，怎么解锁呢？也一样简单，使用如下命令即可：\n# 解锁 lamp 用户 [root@localhost ~]# passwd -u lamp Unlocking password for user lamp. passwd:Success [root@localhost ~]# passwd -S lamp lamp PS 2013-01-06 0 99999 7 -1 (Password set, SHA512 crypt.) # 可以看到，锁定状态消失 [root@localhost ~]# grep \u0026#34;lamp\u0026#34; /etc/shadow lamp: $6$ZTq7cV9o $lj07iZ0bzW.D1zBa9CsY43d04onskUCzjwiFMNt8PX4GXJoHX9zA1S C9.iYz h9LZA4fEM2lg92hM9w/p6NS50.:15711:0:99999:7::: # 密码前面的 \u0026#34;!!\u0026#34; 删除了 11. usermod命令：修改用户信息 前面章节介绍了如何利用 useradd 命令添加用户，但如果不小心添错用户信息，后期如何修改呢？\n办法有两个，一个是使用 Vim 文本编辑器手动修改涉及用户信息的相关文件（/etc/passwd、/etc/shadow、/etc/group、/etc/gshadow），另一个方法就是使用本节介绍了 usermod 命令，该命令专门用于修改用户信息。 这里一定要分清 useradd 命令和 usermod 命令的区别，前者用于添加用户，当然，添加用户时可以对用户信息进行定制；后者针对与已存在的用户，使用该命令可以修改它们的信息。\nusermod 命令的基本格式如下：\n[root@localhost ~]#usermod [选项] 用户名 选项：\n -c 用户说明：修改用户的说明信息，即修改 /etc/passwd 文件目标用户信息的第 5 个字段； -d 主目录：修改用户的主目录，即修改 /etc/passwd 文件中目标用户信息的第 6 个字段，需要注意的是，主目录必须写绝对路径； -e 日期：修改用户的失效曰期，格式为 \u0026ldquo;YYYY-MM-DD\u0026rdquo;，即修改 /etc/shadow 文件目标用户密码信息的第 8 个字段； -g 组名：修改用户的初始组，即修改 /etc/passwd 文件目标用户信息的第 4 个字段（GID）； -u UID：修改用户的UID，即修改 /etc/passwd 文件目标用户信息的第 3 个字段（UID）； -G 组名：修改用户的附加组，其实就是把用户加入其他用户组，即修改 /etc/group 文件； -l 用户名：修改用户名称； -L：临时锁定用户（Lock）； -U：解锁用户（Unlock），和 -L 对应； -s shell：修改用户的登录 Shell，默认是 /bin/bash。  如果你仔细观察会发现，其实 usermod 命令提供的选项和 useradd 命令的选项相似，因为 usermod 命令就是用来调整使用 useradd 命令添加的用户信息的。\n不过，相比 useradd 命令，usermod 命令还多出了几个选项，即 -L 和 -U，作用分别与 passwd 命令的 -l 和-u 相同。需要注意的是，并不是所有的 Linux 发行版都包含这个命令，因此，使用前可以使用 man usermod 命令确定系统是否支持。\n 此命令对用户的临时锁定，同 passwd 命令一样，都是在 /etc/passwd 文件目标用户的加密密码字段前添加 \u0026ldquo;!\u0026quot;，使密码失效；反之，解锁用户就是将添加的 \u0026ldquo;!\u0026rdquo; 去掉。\n 实例 【例 1】\n# 锁定用户 [root@localhost ~]# usermod -L lamp [root@localhost ~]# grep \u0026#34;lamp\u0026#34; /etc/shadow lamp:!$6$YrPj8g0w$ChRVASybEncU24hkYFqxREH3NnzhAVDJSQLwRwTSbcA2N8UbPD9bBKVQSky xlaMGs/Eg5AQwO.UokOnKqaHFa/:15711:0:99999:7::: # 其实锁定就是在密码字段前加入\u0026#34;!\u0026#34;，这时lamp用户就暂时不能登录了 # 解锁用户 [root@localhost ~]# usermod -U lamp [root@localhost ~]# grep \u0026#34;lamp\u0026#34; /etc/shadow lamp:$6$YrPj8g0w$ChRVASybEncU24hkYFqxREH3NnzhAVDJSQLwRwTSbcA2N8UbPD9bBKVQSkyx laMGs/Eg5AQwO.UokOnKqaHFa/:15711:0:99999:7::: # 取消了密码字段前的 \u0026#34;!\u0026#34; 【例 2】\n# 把lamp用户加入root组 [root@localhost ~]# usermod -G root lamp [root@localhost ~]# grep \u0026#34;lamp\u0026#34; /etc/group root:x:0:lamp # lamp用户已经加入了root组 lamp:x:501: 【例 3】\n# 修改用户说明 [root@localhost ~]# usermod -c \u0026#34;test user\u0026#34; lamp [root@localhost ~]# grep \u0026#34;lamp\u0026#34; /etc/passwd lamp:x:501:501:test user:/home/lamp:/bin/bash # 查看一下，用户说明已经被修改了 12. chage命令：修改用户密码状态 除了 passwd -S 命令可以查看用户的密码信息外，还可以利用 chage 命令，它可以显示更加详细的用户密码信息，并且和 passwd 命令一样，提供了修改用户密码信息的功能。 如果你要修改用户的密码信息，我个人建议，还是直接修改 /etc/shadow 文件更加方便。\n首先，我们来看 chage 命令的基本格式：\n[root@localhost ~]#chage [选项] 用户名 选项：\n -l：列出用户的详细密码状态; -d 日期：修改 /etc/shadow 文件中指定用户密码信息的第 3 个字段，也就是最后一次修改密码的日期，格式为 YYYY-MM-DD； -m 天数：修改密码最短保留的天数，也就是 /etc/shadow 文件中的第 4 个字段； -M 天数：修改密码的有效期，也就是 /etc/shadow 文件中的第 5 个字段； -W 天数：修改密码到期前的警告天数，也就是 /etc/shadow 文件中的第 6 个字段； -i 天数：修改密码过期后的宽限天数，也就是 /etc/shadow 文件中的第 7 个字段； -E 日期：修改账号失效日期，格式为 YYYY-MM-DD，也就是 /etc/shadow 文件中的第 8 个字段。  【例 1】\n# 查看一下用户密码状态 [root@localhost ~]# chage -l lamp Last password change:Jan 06, 2013 Password expires:never Password inactive :never Account expires :never Minimum number of days between password change :0 Maximum number of days between password change :99999 Number of days of warning before password expires :7 13. userdel命令：删除用户 userdel 命令功能很简单，就是删除用户的相关数据。此命令只有 root 用户才能使用。\n通过前面的学习我们知道，用户的相关数据包含如下几项：\n用户基本信息：存储在 /etc/passwd 文件中； 用户密码信息：存储在 /etc/shadow 文件中； 用户群组基本信息：存储在 /etc/group 文件中； 用户群组信息信息：存储在 /etc/gshadow 文件中； 用户个人文件：主目录默认位于 /home/用户名，邮箱位于 /var/spool/mail/用户名。 其实，userdel 命令的作用就是从以上文件中，删除与指定用户有关的数据信息。 userdel 命令的语法很简单，基本格式如下：\n[root@localhost ~]# userdel -r 用户名  -r 选项表示在删除用户的同时删除用户的家目录。  注意，在删除用户的同时如果不删除用户的家目录，那么家目录就会变成没有属主和属组的目录，也就是垃圾文件。\n例如，删除前面章节中创建的 lamp 用户，只需执行如下命令：\n[root@localhost ~]# userdel -r lamp 手动删除指定用户的具体操作如下\n# 建立新 lamp 用户 [root@localhost ~]# useradd lamp [root@localhost ~]# passwd lamp # 为 lamp 用户设置密码，由此 lamp 用户才算是创建成功 # 下面开始手动删除 lamp [root@localhost ~]# vi /etc/passwd lamp:x:501:501::/home/lamp:/bin/bash \u0026lt;--删除此行 # 修改用户信息文件，删除lamp用户行 [root@localhost ~]#vi /etc/shadow lamp:$6$KoOYtcOJ $56Xk9vp3D2vMRBxibNOn.21cVJ9onbW8IHx4WrOx6qBqfGa9U3mjMsGjqYnj L/4t3zt3YxElce2X8rbb12x4a0:15716:0:99999:7::: \u0026lt;--删除此行 # 修改影子文件，删除lamp用户密码行，注意，这个文件的权限是000，所以要强制保存 [root@localhost ~]#vi /etc/group lamp:x:501: \u0026lt;--删除此行 # 修改组信息文件，删除lamp群组信息 [root@localhost ~]#vi /etc/gshadow lamp:!:: \u0026lt;--删除此行 # 修改组影子文件，删除lamp群组密码信息。同样注意需要强制保存 [root@localhost ~]# rm -rf /var/spod/mail/lamp #删除用户邮箱 [root@localhost ~]# rm -rf/home/lamp/ #删除用户的家目录 # 至此，用户彻底删除，再新建用户lamp。如果可以正常建立，则说明我们手工删除干净了 [root@localhost ~]# useradd lamp [root@localhost ~]# passwd lamp # 重新建立同名用户，没有报错，说明前面的手工删除是可以完全删除用户的 14. id命令：查看用户的UID和GID id 命令可以查询用户的UID、GID 和附加组的信息。命令比较简单，格式如下：\n[root@localhost ~]# id 用户名 【例 1】\n[root@localhost ~]# id lamp uid=501(lamp) gid=501(lamp) groups=501(lamp) # 能看到uid(用户ID)、gid(初始组ID), groups是用户所在组，这里既可以看到初始组，如果有附加组，则也能看到附加组 【例 2】 [root@localhost ~]# usermod -G root lamp # 把用户加入root组 [root@localhost ~]# id lamp uid=501(lamp) gid=501(lamp) groups=501(lamp),0(root) # 大家发现root组中加入了lamp用户的附加组信息 15. su命令：用户间切换 su 是最简单的用户切换命令，通过该命令可以实现任何身份的切换，包括从普通用户切换为 root 用户、从 root 用户切换为普通用户以及普通用户之间的切换。 普通用户之间切换以及普通用户切换至 root 用户，都需要知晓对方的密码，只有正确输入密码，才能实现切换；从 root 用户切换至其他用户，无需知晓对方密码，直接可切换成功。\nsu 命令的基本格式如下：\n[root@localhost ~]# su [选项] 用户名 选项：\n -：当前用户不仅切换为指定用户的身份，同时所用的工作环境也切换为此用户的环境（包括 PATH 变量、MAIL 变量等），使用 - 选项可省略用户名，默认会切换为 root 用户。 -l：同 - 的使用类似，也就是在切换用户身份的同时，完整切换工作环境，但后面需要添加欲切换的使用者账号。 -p：表示切换为指定用户的身份，但不改变当前的工作环境（不使用切换用户的配置文件）。 -m：和 -p 一样； -c 命令：仅切换用户执行一次命令，执行后自动切换回来，该选项后通常会带有要执行的命令。  【例 1】\n[lamp@localhost ~]$ su -root 密码： \u0026lt;-- 输入 root 用户的密码 # \u0026#34;-\u0026#34;代表连带环境变量一起切换，不能省略 16. groupadd命令：添加用户组 添加用户组的命令是 groupadd，命令格式如下:\n[root@localhost ~]# groupadd [选项] 组名 选项：\n -g GID：指定组 ID； -r：创建系统群组。 使用 groupadd 命令创建新群组非常简单，例如：  [root@localhost ~]# groupadd group1 # 添加group1组 [root@localhost ~]# grep \u0026#34;group1\u0026#34; /etc/group /etc/group:group1:x:502: /etc/gshadow:group1:!:: 17. groupmod命令：修改用户组 groupmod 命令用于修改用户组的相关信息，命令格式如下：\n[root@localhost ~]# groupmod [选现] 组名 选项：\n -g GID：修改组 ID； -n 新组名：修改组名；  例子：\n[root@localhost ~]# groupmod -n testgrp group1 # 把组名group1修改为testgrp [root@localhost ~]# grep \u0026#34;testgrp\u0026#34; /etc/group testgrp:x:502: # 注意GID还是502，但是组名已经改变 不过大家还是要注意，用户名不要随意修改，组名和 GID 也不要随意修改，因为非常容易导致管理员逻辑混乱。如果非要修改用户名或组名，则建议大家先删除旧的，再建立新的。\n18. groupdel命令：刪除用户组 groupdel 命令用于删除用户组（群组），此命令基本格式为：\n[root@localhost ~]#groupdel 组名 通过前面的学习不难猜测出，使用 groupdel 命令删除群组，其实就是删除 /etc/gourp 文件和 /etc/gshadow 文件中有关目标群组的数据信息。\n例如，删除前面章节中用 groupadd 命令创建的群组 group1，执行命令如下：\n[root@localhost ~]#grep \u0026#34;testgrp\u0026#34; /etc/group /etc/gshadow /etc/group:testgrp:x:1002: /etc/gshadow:testgrp:!:: [root@localhost ~]#groupdel testgrp [root@localhost ~]#grep \u0026#34;testgrp\u0026#34; /etc/group /etc/gshadow [root@localhost ~]# 注意，不能使用 groupdel 命令随意删除群组。此命令仅适用于删除那些 \u0026ldquo;不是任何用户初始组\u0026rdquo; 的群组，换句话说，如果有群组还是某用户的初始群组，则无法使用 groupdel 命令成功删除。例如：\n[root@localhost ~]# useradd temp # 运行如下命令，可以看到 temp 用户建立的同时，还创建了 temp 群组，且将其作为 temp用户的初始组（组ID都是 505） [root@localhost ~]# grep \u0026#34;temp\u0026#34; /etc/passwd /etc/group /etc/gshadow /etc/passwd:temp:x:505:505::/home/temp:/bin/bash /etc/group:temp:x:505: /etc/gshadow:temp:!:: # 下面尝试删除 temp 群组 [root@localhost ~]# groupdel temp groupdel:cannot remove the primary group of user \u0026#39;temp\u0026#39; 可以看到，groupdel 命令删除 temp 群组失败，且提示“不能删除 temp 用户的初始组”。如果一定要删除 temp 群组，要么修改 temp 用户的 GID，也就是将其初始组改为其他群组，要么先删除 temp 用户。\n切记，虽然我们已经学了如何手动删除群组数据，但胡乱地删除群组可能会给其他用户造成不小的麻烦，因此更改文件数据要格外慎重。\n19. gpasswd命令：把用户添加进组或从组中删除 为了避免系统管理员（root）太忙碌，无法及时管理群组，我们可以使用 gpasswd 命令给群组设置一个群组管理员，代替 root 完成将用户加入 或移出群组的操作。\ngpasswd 命令的基本格式如下：\n[root@localhost ~]# gpasswd 选项 组名 选项：\n 选项为空时，表示给群组设置密码，仅 root 用户可用。 -A user1,\u0026hellip; 将群组的控制权交给 user1,\u0026hellip; 等用户管理，也就是说，设置 user1,\u0026hellip; 等用户为群组的管理员，仅 root 用户可用。 -M user1,\u0026hellip; 将 user1,\u0026hellip; 加入到此群组中，仅 root 用户可用。 -r 移除群组的密码，仅 root 用户可用。 -R 让群组的密码失效，仅 root 用户可用。 -a user 将 user 用户加入到群组中。 -d user 将 user 用户从群组中移除。 【例 1】  # 创建新群组 group1，并将群组交给 lamp 管理 [root@localhost ~]# groupadd group1 \u0026lt;-- 创建群组 [root@localhost ~]# gpasswd group1 \u0026lt;-- 设置密码吧！ Changing the password for group group1 New Password: Re-enter new password: [root@localhost ~]# gpasswd -A lamp group1 \u0026lt;==加入群组管理员为 lamp [root@localhost ~]# grep \u0026#34;group1\u0026#34; /etc/group /etc/gshadow /etc/group:group1:x:506: /etc/gshadow:group1:$1$I5ukIY1.$o5fmW.cOsc8.K.FHAFLWg0:lamp: 可以看到，此时 lamp 用户即为 group1 群组的管理员。\n【例 2】\n# 以lamp用户登陆系统，并将用户 lamp 和 lamp1 加入group1群组。 [lamp@localhost ~]#gpasswd -a lamp group1 [lamp@localhost ~]#gpasswd -a lamp1 group1 [lamp@localhost ~]#grep \u0026#34;group1\u0026#34; /etc/group group1:x:506:lamp,lamp1 20. newgrp命令：切换用户的有效组 我们知道，每个用户可以属于一个初始组（用户是这个组的初始用户），也可以属于多个附加组（用户是这个组的附加用户）。既然用户可以属于这么多用户组，那么用户在创建文件后，默认生效的组身份是哪个呢？\n当然是初始用户组的组身份生效，因为初始组是用户一旦登陆就获得的组身份。也就是说，用户的有效组默认是初始组，因此所创建文件的属组是用户的初始组。那么，既然用户属于多个用户组，能不能改变用户的初始组呢？使用命令 newgrp 就可以。\nnewgrp 命令可以从用户的附加组中选择一个群组，作为用户新的初始组。此命令的基本格式如下：\n[root@localhost ~]# newgrp 组名 下面，我们通过实例，讲解此命令的具体用法和功能：\n首先，建立 3 个用户组 group1、group2 和 group3，命令如下：\n[root@localhost ~]# groupadd group1 [root@localhost ~]# groupadd group2 [root@localhost ~]# groupadd group3 创建一个用户 user1，同时指定 user1 的初始组为 group1，附加组为 group2 和 group3，执行命令如下：\n[root@localhost ~]# useradd -g group1 -G group2,group3 user1 # 由于指定了初始组，因此不会在创建 user1 默认群组 [root@localhost ~]# more /etc/group | grep user1 group2:x:501:user1 group3:x:502:user1 对用户 user1 设置密码，执行命令如下：\n[root@localhost ~]# passwd user1 Changing password for user user1. New password: Retype new password: passwd: all authentication tokens updated successfully. 切换至 user1 用户，通过 newgrp 切换用户组进行下列操作，读者可从中体会出 newgrp 命令的作用。\n# 切换至 user1 用户 [root@localhost ~]# su - user1 [root@localhost ~]# whoami user1 # 使用 newgrp 命令一边切换 user1 的初始组，一边创建文件 [root@localhost ~]# mkdir user1_doc [root@localhost ~]# newgrp group2 [root@localhost ~]# mkdir user2_doc [root@localhost ~]# newgrp group3 [root@localhost ~]# mkdir user3_doc # 查看各文件的详细信息 [root@localhost ~]# ll total 12 drwxr-xr-x 2 user1 group1 4096 Oct 24 01:18 user1_doc drwxr-xr-x 2 user1 group2 4096 Oct 24 01:18 user2_doc drwxr-xr-x 2 user1 group3 4096 Oct 24 01:19 user3_doc 可以看到，通过使用 newgrp 命令切换用户的初始组，所创建的文件各自属于不同的群组，这就是 newgrp 所发挥的作用，即通过切换附加组成为新的初始组，从而让用户获得使用各个附加组的权限。\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/linux%E4%B8%8B%E7%9A%84%E7%94%A8%E6%88%B7%E5%92%8C%E7%94%A8%E6%88%B7%E7%BB%84%E7%AE%A1%E7%90%86/","summary":"1.概述 Linux 是多用户多任务操作系统，换句话说，Linux 系统支持多个用户在同一时间内登陆，不同用户可以执行不同的任务，并且互不影响。\n例如，某台 Linux 服务器上有 4 个用户，分别是 root、www、ftp 和 mysql，在同一时间内，root 用户可能在查看系统日志、管理维护系统；www 用户可能在修改自己的网页程序；ftp 用户可能在上传软件到服务器；mysql 用户可能在执行自己的 SQL 查询，每个用户互不干扰，有条不紊地进行着自己的工作。与此同时，每个用户之间不能越权访问，比如 www 用户不能执行 mysql 用户的 SQL 查询操作，ftp 用户也不能修改 www 用户的网页程序。\n不同用户具有不问的权限，毎个用户在权限允许的范围内完成不间的任务，Linux 正是通过这种权限的划分与管理，实现了多用户多任务的运行机制。\n因此，如果要使用 Linux 系统的资源，就必须向系统管理员申请一个账户，然后通过这个账户进入系统（账户和用户是一个概念）。通过建立不同属性的用户，一方面可以合理地利用和控制系统资源，另一方面也可以帮助用户组织文件，提供对用户文件的安全性保护。\n每个用户都有唯一的用户名和密码。在登录系统时，只有正确输入用户名和密码，才能进入系统和自己的主目录。\n用户组是具有相同特征用户的逻辑集合。简单的理解，有时我们需要让多个用户具有相同的权限，比如查看、修改某一个文件的权限，一种方法是分别对多个用户进行文件访问授权，如果有 10 个用户的话，就需要授权 10 次，那如果有 100、1000 甚至更多的用户呢？\n显然，这种方法不太合理。最好的方式是建立一个组，让这个组具有查看、修改此文件的权限，然后将所有需要访问此文件的用户放入这个组中。那么，所有用户就具有了和组一样的权限，这就是用户组。\n将用户分组是 Linux 系统中对用户进行管理及控制访问权限的一种手段，通过定义用户组，很多程序上简化了对用户的管理工作。\n2.用户和用户组的关系 用户和用户组的对应关系有以下 4 种：\n一对一：一个用户可以存在一个组中，是组中的唯一成员； 一对多：一个用户可以存在多个用户组中，此用户具有这多个组的共同权限； 多对一：多个用户可以存在一个组中，这些用户具有和组相同的权限； 多对多：多个用户可以存在多个组中，也就是以上 3 种关系的扩展。 用户和组之间的关系可用下图表示：图 1 Linux用户和用户组 3. UID和GID(用户ID和组ID) 登陆 Linux 系统时，虽然输入的是自己的用户名和密码，但其实 Linux 并不认识你的用户名称，它只认识用户名对应的 ID 号（也就是一串数字）。Linux 系统将所有用户的名称与 ID 的对应关系都存储在 /etc/passwd 文件中。","title":"Linux下的用户和用户组管理"},{"content":" 极客时间《MySQL实战45讲》笔记\n 主备的基本原理 在状态 1 中，客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。\n当需要切换的时候，就切成状态 2。这时候客户端读写访问的都是节点 B，而节点 A 是 B 的备库。\n那么从状态1切换到状态2的内部流程是什么样的？\n备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：\n 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。 sql_thread 读取中转日志，解析出日志里的命令，并执行。  binlog 的三种格式对比 主备复制依赖于bin log，那么bin log中是什么内容。\nbin log的三种格式：\n statement row mixed  对于下表：\nmysql\u0026gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `t_modified`(`t_modified`) ) ENGINE=InnoDB; insert into t values(1,1,\u0026#39;2018-11-13\u0026#39;); insert into t values(2,2,\u0026#39;2018-11-12\u0026#39;); insert into t values(3,3,\u0026#39;2018-11-11\u0026#39;); insert into t values(4,4,\u0026#39;2018-11-10\u0026#39;); insert into t values(5,5,\u0026#39;2018-11-09\u0026#39;); 如果要在表中删除一行数据的话，我们来看看这个 delete 语句的 binlog 是怎么记录的：\nmysql\u0026gt; delete from t /*comment*/ where a\u0026gt;=4 and t_modified\u0026lt;=\u0026#39;2018-11-10\u0026#39; limit 1; 当binlog_format=statement时，binlog 里面记录的就是 SQL 语句的原文\nmysql\u0026gt; show binlog events in \u0026#39;binlog.000002\u0026#39;; +---------------+-----+----------------+-----------+-------------+------------------------------------------------------------------------------------------+ | Log_name | Pos | Event_type | Server_id | End_log_pos | Info | +---------------+-----+----------------+-----------+-------------+------------------------------------------------------------------------------------------+ | binlog.000002 | 4 | Format_desc | 1 | 126 | Server ver: 8.0.28, Binlog ver: 4 | | binlog.000002 | 126 | Previous_gtids | 1 | 157 | | | binlog.000002 | 157 | Anonymous_Gtid | 1 | 236 | SET @@SESSION.GTID_NEXT= \u0026#39;ANONYMOUS\u0026#39; | | binlog.000002 | 236 | Query | 1 | 332 | BEGIN | | binlog.000002 | 332 | Query | 1 | 496 | use `test_db`; delete from t /*comment*/ where a\u0026gt;=4 and t_modified\u0026lt;=\u0026#39;2018-11-10\u0026#39; limit 1 | | binlog.000002 | 496 | Xid | 1 | 527 | COMMIT /* xid=12 */ | +---------------+-----+----------------+-----------+-------------+------------------------------------------------------------------------------------------+  当使用steaement格式日志时，可能会存在主备数据不一致的情况，例如：\n 如果 delete 语句使用的是索引 a，那么会根据索引 a 找到第一个满足条件的行，也就是说删除的是 a=4 这一行； 但如果使用的是索引 t_modified，那么删除的就是 t_modified=\u0026lsquo;2018-11-09’也就是 a=5 这一行。  由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 t_modified。因此，MySQL 认为这样写是有风险的。\n当binlog_format=‘row’时：\nmysql\u0026gt; show binlog events in \u0026#39;binlog.000002\u0026#39;; +---------------+-----+----------------+-----------+-------------+--------------------------------------+ | Log_name | Pos | Event_type | Server_id | End_log_pos | Info | +---------------+-----+----------------+-----------+-------------+--------------------------------------+ | binlog.000002 | 4 | Format_desc | 1 | 126 | Server ver: 8.0.28, Binlog ver: 4 | | binlog.000002 | 126 | Previous_gtids | 1 | 157 | | | binlog.000002 | 157 | Anonymous_Gtid | 1 | 236 | SET @@SESSION.GTID_NEXT= \u0026#39;ANONYMOUS\u0026#39; | | binlog.000002 | 236 | Query | 1 | 322 | BEGIN | | binlog.000002 | 322 | Table_map | 1 | 375 | table_id: 88 (test_db.t) | | binlog.000002 | 375 | Delete_rows | 1 | 423 | table_id: 88 flags: STMT_END_F | | binlog.000002 | 423 | Xid | 1 | 454 | COMMIT /* xid=8 */ | +---------------+-----+----------------+-----------+-------------+--------------------------------------+ 与 statement 格式的 binlog 相比，前后的 BEGIN 和 COMMIT 是一样的。但是，row 格式的 binlog 里没有了 SQL 语句的原文，而是替换成了两个 event：Table_map 和 Delete_rows。\n Table_map event，用于说明接下来要操作的表是 test 库的表 t; Delete_rows event，用于定义删除的行为。  但这里我们看不到详细信息，需要借助mysqlbinlog工具。用下面这个命令解析和查看 binlog 中的内容。\nmysqlbinlog -vv data/master.000001 --start-position=8900; # The proper term is pseudo_replica_mode, but we use this compatibility alias # to make the statement usable on server versions 8.0.24 and older. /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/; /*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/; DELIMITER /*!*/; # at 126 #220615 6:51:35 server id 1 end_log_pos 126 CRC32 0xc895f13c Start: binlog v 4, server v 8.0.28 created 220615 6:51:35 at startup # Warning: this binlog is either in use or was not closed properly. ROLLBACK/*!*/; BINLOG ' d4GpYg8BAAAAegAAAH4AAAABAAQAOC4wLjI4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAB3galiEwANAAgAAAAABAAEAAAAYgAEGggAAAAICAgCAAAACgoKKioAEjQA CigAATzxlcg= '/*!*/; # at 157 #220615 6:52:14 server id 1 end_log_pos 236 CRC32 0x245f18b1 Anonymous_GTID last_committed=0 sequence_number=1 rbr_only=yes original_committed_timestamp=1655275934841953immediate_commit_timestamp=1655275934841953 transaction_length=297 /*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/; # original_commit_timestamp=1655275934841953 (2022-06-15 06:52:14.841953 UTC) # immediate_commit_timestamp=1655275934841953 (2022-06-15 06:52:14.841953 UTC) /*!80001 SET @@session.original_commit_timestamp=1655275934841953*//*!*/; /*!80014 SET @@session.original_server_version=80028*//*!*/; /*!80014 SET @@session.immediate_server_version=80028*//*!*/; SET @@SESSION.GTID_NEXT= 'ANONYMOUS'/*!*/; # at 236 #220615 6:52:14 server id 1 end_log_pos 322 CRC32 0xfa351183 Query thread_id=8 exec_time=0 error_code=0 SET TIMESTAMP=1655275934/*!*/; SET @@session.pseudo_thread_id=8/*!*/; SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/; SET @@session.sql_mode=1168113696/*!*/; SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/; /*!\\C latin1 *//*!*/; SET @@session.character_set_client=8,@@session.collation_connection=8,@@session.collation_server=255/*!*/; SET @@session.time_zone='SYSTEM'/*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; /*!80011 SET @@session.default_collation_for_utf8mb4=255*//*!*/; BEGIN /*!*/; # at 322 #220615 6:52:14 server id 1 end_log_pos 375 CRC32 0xe571a47d Table_map: `test_db`.`t` mapped to number 88 # at 375 #220615 6:52:14 server id 1 end_log_pos 423 CRC32 0xafba99da Delete_rows: table id 88 flags: STMT_END_F BINLOG ' noGpYhMBAAAANQAAAHcBAAAAAFgAAAAAAAEAB3Rlc3RfZGIAAXQAAwMDEQEAAgEBAH2kceU= noGpYiABAAAAMAAAAKcBAAAAAFgAAAAAAAEAAgAD/wAEAAAABAAAAFvmH4Dambqv '/*!*/; ### DELETE FROM `test_db`.`t` ### WHERE ### @1=4 /* INT meta=0 nullable=0 is_null=0 */ ### @2=4 /* INT meta=0 nullable=1 is_null=0 */ ### @3=1541808000 /* TIMESTAMP(0) meta=0 nullable=0 is_null=0 */ # at 423 #220615 6:52:14 server id 1 end_log_pos 454 CRC32 0x7e73732f Xid = 8 COMMIT/*!*/; SET @@SESSION.GTID_NEXT= 'AUTOMATIC' /* added by mysqlbinlog */ /*!*/; DELIMITER ; # End of log file /*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/; /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 从上文日志中可以看出：\n server id 1，表示这个事务是在 server_id=1 的这个库上执行的。 每个 event 都有 CRC32 的值，这是因为我把参数 binlog_checksum 设置成了 CRC32。 Table_map event 跟在statement类型日志中看到的相同，显示了接下来要打开的表，map 到数字 226。现在我们这条 SQL 语句只操作了一张表，如果要操作多张表呢？每个表都有一个对应的 Table_map event、都会 map 到一个单独的数字，用于区分对不同表的操作。 在 mysqlbinlog 的命令中，使用了 -vv 参数是为了把内容都解析出来，所以从结果里面可以看到各个字段的值（比如，@1=4、 @2=4 这些值）。 binlog_row_image 的默认配置是 FULL，因此 Delete_event 里面，包含了删掉的行的所有字段的值。如果把 binlog_row_image 设置为 MINIMAL，则只会记录必要的信息，在这个例子里，就是只会记录 id=4 这个信息。 最后的 Xid event，用于表示事务被正确地提交了。  当 binlog_format 使用 row 格式的时候，binlog 里面记录了真实删除行的主键 id，这样 binlog 传到备库去的时候，就肯定会删除 id=4 的行，不会有主备删除不同行的问题。\n为什么会有 mixed 格式的 binlog？  因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。 但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。 所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。  也就是说，mixed 格式可以利用 statment 格式的优点，同时又避免了数据不一致的风险。\n因此，如果你的线上 MySQL 设置的 binlog 格式是 statement 的话，那基本上就可以认为这是一个不合理的设置。你至少应该把 binlog 的格式设置为 mixed。\n但现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row。这么做的理由有很多，一个最直接的好处：恢复数据。\n  从row格式的日志可以看出，binlog会把删掉的整行信息都保存下来，所以，如果你在执行完一条 delete 语句以后，发现删错数据了，可以直接把 binlog 中记录的 delete 语句转成 insert，把被错删的数据插入回去就可以恢复了。\n  如果你是执行错了 insert 语句呢？那就更直接了。row 格式下，insert 语句的 binlog 里会记录所有的字段信息，这些信息可以用来精确定位刚刚被插入的那一行。这时，你直接把 insert 语句转成 delete 语句，删除掉这被误插入的一行数据就可以了。\n  如果执行的是 update 语句的话，binlog 里面会记录修改前整行的数据和修改后的整行数据。所以，如果你误执行了 update 语句的话，只需要把这个 event 前后的两行信息对调一下，再去数据库里面执行，就能恢复这个更新操作了。\n  循环复制问题 现在可以知道，binlog 的特性确保了在备库执行相同的 binlog，可以得到与主库相同的状态。因此我们可以认为图1中所示的M-S结构中A、B两个节点的内容是一致的。\n但实际生产上使用比较多的是双 M 结构，也就是下图所示的主备切换流程。\n对比上图和图 1，你可以发现，双 M 结构和 M-S 结构，其实区别只是多了一条线，即：节点 A 和 B 之间总是互为主备关系。这样在切换的时候就不用再修改主备关系。\n但是，双 M 结构还有一个问题需要解决。\n业务逻辑在节点 A 上更新了一条语句，然后再把生成的 binlog 发给节点 B，节点 B 执行完这条更新语句后也会生成 binlog。（我建议你把参数 log_slave_updates 设置为 on，表示备库执行 relay log 后生成 binlog）。\n那么，如果节点 A 同时是节点 B 的备库，相当于又把节点 B 新生成的 binlog 拿过来执行了一次，然后节点 A 和 B 间，会不断地循环执行这个更新语句，也就是循环复制了。这个要怎么解决呢？\n从row格式的日志中可以看到，MySQL 在 binlog 中记录了这个命令第一次执行时所在实例的 server id。因此，可以用下面的逻辑，来解决两个节点间的循环复制的问题：\n 规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系； 一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog； 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。  按照这个逻辑，如果我们设置了双 M 结构，日志的执行流就会变成这样：\n 从节点 A 更新的事务，binlog 里面记的都是 A 的 server id； 传到节点 B 执行一次以后，节点 B 生成的 binlog 的 server id 也是 A 的 server id； 再传回给节点 A，A 判断到这个 server id 与自己的相同，就不会再处理这个日志。所以，死循环在这里就断掉了。  ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%A6%82%E8%A7%88mysql%E7%AF%87%E5%9B%9B%E4%B8%BB%E5%A4%87%E4%B8%8E%E9%AB%98%E5%8F%AF%E7%94%A8/","summary":"极客时间《MySQL实战45讲》笔记\n 主备的基本原理 在状态 1 中，客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。\n当需要切换的时候，就切成状态 2。这时候客户端读写访问的都是节点 B，而节点 A 是 B 的备库。\n那么从状态1切换到状态2的内部流程是什么样的？\n备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：\n 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。 sql_thread 读取中转日志，解析出日志里的命令，并执行。  binlog 的三种格式对比 主备复制依赖于bin log，那么bin log中是什么内容。","title":"概览MySQL篇三：主备与高可用"},{"content":" 极客时间《MySQL实战45讲》笔记\n MySQL中的隔离级别 见本地事务的隔离\n事务隔离的实现 事务之间的隔离是如何实现的?\n在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。\n假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。\n当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。\n但这种回滚日志不能一直保留，当系统判断，没有事务再需要用到这些回滚日志时，回滚日志会被删除，也即当系统中没有比这个回滚日志更早的read-view时。\n如何尽量避免长事务 为什么要避免长事务？\n长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。\n在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。\n除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。\n如何避免长事务？\nMySQL 的事务启动方式有以下几种：\n  显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。\n  set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。\n  因此，建议总是使用 set autocommit=1, 通过显式语句的方式来启动事务。\n但这样会多一次“交互”，针对这个问题，使用commit work and chain 语法。\n在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。\n可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。\nselect * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))\u0026gt;60 MVCC 在 MySQL 里，有两个“视图”的概念：\n  一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。\n  另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。\n  它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。\n在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。\nInnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。而每行数据也都是有多个版本的。\n每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。\n也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。如图 2 所示，就是一个记录被多个事务连续更新后的状态。\n如图所示，就是一个记录被多个事务连续更新后的状态。\n图中的三个虚线箭头，就是 undo log；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。\n按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。这是怎么实现的呢？\nInnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。\n数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。\n这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。\n而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。\n这个视图数组把所有的 row trx_id 分成了几种不同的情况。\n这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：\n 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况  a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。    一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：\n 版本未提交，不可见； 版本已提交，但是是在视图创建后提交的，不可见； 版本已提交，而且是在视图创建前提交的，可见。  更新操作\n对于更新操作来说，其显然不可能在历史数据上更新，否则可能会造成某个更新丢失，而必须在当前版本上进行更新。更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）\n如上图中，事务B的更新结果为（1, 3）。\n除了 update 语句外，select 语句如果加锁，也是当前读。\nMySQl中有哪些锁，分别有什么作用 全局锁 顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。\n全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。\n使用全局锁存在的问题 以前有一种做法，是通过 FTWRL 确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。\n但是让整库都只读，听上去就很危险：\n 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。  因此加全局锁可能不是太好。\n全局备份 备份时我们需要拿到一个一致性视图，保证在备份期间读取到的数据不发生变化，我们可以在可重复读隔离级别下开启一个事务。\n官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。\n有了这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。\n所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。\n表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL）。\n表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。\n举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。\n另一类表级的锁是 MDL（metadata lock）。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。\n因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。\n 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。  行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。\n顾名思义，行锁就是针对数据表中行记录的锁。这很好理解，比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。\n两阶段锁 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。\n 因此，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。\n 死锁和死锁检测 死锁现象 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。\n这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。\n死锁的处理 当出现死锁以后，有两种策略：\n  一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。\n  另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。\n  在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。\n但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。\n所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。\n每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。\n针对死锁检测造成的性能损耗，可以选择直接关闭死锁检测，但这必然带来一定的风险；或者我们可以选择控制并发度，如果同一行每次最多只有10个线程在更新，那么死锁检测的成本很低，就不会出现死锁检测占用太多资源的问题，但这种方式也很不好控制。\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%A6%82%E8%A7%88mysql%E7%AF%87%E4%B8%89%E9%94%81/","summary":"极客时间《MySQL实战45讲》笔记\n MySQL中的隔离级别 见本地事务的隔离\n事务隔离的实现 事务之间的隔离是如何实现的?\n在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。\n假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。\n当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。\n但这种回滚日志不能一直保留，当系统判断，没有事务再需要用到这些回滚日志时，回滚日志会被删除，也即当系统中没有比这个回滚日志更早的read-view时。\n如何尽量避免长事务 为什么要避免长事务？\n长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。\n在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。\n除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。\n如何避免长事务？\nMySQL 的事务启动方式有以下几种：\n  显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。\n  set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。\n  因此，建议总是使用 set autocommit=1, 通过显式语句的方式来启动事务。\n但这样会多一次“交互”，针对这个问题，使用commit work and chain 语法。\n在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。","title":"概览MySQL篇三：锁、事务和隔离"},{"content":" 极客时间《MySQL实战45讲》笔记\n 什么是change buffer，有什么作用 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。这个操作称为merge.\n这里需要注意的是，change buffer看起来像是内存缓存一类的东西，但是change buffer是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。\n如果将change buffer也存在磁盘上，而数据也是存储在磁盘上，那么change buffer相比直接读取磁盘数据快在哪里呢？\n从磁盘读取一条记录，是随机读写，而写change buffer，是顺序读写。这二者的速度存在较大差异。随机读写由于存在磁头移道等物理操作，因此比较慢，但顺序读写比较快速。\nchange buffer的限制 change buffer只是暂时的将更新操作保存下来，而并没有去读取真正的数据。考虑以下情况，表中要求某一字段为唯一的，而在更新时不小心插入了一个与原有某数据重复的条目，这显然是不被允许的。\n因此，当表中存在唯一索引、唯一值等限制，这时候就不能用change buffer了。只有普通索引和不存在值唯一性约束的列，才可以用change buffer。\nchange buffer用的是buffer pool中的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。\nchange buffer的使用场景 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。\n因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。\n反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。\n什么是redo log， 有什么作用 在MySQL中，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。因此，我们可以使用类似“缓存”的思路来解决这个问题。\nWAL:Write-Ahead Logging，先写日志，再写磁盘。注意这里不是Write ahead logging(在记日志之前写)，而是Write-Ahead Logging：在写之前记日志。\n具体说来，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这个时候更新就算完成了（WAL）。同时，InnoDB 引擎会在适当的时候(系统比较空闲的时候或其他情况)，将这个操作记录更新到磁盘里面.\n但redo log并不是无限大的，如果当前系统比较繁忙，redo log很快就被写满，那么这时候系统只能先暂停处理请求，转而把redo log中的内容刷到磁盘上。redo log的结构类似循环队列。\nredo log并不是在内存里，而是存储在磁盘上，其提高读写速度的关键在于将原来查询数据的随机读写转化为顺序读写，因此速度快的多。\nredo log为MySQL提供了crash-safe的能力，当系统突然宕机，数据库中的更新不会丢失，可以完整的恢复。\n什么是binlog，有什么作用 redo log是InnoDB引擎特有的日志系统，而binlog才是MySQL的“亲儿子”。\nbinlog是记录所有数据库表结构变更以及表数据修改的二进制日志，不会记录SELECT和SHOW这类操作。\nbinlog日志是以事件形式记录，还包含语句所执行的消耗时间。\nbinlog对数据进行“存档”，从而可通过binlog对数据进行恢复，同时通过binlog还可进行主从备份、复制等。\n为什么MySQL有两个日志系统，有什么差别吗，能不能只用其中一个 MySQL分为server层和引擎层，而引擎层是可被替换的。MySQL自带的引擎叫MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档. InnoDB是另一家公司以插件形式引入MySQL的，而redo log就是InnoDB用来实现crash-safe的日志系统。\n这两种日志有以下三点不同：\n  redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。\n  redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。\n  redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n  能不能只使用其中某一个日志？答曰：最好不要.\n 只使用binlog，则MySQL不能做到crash-safe，也就是说如果数据库宕机，那么可能会造成数据的丢失。 只使用redo log，则MySQL不能做到数据存档，无法进行数据恢复，主从备份等操作。  因此，binlog和redo log缺一不可，redo log虽然是“后娘养的”，但是对MySQL有很大的作用。\n什么是两阶段提交，为什么需要两阶段提交 一个更新语句的执行流程如下：\n  执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。\n  执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。\n  引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。\n  执行器生成这个操作的 binlog，并把 binlog 写入磁盘。\n  执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。\n  在上面步骤的后三部中，我们先是将更新操作记录到redo log中，但并不提交事务，再写入binlog，最后再把刚才写入的redo log提交。这个步骤称为两阶段提交\n为什么要使用两阶段提交？答曰：为了使两份日志之间的逻辑一致。\n为了说明这个问题。对于如下语句：update a = 1 where a = 0, 考虑下面两种情况:\n 先提交redo log，再提交binlog。  如果在redo log提交完成后数据库宕机，系统恢复后redo log将数据恢复, 这一行已经成功的被修改为1，但binlog中并没有这条数据，如果这时候用binlog来恢复临时库的话，由于这个语句的binlog丢失，因此刚才的改动并不会应用到临时库上，临时库与原库的值不同，造成了数据不一致。\n先写binlog再写redo log  如果写完binlog之后系统崩溃，这时候binlog已经记录了修改数据的逻辑。但崩溃恢复后由于redo log中没有这条更新，因此改动并不会应用到数据表上,这一行依然是0.如果这时用binlog来恢复数据或主从复制，那么binlog的记录中多了一个事务出来，恢复出来的行为1,造成了数据不一致。\n对于两阶段提交，它是如何保证数据一致性的呢？\n对于两阶段提交：①redo log prepare ----\u0026gt; ②写binlog ----\u0026gt; ③redo log commit\n如果在②之前崩溃，重启恢复后发现没有commit，回滚，事务不成功。备份恢复时，没有binlog，数据一致。\n如果在③之前崩溃，重启恢复后虽然没有commit，但prepare和binlog都完成，将自动commit。备份恢复时，binlog完整，数据一致。\nbinlog的写入机制 binlog的写入逻辑比较简单：事务执行过程中，先把日志写道binlog cache，事务提交后，再把binlog cache写到binlog文件中。\n但，一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了 binlog cache 的保存问题。\n系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。\n事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。状态如图所示。\n可以看到，每个线程有自己 binlog cache，但是共用同一份 binlog 文件。\n 图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。 图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。  write 和 fsync 的时机，是由参数 sync_binlog 控制的：\n sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync； sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N\u0026gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。  因此，在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。\n但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。\nredo log的写入机制 redo log buffer 在一个事务的更新过程中，日志是要写多次的。比如下面这个事务：\nbegin; insert into t1 ... insert into t2 ... commit; 这个事务要往两个表中插入记录，插入数据的过程中，生成的日志都得先保存起来，但又不能在还没 commit 的时候就直接写到 redo log 文件里。\n所以，redo log buffer 就是一块内存，用来先存 redo 日志的。也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。\n但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。\n但redo log buffer 里面的内容，不是每次生成后都要直接持久化到磁盘。\n如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。\nredo log的三种状态  存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分； 写到磁盘 (write)，但是没有持久化（fsync），物理上是在文件系统的 page cache 里面，也就是图中的黄色部分； 持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。  日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。\nredo log 的写入策略 为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：\n 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘； 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。  redo log的写入时机 InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。\n注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。\n也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。\n实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。\n  一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。\n  另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。\n  双1配置 指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。\n但事务提交时使用的是两阶段提交，redo log除了prepare阶段外还需要commit阶段，为什么这里只需要在prepare阶段进行一次fsync呢。\n如果把 innodb_flush_log_at_trx_commit 设置成 1，那么 redo log 在 prepare 阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于 prepare 的 redo log，再加上 binlog 来恢复的。\n每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB 就认为 redo log 在 commit 的时候就不需要 fsync 了，只会 write 到文件系统的 page cache 中就够了。\n组提交 group commit LSN: 日志逻辑序列号（log sequence number，LSN）。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log.\n如图所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160。\n从图中可以看到， trx1 是第一个到达的，会被选为这组的 leader；等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160；trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘；这时候 trx2 和 trx3 就可以直接返回了。所以，一次组提交里面，组员越多，节约磁盘 IOPS 的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了。\nLSN: 日志逻辑序列号（log sequence number，LSN）。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log.\n如图所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160。\n从图中可以看到，\n trx1 是第一个到达的，会被选为这组的 leader； 等 trx1 要开始写盘的时候，这个组里面已经有了三个事务，这时候 LSN 也变成了 160； trx1 去写盘的时候，带的就是 LSN=160，因此等 trx1 返回时，所有 LSN 小于等于 160 的 redo log，都已经被持久化到磁盘；这时候 trx2 和 trx3 就可以直接返回了。 所以，一次组提交里面，组员越多，节约磁盘 IOPS 的效果越好。  但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了。\n因此可能会存在这种现象：从 MySQL 看到的 TPS 是每秒两万的话，也即每秒会写四万次磁盘。但用工具测试出来，磁盘能力两万左右，怎么能实现两万的 TPS？了解了组提交后我们就会知道，这是用组提交完成的。\n在并发更新场景下，第一个事务写完 redo log buffer 以后，接下来这个 fsync 越晚调用，组员可能越多，节约 IOPS 的效果就越好。\nMySQL 对组提交的优化 前面我们知道，日志提交为两阶段提交，①redo log prepare ----\u0026gt; ②写binlog ----\u0026gt; ③redo log commit。但实际上写bin log是分为两步的：\n 先把 binlog 从 binlog cache 中写到磁盘上的 binlog 文件； 调用 fsync 持久化。  MySQL 为了让组提交的效果更好，把 redo log 做 fsync 的时间拖到了步骤 1 之后。也就是说，两阶段提交变成了这样： 这么一来，binlog 也可以组提交了。在执行图中第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog 已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。\n不过通常情况下第 3 步执行得会很快，所以 binlog 的 write 和 fsync 间的间隔时间短，导致能集合到一起持久化的 binlog 比较少，因此 binlog 的组提交的效果通常不如 redo log 的效果那么好。\n如果你想提升 binlog 组提交的效果，可以通过设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 来实现。binlog_group_commit_sync_delay 参数，表示延迟多少微秒后才调用 fsync;binlog_group_commit_sync_no_delay_count 参数，表示累积多少次以后才调用 fsync。\n这两个条件是或的关系，也就是说只要有一个满足条件就会调用 fsync。所以，当 binlog_group_commit_sync_delay 设置为 0 的时候，binlog_group_commit_sync_no_delay_count 也无效了。\n分析到这里可以得知，WAL机制的快速主要得益于两方面：\n redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快； 组提交机制，可以大幅度降低磁盘的 IOPS 消耗。  ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%A6%82%E8%A7%88mysql%E7%AF%87%E4%BA%8C%E6%8C%81%E4%B9%85%E5%8C%96/","summary":"极客时间《MySQL实战45讲》笔记\n 什么是change buffer，有什么作用 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。这个操作称为merge.\n这里需要注意的是，change buffer看起来像是内存缓存一类的东西，但是change buffer是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。\n如果将change buffer也存在磁盘上，而数据也是存储在磁盘上，那么change buffer相比直接读取磁盘数据快在哪里呢？\n从磁盘读取一条记录，是随机读写，而写change buffer，是顺序读写。这二者的速度存在较大差异。随机读写由于存在磁头移道等物理操作，因此比较慢，但顺序读写比较快速。\nchange buffer的限制 change buffer只是暂时的将更新操作保存下来，而并没有去读取真正的数据。考虑以下情况，表中要求某一字段为唯一的，而在更新时不小心插入了一个与原有某数据重复的条目，这显然是不被允许的。\n因此，当表中存在唯一索引、唯一值等限制，这时候就不能用change buffer了。只有普通索引和不存在值唯一性约束的列，才可以用change buffer。\nchange buffer用的是buffer pool中的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。\nchange buffer的使用场景 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。\n因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。\n反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。","title":"概览MySQL篇二：持久化"},{"content":" 极客时间《MySQL实战45讲》笔记\n 索引是什么，为何存在，以什么样的结构组织或存储索引? 简单来说，索引是一个目录，用来对数据进行快速的查找。就像一本厚厚的字典，想要查询某个字或者词语，我们固然可以一页页翻阅整本词典，但更好的方式是通过拼音索引或者笔画索引到这条记录。\n索引可以有效减小查询的资源消耗，但索引不是毫无代价的，大量的创建索引会造成存储空间的损耗，我们要根据业务需求，有目的的创建对业务有帮助的索引。\n在MySQL的InnoDB引擎中，索引是以B+树的形式存在的。B+树的节点存储在物理页上。\n根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。\n非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。\n为什么更推荐使用自增主键? 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的：\nNOT NULL PRIMARY KEY AUTO_INCREMENT。 B+树天然就是有序的，当我们想在上图中插入一个ID=400的记录，那么可能需要进行页分裂操作，这就需要挪动后面的数据。但如果想插入一个ID=700的值，只需要在最后附加一条记录就可以，不需要对前面的值就行操作。\n自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。\n除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。\n使用索引查询的过程是怎么样的，什么叫回表、覆盖索引？ 对于如下表：\nmysql\u0026gt; create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT \u0026#39;\u0026#39;, index k(k)) engine=InnoDB; insert into T values(100,1, \u0026#39;aa\u0026#39;),(200,2,\u0026#39;bb\u0026#39;),(300,3,\u0026#39;cc\u0026#39;),(500,5,\u0026#39;ee\u0026#39;),(600,6,\u0026#39;ff\u0026#39;),(700,7,\u0026#39;gg\u0026#39;); 当执行select * from T where k between 3 and 5时，过程如下：\n 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 再到 ID 索引树查到 ID=300 对应的 R3； 在 k 索引树取下一个值 k=5，取得 ID=500； 再回到 ID 索引树查到 ID=500 对应的 R4； 在 k 索引树取下一个值 k=6，不满足条件，循环结束。  可以看到，MySQL先在k索引树上查找满足条件的记录，拿到主键，然后再到主键索引树上去取整条记录。这个用主键去主键索引上取数据的操作就叫做回表。\n回表的过程重新访问了主键索引，有没有什么办法可以避免回表？\n如果将上面的查询语句改成select ID from T where k between 3 and 5，这时我们查询k索引树的时候，由于ID已经在k键索引上存在了，因此就不需要再进行回表操作。索引覆盖了我们的查询需求，称为覆盖索引。\n什么叫最左前缀，索引下推 对于一个记录居民身份信息的表，其（name, age）索引如下： 当我们想要在这张表上查询name=张三的记录时，可以通过索引快速定位到ID4，然后继续往后查找。类似的，当我们想要查找name like 张%的记录时，也可以利用这个索引快速定位到满足条件的第一个记录ID3.\n可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。\n对于如下语句：\nmysql\u0026gt; select * from tuser where name like \u0026#39;张%\u0026#39; and age=10 and ismale=1; 在MySQL5.6之前，在（name， age）索引上找到符合name like '张%'的记录后，仍然需要根据索引上存储的主键去主索引上回表，待将整行数据全部取出后再进行判断。\n但仔细观察可以发现，（name， age）联合索引上已经存在age字段，也就是说，对于age=10这个条件判断，我们完全不需要进行回表，只用当前索引上的数据就可以进行判断。这叫做索引下推\n扫描行数是如何判断的？ 当一个语句被分析完毕后，会由优化器来选择索引，目的是找出一个最优的执行方案，并用最小的代价去执行语句。其中，扫描行数是一个比较重要的判断标准。\nMySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。\n这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。\n可以使用show index方法，查看索引的基数。但，基数可能会不准确。\nMySQL采用采样的统计的方法得到索引的基数。\n为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。\n采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。\n而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。\n在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数innodb_stats_persistent 的值来选择：\n 设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。 设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。  由于是采样统计，所以不管 N 是 20 还是 8，这个基数都是很容易不准的。\n如何修正索引统计信息？\nanalyze table \u0026lt;table_name\u0026gt;; 对于字符串索引，有哪些优化方式 对于邮箱等字段，可以使用前缀索引。但使用前缀索引存在一个问题：即使前缀索引中已经包含了查询所需要的全部信息，但MySQL仍然要进行回表，因为系统并不确定前缀索引的定义是否截断了完整信息。也就是说，使用前缀索引就用不上覆盖索引对查询性能的优化了.\n对于身份证号这种类型，可以使用倒序存储，这样可以更好的使用前缀索引。\n还可以使用hash字段，在表上再创建一个整数字段，来保存字段的校验码，同时在这个字段上创建索引。\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%A6%82%E8%A7%88mysql%E7%AF%87%E4%B8%80%E7%B4%A2%E5%BC%95/","summary":"极客时间《MySQL实战45讲》笔记\n 索引是什么，为何存在，以什么样的结构组织或存储索引? 简单来说，索引是一个目录，用来对数据进行快速的查找。就像一本厚厚的字典，想要查询某个字或者词语，我们固然可以一页页翻阅整本词典，但更好的方式是通过拼音索引或者笔画索引到这条记录。\n索引可以有效减小查询的资源消耗，但索引不是毫无代价的，大量的创建索引会造成存储空间的损耗，我们要根据业务需求，有目的的创建对业务有帮助的索引。\n在MySQL的InnoDB引擎中，索引是以B+树的形式存在的。B+树的节点存储在物理页上。\n根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。\n非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。\n为什么更推荐使用自增主键? 自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的：\nNOT NULL PRIMARY KEY AUTO_INCREMENT。 B+树天然就是有序的，当我们想在上图中插入一个ID=400的记录，那么可能需要进行页分裂操作，这就需要挪动后面的数据。但如果想插入一个ID=700的值，只需要在最后附加一条记录就可以，不需要对前面的值就行操作。\n自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。\n除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。\n使用索引查询的过程是怎么样的，什么叫回表、覆盖索引？ 对于如下表：\nmysql\u0026gt; create table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT \u0026#39;\u0026#39;, index k(k)) engine=InnoDB; insert into T values(100,1, \u0026#39;aa\u0026#39;),(200,2,\u0026#39;bb\u0026#39;),(300,3,\u0026#39;cc\u0026#39;),(500,5,\u0026#39;ee\u0026#39;),(600,6,\u0026#39;ff\u0026#39;),(700,7,\u0026#39;gg\u0026#39;); 当执行select * from T where k between 3 and 5时，过程如下：\n 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 再到 ID 索引树查到 ID=300 对应的 R3； 在 k 索引树取下一个值 k=5，取得 ID=500； 再回到 ID 索引树查到 ID=500 对应的 R4； 在 k 索引树取下一个值 k=6，不满足条件，循环结束。  可以看到，MySQL先在k索引树上查找满足条件的记录，拿到主键，然后再到主键索引树上去取整条记录。这个用主键去主键索引上取数据的操作就叫做回表。","title":"概览MySQL篇一：索引"},{"content":" 大约2021年8月份，go社区对切片容量增长的方式进行了一次调整。具体讨论可见：https://groups.google.com/g/golang-nuts/c/UaVlMQ8Nz3o\n 1. 之前的增长规则 先看源码\n runtime/slice.go\n func growslice(et *_type, old slice, cap int) slice { // 省略部分条件检查  // ...  newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { if old.cap \u0026lt; 1024 { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow \t// and prevent an infinite loop. \tfor 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { newcap += newcap / 4 } // Set newcap to the requested cap when \t// the newcap calculation overflowed. \tif newcap \u0026lt;= 0 { newcap = cap } } } // 对切片进行内存对齐，可能会调整切片容量大小  // ...  return slice{p, old.len, newcap} } 从代码中可以看出，当cap \u0026lt; 1024时，切片容量增长2倍，当cap \u0026gt; 1024时增长1.25倍，当然，在经过内存对齐等操作后，切片的长度可能不会严格等于原来的2倍或1.25倍。\n存在的问题：\n not monotonically increasing （不平滑）  x1 := make([]int, 897) x2 := make([]int, 1024) y := make([]int, 100) println(cap(append(x1, y...))) // 2048 println(cap(append(x2, y...))) // 1280 对于append(x1, y...), 进行growslice，传入的cap为所需的最小容量，为：cap(x1) + cap(y) = 997。 此时newcap为897, doublecap为897×2 = 1794。由于old.cap = 897 \u0026lt; 1024, 所以newcap = doublecap，为1794,再经过内存对齐后，最终的容量为2048.\n对于append(x2, y...), 进行growslice，传入的cap为1124,此时newcap为1024, doublecap为1024×2 = 2048. 由于cap \u0026lt; doublecap且newcap\u0026lt; cap,因此newcap += newcap / 4 = 1280.经过内存对齐后，仍为1280.。\n这样就产生了一个问题，在一定条件下，如果cap(A) \u0026lt; cap(B), 但二者都append一个相同长度的C后，反而cap(A) \u0026gt; cap(B)。\nnot symmetrical （不对称）  x := make([]int, 98) y := make([]int, 666) println(cap(append(x, y...))) // 768 println(cap(append(y, x...))) // 1360 2. 调整之后 源码：\nfunc growslice(et *_type, old slice, cap int) slice { // 省略部分条件检查  // ...  newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { const threshold = 256 if old.cap \u0026lt; threshold { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow \t// and prevent an infinite loop. \tfor 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { // Transition from growing 2x for small slices \t// to growing 1.25x for large slices. This formula \t// gives a smooth-ish transition between the two. \tnewcap += (newcap + 3*threshold) / 4 } // Set newcap to the requested cap when \t// the newcap calculation overflowed. \tif newcap \u0026lt;= 0 { newcap = cap } } } // 对切片进行内存对齐，可能会调整切片容量大小  // ...  return slice{p, old.len, newcap} } 进行的调整：\n 增加了threshold = 256 增长策略不同的阈值从1024变为256 当切片容量大于阈值时，增长策略变化  原来的问题：\n 增长不平滑  x1 := make([]int, 897) x2 := make([]int, 1024) y := make([]int, 100) println(cap(append(x1, y...))) // 1360 println(cap(append(x2, y...))) // 1536 增长不对称  x := make([]int, 98) y := make([]int, 666) println(cap(append(x, y...))) // 768 println(cap(append(y, x...))) // 1024 增长不平滑的问题得到了较好的解决，同时不对称的问题也得到了缓解。\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E5%88%87%E7%89%87append%E8%A7%84%E5%88%99/","summary":"大约2021年8月份，go社区对切片容量增长的方式进行了一次调整。具体讨论可见：https://groups.google.com/g/golang-nuts/c/UaVlMQ8Nz3o\n 1. 之前的增长规则 先看源码\n runtime/slice.go\n func growslice(et *_type, old slice, cap int) slice { // 省略部分条件检查  // ...  newcap := old.cap doublecap := newcap + newcap if cap \u0026gt; doublecap { newcap = cap } else { if old.cap \u0026lt; 1024 { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow \t// and prevent an infinite loop. \tfor 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { newcap += newcap / 4 } // Set newcap to the requested cap when \t// the newcap calculation overflowed.","title":"切片append规则"},{"content":"反射是一个接口，其定义如下：\ntype Type interface { // 返回具体类型在内存分配时的字节分配方式 \tAlign() int // 返回具体类型在结构体中作为一个字段是内存对齐方式 \tFieldAlign() int // 返回具体类型的第x个方法 \tMethod(int) Method // 根据函数名返回具体类型的方法 \tMethodByName(string) (Method, bool) // 返回类型的方法个数 \tNumMethod() int // 返回类型的名字 \tName() string // 返回类型的包名 \tPkgPath() string // 返回类型所占内存字节大小 \tSize() uintptr // 返回类型的简单描述，如：main.User \tString() string // 返回这个类型的Kind \tKind() Kind // 检查类型是否实现了某个接口 \t//stringer := reflect.TypeOf((*fmt.Stringer)(nil)).Elem() \t//fmt.Println(reflect.ValueOf(u).Type().Implements(stringer)) \tImplements(u Type) bool // 检查类型是否可以被赋值给某个类型 \tAssignableTo(u Type) bool // 检查类型是否可以转换到类型u \tConvertibleTo(u Type) bool // 检查类型是否可比较 \tComparable() bool // 返回Int, Uint, Float, or Complex kinds.的字节大小 \tBits() int // 返回一个通道类型的方向 \tChanDir() ChanDir // 判断一个函数类型的最后一个参数是否是\u0026#34;...\u0026#34;参数 \tIsVariadic() bool // 返回Array, Chan, Map, Ptr, or Slice类型的元素的类型 \tElem() Type // 返回结构体的第i个字段的详细信息 \tField(i int) StructField // 返回结构体内的嵌套结构体 \tFieldByIndex(index []int) StructField // 根据名字返回结构体内的某个字段 \tFieldByName(name string) (StructField, bool) // 根据match函数的返回值返回结构体内某个字段 \tFieldByNameFunc(match func(string) bool) (StructField, bool) // 返回函数的第i个参数类型 \tIn(i int) Type // 返回map的键的类型 \tKey() Type // 返回数组的长度 \tLen() int // 返回结构体的字段个数 \tNumField() int // 返回参数的入参个数 \tNumIn() int // 返回函数的返回值个数 \tNumOut() int // 返回函数第i个返回值的类型 \tOut(i int) Type common() *rtype uncommon() *uncommonType } 从接口定义中我们可以知道:\n对于所有的类型：\n 字节对齐方式  具体类型在内存分配时的字节分配方式 返回具体类型在结构体中作为一个字段是内存对齐方式   方法  具体类型的第x个方法 根据函数名获取具体类型的方法 检查类型是否实现了某个接口   基本信息  方法个数 类型名 类型包名 类型所占字节大小 类型的简单描述（如main.User）   获取类型的Kind  关于类型转换：\n 检查类型是否实现了某个接口 检查类型是否可以被赋值给某个类型 检查类型是否可以转换到类型u  对于通道:\n 获取通道的方向 获取通道内元素的类型  对于函数：\n 检查其最后一个参数是否是...Type类型 获取函数的入参、返回函数的返回值个数 根据下标获取函数入参、返回值的类型  对于结构体：\n 获取结构体的第i个字段的详细信息 获取结构体内的嵌套结构体 根据名字获取结构体内的某个字段 根据match函数的返回值获取结构体内某个字段  对于map:\n 获取map的键的类型  对于Array:\n 获取Array的长度  其他:\n 检查类型是否可比较  ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E7%90%86%E8%A7%A3%E5%8F%8D%E5%B0%84%E4%B9%8B%E4%B8%80%E4%B8%AAreflect.type%E5%8F%AF%E4%BB%A5%E5%81%9A%E4%BB%80%E4%B9%88/","summary":"反射是一个接口，其定义如下：\ntype Type interface { // 返回具体类型在内存分配时的字节分配方式 \tAlign() int // 返回具体类型在结构体中作为一个字段是内存对齐方式 \tFieldAlign() int // 返回具体类型的第x个方法 \tMethod(int) Method // 根据函数名返回具体类型的方法 \tMethodByName(string) (Method, bool) // 返回类型的方法个数 \tNumMethod() int // 返回类型的名字 \tName() string // 返回类型的包名 \tPkgPath() string // 返回类型所占内存字节大小 \tSize() uintptr // 返回类型的简单描述，如：main.User \tString() string // 返回这个类型的Kind \tKind() Kind // 检查类型是否实现了某个接口 \t//stringer := reflect.TypeOf((*fmt.Stringer)(nil)).Elem() \t//fmt.Println(reflect.ValueOf(u).Type().Implements(stringer)) \tImplements(u Type) bool // 检查类型是否可以被赋值给某个类型 \tAssignableTo(u Type) bool // 检查类型是否可以转换到类型u \tConvertibleTo(u Type) bool // 检查类型是否可比较 \tComparable() bool // 返回Int, Uint, Float, or Complex kinds.","title":"理解反射之：一个reflect.Type可以做什么"},{"content":" range循环时，使用的是被迭代的元素的副本  type T struct { n int } func main() { ts := [2]T{} for i, t := range ts { switch i { case 0: t.n = 3 // 被访问的是ts的副本  ts[1].n = 9 case 1: fmt.Print(t.n, \u0026#34; \u0026#34;) } } fmt.Print(ts) } 输出：0 {{0} {9}} range 循环语句使用的临时变量  func main() { h := make([]*int, 3) u := []int{1, 2, 3} for i, v := range u { h[i] = \u0026amp;v } for i := range h { fmt.Println(*h[i]) } } 输出： 3 3 3 ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/for...range%E8%A6%81%E7%82%B9/","summary":"range循环时，使用的是被迭代的元素的副本  type T struct { n int } func main() { ts := [2]T{} for i, t := range ts { switch i { case 0: t.n = 3 // 被访问的是ts的副本  ts[1].n = 9 case 1: fmt.Print(t.n, \u0026#34; \u0026#34;) } } fmt.Print(ts) } 输出：0 {{0} {9}} range 循环语句使用的临时变量  func main() { h := make([]*int, 3) u := []int{1, 2, 3} for i, v := range u { h[i] = \u0026amp;v } for i := range h { fmt.","title":"for...range要点"},{"content":"1. Go中对指针的限制  Go 的指针不能进行数学运算。 不同类型的指针不能相互转换。 不同类型的指针不能使用 == 或 != 比较。只有在两个指针类型相同或者可以相互转换的情况下，才可以对两者进行比较。另外，指针可以通过 == 和 != 直接和 nil 作比较。 不同类型的指针变量不能相互赋值。  使用unsafe包，可以一定程度上打破这些限制，那么为什么要打破这些限制。请看下文。\n2. unsafe.Pointer unsafe.Pointer的定义\ntype ArbitraryType int type Pointer *ArbitraryType unsafe 包提供了 2 点重要的能力：\n 任何类型的指针和 unsafe.Pointer 可以相互转换。 uintptr 类型和 unsafe.Pointer 可以相互转换。  pointer 不能直接进行数学运算，但可以把它转换成 uintptr，对 uintptr 类型进行数学运算，再转换成 pointer 类型。利用这两个对象的相互转换，就可以打破上述4个限制。\n// uintptr 是一个整数类型，它足够大，可以存储 type uintptr uintptr 还有一点要注意的是，uintptr 并没有指针的语义，意思就是 uintptr 所指向的对象会被 gc 无情地回收.而 unsafe.Pointer 有指针语义，可以保护它所指向的对象在“有用”的时候不会被垃圾回收。\n3. 利用unsafe获取slice和map的长度 slice和map的长度都存储在其内部变量中，因此我们先来看这两个结构体定义：\n// runtime/slice.go type slice struct { array unsafe.Pointer // 元素指针  len int // 长度  cap int // 容量 } 调用 make 函数新建一个 slice，底层调用的是 makeslice 函数，返回的是 slice 结构体：\nfunc makeslice(et *_type, len, cap int) slice 因此我们可以通过 unsafe.Pointer 和 uintptr 进行转换，得到 slice 的字段值。\nfunc main() { s := make([]int, 9, 20) // Len: \u0026amp;s =\u0026gt; pointer =\u0026gt; uintptr =\u0026gt; pointer =\u0026gt; *int =\u0026gt; int \tvar Len = *(*int)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026amp;s)) + uintptr(8))) fmt.Println(Len, len(s)) // 9 9  // Cap: \u0026amp;s =\u0026gt; pointer =\u0026gt; uintptr =\u0026gt; pointer =\u0026gt; *int =\u0026gt; int \tvar Cap = *(*int)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026amp;s)) + uintptr(16))) fmt.Println(Cap, cap(s)) // 20 20 } 总体思路是，先对结构体取指针，再转换成unsafe.Pointer，再转换成uintptr,然后做加法使指针偏移，然后重新转换为unsafe.Pointer,再转换成（*int）类型，最后取引用，得到值。可以看出，其中的重点在于计算偏移量。\n对于map\ntype hmap struct { count int flags uint8 B uint8 noverflow uint16 hash0 uint32 buckets unsafe.Pointer oldbuckets unsafe.Pointer nevacuate uintptr extra *mapextra } 和 slice 不同的是，makemap 函数返回的是 hmap 的指针，注意是指针：\nfunc makemap(t *maptype, hint int64, h *hmap, bucket unsafe.Pointer) *hmap 我们依然能通过 unsafe.Pointer 和 uintptr 进行转换，得到 hamp 字段的值，只不过，现在 count 变成二级指针了：\nfunc main() { mp := make(map[string]int) mp[\u0026#34;q\u0026#34;] = 100 mp[\u0026#34;s\u0026#34;] = 18 // \u0026amp;mp =\u0026gt; pointer =\u0026gt; **int =\u0026gt; int \tcount := **(**int)(unsafe.Pointer(\u0026amp;mp)) fmt.Println(count, len(mp)) // 2 2 } 4. 修改私有成员 总体思路是：通过 offset 函数获取结构体成员的偏移量，进而获取成员的地址，读写该地址的内存，就可以达到改变成员值的目的。\n这里需要注意的是：结构体会被分配一块连续的内存，结构体的地址也代表了第一个成员的地址。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;unsafe\u0026#34; ) type Programmer struct { name string language string } func main() { p := Programmer{\u0026#34;a\u0026#34;, \u0026#34;go\u0026#34;} fmt.Println(p) name := (*string)(unsafe.Pointer(\u0026amp;p)) *name = \u0026#34;b\u0026#34; lang := (*string)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026amp;p)) + unsafe.Offsetof(p.language))) // lang := (*string)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026amp;p)) + unsafe.Sizeof(string(\u0026#34;\u0026#34;)))) \t*lang = \u0026#34;Golang\u0026#34; fmt.Println(p) } 5. 字符串和byte切片的零拷贝转换 string和byte切片的内部结构如下：\ntype StringHeader struct { Data uintptr Len int } type SliceHeader struct { Data uintptr Len int Cap int } 转换：\nfunc string2bytes(s string) []byte { return *(*[]byte)(unsafe.Pointer(\u0026amp;s)) } func bytes2string(b []byte) string{ return *(*string)(unsafe.Pointer(\u0026amp;b)) } 但这样转换会存在一个问题，字符串被转换为切片后，切片的cap无法准确得出。\nReferences https://golang.design/go-questions/stdlib/unsafe/zero-conv/\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E6%A0%87%E5%87%86%E5%BA%93%E4%B9%8Bunsafe/","summary":"1. Go中对指针的限制  Go 的指针不能进行数学运算。 不同类型的指针不能相互转换。 不同类型的指针不能使用 == 或 != 比较。只有在两个指针类型相同或者可以相互转换的情况下，才可以对两者进行比较。另外，指针可以通过 == 和 != 直接和 nil 作比较。 不同类型的指针变量不能相互赋值。  使用unsafe包，可以一定程度上打破这些限制，那么为什么要打破这些限制。请看下文。\n2. unsafe.Pointer unsafe.Pointer的定义\ntype ArbitraryType int type Pointer *ArbitraryType unsafe 包提供了 2 点重要的能力：\n 任何类型的指针和 unsafe.Pointer 可以相互转换。 uintptr 类型和 unsafe.Pointer 可以相互转换。  pointer 不能直接进行数学运算，但可以把它转换成 uintptr，对 uintptr 类型进行数学运算，再转换成 pointer 类型。利用这两个对象的相互转换，就可以打破上述4个限制。\n// uintptr 是一个整数类型，它足够大，可以存储 type uintptr uintptr 还有一点要注意的是，uintptr 并没有指针的语义，意思就是 uintptr 所指向的对象会被 gc 无情地回收.而 unsafe.Pointer 有指针语义，可以保护它所指向的对象在“有用”的时候不会被垃圾回收。\n3. 利用unsafe获取slice和map的长度 slice和map的长度都存储在其内部变量中，因此我们先来看这两个结构体定义：\n// runtime/slice.go type slice struct { array unsafe.","title":"标准库之unsafe"},{"content":"1. 简介 golangci-lint 是对golang进行静态代码检查的工具。其具有以下特性：\n 速度非常快：golangci-lint 是基于 gometalinter 开发的，但是平均速度要比 gometalinter 快 5 倍。golangci-lint 速度快的原因有三个：可以并行检查代码；可以复用 go build 缓存；会缓存分析结果。 可配置：支持 YAML 格式的配置文件，让检查更灵活，更可控。 IDE 集成：可以集成进多个主流的 IDE，例如 VS Code、GNU Emacs、Sublime Text、Goland 等。 linter 聚合器：1.41.1 版本的 golangci-lint 集成了 76 个 linter，不需要再单独安装这 76 个 linter。并且 golangci-lint 还支持自定义 linter。 最小的误报数：golangci-lint 调整了所集成 linter 的默认设置，大幅度减少了误报。 良好的输出：输出的结果带有颜色、代码行号和 linter 标识，易于查看和定位。  2. 安装 # 安装 go get github.com/golangci/golangci-lint/cmd/golangci-lint@v1.41.1 # 检查是否安装成功 golangci-lint version # 输出 golangci-lint 版本号，说明安装成功 golangci-lint has version v1.44.0 built from (unknown, mod sum: \u0026#34;h1:YJPouGNQEdK+x2KsCpWMIBy0q6MSuxHjkWMxJMNj/DU=\u0026#34;) on (unknown) 3. 基本使用 对项目做静态检查，非常简单, 在项目跟目录：\ngolangci-lint run 等同于\ngolangci-lint run ./... 指定检查目录：\ngolangci-lint run dir1 dir2/... dir3/file1.go 这里需要注意的是，golangci-lint不会递归检查目录下的子目录，要想递归检查，可使用dir/...\n4. linter golangci-lint预定义了一些检查规则, 称为linter\ngolangci-lint help linters Enabled by default linters: deadcode: Finds unused code [fast: false, auto-fix: false] errcheck: Errcheck is a program for checking for unchecked errors in go programs. These unchecked errors can be critical bugs in some cases [fast: false, auto-fix: false] gosimple (megacheck): Linter for Go source code that specializes in simplifying a code [fast: false, auto-fix: false] govet (vet, vetshadow): Vet examines Go source code and reports suspicious constructs, such as Printf calls whose arguments do not align with the format string [fast: false, auto-fix: false] ineffassign: Detects when assignments to existing variables are not used [fast: true, auto-fix: false] staticcheck (megacheck): Staticcheck is a go vet on steroids, applying a ton of static analysis checks [fast: false, auto-fix: false] structcheck: Finds unused struct fields [fast: false, auto-fix: false] ... 你可以选择只使用某些linter(使用-E/--enable)或不使用某些linter(使用-D/--disable)\n你还可以对这些linter进行更细粒度的配置，例如：\nlinters-settings: bidichk: # The following configurations check for all mentioned invisible unicode runes. # All runes are enabled by default. left-to-right-embedding: false right-to-left-embedding: false pop-directional-formatting: false left-to-right-override: false right-to-left-override: false left-to-right-isolate: false right-to-left-isolate: false first-strong-isolate: false pop-directional-isolate: false 访问https://golangci-lint.run/usage/linters/#errorlint查看更多示例\n5. 配置 配置文件的优先级低于命令行配置，使用文件配置后再次在命令行中指定配置，则命令行中指定的配置将覆盖文件中的配置。\n配置文件名称：\n .golangci.yml .golangci.yaml .golangci.toml .golangci.json  可用的设置如下：\n# Options for analysis running. run: # See the dedicated \u0026#34;run\u0026#34; documentation section. option: value # output configuration options output: # See the dedicated \u0026#34;output\u0026#34; documentation section. option: value # All available settings of specific linters. linters-settings: # See the dedicated \u0026#34;linters-settings\u0026#34; documentation section. option: value linters: # See the dedicated \u0026#34;linters\u0026#34; documentation section. option: value issues: # See the dedicated \u0026#34;issues\u0026#34; documentation section. option: value severity: # See the dedicated \u0026#34;severity\u0026#34; documentation section. option: value 具体设置可查看文档https://golangci-lint.run/usage/configuration\n6. 对某些代码忽略检查  忽略某一行的检查  var bad_name int //nolint 忽略某一行指定 linter 的检查，可以指定多个 linter，用逗号 , 隔开  var bad_name int //nolint:golint,unused 忽略某个代码块的检查  //nolint func allIssuesInThisFunctionAreExcluded() *string { // ... } //nolint:govet var ( a int b int ) 忽略某个文件的指定linter检查 在 package xx 上面一行添加//nolint注释。  //nolint:unparam package pkg ... 在使用 nolint 的过程中，有 3 个地方需要你注意。\n首先，如果启用了 nolintlint，你就需要在//nolint后面添加 nolint 的原因// xxxx。\n其次，你使用的应该是//nolint而不是// nolint。因为根据 Go 的规范，需要程序读取的注释 // 后面不应该有空格。\n最后，如果要忽略所有 linter，可以用//nolint；如果要忽略某个指定的 linter，可以用//nolint:,。\nReferences  https://golangci-lint.run/ https://time.geekbang.org/column/article/390401  ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E9%9D%99%E6%80%81%E4%BB%A3%E7%A0%81%E6%A3%80%E6%9F%A5-golangci-lint/","summary":"1. 简介 golangci-lint 是对golang进行静态代码检查的工具。其具有以下特性：\n 速度非常快：golangci-lint 是基于 gometalinter 开发的，但是平均速度要比 gometalinter 快 5 倍。golangci-lint 速度快的原因有三个：可以并行检查代码；可以复用 go build 缓存；会缓存分析结果。 可配置：支持 YAML 格式的配置文件，让检查更灵活，更可控。 IDE 集成：可以集成进多个主流的 IDE，例如 VS Code、GNU Emacs、Sublime Text、Goland 等。 linter 聚合器：1.41.1 版本的 golangci-lint 集成了 76 个 linter，不需要再单独安装这 76 个 linter。并且 golangci-lint 还支持自定义 linter。 最小的误报数：golangci-lint 调整了所集成 linter 的默认设置，大幅度减少了误报。 良好的输出：输出的结果带有颜色、代码行号和 linter 标识，易于查看和定位。  2. 安装 # 安装 go get github.com/golangci/golangci-lint/cmd/golangci-lint@v1.41.1 # 检查是否安装成功 golangci-lint version # 输出 golangci-lint 版本号，说明安装成功 golangci-lint has version v1.44.0 built from (unknown, mod sum: \u0026#34;h1:YJPouGNQEdK+x2KsCpWMIBy0q6MSuxHjkWMxJMNj/DU=\u0026#34;) on (unknown) 3.","title":"静态代码检查: golangci-lint"},{"content":"1. draw.io 在线画图工具 https://app.diagrams.net/\n2. sql to Gorm SQL语句转化为go结构体 https://sql2gorm.mccode.info/\n3. 一些编程字体 https://www.nerdfonts.com/font-downloads\n4. 团队协作工具 https://www.devbefore.com/product\n5. markdown 写作工具（开源免费） https://marktext.app/\n6. z 目录快速跳转 (强烈推荐) https://github.com/rupa/z (安装)https://yangchnet.github.io/Dessert/posts/tool/z/\n7. nvm npm多版本管理 (强烈推荐) https://github.com/nvm-sh/nvm\n8. lazydocker 容器查看工具 https://github.com/jesseduffield/lazydocker\n9. dive 镜像查看工具 https://github.com/wagoodman/dive\n10. corc 文件转发工具（任意主机） (强烈推荐) https://github.com/schollz/croc\n11. asciinema 终端会话记录工具 https://github.com/asciinema/asciinema\n12. tree 目录树工具 sudo apt-get install tree\n13. Dev hints 技术速查表 https://devhints.io/\n15. utools 快捷启动工具 (强烈推荐) https://u.tools/\n16. 语雀 文档写作工具（团队/个人） https://www.yuque.com/\n17. fehelper 一些小工具的集合（浏览器插件）(强烈推荐) https://www.baidufe.com/fehelper/index/index.html\n18. Dark Reader 一键切换页面风格（浏览器插件） https://chrome.google.com/webstore/detail/dark-reader/eimadpbcbfnmbkopoojfekhnkhdbieeh\n19. Octotree github目录树插件（浏览器插件） https://chrome.google.com/webstore/detail/octotree-github-code-tree/bkhaagjahfmjljalopjnoealnfndnagc\n20. Surfingkeys 像操作vim一样操作浏览器（浏览器插件） https://chrome.google.com/webstore/detail/surfingkeys/gfbliohnnapiefjpjlpjnehglfpaknnc\n21. roboform 密码管理器 http://www.roboform.com/\n22. oneTab 标签页管理：一键收集，一键还原 （浏览器插件） https://chrome.google.com/webstore/detail/onetab/chphlpgkkbolifaimnlloiipkdnihall\n23. transform 文本转化工具 https://transform.tools/\n24. smartpdf pdf处理工具 https://smartpdf.org/\n25. (Linux) 项目管理软件 ProjectLibre\n26. (Linux) 日程管理 Planner https://planner-todo.web.app/\n27. (Linux) flathub软件商店 https://flathub.org/home\n28. (Linux) UnOfficial Microsoft Todo https://github.com/davidsmorais/kuro\n","permalink":"http://yangchnet.github.io/Dessert/posts/tool/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E9%9B%86%E5%90%88/","summary":"1. draw.io 在线画图工具 https://app.diagrams.net/\n2. sql to Gorm SQL语句转化为go结构体 https://sql2gorm.mccode.info/\n3. 一些编程字体 https://www.nerdfonts.com/font-downloads\n4. 团队协作工具 https://www.devbefore.com/product\n5. markdown 写作工具（开源免费） https://marktext.app/\n6. z 目录快速跳转 (强烈推荐) https://github.com/rupa/z (安装)https://yangchnet.github.io/Dessert/posts/tool/z/\n7. nvm npm多版本管理 (强烈推荐) https://github.com/nvm-sh/nvm\n8. lazydocker 容器查看工具 https://github.com/jesseduffield/lazydocker\n9. dive 镜像查看工具 https://github.com/wagoodman/dive\n10. corc 文件转发工具（任意主机） (强烈推荐) https://github.com/schollz/croc\n11. asciinema 终端会话记录工具 https://github.com/asciinema/asciinema\n12. tree 目录树工具 sudo apt-get install tree\n13. Dev hints 技术速查表 https://devhints.io/\n15. utools 快捷启动工具 (强烈推荐) https://u.tools/\n16. 语雀 文档写作工具（团队/个人） https://www.yuque.com/\n17. fehelper 一些小工具的集合（浏览器插件）(强烈推荐) https://www.","title":"常用工具集合"},{"content":"1. nil channel   接收 接收goroutine阻塞\n  发送 发送个goroutine阻塞\n  2. 向无缓冲channel发送消息   接受队列有goroutine 接收端将收到消息\n  接收队列无goroutine 发送goroutine将阻塞\n  已有发送goroutine阻塞 发送goroutine将阻塞\n  3. 从无缓冲channel接收消息   无发送goroutine 接收端阻塞\n  有发送goroutine 收到消息\n  4. 向有缓冲channel发送消息   队列未满 正常发送\n  队列已满 发送端阻塞\n  5. 从有缓冲channel接收消息   队列中有消息 正常接收\n  队列中无消息 接收端阻塞\n  6. 对close channel的操作   向closed channel发送 panic\n  从无缓冲closed channel接收 接收到对应类型0值\n  从有缓冲closed channel接收\n 若channel中仍有消息，则仍可接收到消息 channel已空, 接收到对应类型零值    7. 关闭channel   关闭nll channel panic\n  关闭一个已经关闭的channel panic\n  ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/channel%E7%9A%84%E8%A1%8C%E4%B8%BA/","summary":"1. nil channel   接收 接收goroutine阻塞\n  发送 发送个goroutine阻塞\n  2. 向无缓冲channel发送消息   接受队列有goroutine 接收端将收到消息\n  接收队列无goroutine 发送goroutine将阻塞\n  已有发送goroutine阻塞 发送goroutine将阻塞\n  3. 从无缓冲channel接收消息   无发送goroutine 接收端阻塞\n  有发送goroutine 收到消息\n  4. 向有缓冲channel发送消息   队列未满 正常发送\n  队列已满 发送端阻塞\n  5. 从有缓冲channel接收消息   队列中有消息 正常接收\n  队列中无消息 接收端阻塞\n  6. 对close channel的操作   向closed channel发送 panic","title":"channel的行为"},{"content":"1. 安装插件系统  使用的是vim-plug\n curl -fLo ~/.vim/autoload/plug.vim --create-dirs \\  https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 2. 安装插件 打开~/.vimrc, 在其中写入：\n\u0026quot; Specify a directory for plugins \u0026quot; - For Neovim: stdpath('data') . '/plugged' \u0026quot; - Avoid using standard Vim directory names like 'plugin' call plug#begin('~/.vim/plugged') \u0026quot; Make sure you use single quotes \u0026quot; Shorthand notation; fetches https://github.com/junegunn/vim-easy-align Plug 'junegunn/vim-easy-align' \u0026quot; Any valid git URL is allowed Plug 'https://github.com/junegunn/vim-github-dashboard.git' \u0026quot; Multiple Plug commands can be written in a single line using | separators Plug 'SirVer/ultisnips' | Plug 'honza/vim-snippets' \u0026quot; On-demand loading Plug 'scrooloose/nerdtree', { 'on': 'NERDTreeToggle' } Plug 'tpope/vim-fireplace', { 'for': 'clojure' } \u0026quot; Using a non-default branch Plug 'rdnetto/YCM-Generator', { 'branch': 'stable' } \u0026quot; Using a tagged release; wildcard allowed (requires git 1.9.2 or above) Plug 'fatih/vim-go', { 'tag': '*' } \u0026quot; Plugin options Plug 'nsf/gocode', { 'tag': 'v.20150303', 'rtp': 'vim' } \u0026quot; Plugin outside ~/.vim/plugged with post-update hook Plug 'junegunn/fzf', { 'dir': '~/.fzf', 'do': './install --all' } \u0026quot; Unmanaged plugin (manually installed and updated) Plug '~/my-prototype-plugin' \u0026quot; auto complete Plug 'Valloric/YouCompleteMe' \u0026quot; auto complete ()[]{} Plug 'tmsvg/pear-tree' Plug 'preservim/nerdtree' Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' Plug 'preservim/tagbar' \u0026quot; Initialize plugin system call plug#end() :colorscheme atom-dark \u0026quot; 设置行号 set nu \u0026quot; 设置tab set ts=4 %retab! \u0026quot; 设置YouCompleteMe不需要预览 set completeopt-=preview if !isdirectory(\u0026amp;undodir) call mkdir(\u0026amp;undodir, 'p', 0700) endif set nobackup set undodir=~/.vim/undodir \u0026quot; 设置ctrl+n打开目录 map \u0026lt;C-n\u0026gt; :NERDTreeToggle\u0026lt;CR\u0026gt; \u0026quot; 设置F8打开tag bar nmap \u0026lt;F8\u0026gt; :TagbarToggle\u0026lt;CR\u0026gt; \u0026quot; 设置搜索高亮 set hlsearch \u0026quot; 停止搜索高亮的键映 nnoremap \u0026lt;silent\u0026gt; \u0026lt;BS\u0026gt; \u0026lt;BS\u0026gt;:nohlsearch\u0026lt;CR\u0026gt; 然后在命令模式下执行：PlugInstall\n等待一段时间，一些插件将会被安装到vim上\n3. 使用主题  这里选择molokai主题\n 下载主题文件\ngit clone https://github.com/tomasr/molokai.git cd molokai \u0026amp;\u0026amp; mv colors/molokai.vim ~/.vim/colors/ 使用主题\n在~/.vimrc文件中写入：\n:colorscheme molokai 重新进入vim，主题已经生效\n4. 自动补全  这里使用的是YouCompleteMe 在vim plug中添加插件\n Plug 'Valloric/YouCompleteMe' vim运行:PlugInstall\n添加完成后还需要手动编译\ncd ~/.vim/plugged/YouCompleteMe \u0026amp;\u0026amp; python3 install.py --all --verbose --all代表要安装所有语言的自动补全功能，如果你只需要某个语言的(如C语言)，可以使用--clang参数代替--all\n使用YouCompleteMe时会跳出函数文档，可以设置不显示：\nset completeopt-=preview 5. 其他配置  括号自动补全  Plug 'tmsvg/pear-tree' 显示行号  set nu tab替换为4个空格  set ts=4 %retab! ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/vim%E9%85%8D%E7%BD%AE/","summary":"1. 安装插件系统  使用的是vim-plug\n curl -fLo ~/.vim/autoload/plug.vim --create-dirs \\  https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 2. 安装插件 打开~/.vimrc, 在其中写入：\n\u0026quot; Specify a directory for plugins \u0026quot; - For Neovim: stdpath('data') . '/plugged' \u0026quot; - Avoid using standard Vim directory names like 'plugin' call plug#begin('~/.vim/plugged') \u0026quot; Make sure you use single quotes \u0026quot; Shorthand notation; fetches https://github.com/junegunn/vim-easy-align Plug 'junegunn/vim-easy-align' \u0026quot; Any valid git URL is allowed Plug 'https://github.com/junegunn/vim-github-dashboard.git' \u0026quot; Multiple Plug commands can be written in a single line using | separators Plug 'SirVer/ultisnips' | Plug 'honza/vim-snippets' \u0026quot; On-demand loading Plug 'scrooloose/nerdtree', { 'on': 'NERDTreeToggle' } Plug 'tpope/vim-fireplace', { 'for': 'clojure' } \u0026quot; Using a non-default branch Plug 'rdnetto/YCM-Generator', { 'branch': 'stable' } \u0026quot; Using a tagged release; wildcard allowed (requires git 1.","title":"vim配置"},{"content":"1. 用例图 用例图主要用于定义系统的功能需求，它描述了系统的参与者与系统提供的用例之间的关系，用例图仅从参与者使用系统的角度描述系统中的信息。\n图例\n示例\n2. 时序图（顺序图） 顺序图描述了对象以及对象之间传递的消息，强调对象之间的交互是按照时间的先后顺序发生的，这些特定顺序发生的交互序列从开始到结束需要一定的时间。在顺序图中主要包括了以下 4 种元素。 ● 对象 ● 生命线 ● 激活 ● 消息\n图例\n示例\n3. 协作图 协作图与顺序图一样，也是用于描述系统中各对象的交互关系并展现对象间的消息传递，但两者侧重点不同，顺序图着重于描述交互的时间顺序，而协作图着重于描述协作对象间的交互和连接。还可以从另一个角度来看两种图的定义，顺序图是按照时间的顺序布图，而协作图是按照空间来布图。\n图例\n示例\n顺序图与协作图的关系\n顺序图和协作图在语义上是等价的，它们之间可以进行互相转换。 例如上面的协作图可以等价转化为顺序图：\n4. 类图 类图描述了类和类间关系，它从静态角度来表示一个系统，因此类图属于一种静态图。类图是 UML 建模中最基本和最重要的一类图。\n图例\n示例\n5. 对象图 对象图是类图的实例，几乎使用与类图完全相同的标识。它们的不同点在于对象图显示类的多个对象实例，而不是实际的类。一个对象图是类图的一个实例。由于对象存在生命周期，因此对象图也是有生命周期的，它只能在系统某一时间段存在。\n示例 6. 包图 创建包图的主要作用是：\n 描述需求的高阶概述。 描述设计的高阶概述。 在逻辑上把一个复杂的图模块化。 组织源代码。 对框架进行建模。  图例\n示例\n7. 状态图 状态图主要用来描述一个特定对象的所有可能状态以及由于各种事件的发生而引起状态之间的转移。通过状态图可以知道一个对象、子系统、系统的各种状态及其收到的消息对其状态的影响。通常创建一个 UML 状态图是为了以下的研究目的：研究类、角色、子系统或构件的复杂行为。\n状态图主要由起点、终点和状态组成，各状态由转移连接在一起。状态是对象执行某项活动或等待某个事件时的条件。转换是两个状态之间的关系，它由某个事件触发，然后执行特定的操作或评估并导致特定的结束状态。\n状态图适合于描述跨越多个用例的单个对象的行为，而不适合描述多个对象之间的行为协作。为此，常常将状态图与其他技术组合使用。\n图例\n示例\n8. 活动图 活动图是用来描述达到一个目标所实施一系列活动的过程，描述了系统的动态特征。活动图类似结构化程序课程中的流程图，不同之处在于它支持并行活动。活动图和状态图的主要区别在于状态图侧重从行为的结果来描述，以状态为中心；活动图侧重从行为的动作来描述，以活动为中心。活动图用来为一个过程中的活动序列建模，而状态图用来为对象生命期中的各离散状态建模。\n图例\n示例\n9. 构件图 构件是系统的模块化部分，它封装了自己的内容，且它的声明在其环境中是可以替换的；构件利用提供接口和请求接口定义自身的行为，它起类型的作用。\n图例\n示例\n10. 部署图 部署图描述了整个系统的软硬件的实际配置，它表示了系统在运行期间的体系结构、硬件元素（节点）的构造和软件元素是如何被映射在那些节点之上的。\n部署图可以帮助系统的有关人员了解系统中各个构件部署在什么硬件上，以及这些硬件之间的交互关系。\n图例 示例\nReferences 《面向对象技术及UML教程》 李磊，王养廷主编\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%9D%82%E8%AE%B0/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1uml%E5%9B%BE/","summary":"1. 用例图 用例图主要用于定义系统的功能需求，它描述了系统的参与者与系统提供的用例之间的关系，用例图仅从参与者使用系统的角度描述系统中的信息。\n图例\n示例\n2. 时序图（顺序图） 顺序图描述了对象以及对象之间传递的消息，强调对象之间的交互是按照时间的先后顺序发生的，这些特定顺序发生的交互序列从开始到结束需要一定的时间。在顺序图中主要包括了以下 4 种元素。 ● 对象 ● 生命线 ● 激活 ● 消息\n图例\n示例\n3. 协作图 协作图与顺序图一样，也是用于描述系统中各对象的交互关系并展现对象间的消息传递，但两者侧重点不同，顺序图着重于描述交互的时间顺序，而协作图着重于描述协作对象间的交互和连接。还可以从另一个角度来看两种图的定义，顺序图是按照时间的顺序布图，而协作图是按照空间来布图。\n图例\n示例\n顺序图与协作图的关系\n顺序图和协作图在语义上是等价的，它们之间可以进行互相转换。 例如上面的协作图可以等价转化为顺序图：\n4. 类图 类图描述了类和类间关系，它从静态角度来表示一个系统，因此类图属于一种静态图。类图是 UML 建模中最基本和最重要的一类图。\n图例\n示例\n5. 对象图 对象图是类图的实例，几乎使用与类图完全相同的标识。它们的不同点在于对象图显示类的多个对象实例，而不是实际的类。一个对象图是类图的一个实例。由于对象存在生命周期，因此对象图也是有生命周期的，它只能在系统某一时间段存在。\n示例 6. 包图 创建包图的主要作用是：\n 描述需求的高阶概述。 描述设计的高阶概述。 在逻辑上把一个复杂的图模块化。 组织源代码。 对框架进行建模。  图例\n示例\n7. 状态图 状态图主要用来描述一个特定对象的所有可能状态以及由于各种事件的发生而引起状态之间的转移。通过状态图可以知道一个对象、子系统、系统的各种状态及其收到的消息对其状态的影响。通常创建一个 UML 状态图是为了以下的研究目的：研究类、角色、子系统或构件的复杂行为。\n状态图主要由起点、终点和状态组成，各状态由转移连接在一起。状态是对象执行某项活动或等待某个事件时的条件。转换是两个状态之间的关系，它由某个事件触发，然后执行特定的操作或评估并导致特定的结束状态。\n状态图适合于描述跨越多个用例的单个对象的行为，而不适合描述多个对象之间的行为协作。为此，常常将状态图与其他技术组合使用。\n图例\n示例\n8. 活动图 活动图是用来描述达到一个目标所实施一系列活动的过程，描述了系统的动态特征。活动图类似结构化程序课程中的流程图，不同之处在于它支持并行活动。活动图和状态图的主要区别在于状态图侧重从行为的结果来描述，以状态为中心；活动图侧重从行为的动作来描述，以活动为中心。活动图用来为一个过程中的活动序列建模，而状态图用来为对象生命期中的各离散状态建模。\n图例\n示例\n9. 构件图 构件是系统的模块化部分，它封装了自己的内容，且它的声明在其环境中是可以替换的；构件利用提供接口和请求接口定义自身的行为，它起类型的作用。\n图例\n示例","title":"面向对象UML图"},{"content":" 逛GitHub的时候无意中发现微软有一个Web-Dev-For-Beginners的库，然后随手搜了一下，发现还有很多类似的，这里纪录一下其中比较好的，有备无患。\n  可以Google搜索Beginners site:github.com/microsoft查看相关的信息\n 1. Web-Dev-For-Beginners web开发课程，一共12周，24节课，每节课都包括课前和课后测验、完成课程的书面说明、解决方案、作业等。源文件为英文版，但每个章节都配备有不同语言的翻译版。同时也提供pdf版本下载。\n项目地址：https://github.com/microsoft/Web-Dev-For-Beginners\n2. ML-For-Beginners 机器学习课程，12周，26节课。主要使用sklearn库。\n项目地址：https://github.com/microsoft/Data-Science-For-Beginners\n3. Data-Science-For-Beginners 数据科学课程，10周，20节课。每节课包括课前和课后测验、完成课程的书面说明、解决方案和作业。\n项目地址：https://github.com/microsoft/Data-Science-For-Beginners\n4. IoT-For-Beginners 物联网开发课程，12周，24节课。每节课都包括课前和课后测验、完成课程的书面说明、解决方案、作业等。\n项目地址：https://github.com/microsoft/IoT-For-Beginners\n5. beginners-intro-javascript-node nodejs课程\n项目地址：https://github.com/microsoft/beginners-intro-javascript-node\n6. beginners-series-rust rust课程。这个项目感觉还不是太完善，但依然可以作为参考。\n项目地址：https://github.com/microsoft/beginners-series-rust\n7. beginners-django django课程。这个项目就厉害了，啥也没有，不过创建时间还短，先插个眼。\n项目地址：https://github.com/microsoft/beginners-django\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%9D%82%E8%AE%B0/tutorial-for-beginner/","summary":"逛GitHub的时候无意中发现微软有一个Web-Dev-For-Beginners的库，然后随手搜了一下，发现还有很多类似的，这里纪录一下其中比较好的，有备无患。\n  可以Google搜索Beginners site:github.com/microsoft查看相关的信息\n 1. Web-Dev-For-Beginners web开发课程，一共12周，24节课，每节课都包括课前和课后测验、完成课程的书面说明、解决方案、作业等。源文件为英文版，但每个章节都配备有不同语言的翻译版。同时也提供pdf版本下载。\n项目地址：https://github.com/microsoft/Web-Dev-For-Beginners\n2. ML-For-Beginners 机器学习课程，12周，26节课。主要使用sklearn库。\n项目地址：https://github.com/microsoft/Data-Science-For-Beginners\n3. Data-Science-For-Beginners 数据科学课程，10周，20节课。每节课包括课前和课后测验、完成课程的书面说明、解决方案和作业。\n项目地址：https://github.com/microsoft/Data-Science-For-Beginners\n4. IoT-For-Beginners 物联网开发课程，12周，24节课。每节课都包括课前和课后测验、完成课程的书面说明、解决方案、作业等。\n项目地址：https://github.com/microsoft/IoT-For-Beginners\n5. beginners-intro-javascript-node nodejs课程\n项目地址：https://github.com/microsoft/beginners-intro-javascript-node\n6. beginners-series-rust rust课程。这个项目感觉还不是太完善，但依然可以作为参考。\n项目地址：https://github.com/microsoft/beginners-series-rust\n7. beginners-django django课程。这个项目就厉害了，啥也没有，不过创建时间还短，先插个眼。\n项目地址：https://github.com/microsoft/beginners-django","title":"Tutorial for Beginner"},{"content":"1. site site: 搜索指定站点\n用法:\nsite:[example.com] 示例:\ngolang site:github.com 2. source source: 在谷歌新闻中指定来源\n用法:\nsource:[sourcesite] 示例:\nCOVID source:yahoo 3. intext intext: 查询的内容必须出现在正文中\n用法:\nintext:[somewords] 示例:\nintext:xiaomi 4. allintext allintext: 查询的每个单词都必须包含在页面中\n用法:\nallintext:[somewords] 示例:\nallintext:Quantum Network Coding 5. intitle intitle: 标题中包含要查询的内容\n用法:\nintitle:[somewords] 示例:\nintitle:Quantum 6. allintitle allintitle: 类似allintext\n7. url url:结果的url中必须包含某些内容\n用法:\nurl:[somewords] 示例:\nurl:airpods 8. allinurl allinurl: 结果的url必须包含所有查询内容\n9. filetype filetype: 查询的结果满足某种文件类型\n用法:\nfiletype:[filetype] 示例:\ngolang filetype:pdf 10. related related: 查找有关内容\n用法:\nrelated:[somewords] 示例:\nrelated:[airpods] 11. AROUND(X) AROUND(X): 将结果限制为:包含彼此相差X个单词的搜索词的页面。\n用法:\nAROUND(X) [somewords] 示例:\ngolang AROUND(10) best 12. 引号\u0026quot;\u0026quot; \u0026ldquo;\u0026quot;: 将搜索词包含在引号中将启动对该短语的完全匹配搜索，确保引号中的内容必须在页面上\n用法:\n\u0026quot;[somewords]\u0026quot; 示例:\n\u0026quot;Quantum Network Coding\u0026quot; 13. AND AND: 搜索并集\n用法:\n[x] AND [y] 示例:\ngolang AND tutorial 14. -(横线) -: 差集操作\n用法:\n[x] -[y] 示例:\ngolang tutorial -site:youtube.com golang -tutorial 15. *(星号) *:通配\n用法:\n[somewords] * [somewords] 示例:\nwsl * tutorial 16. () 括号 (): 将术语或搜索运算符组合在一起帮助构建搜索指令\n用法:\n[somewords] ([somewords]) 示例:\ngolang site:github.com (pr OR commit) 17. OR(或|竖线) OR/|: 或\n用法:\n[x] OR [y] [x] | [y] 示例:\ngolang (user OR org) golang (user | org) 18. $/€ (美元符或英镑符) $/€: 搜索商品的价格\n用法:\n[merchandise] [price]$ 示例:\nmac pro 2000$ 19. Year..Year Year..Year: 在两年之间放置两个点会为该年份范围内的结果创建一个 Google 搜索命令\n用法:\n[somewords] [Year]..[Year] 示例:\nmac 2019..2021 20. info info: 查询网站的相关信息\n用法:\ninfo:[site] 示例:\ninfo:google.com 21. link link: 查询显示指向那个的页面\n用法:\nlink:[site] 示例:\nlink:golang.dev 22. movie movie: 查询电影\n用法:\nmovie:[movie name or other information about it] 示例:\nmovie:Titanic 23. insubject insubject: 查询主题\n用法:\ninsubject:[subject] 示例:\nsite:github.com insubject:Linux 24. define define: 查询术语定义\n用法:\ndefine:[somewords] 示例:\ndefine:(Quantum Network) 25. cache cache: 查询Google缓存的网页版本，而不是网页的当前版本\n用法:\ncache:[site] 示例:\ncache:juejin.cn 26. author author: 如果在查询中包含author:，Google将网上论坛结果限制为包含指定的作者的新闻组文章。用于网上论坛查询\n用法:\nauthor:[author] 示例:\n children author:doe@someaddress.com References http://www.googleguide.com/advanced_operators_reference.html\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%9D%82%E8%AE%B0/google%E6%90%9C%E7%B4%A2%E6%8C%87%E4%BB%A4/","summary":"1. site site: 搜索指定站点\n用法:\nsite:[example.com] 示例:\ngolang site:github.com 2. source source: 在谷歌新闻中指定来源\n用法:\nsource:[sourcesite] 示例:\nCOVID source:yahoo 3. intext intext: 查询的内容必须出现在正文中\n用法:\nintext:[somewords] 示例:\nintext:xiaomi 4. allintext allintext: 查询的每个单词都必须包含在页面中\n用法:\nallintext:[somewords] 示例:\nallintext:Quantum Network Coding 5. intitle intitle: 标题中包含要查询的内容\n用法:\nintitle:[somewords] 示例:\nintitle:Quantum 6. allintitle allintitle: 类似allintext\n7. url url:结果的url中必须包含某些内容\n用法:\nurl:[somewords] 示例:\nurl:airpods 8. allinurl allinurl: 结果的url必须包含所有查询内容\n9. filetype filetype: 查询的结果满足某种文件类型\n用法:\nfiletype:[filetype] 示例:\ngolang filetype:pdf 10. related related: 查找有关内容","title":"Google搜索指令"},{"content":"1. install curl https://getcroc.schollz.com | bash 或\ngo install github.com/schollz/croc/v9@latest 2. basic usage sender:\n$ croc send [file(s)-or-folder] Sending \u0026#39;file-or-folder\u0026#39; (X MB) Code is: code-phrase receiver:\ncroc code-phrase 3. comment 可用于替代ftp上传文件，用于不同主机之间文件共享，类似“空投”的效果\n","permalink":"http://yangchnet.github.io/Dessert/posts/tool/croc-%E8%B7%A8%E7%BD%91%E7%BB%9C%E8%B7%A8%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%A9%BA%E6%8A%95/","summary":"1. install curl https://getcroc.schollz.com | bash 或\ngo install github.com/schollz/croc/v9@latest 2. basic usage sender:\n$ croc send [file(s)-or-folder] Sending \u0026#39;file-or-folder\u0026#39; (X MB) Code is: code-phrase receiver:\ncroc code-phrase 3. comment 可用于替代ftp上传文件，用于不同主机之间文件共享，类似“空投”的效果","title":"croc: 跨网络、跨系统的空投"},{"content":"install z # Download to latest to home dir wget https://raw.githubusercontent.com/rupa/z/master/z.sh -O ~/z.sh # Add to .bashrc echo . /path/to/z.sh \u0026gt;\u0026gt; ~/.bashrc # Add to .zshrc echo . /path/to/z.sh \u0026gt;\u0026gt; ~/.zshrc ","permalink":"http://yangchnet.github.io/Dessert/posts/tool/z/","summary":"install z # Download to latest to home dir wget https://raw.githubusercontent.com/rupa/z/master/z.sh -O ~/z.sh # Add to .bashrc echo . /path/to/z.sh \u0026gt;\u0026gt; ~/.bashrc # Add to .zshrc echo . /path/to/z.sh \u0026gt;\u0026gt; ~/.zshrc ","title":"z"},{"content":" 这是一篇理论性较强的文章\n 1. HTTP/1的不足与面临的问题  此部分内容来自：https://segmentfault.com/a/1190000013519925\n 1.1 HTTP/1概述 1.2 队头阻塞 浏览器很少只从一个域名获取一份资源。大多数时候，它希望能同时获取许多资源。设想这样一个网站，它把所有图片放在单个特定域名下。HTTP/1 并未提供机制来同时请求这些资源。如果仅仅使用一个连接，它需要发起请求、等待响应，之后才能发起下一个请求。这显然会在加载页面时造成较大的延迟，降低了用户体验。\nh1 有个特性叫 管道化(pipelining)，允许一次发送一组连续的请求，而不用等待应答返回。这样可以避免连接延迟。但是该特性只能按照发送顺序依次接收响应。而且，管道化备受互操作性和部署的各种问题的困扰，基本没有实用价值。\n在请求应答过程中，如果出现任何状况，剩下所有的工作都会被阻塞在那次请求应答之后。这就是队头阻塞（Head-of-line blocking或缩写为HOL blocking），它会阻碍网络传输和 Web 页面渲染，直至失去响应。\n为了防止这种问题，现代浏览器会针对单个域名开启 6 个连接，通过各个连接分别发送请求。它实现了某种程度上的并行，但是每个连接仍会受到 队头阻塞 的影响。另外，这也没有高效利用有限的设备资源。\n1.3 TCP复用 传输控制协议(TCP) 的设计思路是：对假设情况很保守，并能够公平对待同一网络的不同流量的应用。它的避免拥塞机制被设计成即使在最差的网络状况下仍能起作用，并且如果有需求冲突也保证相对公平。这是它取得成功的原因之一。\n它的成功并不是因为传输数据最快，而是因为它是最可靠的协议之一，涉及的核心概念就是 拥塞窗口(congestion window) 。拥塞窗口是指，在接收方确认数据包之前，发送方可以发出的 TCP 包的数量。 例如，如果拥塞窗口指定为 1，那么发送方发出 1 个数据包之后，只有接收方确认了那个包，才能发送下一个。\n一般来讲，每次发送一个数据包并不是非常低效。TCP 有个概念叫 慢启动(Slow Start)， 它用来探索当前连接对应拥塞窗口的合适大小。慢启动的设计目标是为了让新连接搞清楚当前网络状况，避免给已经拥堵的网络继续添乱。它允许发送者在收到每个确认回复后额外发送 1 个未确认包。这意味着新连接在收到 1 个确认回复之后，可以发送 2 个数据包; 在收到 2 个确认回复之后，可以发 4 个；以此类推。这种几何级数增长很快就会到达协议规定的发包数上限，这时候连接将进入拥塞避免阶段 这种机制需要几次往返数据请求才能得知最佳拥塞窗口大小。但在解决性能问题时，就这 区区几次数据往返也是非常宝贵的时间(成本)。现代操作系统一般会取 4~10 个数据包作为初始拥塞窗口大小。如果你把一个数据包设置为最大值下限 1460 字节(也就是 最大有效负载)，那么只能先发送 5840 字节(假定拥塞窗口为 4)，然后就需要等待接收确认回复。\n如今的 Web 页面平均大小约 2MB，包括 HTML 和所有依赖的资源。在理想情况下， 这需要大约 9 次往返请求来传输完整个页面。除此之外，浏览器一般会针对同一个域名开启 6 个并发连接，这意味着拥塞窗口波动也会并行发生 6 次。TCP 协议保证那些连接都能正常工作， 但是不能保证它们的性能是最优的。\n1.4 消息首部 虽然 h1 提供了压缩被请求内容的机制，但是消息首部却无法压缩。消息首部可不能忽略， 尽管它比响应资源小很多，但它可能占据请求的绝大部分(有时候可能是全部)。如果算 上 cookie，有个几千字节就很正常了。\n据 HTTP 历史存档记录，2016 年末，请求首部一般集中在 460 字节左右。对于包含 140 个 资源的普通Web 页面，意味着它在发起的所有请求中大约占 63KB。\n想想之前关于 TCP 拥塞窗口管理的讨论，发送该页面相关的所有请求可能需要 3~4 轮往返，因此网络延迟的损耗会被迅速放大。此外，上行带宽通常会受到网络限制，尤其是在移动网络环境中，于是拥塞窗口机制根本来不及起作用，导致更多的请求和响应。\n消息首部压缩的缺失也容易导致客户端到达带宽上限，对于低带宽或高拥堵的链路尤其如此。体育馆效应(Stadium Effect) 就是一个经典例子。如果成千上万人同一时间出现在同一地点(例如重大体育赛事)，会迅速耗尽无线蜂窝网络带宽。这时候，如果能压缩请求首部，把请求变得更小，就能够缓解带宽压力，降低系统的总负载。\n1.5 优先级设置受限 如果浏览器针对指定域名开启了多个 socket(每个都会受队头阻塞问题的困扰)，开始请求资源，这时候浏览器能指定优先级的方式是有限的：要么发起请求，要么不发起。\n然而 Web 页面上某些资源会比另一些更重要，这必然会加重资源的 排队效应。这是因为浏览器为了先请求优先级高的资源，会推迟请求其他资源。\n但是优先级高的资源获取之后，在处理的过程中，浏览器并不会发起新的资源请求，所以服务器无法利用这段时间发送优先级低的资源，总的页面下载时间因此延长了。还会出现这样的情况：一个高优先级资源被浏览器发现，但是受制于浏览器处理的方式，它被排在了一个正在获取的低优先级资源之后。\n  以下内容来自：RFC7540\n 2. HTTP/2概述 HTTP/2 为 HTTP 语义提供了优化的传输。HTTP/2 支持 HTTP/1.1 的所有核心功能，但旨在通过多种方式提高效率。\nHTTP/2 中的基本协议单元是帧。每种帧类型都有不同的用途。例如，HEADERS和DATA帧构成了 HTTP 请求和响应的基础；其他帧类型如SETTINGS、WINDOW_UPDATE和PUSH_PROMISE用于支持其他 HTTP/2 功能。\n请求的复用是通过让每个 HTTP 请求/响应交换与其自己的流相关联来实现的。流在很大程度上彼此独立，因此阻塞或停滞的请求或响应不会阻止其他流的进展。\n流控制和优先级确保可以有效地使用多路复用流。流量控制有助于确保仅传输接收器可以使用的数据。优先级确保可以将有限的资源首先用于最重要的流。\nHTTP/2 添加了一种新的交互模式，服务器可以通过这种模式将响应推送到客户端。服务器推送允许服务器推测性地将数据发送到服务器预期客户端需要的客户端，权衡一些网络使用与潜在的延迟增益。服务器通过合成一个请求来做到这一点，它作为一个PUSH_PROMISE帧发送。然后，服务器能够在单独的流上发送对合成请求的响应。\n由于连接中使用的 HTTP 标头字段可能包含大量冗余数据，因此包含它们的帧会被压缩。这对常见情况下的请求大小具有特别有利的影响，允许将许多请求压缩为一个数据包。\n3. HTTP/2帧 3.1 帧格式 所有帧都以固定的 9 个八位字节标头开始，后跟可变长度的有效载荷。\n +-----------------------------------------------+ | 长度 (24) | +---------------+---------------+---------------+ | 类型 (8) | 旗帜 (8) | +-+-------------+---------------+-------------------------------+ |R| 流标识符 (31) | +=+=============================================================+ | 帧有效载荷 (0...) ... +---------------------------------------------------------------+ 帧头的字段定义为：\n  长度： 以无符号 24 位整数表示的帧有效载荷的长度。除非接收方为SETTINGS_MAX_FRAME_SIZE设置了更大的值，否则不得发送大于 $2^14$ (16,384) 的值。 该值中不包括帧头的 9 个八位字节。\n  类型： 帧的 8 位类型。帧类型决定了帧的格式和语义。实现必须忽略并丢弃任何类型未知的帧。\n  标志： 为特定于帧类型的布尔标志保留的 8 位字段。\n  标志被分配特定于指示的帧类型的语义。对于特定帧类型没有定义语义的标志必须被忽略并且在发送时必须保持未设置 (0x0)。\n  回复： 保留的 1 位字段。该位的语义未定义，并且该位在发送时必须保持未设置 (0x0)，在接收时必须被忽略。\n  流标识符： 表示为无符号 31 位整数的流标识符（参见第 5.1.1 节）。值 0x0 保留用于与整个连接相关联的帧，而不是单个流。\n  帧有效载荷的结构和内容完全取决于帧类型\n3.2 帧大小 帧有效载荷的大小受接收器在SETTINGS_MAX_FRAME_SIZE设置中通告的最大大小的限制。此设置可以具有 $2^14$ (16,384) 和 $2^24 -1$ (16,777,215) 个八位字节之间的任何值，包括两个字节。\n所有实现都必须能够接收和最少处理长达 $2^14$ 个八位字节的帧，加上 9 个八位字节的帧头。描述帧大小时不包括帧头的大小。\n注意：某些帧类型，例如 PING，对允许的有效载荷数据量施加了额外的限制。\n如果帧超过SETTINGS_MAX_FRAME_SIZE 中定义的大小，超过为帧类型定义的任何限制，或者太小而无法包含强制性帧数据，则端点必须发送错误代码FRAME_SIZE_ERROR。帧中可能改变整个连接状态的帧大小错误必须被视为连接错误；这包括任何带有标题块（即HEADERS、PUSH_PROMISE和CONTINUATION）、SETTINGS 的帧，以及任何流标识符为 0 的帧。\n端点没有义务使用帧中的所有可用空间。可以通过使用小于允许的最大尺寸的帧来提高响应能力。发送大帧可能会导致发送时间敏感帧（例如RST_STREAM、WINDOW_UPDATE或PRIORITY）的延迟，如果被大帧的传输阻塞，则可能会影响性能\n3.3 头压缩和解压 就像在 HTTP/1 中一样，HTTP/2 中的标头字段是一个具有一个或多个关联值的名称。标头字段用于 HTTP 请求和响应消息以及服务器推送操作。\n标题列表是零个或多个标题字段的集合。当通过连接传输时，标头列表使用HTTP 标头压缩 [COMPRESSION]序列化为标头块。然后将序列化的头块分成一个或多个八位字节序列，称为头块片段，并在 HEADERS、PUSH_PROMISE或 CONTINUATION帧的有效载荷内传输。\n该Cookie头字段 [COOKIE]是由HTTP映射特殊处理。\n接收端点通过连接其片段来重新组装头块，然后解压缩块以重建头列表。\n一个完整的头块包括：\n 设置了 END_HEADERS 标志的单个HEADERS或PUSH_PROMISE帧，或 清除了 END_HEADERS 标志的HEADERS或PUSH_PROMISE帧和一个或多个CONTINUATION帧，其中最后一个CONTINUATION帧设置了 END_HEADERS 标志。  标头压缩是有状态的。整个连接使用一个压缩上下文和一个解压缩上下文。头块中的解码错误必须被视为COMPRESSION_ERROR类型的连接错误。\n每个标题块都作为一个离散单元进行处理。头块必须作为连续的帧序列传输，没有任何其他类型的交错帧或来自任何其他流。HEADERS或CONTINUATION帧序列中的最后一帧设置了 END_HEADERS 标志。PUSH_PROMISE或CONTINUATION帧序列中的最后一帧设置了 END_HEADERS 标志。这允许标题块在逻辑上等同于单个帧。\n标头块片段只能作为HEADERS、PUSH_PROMISE或CONTINUATION帧的有效载荷发送，因为这些帧携带的数据可以修改接收器维护的压缩上下文。接收HEADERS、PUSH_PROMISE或CONTINUATION帧的端点需要重新组装头块并执行解压缩，即使这些帧将被丢弃。如果接收方没有解压缩头块，则接收方必须以COMPRESSION_ERROR类型的连接错误终止连接。\n4. 流和复用 “流”是在 HTTP/2 连接内在客户端和服务器之间交换的独立的双向帧序列。流有几个重要的特征：\n 单个 HTTP/2 连接可以包含多个并发打开的流，其中任一端点交错来自多个流的帧。 流可以单方面建立和使用，也可以由客户端或服务器共享。 流可以被任一端点关闭。 在流上发送帧的顺序很重要。接收者按照接收到的顺序处理帧。特别是，HEADERS和DATA帧的顺序在语义上很重要。 流由一个整数标识。流标识符由发起流的端点分配给流。  4.1 流状态 流的生命周期如图所示:\n +--------+ 送PP | | 接收PP ,--------| 闲置|--------。 / | | \\ v + -------- + v +----------+ | +----------+ | | | 发送 H / | | ,------| 保留 | | 接收 H | 保留|------。 | | (本地) | | | （远程） | | | + ---------- + v + ---------- + | | | +--------+ | | | | 接收 EN | | 发送 EN | | | 送H | ,-------| 打开|-------。| 接收 H | | | / | | \\ | | | vv + -------- + vv | | +----------+ | +----------+ | | | 一半| | | 一半| | | | 关闭 | | 发送 R / | 关闭 | | | | （远程） | | 接收 R | (本地) | | | +----------+ | +----------+ | | | | | | | | 发送 ES / | 接收 EN / | | | | 发送 R / v 发送 R / | | | | 接收 R +--------+ 接收 R | | | 发送 R /`-----------\u0026gt;| |\u0026lt;-----------' 发送 R / | | 接收 R | 关闭 | 接收 R | `-----------------------\u0026gt;| |\u0026lt;----------------------' +--------+ 发送：端点发送此帧 recv：端点接收此帧 H：HEADERS 框架（带有隐含的 CONTINUATION） PP：PUSH_PROMISE 框架（带有隐含的 CONTINUATION） ES：END_STREAM 标志 R：RST_STREAM 帧 请注意，此图仅显示了流状态转换以及影响这些转换的帧和标志。在这方面，CONTINUATION帧不会导致状态转换；它们实际上是它们遵循的HEADERS或PUSH_PROMISE 的一部分。出于状态转换的目的，END_STREAM 标志作为单独的事件处理到承载它的帧；设置了 END_STREAM 标志的HEADERS帧会导致两个状态转换。\n两个端点都对流的状态有主观看法，当帧在传输时可能会有所不同。端点不协调流的创建；它们由任一端点单方面创建。状态不匹配的负面影响仅限于发送RST_STREAM后的“关闭”状态，在关闭后的一段时间内可能会收到帧。\n流具有以下状态：\n idle reserved(local) reserved(remote) open half-closed(local) half-open(remote) closed  4.1.1 流标识符 流由一个无符号的 31 位整数标识。客户端发起的流必须使用奇数流标识符；那些由服务器发起的必须使用偶数流标识符。零 (0x0) 的流标识符用于连接控制消息；零的流标识符不能用于建立新的流。\n升级到 HTTP/2（参见第 3.2 节）的HTTP/1.1 请求将使用流标识符 1 (0x1) 进行响应。升级完成后，流 0x1 对客户端来说是“半关闭（本地）”。因此，从 HTTP/1.1 升级的客户端不能选择流 0x1 作为新的流标识符。\n新建立的流的标识符在数字上必须大于发起端点已打开或保留的所有流。这控制使用HEADERS帧打开的流和使用PUSH_PROMISE保留的流。接收到意外流标识符的端点必须响应类型为PROTOCOL_ERROR的连接错误（第 5.4.1 节）。\n新流标识符的第一次使用会隐式关闭所有处于“空闲”状态的流，这些流可能已由具有较低值的流标识符的对等方启动。例如，如果客户端在流 7 上发送HEADERS帧，但从未在流 5 上发送过帧，则当发送或接收流 7 的第一个帧时，流 5 将转换为“关闭”状态。\n流标识符不能重复使用。长期连接可能导致端点耗尽流标识符的可用范围。无法建立新流标识符的客户端可以为新流建立新连接。无法建立新流标识符的服务器可以发送GOAWAY帧，以便客户端被迫为新流打开新连接。\n4.1.2 流并发 对等方可以使用SETTINGS帧内的SETTINGS_MAX_CONCURRENT_STREAMS参数（参见第 6.5.2 节）限制并发活动流的数量。最大并发流设置特定于每个端点，并且仅适用于接收设置的对等方。也就是说，客户端指定服务器可以启动的最大并发流数，服务器指定客户端可以启动的最大并发流数。\n处于“打开”状态或处于“半关闭”状态之一的流计入允许端点打开的最大流数。处于这三种状态中的任何一种状态的流都计入SETTINGS_MAX_CONCURRENT_STREAMS设置中公布的限制。处于任一“保留”状态的流不计入流限制。\n端点不得超过其对等方设置的限制。收到HEADERS帧导致超出其通告的并发流限制的端点必须将此视为PROTOCOL_ERROR或REFUSED_STREAM类型的流错误（第 5.4.2 节）。错误代码的选择决定了端点是否希望启用自动重试（详细信息请参见第 8.1.4 节）。\n希望将SETTINGS_MAX_CONCURRENT_STREAMS的值减少到低于当前打开流数的值的端点可以关闭超过新值的流或允许流完成\n4.2 流量控制 使用流进行多路复用会引入对 TCP 连接使用的争用，从而导致流被阻塞。流量控制方案确保同一连接上的流不会相互破坏性地干扰。流控制用于单个流和整个连接。\nHTTP/2 通过使用 WINDOW_UPDATE 帧提供流量控制\n4.2.1 流量控制原理 HTTP/2 流控制旨在允许使用各种流控制算法而无需更改协议。HTTP/2 中的流量控制具有以下特点：\n 流控制特定于连接。两种类型的流量控制都在单跳的端点之间，而不是在整个端到端路径上。 流量控制基于WINDOW_UPDATE帧。接收器通告他们准备在一个流上和整个连接上接收多少个八位字节。这是一个基于信用的计划。 流量控制是定向的，由接收器提供整体控制。接收者可以选择为每个流和整个连接设置它所需的任何窗口大小。发送方必须遵守接收方施加的流量控制限制。客户端、服务器和中介都独立地将其流量控制窗口作为接收方进行通告，并在发送时遵守其对等方设置的流量控制限制。 对于新流和整个连接，流控制窗口的初始值是 65,535 个八位字节。 帧类型决定流控制是否适用于帧。在本文档中指定的帧中，只有DATA帧受到流控；所有其他帧类型不占用广告流控制窗口中的空间。这确保重要的控制帧不会被流量控制阻塞。 无法禁用流量控制。 HTTP/2 只定义了WINDOW_UPDATE帧的格式和语义（第 6.9 节）。本文档没有规定接收方如何决定何时发送该帧或发送的值，也没有规定发送方选择如何发送数据包。实现能够选择任何适合他们需要的算法。  实现还负责管理如何根据优先级发送请求和响应，选择如何避免请求的队头阻塞，以及管理新流的创建。这些的算法选择可以与任何流量控制算法交互\n4.2.2 流优先级 客户端可以通过在打开流的 HEADERS 帧中包含优先级信息来为新流分配优先级。在任何其他时间，PRIORITY 帧可用于更改流的优先级。\n优先级的目的是允许端点在管理并发流时表达它希望其对等方如何分配资源。最重要的是，当发送容量有限时，可以使用优先级来选择传输帧的流。\n流可以通过将它们标记为依赖于其他流的完成来确定优先级。每个依赖项都分配了一个相对权重，该数字用于确定分配给依赖于同一流的流的可用资源的相对比例。\n显式设置流的优先级被输入到优先级处理过程中。它不保证流相对于任何其他流的任何特定处理或传输顺序。端点不能强制对等方使用优先级以特定顺序处理并发流。因此，表达优先级只是一个建议。\n可以从消息中省略优先级信息。在提供任何显式值之前使用默认值\n5. 服务器推送 HTTP/2 允许服务器预先向客户端发送（或“推送”）响应（连同相应的“承诺”请求）与先前客户端发起的请求相关联。当服务器知道客户端需要这些响应可用才能完全处理对原始请求的响应时，这会很有用。\n客户端可以请求禁用服务器推送，尽管这是为每个跃点独立协商的。该SETTINGS_ENABLE_PUSH设置可以设置为0，表明服务器推送被禁用。\n承诺的请求必须是可缓存的，必须是安全的，并且不得包含请求正文。收到不可缓存、未知安全或指示请求正文存在的承诺请求的客户端必须使用PROTOCOL_ERROR类型的流错误重置承诺流。请注意，如果客户端不认为新定义的方法是安全的，这可能会导致承诺的流被重置。\n如果客户端实现了 HTTP 缓存，则可缓存的推送响应可以由客户端存储。推送的响应被认为在源服务器上成功验证（例如，如果存在“无缓存”缓存响应指令，而由承诺的流 ID 标识的流仍处于打开状态。\n不可缓存的推送响应不得由任何 HTTP 缓存存储。它们可以单独提供给应用程序。\n服务器必须在:authority伪标头字段中包含一个服务器对其具有权威性的值。客户端必须将服务器不具有权威性的PUSH_PROMISE视为PROTOCOL_ERROR类型的流错误。\n中介可以接收来自服务器的推送并选择不将它们转发到客户端。换句话说，如何使用推送的信息取决于该中介。同样，中介可能会选择向客户端进行额外的推送，而无需服务器采取任何行动。\n客户端无法推送。因此，服务器必须将PUSH_PROMISE帧的接收视为PROTOCOL_ERROR类型的连接错误。客户端必须通过将消息视为PROTOCOL_ERROR类型的连接错误来拒绝将SETTINGS_ENABLE_PUSH设置更改为0 以外的值的任何尝试。\n5.1 推送请求 服务器推送在语义上等同于服务器响应请求；但是，在这种情况下，该请求也由服务器发送，作为PUSH_PROMISE帧。\n所述PUSH_PROMISE帧包括包含了一套完整的请求报头字段的该服务器的属性对所述请求的首标块。无法推送对包含请求正文的请求的响应。\n推送的响应总是与来自客户端的显式请求相关联。服务器发送的PUSH_PROMISE帧在该显式请求的流上发送。所述PUSH_PROMISE框架还包括一个承诺流标识符，从所述服务器中可用的流标识符选择。\nPUSH_PROMISE和任何后续CONTINUATION帧中的头字段必须是一组有效且完整的请求头字段。服务器必须在:method伪标头字段中包含一个安全且可缓存的方法。如果客户端收到不包含完整有效的头字段集的PUSH_PROMISE或:method伪头字段标识了不安全的方法，则它必须以PROTOCOL_ERROR类型的流错误进行响应.\n服务器应该在发送任何引用承诺响应的帧之前发送PUSH_PROMISE帧。这避免了客户端在接收任何PUSH_PROMISE帧之前发出请求的竞争。\n例如，如果服务器收到对包含多个图像文件的嵌入链接的文档的请求，并且服务器选择将这些附加图像推送到客户端，则在包含图像链接的DATA帧之前发送PUSH_PROMISE帧可确保客户端能够在发现嵌入链接之前查看资源将被推送。类似地，如果服务器推送标头块引用的响应（例如，在 Link 标头字段中），则在发送标头块之前发送PUSH_PROMISE可确保客户端不会请求这些资源。\nPUSH_PROMISE帧不得由客户端发送。\nPUSH_PROMISE帧可以由服务器发送以响应任何客户端启动的流，但该流必须处于相对于服务器的“打开”或“半关闭（远程）”状态。PUSH_PROMISE帧与组成响应的帧穿插在一起，但它们不能与组成单个标头块的HEADERS和CONTINUATION帧穿插在一起。\n发送PUSH_PROMISE帧会创建一个新流，并将流置于服务器的“保留（本地）”状态和客户端的“保留（远程）”状态\n5.2 推送响应 发送PUSH_PROMISE帧后，服务器可以开始将推送的响应作为响应传送到使用承诺流标识符的服务器启动的流上。服务器使用此流传输 HTTP 响应，使用与第 8.1 节中定义的相同的帧序列。在发送初始HEADERS帧后，此流对客户端变为“半关闭” 。\n一旦客户端收到一个PUSH_PROMISE帧并选择接受推送的响应，客户端不应该在承诺的流关闭之前发出任何对承诺的响应的请求。\n如果客户端出于任何原因确定它不希望接收来自服务器的推送响应，或者如果服务器开始发送承诺的响应时间过长，则客户端可以使用CANCEL或REFUSED_STREAM发送RST_STREAM帧代码并引用推送流的标识符。\n客户端可以使用SETTINGS_MAX_CONCURRENT_STREAMS设置来限制服务器可以同时推送的响应数量。将SETTINGS_MAX_CONCURRENT_STREAMS值设为零会通过阻止服务器创建必要的流来禁用服务器推送。这不会禁止服务器发送PUSH_PROMISE帧；客户端需要重置任何不需要的承诺流。\n接收推送响应的客户端必须验证服务器是否具有权威性或为相应请求配置了提供推送响应的代理。例如，只为example.com DNS-ID 或 Common Name提供证书的服务器不允许推送对https://www.example.org/doc的响应。\nPUSH_PROMISE流的响应以HEADERS帧开始，它立即将流置于服务器的“半关闭（远程）”状态和客户端的“半关闭（本地）”状态，并以帧结束轴承 END_STREAM，它将流置于“关闭”状态。\n注意：客户端永远不会为服务器推送发送带有 END_STREAM 标志的帧。\nReference [1] HTTP/1的缺点总结\n[2] RFC7540 (HTTP/2) [3] HTTP/2\n","permalink":"http://yangchnet.github.io/Dessert/posts/net/http2/","summary":"这是一篇理论性较强的文章\n 1. HTTP/1的不足与面临的问题  此部分内容来自：https://segmentfault.com/a/1190000013519925\n 1.1 HTTP/1概述 1.2 队头阻塞 浏览器很少只从一个域名获取一份资源。大多数时候，它希望能同时获取许多资源。设想这样一个网站，它把所有图片放在单个特定域名下。HTTP/1 并未提供机制来同时请求这些资源。如果仅仅使用一个连接，它需要发起请求、等待响应，之后才能发起下一个请求。这显然会在加载页面时造成较大的延迟，降低了用户体验。\nh1 有个特性叫 管道化(pipelining)，允许一次发送一组连续的请求，而不用等待应答返回。这样可以避免连接延迟。但是该特性只能按照发送顺序依次接收响应。而且，管道化备受互操作性和部署的各种问题的困扰，基本没有实用价值。\n在请求应答过程中，如果出现任何状况，剩下所有的工作都会被阻塞在那次请求应答之后。这就是队头阻塞（Head-of-line blocking或缩写为HOL blocking），它会阻碍网络传输和 Web 页面渲染，直至失去响应。\n为了防止这种问题，现代浏览器会针对单个域名开启 6 个连接，通过各个连接分别发送请求。它实现了某种程度上的并行，但是每个连接仍会受到 队头阻塞 的影响。另外，这也没有高效利用有限的设备资源。\n1.3 TCP复用 传输控制协议(TCP) 的设计思路是：对假设情况很保守，并能够公平对待同一网络的不同流量的应用。它的避免拥塞机制被设计成即使在最差的网络状况下仍能起作用，并且如果有需求冲突也保证相对公平。这是它取得成功的原因之一。\n它的成功并不是因为传输数据最快，而是因为它是最可靠的协议之一，涉及的核心概念就是 拥塞窗口(congestion window) 。拥塞窗口是指，在接收方确认数据包之前，发送方可以发出的 TCP 包的数量。 例如，如果拥塞窗口指定为 1，那么发送方发出 1 个数据包之后，只有接收方确认了那个包，才能发送下一个。\n一般来讲，每次发送一个数据包并不是非常低效。TCP 有个概念叫 慢启动(Slow Start)， 它用来探索当前连接对应拥塞窗口的合适大小。慢启动的设计目标是为了让新连接搞清楚当前网络状况，避免给已经拥堵的网络继续添乱。它允许发送者在收到每个确认回复后额外发送 1 个未确认包。这意味着新连接在收到 1 个确认回复之后，可以发送 2 个数据包; 在收到 2 个确认回复之后，可以发 4 个；以此类推。这种几何级数增长很快就会到达协议规定的发包数上限，这时候连接将进入拥塞避免阶段 这种机制需要几次往返数据请求才能得知最佳拥塞窗口大小。但在解决性能问题时，就这 区区几次数据往返也是非常宝贵的时间(成本)。现代操作系统一般会取 4~10 个数据包作为初始拥塞窗口大小。如果你把一个数据包设置为最大值下限 1460 字节(也就是 最大有效负载)，那么只能先发送 5840 字节(假定拥塞窗口为 4)，然后就需要等待接收确认回复。\n如今的 Web 页面平均大小约 2MB，包括 HTML 和所有依赖的资源。在理想情况下， 这需要大约 9 次往返请求来传输完整个页面。除此之外，浏览器一般会针对同一个域名开启 6 个并发连接，这意味着拥塞窗口波动也会并行发生 6 次。TCP 协议保证那些连接都能正常工作， 但是不能保证它们的性能是最优的。","title":"HTTP2简介"},{"content":"1. 互信息量 信息熵（香农熵） $$ H(A) = - \\sum_{x\\in X}p(x)\\log p(x) $$ 信息熵度量了信息量的多少。如英语有26个字母，加入每个字母在文章中出现次数相同的话，每个字母的信息量为: $$ I_e = -\\log_2{\\frac{1}{26}}=4.7 $$ 若有一个字母出现次数为总次数的一半，则其信息熵为： $$ I_e = -\\log_2{\\frac{1}{2}} = 1 $$ 以上例子说明，某一个信息出现次数越多，其信息量就越少。直观的说，某一篇文章中，“的”，“我”等词语可能出现次数最多，但其能为我们带来的信息可能并不是那么多。\n经典互信息量 假定我们在一系列的不同时刻$(t_1, t_2, \\dots, t_N)$对一个给定的系统进行连续测量，把每次测量结果记为$x_1, x_2,\\dots, x_N$，每个测量序列的结果都有不同的概率输出，将之记为$p(x_1), p(x_2), \\dots, p(x_N)$.\n则关联就意味着对于任意的$1\\le n \\le N$，这些概率分布不会以乘积的形式出现，即$p(x_1, x_2, \\dots, x_n)*p(x_n+1, x_n+2, \\dots, x_N)$。用通俗易懂的话来说，就是这些概率分布不是相互独立的。\n简单起见，我们将所有测量分为两组A和B，这样A和B之间的互信息量就定义为： $$ I(A:B) = H(A)+H(B) - H(A,B), where\\ H(A) = - \\sum_{x\\in X}p(x)\\log p(x) $$\n又，根据贝叶斯定理$H(A|B)=H(A,B)-H(B)$，经典互信息量还有一个等价的表达方式： $$ C(A:B)=H(A)-H(A|B) $$ 其中$H(A|B)$表示在知道B体系测量结果情况下A体系的条件熵。因此，经典互信息量度量了在对B测量时所能提取的A的信息量。\n2. 量子互信息量 将经典互信息量的理论推广到量子系统，这样就能得到量子互信息量的概念，考虑一个两体的量子态$\\rho_{AB}$，量子互信息量定义为： $$ I(\\rho_{AB})=S(\\rho_{A})+S(\\rho_{B})-S(\\rho_{AB}) $$ 其中$S(\\rho)=-tr\\rho\\log(\\rho)$为冯·诺伊曼熵，（$tr$指求迹，即求矩阵对角线元素之和）,$\\rho_A$和$\\rho_B$分别为$\\rho_{AB}$的约化密度矩阵。\n两体系统的总关联为其量子互信息量\n3. 量子关联 对于经典互信息量表达形式的量子推广，我们需要引入一套完备的测量基底${\\Pi_j}$（不一定是正交基底）来对B体系进行测量，对于每次的测量结果$j$，其概率为$p_j=tr_{AB}(\\rho_{AB}\\Pi_j)$，A的态将坍缩到$\\rho_{A}^{j}=\\frac{tr_{B}(\\Pi_{j}\\rho_{AB}\\Pi_j)}{p_j}$，在对所有的测量基进行优化时，量子体系的经典关联就可定义为： $$ C(\\rho_{AB})=S(\\rho_A)-\\min\\limits_{{\\Pi_j}}\\sum_{j}p_j S(\\rho_{A}^{j}) $$\n 关联分为经典关联和量子关联。\n对于两体量子态$\\rho_{AB}$，其经典关联态，又称经典-经典（关联）态定义如下：\n存在系统$A$和$B$上的冯·诺伊曼测量（秩为1的正交投影测量）$\\Pi_{i}^{A}$, $\\Pi_{j}^{B}$，使得该测量不扰动$\\rho_{AB}$，即 $$ \\rho_{AB}=\\sum_{ij}(\\Pi_{i}^{a}\\otimes\\Pi_{j}^{b})\\rho_{AB}(\\Pi_{i}^{a}\\otimes\\Pi_{j}^{b}) $$ 简单来说就是找不到一个测量基，在对两体量子态其中一个系统进行测量时会对另一个系统造成扰动，这种状态成为经典关联态。 若对另一个系统造成了扰动，则为量子关联态。\n4. 量子失协 由于对两个量子关联在一起的体系种一个进行测量，将不可避免地导致对另一个体系的扰动，因此经典互信息量两种等价的表达形式在量子世界中一般是不一致的，它们之间的差值为： $$ Q(\\rho_{AB})=I(\\rho_{AB})-C(\\rho_{AB}) $$\n这就是 量子失协\n量子失协包含量子体系中的量子纠缠和非纠缠的量子关联，它度量了量子体系中总的非经典关联。\n量子失协存在以下性质：\n  对于纯态，量子失协等于量子纠缠；对于混合态，它们一般不相等。\n  一般来说，量子失协不是对称的，这是由于条件熵的定义依赖于测量作用在哪一个系统上，即 $$ D_A(\\rho_{AB})\\ne D_B(\\rho_{AB}) $$ 此处$D_A(\\rho_{AB})$、$D_B(\\rho_{AB})$分别表示对A，B系统做冯·诺伊曼测量得到的量子失协。当然也可以基于双边测量，从而得到对称化的量子失协。\n  $0\\le D(\\rho_{AB}) \\le I(\\rho_{AB})$。进一步，$D(\\rho_{AB}) \\le S(\\rho_A)$，但$D(\\rho_{AB}) \\le S(\\rho_B)$不一定成立。\n  量子失协在局部酉操作下保持不变，即量子态$\\rho_{AB}$和$(U^{A}\\otimes U^{B})\\rho_{AB}(U^{A}\\otimes U^{B})^\\dagger$具有相同的量子失协。此处$U^{A}$和$U^{B}$分别是系统$A$、$B$上的酉算子。\n  在一定的意义下，几乎所有的态都具有非零的量子失协。$D(\\rho_{AB})=0$当且仅当这个态是经典-量子态。\n  对于没有量子关联但有经典关联的量子态，可以通过局域操作产生量子关联。事实上，所有的非纠缠量子关联都可由局域操作在经典态上产生。\n  所有的纠缠态都具有非零的量子失协，但是部分可分态的量子失协也非零。一个很简单的例子是\n$$ \\rho_{AB}=\\frac{1}{2}(\\ket{0}\\bra{0}\\otimes\\ket{+}\\bra{+}+\\ket{+}\\bra{+}\\otimes\\ket{1}\\bra{1}), where\\ \\ket{+}=\\frac{1}{\\sqrt{2}}(\\ket{0}+\\bra{1}) $$\n  通常情况下，量子失协在噪声环境中的演化要比量子纠缠更平稳、更持久，量子失协一般不会像量子纠缠那样出现突然消失的现象。\n  5. 纠缠、经典关联、非经典关联 纠缠 假设一个复合系统是由两个不相互作用的子系统A、B组成，这两个子系统A、B的希尔伯特空间分别为$H_A$, $H_B$, 则复合系统的希尔伯特空间$H_{AB}$为张量积 $$ H_{AB}=H_A\\otimes H_B $$ 设定子系统A、B的量子态分别为$|\\alpha\\rangle_A$, $|\\beta\\rangle_B$,假设符合系统的量子态$|\\psi\\rangle_{AB}$不能写为张量积$|\\alpha\\rangle_A\\otimes|\\beta\\rangle_B$,则称这复合系统为子系统A、B的纠缠系统，两个子系统A、B相互纠缠。\n经典关联 用密度算符描述的两量子复合系统若可描述为： $$ \\rho_{AB} = \\sum_{i,j}p_{ij}|i\\rangle \\langle j|^{A}\\otimes |j\\rangle \\langle j| $$ 其中，$|i\\rangle^A$是希尔伯特空间$H_A$上的正交态，$|j\\rangle^B$是希尔伯特空间$H_B$上的正交态, 则可以说这两个量子之间具有经典关联\n非经典关联 如果$|i\\rangle^A$，$|j\\rangle^B$不是$H_A$,$H_B$上的正交态，则$AB$是非经典关联.\n以上三个定义可以说明，混合态可能具有一些与纠缠态不同的非经典相关，因为这种状态是可分的\n","permalink":"http://yangchnet.github.io/Dessert/posts/quantum/%E9%87%8F%E5%AD%90%E5%85%B3%E8%81%94/","summary":"1. 互信息量 信息熵（香农熵） $$ H(A) = - \\sum_{x\\in X}p(x)\\log p(x) $$ 信息熵度量了信息量的多少。如英语有26个字母，加入每个字母在文章中出现次数相同的话，每个字母的信息量为: $$ I_e = -\\log_2{\\frac{1}{26}}=4.7 $$ 若有一个字母出现次数为总次数的一半，则其信息熵为： $$ I_e = -\\log_2{\\frac{1}{2}} = 1 $$ 以上例子说明，某一个信息出现次数越多，其信息量就越少。直观的说，某一篇文章中，“的”，“我”等词语可能出现次数最多，但其能为我们带来的信息可能并不是那么多。\n经典互信息量 假定我们在一系列的不同时刻$(t_1, t_2, \\dots, t_N)$对一个给定的系统进行连续测量，把每次测量结果记为$x_1, x_2,\\dots, x_N$，每个测量序列的结果都有不同的概率输出，将之记为$p(x_1), p(x_2), \\dots, p(x_N)$.\n则关联就意味着对于任意的$1\\le n \\le N$，这些概率分布不会以乘积的形式出现，即$p(x_1, x_2, \\dots, x_n)*p(x_n+1, x_n+2, \\dots, x_N)$。用通俗易懂的话来说，就是这些概率分布不是相互独立的。\n简单起见，我们将所有测量分为两组A和B，这样A和B之间的互信息量就定义为： $$ I(A:B) = H(A)+H(B) - H(A,B), where\\ H(A) = - \\sum_{x\\in X}p(x)\\log p(x) $$\n又，根据贝叶斯定理$H(A|B)=H(A,B)-H(B)$，经典互信息量还有一个等价的表达方式： $$ C(A:B)=H(A)-H(A|B) $$ 其中$H(A|B)$表示在知道B体系测量结果情况下A体系的条件熵。因此，经典互信息量度量了在对B测量时所能提取的A的信息量。\n2. 量子互信息量 将经典互信息量的理论推广到量子系统，这样就能得到量子互信息量的概念，考虑一个两体的量子态$\\rho_{AB}$，量子互信息量定义为： $$ I(\\rho_{AB})=S(\\rho_{A})+S(\\rho_{B})-S(\\rho_{AB}) $$ 其中$S(\\rho)=-tr\\rho\\log(\\rho)$为冯·诺伊曼熵，（$tr$指求迹，即求矩阵对角线元素之和）,$\\rho_A$和$\\rho_B$分别为$\\rho_{AB}$的约化密度矩阵。","title":"量子关联与量子失协"},{"content":"0. 前记 作为一个技术人员, 一个笔记软件对于我们来说必不可少，每个人对笔记软件的需求又各不相同。对于我来说，我希望一个云笔记的功能包括：markdown支持、latex公式编辑、很好的版本控制、可以方便的分享、可以方便的导出/导入、代码块支持、图片插入等。\n我用过很多云笔记软件，印象笔记、为知笔记、有道云、Notion、OneNote等，这些市面上常见的云笔记软件均有一个问题，就是你用了一段时间后，发现自己被限制在里面了，你无法对你的笔记进行导出，更换一个你觉得更好的笔记软件。这对我来说是不可接受的，我希望我的笔记可以由我自己来管理，我需要它的所有控制权。\n因此，我后来又使用了jupyter notebook，用过jupyter的同学知道，我们可以在上面编辑代码，markdown和latex也被很好的支持，而又由于jupyter的源文件是文本文件，因此我们可以方便的使用git对其进行版本控制，这种方式可以说满足了我的绝大部分需要，是一种非常nice的笔记方式。\n但今天我们要说的是另一种非常纯粹的方式，只把笔记源文件存储为markdown，这样我们可以用任何编辑器书写，然后用git进行版本控制，用Github Pages进行展示与分享，具体过程如下。\n1. hugo的使用  为什么要用hugo？首先我们需要一种分享展示笔记的方式，将其编译为静态网站是一个不错的选择。除了hugo，我们也可以使用hexo等类似的静态网站编译工具。\n 1.1 安装hugo 到GitHub上hugo的release页面下载你的系统需要的版本，比如我的系统为64位Linux，那么我需要下载hugo_*.**.*_Linux-64bit.tar.gz（中间的星号为版本号，这里代表你可以任意选择）。\n将其下载到本地之后，解压安装。\ntar -zxvf hugo_0.89.2_Linux-64bit.tar.gz sudo mv hugo /usr/local/bin 安装完成\n1.2 开始 hugo的使用炒鸡简单，你只需要使用\nhugo new site MySite # 你可以取一个好听的名字 即可新建一个名为MySite的网站\n1.2 为你的网站选择一个theme 进入到我们刚才建立的网站目录\ncd Mysite/ 从GitHub导入你想应用的主题\ngit submodule add git@github.com:adityatelange/hugo-PaperMod.git themes/PaperMod --depth=1 待下载完成后，还需要修改你的配置文件\nbaseURL = \u0026#34;http://your-user-name.github.io/Mysite/\u0026#34; # 使用你自己的用户名 languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; theme = \u0026#34;PaperMod\u0026#34; 1.3 为你的网站添加一些内容 hugo使用我们上传的md文件来自动生成静态网页，而我们上传的md文件的位置在MySite/content/posts/*, 我们可以直接复制已经编辑好的md文件到这个目录，或者使用如下命令：\nhugo new posts/first/my-first-post.md 需要注意的一点：为了让hugo知道更多的信息，我们上传的md文件一般会有一个\u0026quot;standand header\u0026quot;, 如下：\n--- title: \u0026quot;My First Post\u0026quot; date: 2019-03-26T08:47:11+01:00 draft: true --- 这里的头部并不是一成不变的，你可以根据需要自行配置。\n 我们所有的笔记源文件，也就是markdown文件，都存储在post文件夹里，你可以在里面创建不同分类的文件夹\n 1.4 开始让你的网站服务 使用如下命令让你的网站开始服务吧！\nhugo server -D 注意，这个命令只会让你的hugo服务器监听本地访问，也就是127.0.0.1 如果你想要你的hugo服务器为整个网络服务，可以这样：\nhugo server --bind=\u0026#39;0.0.0.0\u0026#39; 这样，你的网站就可以全网访问了。\n 完成上面的工作后，我们就有了自己本地的笔记，你可以打开post文件夹，使用任何你喜欢的编辑器，vscode、sublime、Typora、notepad、甚至是记事本，来书写你的笔记，只需要按照markdown的格式来进行编辑，你就可以通过调整hugo的theme得到许多截然不同的漂亮的页面。\n 2. 使用git进行版本控制 2.1 创建github仓库 打开你的GitHub账号，创建一个仓库，不要勾选Readme.md或LICENSE等，这样就得到了一个空的GitHub仓库。\n2.2 提交本地更改 提交所有更改\ngit add . git commit -m \u0026quot;init commit\u0026quot; 链接到GitHub远程仓库\ngit remote add origin git@github.com:your_name/your_repo.git 推送到github\ngit push 3. 分享你的云笔记 我们将笔记以hugo项目的形式分享到了GitHub，但是这并不是最终的形式，接下来我们通过Github Action + Github Pages的方式，来实现云笔记的自动发布、自动更新。\n在GitHub上打开你的仓库，选择Action选项卡 选择set up a workflow yourself 编辑配置文件，内容如下：\nname: github pages on: push: branches: - master  # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.81.0\u0026#39; # 使用你的版本 # extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: github.ref == \u0026#39;refs/heads/master\u0026#39; with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public 这个文件中有两处需要注意的地方，一个是分支名要与你的分支名一致，二是hugo版本号要尽量与本地一致\n提交更改 提交之后，再次打开GitHub Action，你会发现GitHub在为你自动部署hugo网站\n现在找到GitHub Pages 访问这个地址，不出意外的话，你的网站已经上线了。\n","permalink":"http://yangchnet.github.io/Dessert/posts/env/%E4%BD%93%E9%AA%8C%E8%BF%87%E7%9A%84%E6%9C%80%E5%A5%BD%E7%9A%84%E4%BA%91%E7%AC%94%E8%AE%B0%E6%96%B9%E5%BC%8F/","summary":"0. 前记 作为一个技术人员, 一个笔记软件对于我们来说必不可少，每个人对笔记软件的需求又各不相同。对于我来说，我希望一个云笔记的功能包括：markdown支持、latex公式编辑、很好的版本控制、可以方便的分享、可以方便的导出/导入、代码块支持、图片插入等。\n我用过很多云笔记软件，印象笔记、为知笔记、有道云、Notion、OneNote等，这些市面上常见的云笔记软件均有一个问题，就是你用了一段时间后，发现自己被限制在里面了，你无法对你的笔记进行导出，更换一个你觉得更好的笔记软件。这对我来说是不可接受的，我希望我的笔记可以由我自己来管理，我需要它的所有控制权。\n因此，我后来又使用了jupyter notebook，用过jupyter的同学知道，我们可以在上面编辑代码，markdown和latex也被很好的支持，而又由于jupyter的源文件是文本文件，因此我们可以方便的使用git对其进行版本控制，这种方式可以说满足了我的绝大部分需要，是一种非常nice的笔记方式。\n但今天我们要说的是另一种非常纯粹的方式，只把笔记源文件存储为markdown，这样我们可以用任何编辑器书写，然后用git进行版本控制，用Github Pages进行展示与分享，具体过程如下。\n1. hugo的使用  为什么要用hugo？首先我们需要一种分享展示笔记的方式，将其编译为静态网站是一个不错的选择。除了hugo，我们也可以使用hexo等类似的静态网站编译工具。\n 1.1 安装hugo 到GitHub上hugo的release页面下载你的系统需要的版本，比如我的系统为64位Linux，那么我需要下载hugo_*.**.*_Linux-64bit.tar.gz（中间的星号为版本号，这里代表你可以任意选择）。\n将其下载到本地之后，解压安装。\ntar -zxvf hugo_0.89.2_Linux-64bit.tar.gz sudo mv hugo /usr/local/bin 安装完成\n1.2 开始 hugo的使用炒鸡简单，你只需要使用\nhugo new site MySite # 你可以取一个好听的名字 即可新建一个名为MySite的网站\n1.2 为你的网站选择一个theme 进入到我们刚才建立的网站目录\ncd Mysite/ 从GitHub导入你想应用的主题\ngit submodule add git@github.com:adityatelange/hugo-PaperMod.git themes/PaperMod --depth=1 待下载完成后，还需要修改你的配置文件\nbaseURL = \u0026#34;http://your-user-name.github.io/Mysite/\u0026#34; # 使用你自己的用户名 languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; theme = \u0026#34;PaperMod\u0026#34; 1.3 为你的网站添加一些内容 hugo使用我们上传的md文件来自动生成静态网页，而我们上传的md文件的位置在MySite/content/posts/*, 我们可以直接复制已经编辑好的md文件到这个目录，或者使用如下命令：","title":"体验过的最好的云笔记方式"},{"content":" 摘抄自：极客时间《周志明的软件架构课》\n 1. 数据库中的三种锁   写锁（Write Lock，也叫做排他锁 eXclusive Lock，简写为 X-Lock）：只有持有写锁的事务才能对数据进行写入操作，数据加持着写锁时，其他事务不能写入数据，也不能施加读锁。\n  读锁（Read Lock，也叫做共享锁 Shared Lock，简写为 S-Lock）：多个事务可以对同一个数据添加多个读锁，数据被加上读锁后就不能再被加上写锁，所以其他事务不能对该数据进行写入，但仍然可以读取。对于持有读锁的事务，如果该数据只有一个事务加了读锁，那可以直接将其升级为写锁，然后写入数据。\n  范围锁（Range Lock）：对于某个范围直接加排他锁，在这个范围内的数据不能被读取，也不能被写入。如下语句是典型的加范围锁的例子：\n  SELECT * FROM books WHERE price \u0026lt; 100 FOR UPDATE; 2. 本地事务的四种隔离级别  以下隔离级别从高到低\n 可串行化  顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。\n 串行化访问提供了强度最高的隔离性。可串行化比较符合普通程序员对数据竞争加锁的理解，如果不考虑性能优化的话，对事务所有读、写的数据全都加上读锁、写锁和范围锁即可（这种可串行化的实现方案称为 Two-Phase Lock）。\n但数据库显然不可能不考虑性能，并发控制理论（Concurrency Control）决定了隔离程度与并发能力是相互抵触的，隔离程度越高，并发访问时的吞吐量就越低。现代数据库一定会提供除可串行化以外的其他隔离级别供用户使用，让用户调节隔离级别的选项，这样做的根本目的是让用户可以调节数据库的加锁方式，取得隔离性与吞吐量之间的平衡。\n可重复读  一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。\n 可串行化的下一个隔离级别是可重复读（Repeatable Read）。可重复读的意思就是对事务所涉及到的数据加读锁和写锁，并且一直持续到事务结束，但不再加范围锁。\n可重复读比可串行化弱化的地方在于幻读问题（Phantom Reads），它是指在事务执行的过程中，两个完全相同的范围查询得到了不同的结果集。比如现在准备统计一下书店中售价小于 100 元的书有多少本，就可以执行以下第一条 SQL 语句：\nSELECT count(1) FROM books WHERE price \u0026lt; 100 /* 时间顺序：1，事务： T1 */ INSERT INTO books(name,price) VALUES (\u0026#39;深入理解Java虚拟机\u0026#39;,90) /* 时间顺序：2，事务： T2 */ SELECT count(1) FROM books WHERE price \u0026lt; 100 /* 时间顺序：3，事务： T1 */ 可重复读级别对事务涉及到的数据加读锁和写锁，但不再加范围锁。这里事务T1中涉及的数据是原来数据库中已经存在的数据，但新插入的条目显然不在这个范围内。因此在事务T1再次执行读的时候就会与第一次读的结果不同。原因就是，可重复读没有范围锁来禁止在该范围内插入新的数据。\n这就是一个事务遭到其他事务影响，隔离性被破坏的表现。\n这里的介绍实际上是以 ARIES 理论作为讨论目标的，而具体的数据库并不一定要完全遵照着这个理论去实现。因此在同样的隔离级别下可能会出现与这里不同的行为。\n读已提交  一个事务提交后，它做的变更才会被其他事务看到\n 可重复读的下一个隔离级别是读已提交（Read Committed）。读已提交对事务涉及到的数据加的写锁，会一直持续到事务结束，但加的读锁在查询操作完成后就马上会释放。\n读已提交比可重复读弱化的地方在于不可重复读问题（Non-Repeatable Reads），它是指在事务执行过程中，对同一行数据的两次查询得到了不同的结果。\n比如说，现在我要获取书店中《深入理解 Java 虚拟机》这本书的售价，同样让程序执行了两条 SQL 语句。而在这两条语句执行之间，恰好有另外一个事务修改了这本书的价格，从 90 元调整到了 110 元，如下所示：\nSELECT * FROM books WHERE id = 1; /* 时间顺序：1，事务： T1 */ UPDATE books SET price = 110 WHERE ID = 1; COMMIT; /* 时间顺序：2，事务： T2 */ SELECT * FROM books WHERE id = 1; COMMIT; /* 时间顺序：3，事务： T1 */ 在事务T1执行完第一个查询语句后，读锁被释放，这时事务T2执行了update语句，更新了书的价格，然后事务T1再次去查询，这样两次查询就得到了不同的值。读已提交的隔离级别缺乏贯穿整个事务周期的读锁，无法禁止读取过的数据发生变化。\n不过，假如隔离级别是可重复读的话，由于数据已被事务 T1 施加了读锁，并且读取后不会马上释放，所以事务 T2 无法获取到写锁，更新就会被阻塞，直至事务 T1 被提交或回滚后才能提交。\n读未提交  一个事务还没提交时，它做的变更就能被别的事务看到。\n 读已提交的下一个级别是读未提交（Read Uncommitted）。读未提交对事务涉及到的数据只加写锁，这会一直持续到事务结束，但完全不加读锁。\n读未提交比读已提交弱化的地方在于脏读问题（Dirty Reads），它是指在事务执行的过程中，一个事务读取到了另一个事务未提交的数据。\n比如说，我觉得《深入理解 Java 虚拟机》从 90 元涨价到 110 元是损害消费者利益的行为，又执行了一条更新语句，把价格改回了 90 元。而在我提交事务之前，同事过来告诉我，这并不是随便涨价的，而是印刷成本上升导致的，按 90 元卖要亏本，于是我随即回滚了事务。那么在这个场景下，程序执行的 SQL 语句是这样的：\nSELECT * FROM books WHERE id = 1; /* 时间顺序：1，事务： T1 */ /* 注意没有COMMIT */ UPDATE books SET price = 90 WHERE ID = 1; /* 时间顺序：2，事务： T2 */ /* 这条SELECT模拟购书的操作的逻辑 */ SELECT * FROM books WHERE id = 1; /* 时间顺序：3，事务： T1 */ ROLLBACK; /* 时间顺序：4，事务： T2 */ 不过，在我修改完价格之后，事务 T1 已经按 90 元的价格卖出了几本。出现这个问题的原因就在于，读未提交在数据上完全不加读锁，这反而令它能读到其他事务加了写锁的数据，也就是我前面所说的，事务 T1 中两条查询语句得到的结果并不相同。\n这里，你可能会有点疑问，“为什么完全不加读锁，反而令它能读到其他事务加了写锁的数据”，这句话中的“反而”代表的是什么意思呢？不理解也没关系，我们再来重新读一遍写锁的定义：写锁禁止其他事务施加读锁，而不是禁止事务读取数据。\n所以说，如果事务 T1 读取数据时，根本就不用去加读锁的话，就会导致事务 T2 未提交的数据也能马上就被事务 T1 所读到。这同样是一个事务遭到其他事务影响，隔离性被破坏的表现。\n那么，这里我们假设隔离级别是读已提交的话，由于事务 T2 持有数据的写锁，所以事务 T1 的第二次查询就无法获得读锁。而读已提交级别是要求先加读锁后读数据的，所以 T1 中的查询就会被阻塞，直到事务 T2 被提交或者回滚后才能得到结果。\n总结  可串行化： 所有读写数据都加上读锁、写锁，范围锁。 可重复读： 对事务涉及到的数据加读锁和写锁，但不加范围锁。（幻读问题） 读已提交： 对事务涉及到的数据加的写锁持续到事务结束，但读锁在查询完成后马上释放。（不可重复读问题） 读未提交： 对事务涉及到的数据加写锁，完全不加读锁。（脏读问题）  References 《周志明的软件架构课》\nMySQL45讲\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB/","summary":"摘抄自：极客时间《周志明的软件架构课》\n 1. 数据库中的三种锁   写锁（Write Lock，也叫做排他锁 eXclusive Lock，简写为 X-Lock）：只有持有写锁的事务才能对数据进行写入操作，数据加持着写锁时，其他事务不能写入数据，也不能施加读锁。\n  读锁（Read Lock，也叫做共享锁 Shared Lock，简写为 S-Lock）：多个事务可以对同一个数据添加多个读锁，数据被加上读锁后就不能再被加上写锁，所以其他事务不能对该数据进行写入，但仍然可以读取。对于持有读锁的事务，如果该数据只有一个事务加了读锁，那可以直接将其升级为写锁，然后写入数据。\n  范围锁（Range Lock）：对于某个范围直接加排他锁，在这个范围内的数据不能被读取，也不能被写入。如下语句是典型的加范围锁的例子：\n  SELECT * FROM books WHERE price \u0026lt; 100 FOR UPDATE; 2. 本地事务的四种隔离级别  以下隔离级别从高到低\n 可串行化  顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。\n 串行化访问提供了强度最高的隔离性。可串行化比较符合普通程序员对数据竞争加锁的理解，如果不考虑性能优化的话，对事务所有读、写的数据全都加上读锁、写锁和范围锁即可（这种可串行化的实现方案称为 Two-Phase Lock）。\n但数据库显然不可能不考虑性能，并发控制理论（Concurrency Control）决定了隔离程度与并发能力是相互抵触的，隔离程度越高，并发访问时的吞吐量就越低。现代数据库一定会提供除可串行化以外的其他隔离级别供用户使用，让用户调节隔离级别的选项，这样做的根本目的是让用户可以调节数据库的加锁方式，取得隔离性与吞吐量之间的平衡。\n可重复读  一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。\n 可串行化的下一个隔离级别是可重复读（Repeatable Read）。可重复读的意思就是对事务所涉及到的数据加读锁和写锁，并且一直持续到事务结束，但不再加范围锁。\n可重复读比可串行化弱化的地方在于幻读问题（Phantom Reads），它是指在事务执行的过程中，两个完全相同的范围查询得到了不同的结果集。比如现在准备统计一下书店中售价小于 100 元的书有多少本，就可以执行以下第一条 SQL 语句：\nSELECT count(1) FROM books WHERE price \u0026lt; 100 /* 时间顺序：1，事务： T1 */ INSERT INTO books(name,price) VALUES (\u0026#39;深入理解Java虚拟机\u0026#39;,90) /* 时间顺序：2，事务： T2 */ SELECT count(1) FROM books WHERE price \u0026lt; 100 /* 时间顺序：3，事务： T1 */ 可重复读级别对事务涉及到的数据加读锁和写锁，但不再加范围锁。这里事务T1中涉及的数据是原来数据库中已经存在的数据，但新插入的条目显然不在这个范围内。因此在事务T1再次执行读的时候就会与第一次读的结果不同。原因就是，可重复读没有范围锁来禁止在该范围内插入新的数据。","title":"本地事务的隔离"},{"content":" 翻译自：TCMalloc : Thread-Caching Malloc（性能测试部分没有翻译）\n 动机 在我测试过的所有malloc(动态内存分配器)中，TCMalloc比glibc 2.3 malloc(作为一个单独的库称作ptmalloc2)以及其他内存分配器都要快。对于小内存对象来说，在Intel® Pentium® 4 Processor 2.80 GHzCPU上ptmalloc2执行一次内存分配/回收操作需要大约300ns，而TCMalloc完成相同的操作只需要50ns。显然对于内存分配操作来说，速度十分重要，因为如果内存分配不够及时，开发人员就倾向于在malloc上编写他们自己的空闲列表，这会造成额外的复杂性以及更多的内存占用，除非开发人员非常小心的估算空闲列表的大小并清理其中的空闲对象。\nTCMalloc也降低了多线程应用中的锁冲突。对于小内存对象来说几乎不存在冲突。对于大内存对象来说，TCMalloc尝试使用细粒度和高效的自旋锁。ptmalloc2也尝试通过一些方法降低锁冲突，其为每个线程分配一个arena空间，但ptmalloc2对于arena空间的使用存在一个大问题：在ptmalloc2中内存将不可能从一个arena空间转移到另一个arena空间，也即内存不可以在线程之间进行二次分配。这会导致巨大的内存浪费。例如，在一个Google应用中，阶段一为其数据结构分配了大约300MB。当其第一阶段结束后，阶段二将在相同的地址空间上开始。如果阶段二分配了一个与阶段一不同的arena空间，那么阶段二的计算将不会重复使用阶段一留下的任何内存空间，而是重新分配另一个300MB内存空间。这种内存的“blowup”问题同样出现在其他应用中。\nTCMolloc的另一个优点是针对小内存对象的空间的有效利用。例如，可以将8N bytes大小的对象分配到8N*1.01bytes的空间上，即只需要1%的空间开销。ptmalloc2对每一个对象分配一个4bytes的头，（我认为）这种方式将本来只需要8N bytes大小对象变成了需要16N bytes\n用法 要想使用TCMalloc，只要使用-l tcmalloc标志将tcmalloc链接到你的应用。\n你也可以在不是你编译的应用中使用tcmalloc，通过使用LD_PRELOAD环境变量\nLD_PRELOAD=\u0026#34;/usr/lib/libtcmalloc.so\u0026#34; 但我们不推荐在非必要的情况下使用这种方式。\nTCMalloc也包括一个堆检查器和一个堆分析器。\n如果你只想要链接一个没有堆检查器和分析器的TCMalloc版本（可能想要减小二进制包的大小），你可以链接libtcmalloc_minimal\n概览 TCMalloc为每个线程分配一个本地线程缓存thread-local cache。小的内存分配将直接被本地线程缓存满足。对象按需从中间部件central data structure移动到本地线程缓存。定期的垃圾收集被用来把内存从本地线程缓存放回中间部件central data structure。\nTCMalloc对于大小\u0026lt;=32K的（小）对象的处理方式与大对象不同。大对象由顶层堆管理器central heap使用页级的分配器直接分配。（一个页面是一个4K对齐的内存区域），同时，大对象总是页对齐并且占据整数个页面。\n页面可被一系列的小对象瓜分为大小相同的区域。例如：一个4K的内存将被32个对象分割为每个128bytes的内存序列。\n小对象的分配 每个小对象都对应于170个可分配内存大小size-classes中的一种，例如，大小范围在961-1024bytes的对象将占据1024bytes。这些内存大小级别被不同大小的间距分隔开，其中较小尺寸为8bytes，大尺寸为16bytes，更大的是32bytes，以此类推。最大的空间是256bytes（对于size-classes）大于等于2k。\n本地线程缓存thread-local cache持有不同size-class的空闲链表。\n当分配一个小对象时：\n 将其大小映射到相应的size-class 为当前线程在其thread-local cache的（内存）空闲链表中寻找对应size-class链表 如果空闲链表非空，那么我们将链表的第一个对象移出并返回之，当执行这种快速路径时，TCMalloc不需要任何锁，因为加锁解锁这一对操作在2.8GHz的机器上大约需要100ns，这使得内存分配速度明显加快。  如果空闲链表为空：\n 从central free list(central data structure)获取一系列对应大小的内存。（central data structure被所有线程共享） 将获取到的内存放入thread-local cache的空闲链表。 返回其中一个新获取的内存对象给应用  如果central free list也为空：\n 从central page allocator(central heap)分配一系列页面 将这些页面分割为对应size-class大小的内存对象 将这些新的内存对象放入central free list 像之前所说将内存对象放入thread-local free list  大内存的分配 大对象被对齐到页大小（4K）,并且被central page heap管理。central page heap同样是一个空闲列表数组。当数组下标i小于256时，第k个数组元素是一个每个节点包含k个页的空闲列表，而第256个数组元素中，链表的节点长度大于256页\n一次分配k个页面的需求将被第k个空闲列表满足。如果这个空闲列表为空，那么将在下一个空闲列表中寻找，以此类推。如果有需要的话，我们将在最后一个空闲列表中获得内存对象。如果这仍然失败，那么我们直接从操作系统中获取内存。\n如果一次要求k个页的请求被长度大于k的空闲链表满足，那么剩下的内存将被插入到central page heap合适的空闲链表中.\n跨度（span） TCMalloc管理的堆中有一系列的页。一系列连续的页由span对象表示。span可以被分配或释放。如果被释放，那么span是页堆链表中的一项。如果被分配，那么他是被应用所持有的一个大内存对象，或是已被分割为一个个小内存对象。如果其被分割为小内存对象，那么它的size-class将被记录下来。\n可以用以页号作为下标的central array来表明页属于哪个span。例如，在下图中，spana 占据了两个页，sapnb占据了1个页, sapnc占据了5个页,sapnd占据了3个页。\n一个32位地址空间可被分为2的20次方个4K大小的页，所以一个central array持有4MB空闲看起来是合理的。在64位机器上，我们使用3级基数树而不是数组来将页号映射到相应的span指针。\n解除分配 当一个对象被解除分配，我们计算它的页号并且在central array寻找其对应的span对象。这个span对象告诉我们这个对象是否是小对象以及它的size-class(如果是小对象的话)。如果对象是小对象，我们将其插入到当前线程的thread-local cache中的空闲链表中。如果当前thread-local cache超出了预设的大小（一般为2MB），那么我们进行一个垃圾回收以将其从thread-local cache移到central free list。\n如果这个对象是大对象，那么sapn告诉我们这个对象包含的页号。假设其包含的页号为[p, q]。我们同样会查看相邻的[p-1][q+1]号页，如果这些相邻的页为空闲状态，那么我们将这些空闲页与[p, q]页合并。最后得到的span将被插入到page heap中合适的空闲列表中。\n面向小对象的central free list 正如之前所说，我们为每个size-class维护了一个central free list。每个central free list被组织为两层数据结构：一系列span对象，以及span中的空闲对象列表。\n被分配的对象来自某些span的空闲链表的头部。（如果所有的span都持有空的空闲链表，那么将从central page heap分配合适大小的span）\n在对象被返回时，其将被添加到span持有的链表中。如果链表的长度等于span中的小对象数目，那么说明这个span现在已经被完全释放，其应该被返还到page heap.\nthread-local cache的垃圾收集 当thread-local cache的总大小超过2MB时，就会触发对其的垃圾收集。当线程增加数时，这个垃圾收集门槛将自动减少，这样我们就不会在具有大量线程的程序中浪费过多的内存。\n我们将遍历所有cache中的空闲列表并且将一些对象从空闲列表中移动到相应的central list。\n被移动对象的数目是使用每个列表上低位标记L确定的，L记录自上次垃圾回收以来列表的最小长度。注意，我们可以在最后一次垃圾回收时将列表缩短L个对象而无需对central list进行任何额外访问。我们将这部分历史记录作为对未来访问的一个预测，将L/2个对象从thread-local cache移动到对应的central free list。这个算法有一个很好的特性，如果一个线程停止使用特定大小的内存，那么所有该大小的对象将被快速的从thread-local cache移动到central free list，从而这些对象可以被其他线程使用。\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/tcmalloc/","summary":"翻译自：TCMalloc : Thread-Caching Malloc（性能测试部分没有翻译）\n 动机 在我测试过的所有malloc(动态内存分配器)中，TCMalloc比glibc 2.3 malloc(作为一个单独的库称作ptmalloc2)以及其他内存分配器都要快。对于小内存对象来说，在Intel® Pentium® 4 Processor 2.80 GHzCPU上ptmalloc2执行一次内存分配/回收操作需要大约300ns，而TCMalloc完成相同的操作只需要50ns。显然对于内存分配操作来说，速度十分重要，因为如果内存分配不够及时，开发人员就倾向于在malloc上编写他们自己的空闲列表，这会造成额外的复杂性以及更多的内存占用，除非开发人员非常小心的估算空闲列表的大小并清理其中的空闲对象。\nTCMalloc也降低了多线程应用中的锁冲突。对于小内存对象来说几乎不存在冲突。对于大内存对象来说，TCMalloc尝试使用细粒度和高效的自旋锁。ptmalloc2也尝试通过一些方法降低锁冲突，其为每个线程分配一个arena空间，但ptmalloc2对于arena空间的使用存在一个大问题：在ptmalloc2中内存将不可能从一个arena空间转移到另一个arena空间，也即内存不可以在线程之间进行二次分配。这会导致巨大的内存浪费。例如，在一个Google应用中，阶段一为其数据结构分配了大约300MB。当其第一阶段结束后，阶段二将在相同的地址空间上开始。如果阶段二分配了一个与阶段一不同的arena空间，那么阶段二的计算将不会重复使用阶段一留下的任何内存空间，而是重新分配另一个300MB内存空间。这种内存的“blowup”问题同样出现在其他应用中。\nTCMolloc的另一个优点是针对小内存对象的空间的有效利用。例如，可以将8N bytes大小的对象分配到8N*1.01bytes的空间上，即只需要1%的空间开销。ptmalloc2对每一个对象分配一个4bytes的头，（我认为）这种方式将本来只需要8N bytes大小对象变成了需要16N bytes\n用法 要想使用TCMalloc，只要使用-l tcmalloc标志将tcmalloc链接到你的应用。\n你也可以在不是你编译的应用中使用tcmalloc，通过使用LD_PRELOAD环境变量\nLD_PRELOAD=\u0026#34;/usr/lib/libtcmalloc.so\u0026#34; 但我们不推荐在非必要的情况下使用这种方式。\nTCMalloc也包括一个堆检查器和一个堆分析器。\n如果你只想要链接一个没有堆检查器和分析器的TCMalloc版本（可能想要减小二进制包的大小），你可以链接libtcmalloc_minimal\n概览 TCMalloc为每个线程分配一个本地线程缓存thread-local cache。小的内存分配将直接被本地线程缓存满足。对象按需从中间部件central data structure移动到本地线程缓存。定期的垃圾收集被用来把内存从本地线程缓存放回中间部件central data structure。\nTCMalloc对于大小\u0026lt;=32K的（小）对象的处理方式与大对象不同。大对象由顶层堆管理器central heap使用页级的分配器直接分配。（一个页面是一个4K对齐的内存区域），同时，大对象总是页对齐并且占据整数个页面。\n页面可被一系列的小对象瓜分为大小相同的区域。例如：一个4K的内存将被32个对象分割为每个128bytes的内存序列。\n小对象的分配 每个小对象都对应于170个可分配内存大小size-classes中的一种，例如，大小范围在961-1024bytes的对象将占据1024bytes。这些内存大小级别被不同大小的间距分隔开，其中较小尺寸为8bytes，大尺寸为16bytes，更大的是32bytes，以此类推。最大的空间是256bytes（对于size-classes）大于等于2k。\n本地线程缓存thread-local cache持有不同size-class的空闲链表。\n当分配一个小对象时：\n 将其大小映射到相应的size-class 为当前线程在其thread-local cache的（内存）空闲链表中寻找对应size-class链表 如果空闲链表非空，那么我们将链表的第一个对象移出并返回之，当执行这种快速路径时，TCMalloc不需要任何锁，因为加锁解锁这一对操作在2.8GHz的机器上大约需要100ns，这使得内存分配速度明显加快。  如果空闲链表为空：\n 从central free list(central data structure)获取一系列对应大小的内存。（central data structure被所有线程共享） 将获取到的内存放入thread-local cache的空闲链表。 返回其中一个新获取的内存对象给应用  如果central free list也为空：\n 从central page allocator(central heap)分配一系列页面 将这些页面分割为对应size-class大小的内存对象 将这些新的内存对象放入central free list 像之前所说将内存对象放入thread-local free list  大内存的分配 大对象被对齐到页大小（4K）,并且被central page heap管理。central page heap同样是一个空闲列表数组。当数组下标i小于256时，第k个数组元素是一个每个节点包含k个页的空闲列表，而第256个数组元素中，链表的节点长度大于256页","title":"TCMalloc : Thread-Caching Malloc"},{"content":"1. 两种端口 我们知道TCP/UDP在工作时都需要一个端口来进行收发信息，有两种类型的端口：\n 临时端口或者叫动态端口，是默认情况下计算机进行出站连接时所有的端口集 已知端口，是特定应用程序或服务的定义端口。 例如，文件服务器服务在端口 445 上，HTTPS 为 443，HTTP 为 80，RPC 为 135。 自定义应用程序还将具有其定义的端口号。  客户端要想连接到应用程序或服务，需要使用计算机中的临时端口去连接服务器的已知端口。如：客户端计算机上的浏览器将使用临时端口连接到端口 https://www.microsoft.com 443。\n当浏览器创建与多个网站的大量连接的情况下，其所尝试的任何新连接都将使用临时端口。 一段时间之后，连接将开始失败，并且出现此故障的可能性很高，因为浏览器已使用所有可用端口进行外部连接，并且建立连接的任何新尝试都将失败，因为没有更多的端口可用。 当使用计算机上的所有端口时，我们将它视为端口耗尽。\n2. TCP/IP的默认动态端口范围 window上有两种动态端口范围：\n 起始端口49152，结束端口65535 （新版） 起始端口1025，结束端口5000（旧版）  可使用如下命令查看计算机上动态端口范围：\nnetsh int ipv4 show dynamicport tcp netsh int ipv4 show dynamicport udp netsh int ipv6 show dynamicport tcp netsh int ipv6 show dynamicport udp 也可以手动更改动态端口的范围：\nnetsh int \u0026lt;ipv4|ipv6\u0026gt; set dynamic \u0026lt;tcp|udp\u0026gt; start=number num=range 其中start是起始端口号，num是范围\n例如：\nnetsh int ipv4 set dynamicport tcp start=10000 num=1000 netsh int ipv4 set dynamicport udp start=10000 num=1000 netsh int ipv6 set dynamicport tcp start=10000 num=1000 netsh int ipv6 set dynamicport udp start=10000 num=1000 这些示例命令将动态端口范围设置为从10000开始，分配1000个动态端口，即：10000-19999。可以设置的最小端口范围是255，可以设置的最小起始端口为1025。若要复制 Windows Server 2003 的默认行为，请使用 1025 作为起始端口，然后使用 3976 作为 TCP 和 UDP 的范围。 这导致起始端口为 1025，结束端口为 5000。\n","permalink":"http://yangchnet.github.io/Dessert/posts/windows/%E7%AB%AF%E5%8F%A3%E6%B6%88%E8%80%97%E9%97%AE%E9%A2%98/","summary":"1. 两种端口 我们知道TCP/UDP在工作时都需要一个端口来进行收发信息，有两种类型的端口：\n 临时端口或者叫动态端口，是默认情况下计算机进行出站连接时所有的端口集 已知端口，是特定应用程序或服务的定义端口。 例如，文件服务器服务在端口 445 上，HTTPS 为 443，HTTP 为 80，RPC 为 135。 自定义应用程序还将具有其定义的端口号。  客户端要想连接到应用程序或服务，需要使用计算机中的临时端口去连接服务器的已知端口。如：客户端计算机上的浏览器将使用临时端口连接到端口 https://www.microsoft.com 443。\n当浏览器创建与多个网站的大量连接的情况下，其所尝试的任何新连接都将使用临时端口。 一段时间之后，连接将开始失败，并且出现此故障的可能性很高，因为浏览器已使用所有可用端口进行外部连接，并且建立连接的任何新尝试都将失败，因为没有更多的端口可用。 当使用计算机上的所有端口时，我们将它视为端口耗尽。\n2. TCP/IP的默认动态端口范围 window上有两种动态端口范围：\n 起始端口49152，结束端口65535 （新版） 起始端口1025，结束端口5000（旧版）  可使用如下命令查看计算机上动态端口范围：\nnetsh int ipv4 show dynamicport tcp netsh int ipv4 show dynamicport udp netsh int ipv6 show dynamicport tcp netsh int ipv6 show dynamicport udp 也可以手动更改动态端口的范围：\nnetsh int \u0026lt;ipv4|ipv6\u0026gt; set dynamic \u0026lt;tcp|udp\u0026gt; start=number num=range 其中start是起始端口号，num是范围\n例如：\nnetsh int ipv4 set dynamicport tcp start=10000 num=1000 netsh int ipv4 set dynamicport udp start=10000 num=1000 netsh int ipv6 set dynamicport tcp start=10000 num=1000 netsh int ipv6 set dynamicport udp start=10000 num=1000 这些示例命令将动态端口范围设置为从10000开始，分配1000个动态端口，即：10000-19999。可以设置的最小端口范围是255，可以设置的最小起始端口为1025。若要复制 Windows Server 2003 的默认行为，请使用 1025 作为起始端口，然后使用 3976 作为 TCP 和 UDP 的范围。 这导致起始端口为 1025，结束端口为 5000。","title":"端口消耗问题"},{"content":"1. tag的基本介绍 字段标签可以存储元信息，这些元信息可以使用反射来访问。通常这些元信息用来提供一个字段如何从一种格式编码至另一种格式的相关信息（或是数据应如何在数据库中存储等）。但实际上标签可以存储任何你想要的元信息，无论是你自己使用还是由另一个包使用。\n就像reflect.StructTag文档中提到的那样，字段标签通常是由空格分割的key:\u0026quot;value\u0026quot;列表，例如：\ntype User struct { Name string `json:\u0026#34;name\u0026#34; xml:\u0026#34;name\u0026#34;` } 其中的key通常表示后面\u0026quot;value\u0026quot;所对应的包，例如json这个key将被encoding/json这个包使用。\n如果需要在\u0026quot;value\u0026quot;中传递多个值，那么通常使用,逗号来分割，例如：\nName string `json:\u0026#34;name,omitempty\u0026#34; xml:\u0026#34;name\u0026#34;` 值为破折号通常代表在处理时忽略该字段，例如在json中代表不要序列化这个字段\n2. 例子：获取自定义tag 我们可以使用反射包来获取结构体字段的值。首先我们需要获取结构体的Type，然后查询字段，可以使用Type.Field(i int)或者Type.FieldByName(name string)。这些方法返回一个代表结构体字段的StructField值和一个代表tag的类型为StructTag的StructField.Tag值。\n前面我们提到，字段标签通常是由空格分割的key:\u0026quot;value\u0026quot;列表，如果你的确是这么做的，你可以使用StructTag.Get(key string)这个方法来获取这个key对应的value。如果你不是这么做的，Get()方法可能不能解析key:\u0026quot;value\u0026quot;对并找到你想要的标签。如果你没有遵循字段标签通常是由空格分割的key:\u0026quot;value\u0026quot;列表，那么你可能需要实现自己的解析逻辑。\ngo1.7中添加了StructTag.Lookup()方法，这个方法的行为类似于Get()，但其将不包含给定键的标签与将空字符串与给定键相关联的标签区分开来。\n来看下面这个例子：\ntype User struct { Name string `mytag:\u0026#34;MyName\u0026#34;` Email stirng `mytag:\u0026#34;MyEmail\u0026#34;` } u := User{\u0026#34;Bob\u0026#34;, \u0026#34;bob@cc.com\u0026#34;} t := reflect.TypeOf(u) for _ fieldName := range []string{\u0026#34;Name\u0026#34;, \u0026#34;Email\u0026#34;} { field, found := t.FieldByName(fieldName) if !found { continue } fmt.Printf(\u0026#34;\\nField: User.%s\\n\u0026#34;, fieldName) fmt.Printf(\u0026#34;\\tWhole tag value : %q\\n\u0026#34;, field.Tag) fmt.Printf(\u0026#34;\\tValue of \u0026#39;mytag\u0026#39;: %q\\n\u0026#34;, field.Tag.Get(\u0026#34;mytag\u0026#34;)) } 输出为：\nField: User.Name Whole tag value : \u0026quot;mytag:\\\u0026quot;MyName\\\u0026quot;\u0026quot; Value of 'mytag': \u0026quot;MyName\u0026quot; Field: User.Email Whole tag value : \u0026quot;mytag:\\\u0026quot;MyEmail\\\u0026quot;\u0026quot; Value of 'mytag': \u0026quot;MyEmail\u0026quot; 3. 常见的tag  json - used by the encoding/json package, detailed at json.Marshal() xml - used by the encoding/xml package, detailed at xml.Marshal() bson - used by gobson, detailed at bson.Marshal() protobuf - used by github.com/golang/protobuf/proto, detailed in the package doc yaml - used by the gopkg.in/yaml.v2 package, detailed at yaml.Marshal() db - used by the github.com/jmoiron/sqlx package; also used by github.com/go-gorp/gorp package orm - used by the github.com/astaxie/beego/orm package, detailed at Models – Beego ORM gorm - used by gorm.io/gorm, examples can be found in their docs valid - used by the github.com/asaskevich/govalidator package, examples can be found in the project page datastore - used by appengine/datastore (Google App Engine platform, Datastore service), detailed at Properties schema - used by github.com/gorilla/schema to fill a struct with HTML form values, detailed in the package doc asn - used by the encoding/asn1 package, detailed at asn1.Marshal() and asn1.Unmarshal() csv - used by the github.com/gocarina/gocsv package env - used by the github.com/caarlos0/env package  Translated from: https://stackoverflow.com/questions/10858787/what-are-the-uses-for-tags-in-go\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/golang%E4%B8%AD%E7%9A%84tag/","summary":"1. tag的基本介绍 字段标签可以存储元信息，这些元信息可以使用反射来访问。通常这些元信息用来提供一个字段如何从一种格式编码至另一种格式的相关信息（或是数据应如何在数据库中存储等）。但实际上标签可以存储任何你想要的元信息，无论是你自己使用还是由另一个包使用。\n就像reflect.StructTag文档中提到的那样，字段标签通常是由空格分割的key:\u0026quot;value\u0026quot;列表，例如：\ntype User struct { Name string `json:\u0026#34;name\u0026#34; xml:\u0026#34;name\u0026#34;` } 其中的key通常表示后面\u0026quot;value\u0026quot;所对应的包，例如json这个key将被encoding/json这个包使用。\n如果需要在\u0026quot;value\u0026quot;中传递多个值，那么通常使用,逗号来分割，例如：\nName string `json:\u0026#34;name,omitempty\u0026#34; xml:\u0026#34;name\u0026#34;` 值为破折号通常代表在处理时忽略该字段，例如在json中代表不要序列化这个字段\n2. 例子：获取自定义tag 我们可以使用反射包来获取结构体字段的值。首先我们需要获取结构体的Type，然后查询字段，可以使用Type.Field(i int)或者Type.FieldByName(name string)。这些方法返回一个代表结构体字段的StructField值和一个代表tag的类型为StructTag的StructField.Tag值。\n前面我们提到，字段标签通常是由空格分割的key:\u0026quot;value\u0026quot;列表，如果你的确是这么做的，你可以使用StructTag.Get(key string)这个方法来获取这个key对应的value。如果你不是这么做的，Get()方法可能不能解析key:\u0026quot;value\u0026quot;对并找到你想要的标签。如果你没有遵循字段标签通常是由空格分割的key:\u0026quot;value\u0026quot;列表，那么你可能需要实现自己的解析逻辑。\ngo1.7中添加了StructTag.Lookup()方法，这个方法的行为类似于Get()，但其将不包含给定键的标签与将空字符串与给定键相关联的标签区分开来。\n来看下面这个例子：\ntype User struct { Name string `mytag:\u0026#34;MyName\u0026#34;` Email stirng `mytag:\u0026#34;MyEmail\u0026#34;` } u := User{\u0026#34;Bob\u0026#34;, \u0026#34;bob@cc.com\u0026#34;} t := reflect.TypeOf(u) for _ fieldName := range []string{\u0026#34;Name\u0026#34;, \u0026#34;Email\u0026#34;} { field, found := t.FieldByName(fieldName) if !found { continue } fmt.Printf(\u0026#34;\\nField: User.%s\\n\u0026#34;, fieldName) fmt.Printf(\u0026#34;\\tWhole tag value : %q\\n\u0026#34;, field.","title":"golang中的tag"},{"content":"已过时，不可用\n 部署环境： ubuntu20.04， 8G+4核 kubernete版本： 1.22.1\n 1. 安装vagrant和ansible 按官网教程即可\n2. Vagrantfile 建立如下目录\nk8s-cluster ├── kubernetes-setup │ ├── master-playbook.yml │ └── node-playbook.yml └── Vagrantfile 其中，Vagrantfile内容如下：\nIMAGE_NAME = \u0026quot;bento/ubuntu-16.04\u0026quot; N = 2 Vagrant.configure(\u0026quot;2\u0026quot;) do |config| config.ssh.insert_key = false config.vm.provider \u0026quot;virtualbox\u0026quot; do |v| v.memory = 2048 v.cpus = 2 end config.vm.define \u0026quot;k8s-master\u0026quot; do |master| master.vm.box = IMAGE_NAME master.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;192.168.50.10\u0026quot; master.vm.hostname = \u0026quot;k8s-master\u0026quot; master.vm.provision \u0026quot;ansible\u0026quot; do |ansible| ansible.playbook = \u0026quot;kubernetes-setup/master-playbook.yml\u0026quot; ansible.extra_vars = { node_ip: \u0026quot;192.168.50.10\u0026quot;, } end end (1..N).each do |i| config.vm.define \u0026quot;node-#{i}\u0026quot; do |node| node.vm.box = IMAGE_NAME node.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;192.168.50.#{i + 10}\u0026quot; node.vm.hostname = \u0026quot;node-#{i}\u0026quot; node.vm.provision \u0026quot;ansible\u0026quot; do |ansible| ansible.playbook = \u0026quot;kubernetes-setup/node-playbook.yml\u0026quot; ansible.extra_vars = { node_ip: \u0026quot;192.168.50.#{i + 10}\u0026quot;, } end end end end master-playbook.ym内容如下：\n--- - hosts: all become: true tasks: - name: Install packages that allow apt to be used over HTTPS apt: name: \u0026#34;{{ packages }}\u0026#34; state: present update_cache: yes vars: packages: - apt-transport-https - ca-certificates - curl - gnupg-agent - software-properties-common - name: Add an apt signing key for Docker apt_key: url: http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg state: present - name: Add apt repository for stable version apt_repository: repo: deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial stable state: present - name: Install docker and its dependecies apt: name: \u0026#34;{{ packages }}\u0026#34; state: present update_cache: yes vars: packages: - docker-ce - docker-ce-cli - containerd.io notify: - docker status - name: Add vagrant user to docker group user: name: vagrant group: docker - name: Change Cgroup Driver from cgroupfs to systemd shell: |echo \u0026#39;{ \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] }\u0026#39; \u0026gt;\u0026gt; /etc/docker/daemon.json - name: Restart docker shell: |systemctl daemon-reload systemctl restart docker - name: Remove swapfile from /etc/fstab mount: name: \u0026#34;{{ item }}\u0026#34; fstype: swap state: absent with_items: - swap - none - name: Disable swap command: swapoff -a when: ansible_swaptotal_mb \u0026gt; 0 - name: Add an apt signing key for Kubernetes apt_key: url: https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg state: present - name: Adding apt repository for Kubernetes apt_repository: repo: deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main state: present filename: kubernetes.list - name: Install Kubernetes binaries apt: name: \u0026#34;{{ packages }}\u0026#34; state: present update_cache: yes vars: packages: - kubelet - kubeadm - kubectl - name: Configure node ip lineinfile: path: /usr/bin/kubelet line: KUBELET_EXTRA_ARGS=--node-ip={{ node_ip }} - name: Restart kubelet service: name: kubelet daemon_reload: yes state: restarted - name: Pull CoreDns:v1.8.4 shell: |docker pull registry.aliyuncs.com/google_containers/coredns:latest docker tag registry.aliyuncs.com/google_containers/coredns:latest registry.aliyuncs.com/google_containers/coredns:v1.8.4 docker rmi registry.aliyuncs.com/google_containers/coredns:latest - name: Initialize the Kubernetes cluster using kubeadm command: kubeadm init --apiserver-advertise-address=\u0026#34;192.168.50.10\u0026#34; --apiserver-cert-extra-sans=\u0026#34;192.168.50.10\u0026#34; --node-name k8s-master --pod-network-cidr=192.168.0.0/16 --image-repository=registry.aliyuncs.com/google_containers - name: Setup kubeconfig for vagrant user command: \u0026#34;{{ item }}\u0026#34; with_items: - mkdir -p /home/vagrant/.kube - cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config - chown vagrant:vagrant /home/vagrant/.kube/config - name: Install calico pod network become: false command: kubectl create -f https://docs.projectcalico.org/manifests/calico.yaml - name: Generate join command command: kubeadm token create --print-join-command register: join_command - name: Copy join command to local file shell: |echo hello | tee /vagrant/hello kubeadm token create --print-join-command | tee /vagrant/join-command # local_action: copy content=\u0026#34;{{ join_command.stdout_lines[0] }}\u0026#34; dest=\u0026#34;{{ playbook_dir }}/join-command\u0026#34; handlers: - name: docker status service: name=docker state=started node-playbook.yml内容如下：\n--- - hosts: all become: true tasks: - name: Install packages that allow apt to be used over HTTPS apt: name: \u0026#34;{{ packages }}\u0026#34; state: present update_cache: yes vars: packages: - apt-transport-https - ca-certificates - curl - gnupg-agent - software-properties-common - name: Add an apt signing key for Docker apt_key: url: http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg state: present - name: Add apt repository for stable version apt_repository: repo: deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial stable state: present - name: Install docker and its dependecies apt: name: \u0026#34;{{ packages }}\u0026#34; state: present update_cache: yes vars: packages: - docker-ce - docker-ce-cli - containerd.io notify: - docker status - name: Add vagrant user to docker group user: name: vagrant group: docker - name: Change Cgroup Driver from cgroupfs to systemd shell: |echo \u0026#39;{ \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] }\u0026#39; \u0026gt;\u0026gt; /etc/docker/daemon.json - name: Restart docker shell: |systemctl daemon-reload systemctl restart docker - name: Remove swapfile from /etc/fstab mount: name: \u0026#34;{{ item }}\u0026#34; fstype: swap state: absent with_items: - swap - none - name: Disable swap command: swapoff -a when: ansible_swaptotal_mb \u0026gt; 0 - name: Add an apt signing key for Kubernetes apt_key: url: https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg state: present - name: Adding apt repository for Kubernetes apt_repository: repo: deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main state: present filename: kubernetes.list - name: Install Kubernetes binaries apt: name: \u0026#34;{{ packages }}\u0026#34; state: present update_cache: yes vars: packages: - kubelet - kubeadm - kubectl - name: Configure node ip lineinfile: path: /usr/bin/kubelet line: KUBELET_EXTRA_ARGS=--node-ip={{ node_ip }} - name: Restart kubelet service: name: kubelet daemon_reload: yes state: restarted - name: Copy the join command to server location shell: |cat /vagrant/join-command \u0026gt;\u0026gt; /tmp/join-command.sh chmod +x /tmp/join-command.sh - name: Join the node to cluster command: sh /tmp/join-command.sh handlers: - name: docker status service: name=docker state=started 3. 启动集群 cd /path/to/k8s-cluster/ vagrant up 耐心等待一段时间，三个节点的k8s集群就启动成功了。\n","permalink":"http://yangchnet.github.io/Dessert/posts/cloud/vagrant+ansible%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4/","summary":"已过时，不可用\n 部署环境： ubuntu20.04， 8G+4核 kubernete版本： 1.22.1\n 1. 安装vagrant和ansible 按官网教程即可\n2. Vagrantfile 建立如下目录\nk8s-cluster ├── kubernetes-setup │ ├── master-playbook.yml │ └── node-playbook.yml └── Vagrantfile 其中，Vagrantfile内容如下：\nIMAGE_NAME = \u0026quot;bento/ubuntu-16.04\u0026quot; N = 2 Vagrant.configure(\u0026quot;2\u0026quot;) do |config| config.ssh.insert_key = false config.vm.provider \u0026quot;virtualbox\u0026quot; do |v| v.memory = 2048 v.cpus = 2 end config.vm.define \u0026quot;k8s-master\u0026quot; do |master| master.vm.box = IMAGE_NAME master.vm.network \u0026quot;private_network\u0026quot;, ip: \u0026quot;192.168.50.10\u0026quot; master.vm.hostname = \u0026quot;k8s-master\u0026quot; master.vm.provision \u0026quot;ansible\u0026quot; do |ansible| ansible.playbook = \u0026quot;kubernetes-setup/master-playbook.yml\u0026quot; ansible.","title":"vagrant+ansible安装k8s集群"},{"content":" 环境：ubuntu-20.04, kubernetes:v1.22.1\n 1. 安装docker  安装时有可能会遇到网络问题，你可以选择换源或是为apt设置代理，设置代理的方法见这里\n  更新源镜像并安装依赖  sudo apt-get update sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg \\  lsb-release 安装docker 官方GPG密钥  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 设置稳定版本  echo \\  \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 安装docker  sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io 安装docker-compose(可选)  sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 2. 安装kubectl, kubeadm, kubelet  更新源镜像并安装依赖  sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl 下载谷歌公共签名密钥   对于curl, 可使用 -x , \u0026ndash;proxy \u0026lt;[protocol://][user:password@]proxyhost[:port]\u0026gt; 来设置代理\n sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg 将kubernetes增加到apt仓库  echo \u0026#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list 更新软件源并安装  sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl 3. 使用kubeadm安装kubernetes kubeadm init  这一步回到Google的镜像仓库拉取一些镜像，如果拉取不到或者速度过慢，可使用阿里的镜像源 kubeadm init \u0026ndash;image-repository=registry.aliyuncs.com/google_containers\n 若这一步失败，调整设置重新启动之前，需要先\nkubeadm reset 4. 遇到的坑 4.1 swap 要确保swap已被关闭 使用free -m查看swap大小\nfree -m 确保swap那一行为0\n关闭swap方法\nsudo swapoff -a sudo sed -i \u0026#39;/ swap / s/^/#/\u0026#39; /etc/fstab 4.2 cgourp-driver docker的cgroup driver默认是cgroupfs，需要更改为systemed\n使用docker info命令查看： 如果显示的是cgroupfs，那么需要进行更改\n改动方法： 创建/etc/docker/daemon.json，编辑内容为：\n{ \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } 然后重启docker服务：\nsystemctl restart docker 再次重试：\nkubeadm reset kubeadm init 4.3 阿里云镜像tag不对 解决方法是拉取最新版本的镜像，然后更改其tag\ndocker pull registry.aliyuncs.com/google_container/coredns:latest docker tag registry.aliyuncs.com/google_container/coredns:latest registry.aliyuncs.com/google_container/coredns:v1.8.4 docker rmi registry.aliyuncs.com/google_container/coredns:latest 4.4 node NotReady 安装完成后，使用kubectl get nodes查看发现状态为NotReady\n使用kubectl describe ndoes查看报错信息，有如下输出，可以看到问题是network plugin is not ready: cni config uninitialized\n... Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Sat, 04 Sep 2021 10:16:25 +0800 Sat, 04 Sep 2021 09:15:45 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Sat, 04 Sep 2021 10:16:25 +0800 Sat, 04 Sep 2021 09:15:45 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Sat, 04 Sep 2021 10:16:25 +0800 Sat, 04 Sep 2021 09:15:45 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready False Sat, 04 Sep 2021 10:16:25 +0800 Sat, 04 Sep 2021 09:15:45 +0800 KubeletNotReady container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized ... 解决方法： 安装weave\n根据https://www.weave.works/docs/net/latest/kubernetes/kube-addon/，只需运行如下命令:\nkubectl apply -f \u0026#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34; 再次查看nodes\nkubectl get nodes 状态为Ready\n4.5 1 node(s) had taints that the pod didn\u0026rsquo;t tolerate: 这个问题是在创建pod时出现的\n这是因为kubernetes默认不许往master安装，强制允许\nkubectl taint nodes --all node-role.kubernetes.io/master- ","permalink":"http://yangchnet.github.io/Dessert/posts/env/%E4%BD%BF%E7%94%A8kubeadm%E5%AE%89%E8%A3%85%E5%8D%95%E8%8A%82%E7%82%B9kubernetes/","summary":"环境：ubuntu-20.04, kubernetes:v1.22.1\n 1. 安装docker  安装时有可能会遇到网络问题，你可以选择换源或是为apt设置代理，设置代理的方法见这里\n  更新源镜像并安装依赖  sudo apt-get update sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg \\  lsb-release 安装docker 官方GPG密钥  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 设置稳定版本  echo \\  \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs)stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 安装docker  sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.","title":"使用kubeadm安装单节点Kubernetes"},{"content":"1. 临时设置 sudo apt-get -o Acquire::http::proxy=\u0026#34;http://127.0.0.1:8000/\u0026#34; update 2. 永久设置 创建/etc/apt/apt.conf\ntouch /etc/apt/apt.conf 写入如下内容：\nAcquire::http::Proxy \u0026quot;http://yourproxyaddress:proxyport\u0026quot;; 如果proxy需要密码，则格式如下：\nAcquire::http::Proxy \u0026#34;http://username:password@yourproxyaddress:proxyport\u0026#34;; Reference: https://www.jianshu.com/p/fdae9cb5181b\nhttps://askubuntu.com/questions/257290/configure-proxy-for-apt\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/%E4%B8%BAapt%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86/","summary":"1. 临时设置 sudo apt-get -o Acquire::http::proxy=\u0026#34;http://127.0.0.1:8000/\u0026#34; update 2. 永久设置 创建/etc/apt/apt.conf\ntouch /etc/apt/apt.conf 写入如下内容：\nAcquire::http::Proxy \u0026quot;http://yourproxyaddress:proxyport\u0026quot;; 如果proxy需要密码，则格式如下：\nAcquire::http::Proxy \u0026#34;http://username:password@yourproxyaddress:proxyport\u0026#34;; Reference: https://www.jianshu.com/p/fdae9cb5181b\nhttps://askubuntu.com/questions/257290/configure-proxy-for-apt","title":"为apt设置代理"},{"content":"apiVersion可能的字段值：    Kind apiVersion     CertificateSigningRequest certificates.k8s.io/v1beta1   ClusterRoleBinding rbac.authorization.k8s.io/v1   ClusterRole rbac.authorization.k8s.io/v1   ComponentStatus v1   ConfigMap v1   ControllerRevision apps/v1   CronJob batch/v1beta1   DaemonSet extensions/v1beta1   Deployment extensions/v1beta1   Endpoints v1   Event v1   HorizontalPodAutoscaler autoscaling/v1   Ingress extensions/v1beta1   Job batch/v1   LimitRange v1   Namespace v1   NetworkPolicy extensions/v1beta1   Node v1   PersistentVolumeClaim v1   PersistentVolume v1   PodDisruptionBudget policy/v1beta1   Pod v1   PodSecurityPolicy extensions/v1beta1   PodTemplate v1   ReplicaSet extensions/v1beta1   ReplicationController v1   ResourceQuota v1   RoleBinding rbac.authorization.k8s.io/v1   Role rbac.authorization.k8s.io/v1   Secret v1   ServiceAccount v1   Service v1   StatefulSet apps/v1    每个字段的意义 alpha\n这个apiVersion是早期候选版本，可能包含一些bug\nbeta\n已经过alpha版本的使用，未来将会真正包含到kubernetes中，但其工作方式可能会发生一些改变\nstable\n可以安全使用的版本\nv1\nkubernetes第一个稳定的发布版本，包含许多核心对象\napps/v1\napps是kubernetes中最常见的APi组，许多核心对象都是从这里和v1中提取出来的。它包括一些与kubernetes运行相关的功能，比如：Deployments, RollingUpdates, ReplicaSet等\nautoscaling/v1\n这个APi版本允许pods根据不同的资源使用指标自动缩放，\nbatch/v1\nbatchAPI组包含与批处理及类似作业任务相关的对象。\nbatch/v1beta1\nbatch功能对象的beta版本，包含CronJob（轮询任务）\nextensions/v1beta1\n此版本的 API 包括 Kubernetes 的许多新的、常用的功能。 Deployments、DaemonSets、ReplicaSets 和 Ingresses 在此版本中都发生了重大变化。\n请注意，在 Kubernetes 1.6 中，其中一些对象从扩展重新定位到特定 API 组（例如应用程序）。 当这些对象退出 Beta 版时，希望它们位于特定的 API 组中，例如 apps/v1。 使用 extensions/v1beta1 已被弃用——根据您的 Kubernetes 集群版本，尽可能尝试使用特定的 API 组。\npolicy/v1beta1\n此 apiVersion 增加了设置 pod 中断预算和有关 pod 安全性的新规则的功能。\nrbac.authorization.k8s.io/v1 此 apiVersion 包括基于 Kubernetes 角色的访问控制的额外功能。 这有助于您保护集群\n查看可用的apiVersion kubectl api-versions admissionregistration.k8s.io/v1 admissionregistration.k8s.io/v1beta1 apiextensions.k8s.io/v1 apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1 apiregistration.k8s.io/v1beta1 apps/v1 authentication.k8s.io/v1 authentication.k8s.io/v1beta1 authorization.k8s.io/v1 authorization.k8s.io/v1beta1 autoscaling/v1 autoscaling/v2beta1 autoscaling/v2beta2 batch/v1 batch/v1beta1 certificates.k8s.io/v1 certificates.k8s.io/v1beta1 coordination.k8s.io/v1 coordination.k8s.io/v1beta1 discovery.k8s.io/v1 discovery.k8s.io/v1beta1 events.k8s.io/v1 events.k8s.io/v1beta1 extensions/v1beta1 flowcontrol.apiserver.k8s.io/v1beta1 networking.k8s.io/v1 networking.k8s.io/v1beta1 node.k8s.io/v1 node.k8s.io/v1beta1 policy/v1 policy/v1beta1 rbac.authorization.k8s.io/v1 rbac.authorization.k8s.io/v1beta1 scheduling.k8s.io/v1 scheduling.k8s.io/v1beta1 storage.k8s.io/v1 storage.k8s.io/v1beta1 v1 Translated from: https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html\n","permalink":"http://yangchnet.github.io/Dessert/posts/cloud/k8s%E4%B8%AD%E7%9A%84apiversion/","summary":"apiVersion可能的字段值：    Kind apiVersion     CertificateSigningRequest certificates.k8s.io/v1beta1   ClusterRoleBinding rbac.authorization.k8s.io/v1   ClusterRole rbac.authorization.k8s.io/v1   ComponentStatus v1   ConfigMap v1   ControllerRevision apps/v1   CronJob batch/v1beta1   DaemonSet extensions/v1beta1   Deployment extensions/v1beta1   Endpoints v1   Event v1   HorizontalPodAutoscaler autoscaling/v1   Ingress extensions/v1beta1   Job batch/v1   LimitRange v1   Namespace v1   NetworkPolicy extensions/v1beta1   Node v1   PersistentVolumeClaim v1   PersistentVolume v1   PodDisruptionBudget policy/v1beta1   Pod v1   PodSecurityPolicy extensions/v1beta1   PodTemplate v1   ReplicaSet extensions/v1beta1   ReplicationController v1   ResourceQuota v1   RoleBinding rbac.","title":"k8s中的apiVersion"},{"content":"1. 安装sqlc go get github.com/kyleconroy/sqlc/cmd/sqlc 2. 基本使用 建立基本项目结构 mkdir sqlc-demo cd sqlc-demo go mod init sqlc-demo 在sqlc-demo中建立如下目录结构：\n. ├── db │ ├── queries │ ├── schema │ └── sqlc └── go.mod 其中query中存储查询语句，schema中存储数据库表结构，sqlc中存储生成的代码。\n基本表结构  sqlc-demo/db/schema/table.sql\n CREATE TABLE \u0026#34;accounts\u0026#34; ( \u0026#34;id\u0026#34; bigserial PRIMARY KEY, \u0026#34;owner\u0026#34; varchar NOT NULL, \u0026#34;balance\u0026#34; bigint NOT NULL, \u0026#34;currency\u0026#34; varchar NOT NULL, \u0026#34;created_at\u0026#34; timestamptz NOT NULL DEFAULT (now()) ); CREATE TABLE \u0026#34;entries\u0026#34; ( \u0026#34;id\u0026#34; bigserial PRIMARY KEY, \u0026#34;account_id\u0026#34; bigint NOT NULL, \u0026#34;amount\u0026#34; bigint NOT NULL, \u0026#34;created_at\u0026#34; timestamptz NOT NULL DEFAULT (now()) ); CREATE TABLE \u0026#34;transfers\u0026#34; ( \u0026#34;id\u0026#34; bigserial PRIMARY KEY, \u0026#34;from_account_id\u0026#34; bigint NOT NULL, \u0026#34;to_account_id\u0026#34; bigint NOT NULL, \u0026#34;amount\u0026#34; bigint NOT NULL, \u0026#34;created_at\u0026#34; timestamptz NOT NULL DEFAULT (now()) ); 配置文件  sqlc-demo/sqlc.yaml\n version: 1 packages: - name: \u0026#34;db\u0026#34; # 生成文件的包名 path: \u0026#34;db/sqlc\u0026#34; # 生成的文件位置 engine: \u0026#34;postgresql\u0026#34; # 数据库驱动引擎 schema: \u0026#34;./db/schema/\u0026#34; # 数据库表结构 queries: \u0026#34;./db/queries\u0026#34; # 查询语句位置 查询语句 现在有了表结构和配置文件，我们可以开始根据业务需要书写查询语句了。\n sqlc-demo/db/queries/account.sql\n -- name: CreateAccount :one INSERT INTO accounts ( owner, balance, currency ) VALUES ( $1, $2, $3 ) RETURNING *; 在上面这个查询语句中，第一行注释是为sqlc做的标记，告诉sqlc，要生成的函数名字为CreateAccount, 返回单个对象。\n生成代码  sqlc-demo/Makefile\n sqlc: sqlc generate .PHONY: sqlc 查看sqlc-demo/db/sqlc/文件夹下，生成了三个文件：\n. ├── db │ ├── queries │ │ └── account.sql │ ├── schema │ │ └── table.sql │ └── sqlc │ ├── account.sql.go │ ├── db.go │ └── models.go ├── go.mod ├── Makefile └── sqlc.yaml 其中，account.sql.go中包含有生成的CreateAccount函数:\nfunc (q *Queries) CreateAccount(ctx context.Context, arg CreateAccountParams) (Account, error) { row := q.db.QueryRowContext(ctx, createAccount, arg.Owner, arg.Balance, arg.Currency) var i Account err := row.Scan( \u0026amp;i.ID, \u0026amp;i.Owner, \u0026amp;i.Balance, \u0026amp;i.Currency, \u0026amp;i.CreatedAt, ) return i, err } 这样，我们就可以非常方便的通过golang函数调用的方式完成操作数据库的任务。\n使用 func run() error { ctx := context.Background() psql_db, err := sql.Open(\u0026#34;postgres\u0026#34;, \u0026#34;user=pqgotest dbname=pqgotest sslmode=verify-full\u0026#34;) if err != nil { return err } queries := db.New(psql_db) // create an author \tinsertedAccount, err := queries.CreateAccount(ctx, db.CreateAccountParams{ Owner: \u0026#34;name\u0026#34;, Balance: 0, Currency: \u0026#34;RMB\u0026#34;, }) if err != nil { return err } log.Println(insertedAccount) return nil 3. 注释 要想让sqlc为你生成代码，需要在查询语句的上方写上一行注释，其格式为：\n-- name: \u0026lt;name\u0026gt; \u0026lt;command\u0026gt; 其中, 是你想要生成函数的名字，则是对生成函数的返回做简单的设置\n :exec  :exec使函数只执行查询语句，但不返回任何值（除error外）\n-- name: DeleteAuthor :exec DELETE FROM authors WHERE id = $1; func (q *Queries) DeleteAuthor(ctx context.Context, id int64) error { _, err := q.db.ExecContext(ctx, deleteAuthor, id) return err } :execresult :execresult使函数返回一个sql.Result接口，其接口定义如下：  type Result interface { // LastInsertId returns the integer generated by the database \t// in response to a command. Typically this will be from an \t// \u0026#34;auto increment\u0026#34; column when inserting a new row. Not all \t// databases support this feature, and the syntax of such \t// statements varies. \tLastInsertId() (int64, error) // RowsAffected returns the number of rows affected by an \t// update, insert, or delete. Not every database or database \t// driver may support this. \tRowsAffected() (int64, error) } -- name: DeleteAllAuthors :execresult DELETE FROM authors; func (q *Queries) DeleteAllAuthors(ctx context.Context) (sql.Result, error) { return q.db.ExecContext(ctx, deleteAllAuthors) } :execrows :execrows使函数返回受该查询语句影响的行  -- name: DeleteAllAuthors :execrows DELETE FROM authors; func (q *Queries) DeleteAllAuthors(ctx context.Context) (int64, error) { _, err := q.db.ExecContext(ctx, deleteAllAuthors) // ... } :many :many使函数返回一个切片  -- name: ListAuthors :many SELECT * FROM authors ORDER BY name; func (q *Queries) ListAuthors(ctx context.Context) ([]Author, error) { rows, err := q.db.QueryContext(ctx, listAuthors) // ... } :one :one使函数返回单个对象  -- name: GetAuthor :one SELECT * FROM authors WHERE id = $1 LIMIT 1; func (q *Queries) GetAuthor(ctx context.Context, id int64) (Author, error) { row := q.db.QueryRowContext(ctx, getAuthor, id) // ... } 4. 配置文件 version: \u0026#34;1\u0026#34; packages: - name: \u0026#34;db\u0026#34; # 生成代码的包名 path: \u0026#34;internal/db\u0026#34; # 生成代码的位置 queries: \u0026#34;./sql/query/\u0026#34; # 查询语句的位置 schema: \u0026#34;./sql/schema/\u0026#34; # 数据库表的位置 engine: \u0026#34;postgresql\u0026#34; # 数据库引擎 emit_prepared_queries: true # 开启预查询支持，默认为false emit_interface: false # 如果为true，则会生成一个Querier接口，包含所有查询方法，默认为false emit_exact_table_names: false # 如果为true，则结构体会与表名相同，否则将会对复数表名进行单数化，默认为false emit_empty_slices: false # 如果为true，则:many标签若查询为空会返回[]而不是nil, 默认为false emit_exported_queries: false # 如果为true，则生成的函数为可导出的 emit_json_tags: true # 如果为true，则为生成的结构体增加json标签 json_tags_case_style: \u0026#34;camel\u0026#34; # json标签的风格，\u0026#34;camel\u0026#34;：camelCase，\u0026#34;pascal\u0026#34;:PascalCase, \u0026#34;snake\u0026#34;: snake_case output_db_file_name: \u0026#34;db.go\u0026#34; # 自定义db文件的名字 output_models_file_name: \u0026#34;models.go\u0026#34; # 自定义model文件的名字 output_querier_file_name: \u0026#34;querier.go\u0026#34; # 自定义querier文件的名字 ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/orm%E4%B9%8Bsqlc/","summary":"1. 安装sqlc go get github.com/kyleconroy/sqlc/cmd/sqlc 2. 基本使用 建立基本项目结构 mkdir sqlc-demo cd sqlc-demo go mod init sqlc-demo 在sqlc-demo中建立如下目录结构：\n. ├── db │ ├── queries │ ├── schema │ └── sqlc └── go.mod 其中query中存储查询语句，schema中存储数据库表结构，sqlc中存储生成的代码。\n基本表结构  sqlc-demo/db/schema/table.sql\n CREATE TABLE \u0026#34;accounts\u0026#34; ( \u0026#34;id\u0026#34; bigserial PRIMARY KEY, \u0026#34;owner\u0026#34; varchar NOT NULL, \u0026#34;balance\u0026#34; bigint NOT NULL, \u0026#34;currency\u0026#34; varchar NOT NULL, \u0026#34;created_at\u0026#34; timestamptz NOT NULL DEFAULT (now()) ); CREATE TABLE \u0026#34;entries\u0026#34; ( \u0026#34;id\u0026#34; bigserial PRIMARY KEY, \u0026#34;account_id\u0026#34; bigint NOT NULL, \u0026#34;amount\u0026#34; bigint NOT NULL, \u0026#34;created_at\u0026#34; timestamptz NOT NULL DEFAULT (now()) ); CREATE TABLE \u0026#34;transfers\u0026#34; ( \u0026#34;id\u0026#34; bigserial PRIMARY KEY, \u0026#34;from_account_id\u0026#34; bigint NOT NULL, \u0026#34;to_account_id\u0026#34; bigint NOT NULL, \u0026#34;amount\u0026#34; bigint NOT NULL, \u0026#34;created_at\u0026#34; timestamptz NOT NULL DEFAULT (now()) ); 配置文件  sqlc-demo/sqlc.","title":"ORM之sqlc"},{"content":" migrate是一个golang写成的数据库版本迁移工具，可以用来方便的对数据库进行迁移和回退。 Github上有详细的教程等：https://github.com/golang-migrate/migrate\n  建立目录  mkdir -p migrate-demo/db cd migrate-demo/db mkdir ddl mkdir -p schema/blog 现在migrate-demo目录下结构如下：\n. └── db ├── ddl └── schema └── blog 其中，ddl中存储建库的sql文件，schema存放建表的sql文件\n建库  建库\nvim db/ddl/blog.sql CREATE DATABASE IF NOT EXISTS blog DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_unicode_ci; build镜像  编写Dockerfile\nvim db/Dockerfile FROMmysql:5.7COPY ./ddl /docker-entrypoint-initdb.d/ENV MYSQL_ROOT_PASSWORD=admin123 复制到/docker-entrypoint-initdb.d目录下的sql脚本会被自动执行\ndocker build -t mysql-demo -f ./Dockerfile . build成功后，使用docker images命令查看镜像：\nREPOSITORY TAG IMAGE ID CREATED SIZE mysql-demo latest 6a2faae69a6f 26 minutes ago 447MB 启动镜像并查看  docker run --name mysql -p 13306:3306 -d mysql-demo 进入容器查看数据库\ndocker exec -it mysql /bin/sh 可以看到，blog表已被创建\n使用migrate进行数据库迁移  生成sql脚本文件\nmigrate create -ext sql -dir db/blog -seq create_blog_table 会在db/blog目录下生成000001_create_blog_table.down.sql, 000001_create_blog_table.up.sql，其后缀名分别为up.sql, down.sql。\nup.sql文件是你要执行的操作，down.sql中是这个操作的逆操作。\n建表\nvim schema/blog/000001_create_blog_table.up.sql.sql CREATE TABLE `blog_article` ( `id` integer PRIMARY KEY NOT NULL, `tag_id` integer DEFAULT 0, `title` varchar(100) DEFAULT null, `desc` varchar(255) DEFAULT null, `content` text, `created_on` timestamp DEFAULT CURRENT_TIMESTAMP, `created_by` varchar(100) DEFAULT null, `modified_on` timestamp DEFAULT CURRENT_TIMESTAMP, `modified_by` varchar(100) DEFAULT null, `deleted_on` timestamp DEFAULT CURRENT_TIMESTAMP, ); vim schema/blog/000001_create_blog_table.down.sql.sql DROP table IF EXISTS blog_article; 现在的目录结构：\n. └── db ├── ddl │ └── blog.sql ├── Dockerfile └── schema └── blog ├── 000001_create_blog_table.down.sql └── 000001_create_blog_table.up.sql migrate 执行数据库迁移  migrate -source=file://db/schema/blog -database \u0026#34;mysql://root:admin123@tcp(localhost:13306)/blog\u0026#34; -verbose up 再次进入容器查看数据库 ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86-migrate/","summary":"migrate是一个golang写成的数据库版本迁移工具，可以用来方便的对数据库进行迁移和回退。 Github上有详细的教程等：https://github.com/golang-migrate/migrate\n  建立目录  mkdir -p migrate-demo/db cd migrate-demo/db mkdir ddl mkdir -p schema/blog 现在migrate-demo目录下结构如下：\n. └── db ├── ddl └── schema └── blog 其中，ddl中存储建库的sql文件，schema存放建表的sql文件\n建库  建库\nvim db/ddl/blog.sql CREATE DATABASE IF NOT EXISTS blog DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_unicode_ci; build镜像  编写Dockerfile\nvim db/Dockerfile FROMmysql:5.7COPY ./ddl /docker-entrypoint-initdb.d/ENV MYSQL_ROOT_PASSWORD=admin123 复制到/docker-entrypoint-initdb.d目录下的sql脚本会被自动执行\ndocker build -t mysql-demo -f ./Dockerfile . build成功后，使用docker images命令查看镜像：\nREPOSITORY TAG IMAGE ID CREATED SIZE mysql-demo latest 6a2faae69a6f 26 minutes ago 447MB 启动镜像并查看  docker run --name mysql -p 13306:3306 -d mysql-demo 进入容器查看数据库","title":"数据库版本管理-migrate"},{"content":"1. docker for wsl2 在wsl2中使用docker的最佳实践不是在wsl2中安装docker，而是安装docker desktop：\n从docker官网下载并安装完成后，打开docker desktop，选择setting-\u0026gt;General，确保Use the WSL 2 based engine选项被勾选，然后选择右下角Apply\u0026amp;Restart。\n重启docker desktop后，再次打开设置，确保setting-\u0026gt;Resources-\u0026gt;WSL INTEGRATION选项页中你的WSL发行版被勾选。\n完成以上步骤之后，打开你的wsl, 输入docker： 出现这一堆说明安装成功。\n使用docker run helloworld验证你的docker可以正常启动容器。\n 如果输入docker命令后无法启动，可以尝试sudo docker\n 2. k8s for wsl2 安装了docker desktop后，可以通过setting-\u0026gt;Kubernetes，勾选Enable Kubernetes来为你的wsl提供k8s服务，但由于网络问题，通常不可能成功。\n所以我们要\u0026quot;换源\u0026quot;。\n打开setting-\u0026gt;Docker Engine，将右侧配置文件改为：\n{ \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34; ], \u0026#34;insecure-registries\u0026#34;: [], \u0026#34;debug\u0026#34;: false, \u0026#34;experimental\u0026#34;: false, \u0026#34;features\u0026#34;: { \u0026#34;buildkit\u0026#34;: true } } Apply\u0026amp;Restart，重启docker desktop。\n现在我们还需要一些额外的镜像。 clone AliyunContainerService/k8s-for-docker-desktop 这个项目。\ngit clone https://github.com/AliyunContainerService/k8s-for-docker-desktop.git 查看自己的docker desktop上Kubernetes的版本。\n可以看到我们这里是v1.21.2。相应的，我们进入刚才clone的文件夹下，切换到v1.21.2分支\ngit checkout v1.21.2 切换分支后，在当前目录下执行：\n.\\load_images.ps1  如果因为安全策略无法执行 PowerShell 脚本，请在 “以管理员身份运行” 的 PowerShell 中执行 Set-ExecutionPolicy RemoteSigned 命令\n 最后一步，setting-\u0026gt;Kubernetes 确保Enable Kubernetes被勾选，然后Apply\u0026amp;Restart，这时候你的docker desktop左下角会出现k8s的图标，并逐渐从黄色变成绿色，代表你的k8s环境启动成功。\nReferences Get started with Docker remote containers on WSL 2 Docker Desktop WSL 2 backend AliyunContainerService/k8s-for-docker-desktop 在 Docker Desktop 中启用 K8s 服务\n","permalink":"http://yangchnet.github.io/Dessert/posts/env/wsl2-%E4%B8%8D%E8%BE%93mac%E7%9A%84%E5%BC%80%E5%8F%91%E4%BD%93%E9%AA%8C%E4%B8%89wsl2%E4%B8%AD%E4%BD%BF%E7%94%A8dockerk8s/","summary":"1. docker for wsl2 在wsl2中使用docker的最佳实践不是在wsl2中安装docker，而是安装docker desktop：\n从docker官网下载并安装完成后，打开docker desktop，选择setting-\u0026gt;General，确保Use the WSL 2 based engine选项被勾选，然后选择右下角Apply\u0026amp;Restart。\n重启docker desktop后，再次打开设置，确保setting-\u0026gt;Resources-\u0026gt;WSL INTEGRATION选项页中你的WSL发行版被勾选。\n完成以上步骤之后，打开你的wsl, 输入docker： 出现这一堆说明安装成功。\n使用docker run helloworld验证你的docker可以正常启动容器。\n 如果输入docker命令后无法启动，可以尝试sudo docker\n 2. k8s for wsl2 安装了docker desktop后，可以通过setting-\u0026gt;Kubernetes，勾选Enable Kubernetes来为你的wsl提供k8s服务，但由于网络问题，通常不可能成功。\n所以我们要\u0026quot;换源\u0026quot;。\n打开setting-\u0026gt;Docker Engine，将右侧配置文件改为：\n{ \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34; ], \u0026#34;insecure-registries\u0026#34;: [], \u0026#34;debug\u0026#34;: false, \u0026#34;experimental\u0026#34;: false, \u0026#34;features\u0026#34;: { \u0026#34;buildkit\u0026#34;: true } } Apply\u0026amp;Restart，重启docker desktop。\n现在我们还需要一些额外的镜像。 clone AliyunContainerService/k8s-for-docker-desktop 这个项目。\ngit clone https://github.com/AliyunContainerService/k8s-for-docker-desktop.git 查看自己的docker desktop上Kubernetes的版本。\n可以看到我们这里是v1.21.2。相应的，我们进入刚才clone的文件夹下，切换到v1.21.2分支\ngit checkout v1.21.2 切换分支后，在当前目录下执行：","title":"WSL2-不输Mac的开发体验（三）：WSL2中使用docker\u0026k8s"},{"content":"1. 代理服务 1.1 获取Windows主机ip并使用其代理 可以在wsl中安装代理软件，但如果win主机上也有代理软件的话，会感觉有点乱。这里让wsl使用主机的代理。\n我使用的代理软件是clash，这里首先要确保clash允许局域网\n确认完clash支持局域网后，我们来看如何从wsl中获取win主机的ip地址。 在Windows主机上，我们可以用ipconfig命令来查看本地的ip地址\nipconfig 如图所示，命令行输出了在wsl网络中Win主机的ip地址。（wsl和windows处于一个网络中，这里得到的是windows在这个网络中的ip地址）。\n如果我们想使用windows的代理，那么可以使用如下命令(在wsl中)：\nexport ALL_PROXY=\u0026#34;http://172.28.48.1:7890\u0026#34; 这里， 172.28.48.1是Windows在这个网络中的ip地址，7890是代理接口。\n通过访问google查看是否代理成功： 1.2 使用脚本自动获取ip地址并设置代理 通过1.1我们知道，可以在Windows主机中获取其在wsl网络中的ip地址，那我们是否可以在wsl中获取到这个地址呢？\ncat /etc/resolv.conf 得到类似下面的输出：\n# This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf: # [network] # generateResolvConf = false nameserver 172.28.48.1 这里的nameserver，是不是就和我们在Windows下获取到的ip地址是一样的。\n我们通过一个脚本来自动获取到这个ip地址，并将其设置为代理地址：\n#!/bin/bash host_ip=$(cat /etc/resolv.conf |grep \u0026#34;nameserver\u0026#34; |cut -f 2 -d \u0026#34; \u0026#34;) # 获取ip地址 echo $host_ip # 输出ip地址 export ALL_PROXY=\u0026#34;http://$host_ip:7890\u0026#34; # 设置代理，7890为我的代理端口 curl -I https://www.google.com # 尝试访问Google, 验证代理的有效性 将这个脚本移动到某个目录下，例如我将其放在/opt/proxy/proxy.sh\n再为其设置一个快捷启动方式：\nvim ~/.bashrc 在~/.bashrc最后输入下行\nalias proxy=/opt/proxy/proxy.sh source ~/.bashrc 现在，我们可以使用proxy命令来一键设置代理了 2. 从windows主机访问wsl2 2.1 获取WSL的ip地址 当我们使用WSL2时，wsl和Windows相当于两台处于一个网络中但分贝独立的主机（我们称这个网络为WSL网络）。在Windows主机上使用ipconfig我们可以看到Windows主机在WSL网络中的地址，相应的，在wsl中我们可以使用ifconfig命令来查看wsl在WSL网络中的地址。\nifconfig 其中的eth0就是wsl在WSL网络中的IP地址，这里为：172.28.62.74\n打开Windows文件夹，定位到C:\\Windows\\System32\\drivers\\etc\\hosts这个文件，在其最后一行加上：\n172.28.62.74 wsl #wsl2 dns config 配置完成之后，你就可以使用wsl域名来直接访问wsl中的服务。如：https://wsl:8080/index。（如果你的8080端口有服务监听的话）。\n2.2 使用脚本自动为windows配置dns #!/bin/bash HOSTS_FILE_WIN=\u0026#39;/mnt/c/Windows/System32/drivers/etc/hosts\u0026#39; inetIp=`ifconfig eth0 | grep -o \u0026#34;inet [0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]* netmask\u0026#34; | cut -f 2 -d \u0026#34; \u0026#34;` # 获取本机ip nu=`cat -n ${HOSTS_FILE_WIN} | grep wsl2 | awk \u0026#39;{print $1}\u0026#39;` # 获取已设置dns行号 dnsIp=`cat ${HOSTS_FILE_WIN} | grep -o \u0026#34;[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]* wsl #\u0026#34; | cut -f 1 -d \u0026#34; \u0026#34;` # 获取已设置dns ip echo \u0026#34;wsl\u0026#39;s ip is: ${inetIp}\u0026#34; # set -x set -e if [ ${nu} ] # 若已设置 then if [ ${inetIp} != ${dnsIp} ] # 已设置dns不正确 then echo reset sed -i \u0026#34;${nu}d\u0026#34; ${HOSTS_FILE_WIN} # 删除对应行 echo \u0026#34;${inetIp}wsl #wsl2 dns config\u0026#34; \u0026gt;\u0026gt; ${HOSTS_FILE_WIN} # 重新设置 fi else # 未设置 echo \u0026#34;${inetIp}wsl #wsl2 dns config\u0026#34; \u0026gt;\u0026gt; ${HOSTS_FILE_WIN} # 直接设置 fi 同样，将这个脚本保存到/opt/dns/dns.sh，然后为其设置别名（快捷启动）alias dns=\u0026quot;/opt/dns/dns.sh\u0026quot;。\n当需要从Windows访问wsl中的服务时，就可以使用dns命令来设置为通过使用wsl域名来访问。\n 关于这个脚本，我还实现了golang版本，并编译出二进制文件，你可以直接下载二进制文件放在你的/usr/local/bin目录下面，在需要的时候使用命令一键设置dns. 地址在这里：https://github.com/yangchnet/wsl-Ip\n 2.3 遇到访问权限问题怎么办 如果遇到因权限问题不能更改文件，可按照以下步骤更改C:\\Windows\\System32\\drivers\\etc\\hosts文件权限 右击文件，选择属性，选择安全选项卡，点击编辑来更改文件权限\n为User增加写入权限 点击确定保存退出。\n这时候，可以在wsl中cd进入/mnt/c/Windows/System32/drivers/etc，使用ll命令查看hosts文件的权限 可以看到，对于hosts文件，已经有了写权限\n","permalink":"http://yangchnet.github.io/Dessert/posts/env/wsl2-%E4%B8%8D%E8%BE%93mac%E7%9A%84%E5%BC%80%E5%8F%91%E4%BD%93%E9%AA%8C%E4%BA%8Cwsl2%E7%9A%84%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98/","summary":"1. 代理服务 1.1 获取Windows主机ip并使用其代理 可以在wsl中安装代理软件，但如果win主机上也有代理软件的话，会感觉有点乱。这里让wsl使用主机的代理。\n我使用的代理软件是clash，这里首先要确保clash允许局域网\n确认完clash支持局域网后，我们来看如何从wsl中获取win主机的ip地址。 在Windows主机上，我们可以用ipconfig命令来查看本地的ip地址\nipconfig 如图所示，命令行输出了在wsl网络中Win主机的ip地址。（wsl和windows处于一个网络中，这里得到的是windows在这个网络中的ip地址）。\n如果我们想使用windows的代理，那么可以使用如下命令(在wsl中)：\nexport ALL_PROXY=\u0026#34;http://172.28.48.1:7890\u0026#34; 这里， 172.28.48.1是Windows在这个网络中的ip地址，7890是代理接口。\n通过访问google查看是否代理成功： 1.2 使用脚本自动获取ip地址并设置代理 通过1.1我们知道，可以在Windows主机中获取其在wsl网络中的ip地址，那我们是否可以在wsl中获取到这个地址呢？\ncat /etc/resolv.conf 得到类似下面的输出：\n# This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf: # [network] # generateResolvConf = false nameserver 172.28.48.1 这里的nameserver，是不是就和我们在Windows下获取到的ip地址是一样的。\n我们通过一个脚本来自动获取到这个ip地址，并将其设置为代理地址：\n#!/bin/bash host_ip=$(cat /etc/resolv.conf |grep \u0026#34;nameserver\u0026#34; |cut -f 2 -d \u0026#34; \u0026#34;) # 获取ip地址 echo $host_ip # 输出ip地址 export ALL_PROXY=\u0026#34;http://$host_ip:7890\u0026#34; # 设置代理，7890为我的代理端口 curl -I https://www.","title":"WSL2-不输Mac的开发体验（二）：WSL2的网络问题"},{"content":"1. 安装pm2 npm install pm2 -g 或\nyarn global add pm2 使用pm2 -v查看版本号\n2. 基本使用 2.1 启动应用 pm2 start app.js # 不止是js文件，其他可执行文件也可以执行 pm2 start script.sh # 启动bash脚本 pm2 start python3 -- app.py # -- 后跟要传给命令的参数 pm2 start binary -- -port 8080 在启动应用时还有一些参数\n--name \u0026lt;app_name\u0026gt; # 为应用设置一个名字 --watch # 监视源文件并在源文件存在更改时重启应用 --max-memory-restart \u0026lt;200MB\u0026gt; # 设置应用占用内存上限 --log \u0026lt;log_path\u0026gt; # 设置log文件路径 -- arg1 arg2 arg3 # 传递参数 --restart-delay \u0026lt;delay in ms\u0026gt; # 重启前延时 --time 在日志前增加时间戳 --no-autorestart # 不要自动重启 2.2 管理应用 pm2 restart app_name pm2 reload app_name pm2 stop app_name pm2 delete app_name 除了使用app_name，还可以使用应用的id, 或使用all来批量管理所有应用\n2.3 列出被管理的应用 pm2 list|ls|status 2.4 打印log pm2 log 打印最后两百行：\npm2 log --line 200 2.5 显示监控 pm2 monit 3. ecosystems.config.js \u0026ndash; 使用配置文件启动应用 3.1 基本使用 生成一个示例文件\npm2 init simple 生成的示例文件如下：\nmodule.exports = { apps : [{ name : \u0026#34;app1\u0026#34;, script : \u0026#34;./app.js\u0026#34; }] } 上面的配置文件中定义了一个最简单的应用，其name为app1, 启动方式为./app.js\n管理应用：\npm2 start ecosystem.config.js # 启动应用 pm2 stop ecosystem.config.js # 停止应用 pm2 restart ecosystem.config.js # 重启应用 pm2 reload ecosystem.config.js # 重新加载应用 pm2 delete ecosystem.config.js # 删除应用 如果你的配置文件中配置了多个文件，那他看起来可能长这样：\nmodule.exports = { apps : [ { name : \u0026#34;app1\u0026#34;, script : \u0026#34;./app1.js\u0026#34; }, { name: \u0026#34;app2\u0026#34;, script : \u0026#34;./app2.js\u0026#34; }, { name: \u0026#34;app3\u0026#34;, script : \u0026#34;./app3.js\u0026#34; }, { name: \u0026#34;app4\u0026#34;, script : \u0026#34;./app4.js\u0026#34; } ] } 如果想要单独管理配置文件中的某（几）个应用，可以使用--only参数\npm2 stop ecosystem.config.js --only app1 # 只停止app1 pm2 delete ecosystem.config.js --only \u0026#34;app1,app2\u0026#34; # 只删除app1和app2 当然你还可以用他们的id来管理\n3.2 配置不同环境 可能你有开发环境和生产环境两个环境，而二者在启动时可能需要不同的环境变量（如数据库节点），那么你可以为每个应用配置env_*\nmodule.exports = { apps : [ { name : \u0026#34;app1\u0026#34;, script : \u0026#34;./app.js\u0026#34;, env_production: { NODE_ENV: \u0026#34;production\u0026#34;, SQL_ENDPOINT: \u0026#34;192.168.11.11\u0026#34;, }, env_development: { NODE_ENV: \u0026#34;development\u0026#34;, SQL_ENDPOINT: \u0026#34;192.168.22.22\u0026#34;, } } ] } 在这个配置文件中, 定义了两套环境变量，你可以选择其中之一用来启动你的应用\npm2 start ecosystem.config.js --env production # 使用生产环境的环境变量，即：SQL_ENDPOINT = \u0026#34;192.168.11.11\u0026#34; pm2 start ecosystem.config.js --env development # 使用开发环境的环境变量，即：SQL_ENDPOINT = \u0026#34;192.168.22.22\u0026#34; 3.3 其他参数 基本参数\n   Field Type Example Description     name (string) “my-api” application name (default to script filename without extension)   script (string) ”./api/app.js” script path relative to pm2 start   cwd (string) “/var/www/” the directory from which your app will be launched   args (string) “-a 13 -b 12” string containing all arguments passed via CLI to script   interpreter (string) “/usr/bin/python” interpreter absolute path (default to node)   interpreter_args (string) ”–harmony” option to pass to the interpreter   node_args (string)  alias to interpreter_args    高级参数\n   Field Type Example Description     instances number -1 number of app instance to be launched   exec_mode string “cluster” mode to start your app, can be “cluster” or “fork”, default fork   watch boolean or [] true enable watch \u0026amp; restart feature, if a file change in the folder or subfolder, your app will get reloaded   ignore_watch list [”[/\\]./”, “node_modules”] list of regex to ignore some file or folder names by the watch feature   max_memory_restart string “150M” your app will be restarted if it exceeds the amount of memory specified. human-friendly format : it can be “10M”, “100K”, “2G” and so on…   env object {“NODE_ENV”: “development”, “ID”: “42”} env variables which will appear in your app   env_ object {“NODE_ENV”: “production”, “ID”: “89”} inject when doing pm2 restart app.yml \u0026ndash;env   source_map_support boolean true default to true, [enable/disable source map file]   instance_var string “NODE_APP_INSTANCE” see documentation   filter_env array of string [ “REACT_” ] Excludes global variables starting with “REACT_” and will not allow their penetration into the cluster.    log配置    Field Type Example Description     log_date_format (string) “YYYY-MM-DD HH:mm Z” log date format (see log section)   error_file (string)  error file path (default to $HOME/.pm2/logs/XXXerr.log)   out_file (string)  output file path (default to $HOME/.pm2/logs/XXXout.log)   combine_logs boolean true if set to true, avoid to suffix logs file with the process id   merge_logs boolean true alias to combine_logs   pid_file (string)  pid file path (default to $HOME/.pm2/pid/app-pm_id.pid)    控制流参数\n   Field Type Example Description     min_uptime (string)  min uptime of the app to be considered started   listen_timeout number 8000 time in ms before forcing a reload if app not listening   kill_timeout number 1600 time in milliseconds before sending a final SIGKILL   shutdown_with_message boolean false shutdown an application with process.send(‘shutdown’) instead of process.kill(pid, SIGINT)   wait_ready boolean false Instead of reload waiting for listen event, wait for process.send(‘ready’)   max_restarts number 10 number of consecutive unstable restarts (less than 1sec interval or custom time via min_uptime) before your app is considered errored and stop being restarted   restart_delay number 4000 time to wait before restarting a crashed app (in milliseconds). defaults to 0.   autorestart boolean false true by default. if false, PM2 will not restart your app if it crashes or ends peacefully   cron_restart string “1 0 * * *” a cron pattern to restart your app. Application must be running for cron feature to work   vizion boolean false true by default. if false, PM2 will start without vizion features (versioning control metadatas)   post_update list [“npm install”, “echo launching the app”] a list of commands which will be executed after you perform a Pull/Upgrade operation from Keymetrics dashboard   force boolean true defaults to false. if true, you can start the same script several times which is usually not allowed by PM2    部署相关参数\n   Entry name Description Type Default     key SSH key path String $HOME/.ssh   user SSH user String    host SSH host [String]    ssh_options SSH options with no command-line flag, see ‘man ssh’ String or [String]    ref GIT remote/branch String    repo GIT remote String    path path in the server String    pre-setup Pre-setup command or path to a script on your local machine String    post-setup Post-setup commands or path to a script on the host machine String    pre-deploy-local pre-deploy action String    post-deploy post-deploy action String     References https://pm2.keymetrics.io/\n","permalink":"http://yangchnet.github.io/Dessert/posts/tool/pm2%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","summary":"1. 安装pm2 npm install pm2 -g 或\nyarn global add pm2 使用pm2 -v查看版本号\n2. 基本使用 2.1 启动应用 pm2 start app.js # 不止是js文件，其他可执行文件也可以执行 pm2 start script.sh # 启动bash脚本 pm2 start python3 -- app.py # -- 后跟要传给命令的参数 pm2 start binary -- -port 8080 在启动应用时还有一些参数\n--name \u0026lt;app_name\u0026gt; # 为应用设置一个名字 --watch # 监视源文件并在源文件存在更改时重启应用 --max-memory-restart \u0026lt;200MB\u0026gt; # 设置应用占用内存上限 --log \u0026lt;log_path\u0026gt; # 设置log文件路径 -- arg1 arg2 arg3 # 传递参数 --restart-delay \u0026lt;delay in ms\u0026gt; # 重启前延时 --time 在日志前增加时间戳 --no-autorestart # 不要自动重启 2.","title":"pm2使用指南"},{"content":"1. nvm install curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash 安装完毕后会提示你让你将以下命令加入你的配置文件中\nexport NVM_DIR=\u0026#34;$HOME/.nvm\u0026#34; [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/nvm.sh\u0026#34; # This loads nvm [ -s \u0026#34;$NVM_DIR/bash_completion\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/bash_completion\u0026#34; # This loads nvm bash_completion 默认情况下，已经加入你的~/.bashrc文件中，如果你使用的是zsh，那么就需要手动将其添加到~/.zshrc中\n2. 常用操作   列出本地所有npm版本\nnvm ls   列出可获取的所有版本\nnvm ls-remote   安装指定版本\nnvm install 14 # 14是版本号   指定使用某个版本\nnvm use 14   ","permalink":"http://yangchnet.github.io/Dessert/posts/env/%E5%A4%9A%E7%89%88%E6%9C%ACnpm%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","summary":"1. nvm install curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash 安装完毕后会提示你让你将以下命令加入你的配置文件中\nexport NVM_DIR=\u0026#34;$HOME/.nvm\u0026#34; [ -s \u0026#34;$NVM_DIR/nvm.sh\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/nvm.sh\u0026#34; # This loads nvm [ -s \u0026#34;$NVM_DIR/bash_completion\u0026#34; ] \u0026amp;\u0026amp; \\. \u0026#34;$NVM_DIR/bash_completion\u0026#34; # This loads nvm bash_completion 默认情况下，已经加入你的~/.bashrc文件中，如果你使用的是zsh，那么就需要手动将其添加到~/.zshrc中\n2. 常用操作   列出本地所有npm版本\nnvm ls   列出可获取的所有版本\nnvm ls-remote   安装指定版本\nnvm install 14 # 14是版本号   指定使用某个版本\nnvm use 14   ","title":"多版本npm解决方案"},{"content":" 电子数据的认定包含3个重要的部分，称为“三性”：\n 真实性 关联性 合法性   1. 真实性认定 由于电子数据存证的特殊性，其真实性可以从三个不同的层面来认定：\n 电子证据载体的真实性 电子数据的真实性 电子证据内容的真实性  1.1 电子证据载体的真实性 电子证据载体的真实性，是指存储电子数据的媒介、设备在诉讼过程中保持原始性、同一性、完整性，不存在被伪造、变造、替换、破坏等问题。 主要包括两个方面的要求：\n 电子证据载体来源的真实性 电子证据载体在诉讼前和诉讼流转过程中的真实性。  具体来说，法官审查电子证据时往往会关注：提交的电子证据是否包括原始存储介质，原始存储介质的收集程序、方式是否符合法律规定和有关规范；如果无法提取原始存储介质，如何确保其他存储介质能够保障电子数据的真实性。 这是对电子证据载体原始性和同一性的审查。\n二是电子证据载体在诉讼前和诉讼中流转过程中的真实性。诉讼前和诉讼中，证据会在多个主体（如刑事案件的公、检、法和民事、行政案件中原告、法院、被告）间流转。在此过程中需要考察：电子证据载体在移送、 流转中是否保持同一性，是否符合鉴真的要求；电子证据载体是否保持完整性，没有被改变、破坏等。\n区块链技术极大地扩展了电子证据的载体外延，并可以从技术上确保电子证据载体的真实性。具体而言，区块链存证使用分布式存储并附加防篡改校验机制，使电子证据可以脱离原始存储介质而 安全存储，同时无被篡改之虞\n区块链技术有效解决了电子证据载体真实性认定的问题。\n1.2 电子数据的真实性 电子数据的真实性问题，是指作为电子证据信息在技术层面的存在形式的电子数据是否真实，是否与原始数据保持一致，是否存在被修改、删除、增加等问题。\n电子数据的真实性，是区块链存证的真实性评价中的一个关键问题。因为，如前述，区块链技术可以保障电子证据的载体及载体上证据副本的真实性，但载体的真实和副本数据的真实，无法决定电子数据本身的真实性（意思是虽然副本和原件相同，但原件不一定是真实的）。同时，在区块链存证场景下，如果电子证据或其证据指纹（或称校验数据）上链并分布式存储，则证据的真实性已经具有技术保证。\n则欲保障电子数据的真实性，则需要确保作为电子证据的数据信息（或其证据指纹）在生成时即同步上链， 或者确保该数据信息在上链前未被篡改。考虑到确保作为电子证据的数据信息在上链前未被更改实际上是一个传统的电子证据鉴定场景，不能发挥区块链技术的优势，故而在区块链存证领域，比较理想的确保电子数据真实性的方案即为作为电子证据的数据信息（或其证据指纹）在生成时即同步上链。\n1.3 电子证据内容的真实性 电子证据内容的真实性，是指：（1）在“排除合理怀疑”的证明标准场合，电子证据所包含的信息可以与案件中其他证据所包含的信息能够相互印证，从而准确证明案件事实；（2）在“优势证据”证明标准场合， 电子证据所包含的信息可以证明一定的法律事实，特别是证明当事人的意思表示和法律行为。\n电子证据内容的真实性是电子证据真实性的核心问题，不少语境中，电子证据的真实性，往往也是指电子证据内容的真实性。在司法实践中，鉴定意见、证据相互印证是确认电子证据内容真实性的主要方式，有些情况下二者还会同时使用。\n附加可信时间戳的上链数据，可以推定为形成于特定时间点，则该数据中的时间信息就具有内容真实性。\n同理，通过区块链达成的智能合约，在作为证据使用时可以推定合约内容数据真实，合约内容数据也具有了内容真实性。（意思是如果交易通过智能合约达成，可以保证交易是真实的，那么交易的内容就具有了内容真实性）\n2. 关联性认定 证据的关联性，是指证据必须和需要证明的案件事实或其他争议事实具有一定的联系。\n区块链在单纯的存证场景应用，技术本身并不增强电子证据的关联性。如果是一类或一系列业务运行在区块链上，因其全流程留痕，可能因为可追溯性使证据的关联更加明确，方便进行关联性认定。\n无论如何，上链证据和案件无关的情况不可避免，区块链存证不能确保电子证据具有关联性，而是在部分场景下为电子证据的关联性认定提供参考。（证据链）\n3. 合法性认定 证据的合法性认定包括取证主体合法性、证据形式的合法性、取证程序合法性以及证据保存与运用方式合法性四个方面。它是证据认定主体机械式对比法条的过程，其中不掺杂证据认定主体的私人价值评价。因此，与证据的真实性、关联性要求不同，证据的合法性判断不应考虑与案件事实的联系，而与法律规定密切相关。（这就不是区块链要考虑的问题了）\n4. 可信时间戳 由于区块链本身上存储的只是哈希值，而非原件，因此在示证的时候如果没有原件与哈希值相对应，存证也将无法达到目的。因此，电子数据存储电子数据原件也是区块链司法存证系统的重要部分。\n可信时间戳是将用户的电子数据信息和权威时间源绑定，由国家授时中心提供授时信息，将对电子数据信息和授时信息进行数字 签名生成时间戳。通过可信时间戳可确定电子数据信息生成的精确时间，并防止电子文件被篡改，为电子数据提供可信的时间证明和内容真实性、完整性证明。\n可信时间戳是表示电子数据在一个特定时间点已经存在的完整的可验证的数据。\n5. 区块链存证系统对电子数据认定的作用 区块链技术对电子数据证据认定的作用，即对电子证据“三性”的影响，首先在于对电子证据真实性的判定所产出的显著影响。此外，对于某些业务类型，诸如证据在链上形成并同步存储的情形，该系统对于证明所存证据的合法性和关联性也具有一定帮助。\n具体而言，该系统对于证据认定辅助功能主要通过以下四个层面来实现。\n 安全架构确保电子数据载体真实性 关键技术提高电子数据真实性 相关技术提高证据认定效率 相关业务和链下治理辅助证据认定  6. 总结 在理想情况下（即电子数据生成时即同步上链）：\n 区块链系统保证了电子数据载体的真实性与电子数据的真实性 可信时间戳证明电子数据在某一时刻已经存在，保证了电子数据时间内容的真实性，为电子数据提供可信的时间证明和内容真实性、完整性证明 二者可同时保证文件不被篡改，但区块链中只有文件摘要，无法用于“示证”，因此还需要存储电子数据原文  在现有情况下（电子数据生成后上链）：\n 区块链系统保证了电子数据载体真实性 可信时间戳证明了电子数据在某一时刻已经存在 二者可同时保证文件不被篡改（上链并进行时间戳签名后），但区块链中只有文件摘要，无法用于“示证”，因此还需要存储电子数据原文  References 《区块链司法存证应用白皮书（v1.0）》\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%8F%AF%E4%BF%A1%E5%AD%98%E8%AF%81/","summary":"电子数据的认定包含3个重要的部分，称为“三性”：\n 真实性 关联性 合法性   1. 真实性认定 由于电子数据存证的特殊性，其真实性可以从三个不同的层面来认定：\n 电子证据载体的真实性 电子数据的真实性 电子证据内容的真实性  1.1 电子证据载体的真实性 电子证据载体的真实性，是指存储电子数据的媒介、设备在诉讼过程中保持原始性、同一性、完整性，不存在被伪造、变造、替换、破坏等问题。 主要包括两个方面的要求：\n 电子证据载体来源的真实性 电子证据载体在诉讼前和诉讼流转过程中的真实性。  具体来说，法官审查电子证据时往往会关注：提交的电子证据是否包括原始存储介质，原始存储介质的收集程序、方式是否符合法律规定和有关规范；如果无法提取原始存储介质，如何确保其他存储介质能够保障电子数据的真实性。 这是对电子证据载体原始性和同一性的审查。\n二是电子证据载体在诉讼前和诉讼中流转过程中的真实性。诉讼前和诉讼中，证据会在多个主体（如刑事案件的公、检、法和民事、行政案件中原告、法院、被告）间流转。在此过程中需要考察：电子证据载体在移送、 流转中是否保持同一性，是否符合鉴真的要求；电子证据载体是否保持完整性，没有被改变、破坏等。\n区块链技术极大地扩展了电子证据的载体外延，并可以从技术上确保电子证据载体的真实性。具体而言，区块链存证使用分布式存储并附加防篡改校验机制，使电子证据可以脱离原始存储介质而 安全存储，同时无被篡改之虞\n区块链技术有效解决了电子证据载体真实性认定的问题。\n1.2 电子数据的真实性 电子数据的真实性问题，是指作为电子证据信息在技术层面的存在形式的电子数据是否真实，是否与原始数据保持一致，是否存在被修改、删除、增加等问题。\n电子数据的真实性，是区块链存证的真实性评价中的一个关键问题。因为，如前述，区块链技术可以保障电子证据的载体及载体上证据副本的真实性，但载体的真实和副本数据的真实，无法决定电子数据本身的真实性（意思是虽然副本和原件相同，但原件不一定是真实的）。同时，在区块链存证场景下，如果电子证据或其证据指纹（或称校验数据）上链并分布式存储，则证据的真实性已经具有技术保证。\n则欲保障电子数据的真实性，则需要确保作为电子证据的数据信息（或其证据指纹）在生成时即同步上链， 或者确保该数据信息在上链前未被篡改。考虑到确保作为电子证据的数据信息在上链前未被更改实际上是一个传统的电子证据鉴定场景，不能发挥区块链技术的优势，故而在区块链存证领域，比较理想的确保电子数据真实性的方案即为作为电子证据的数据信息（或其证据指纹）在生成时即同步上链。\n1.3 电子证据内容的真实性 电子证据内容的真实性，是指：（1）在“排除合理怀疑”的证明标准场合，电子证据所包含的信息可以与案件中其他证据所包含的信息能够相互印证，从而准确证明案件事实；（2）在“优势证据”证明标准场合， 电子证据所包含的信息可以证明一定的法律事实，特别是证明当事人的意思表示和法律行为。\n电子证据内容的真实性是电子证据真实性的核心问题，不少语境中，电子证据的真实性，往往也是指电子证据内容的真实性。在司法实践中，鉴定意见、证据相互印证是确认电子证据内容真实性的主要方式，有些情况下二者还会同时使用。\n附加可信时间戳的上链数据，可以推定为形成于特定时间点，则该数据中的时间信息就具有内容真实性。\n同理，通过区块链达成的智能合约，在作为证据使用时可以推定合约内容数据真实，合约内容数据也具有了内容真实性。（意思是如果交易通过智能合约达成，可以保证交易是真实的，那么交易的内容就具有了内容真实性）\n2. 关联性认定 证据的关联性，是指证据必须和需要证明的案件事实或其他争议事实具有一定的联系。\n区块链在单纯的存证场景应用，技术本身并不增强电子证据的关联性。如果是一类或一系列业务运行在区块链上，因其全流程留痕，可能因为可追溯性使证据的关联更加明确，方便进行关联性认定。\n无论如何，上链证据和案件无关的情况不可避免，区块链存证不能确保电子证据具有关联性，而是在部分场景下为电子证据的关联性认定提供参考。（证据链）\n3. 合法性认定 证据的合法性认定包括取证主体合法性、证据形式的合法性、取证程序合法性以及证据保存与运用方式合法性四个方面。它是证据认定主体机械式对比法条的过程，其中不掺杂证据认定主体的私人价值评价。因此，与证据的真实性、关联性要求不同，证据的合法性判断不应考虑与案件事实的联系，而与法律规定密切相关。（这就不是区块链要考虑的问题了）\n4. 可信时间戳 由于区块链本身上存储的只是哈希值，而非原件，因此在示证的时候如果没有原件与哈希值相对应，存证也将无法达到目的。因此，电子数据存储电子数据原件也是区块链司法存证系统的重要部分。\n可信时间戳是将用户的电子数据信息和权威时间源绑定，由国家授时中心提供授时信息，将对电子数据信息和授时信息进行数字 签名生成时间戳。通过可信时间戳可确定电子数据信息生成的精确时间，并防止电子文件被篡改，为电子数据提供可信的时间证明和内容真实性、完整性证明。\n可信时间戳是表示电子数据在一个特定时间点已经存在的完整的可验证的数据。\n5. 区块链存证系统对电子数据认定的作用 区块链技术对电子数据证据认定的作用，即对电子证据“三性”的影响，首先在于对电子证据真实性的判定所产出的显著影响。此外，对于某些业务类型，诸如证据在链上形成并同步存储的情形，该系统对于证明所存证据的合法性和关联性也具有一定帮助。\n具体而言，该系统对于证据认定辅助功能主要通过以下四个层面来实现。\n 安全架构确保电子数据载体真实性 关键技术提高电子数据真实性 相关技术提高证据认定效率 相关业务和链下治理辅助证据认定  6. 总结 在理想情况下（即电子数据生成时即同步上链）：","title":"可信存证"},{"content":" 官方教程，写的很好，我就不多说了\n Let\u0026rsquo;s learn to use Wire by example. The Wire guide provides thorough documentation of the tool\u0026rsquo;s usage. For readers eager to see Wire applied to a larger server, the guestbook sample in Go Cloud uses Wire to initialize its components. Here we are going to build a small greeter program to understand how to use Wire. The finished product may be found in the same directory as this README.\nA First Pass of Building the Greeter Program Let\u0026rsquo;s create a small program that simulates an event with a greeter greeting guests with a particular message.\nTo start, we will create three types: 1) a message for a greeter, 2) a greeter who conveys that message, and 3) an event that starts with the greeter greeting guests. In this design, we have three struct types:\ntype Message string type Greeter struct { // ... TBD } type Event struct { // ... TBD } The Message type just wraps a string. For now, we will create a simple initializer that always returns a hard-coded message:\nfunc NewMessage() Message { return Message(\u0026#34;Hi there!\u0026#34;) } Our Greeter will need reference to the Message. So let\u0026rsquo;s create an initializer for our Greeter as well.\nfunc NewGreeter(m Message) Greeter { return Greeter{Message: m} } type Greeter struct { Message Message // \u0026lt;- adding a Message field } In the initializer we assign a Message field to Greeter. Now, we can use the Message when we create a Greet method on Greeter:\nfunc (g Greeter) Greet() Message { return g.Message } Next, we need our Event to have a Greeter, so we will create an initializer for it as well.\nfunc NewEvent(g Greeter) Event { return Event{Greeter: g} } type Event struct { Greeter Greeter // \u0026lt;- adding a Greeter field } Then we add a method to start the Event:\nfunc (e Event) Start() { msg := e.Greeter.Greet() fmt.Println(msg) } The Start method holds the core of our small application: it tells the greeter to issue a greeting and then prints that message to the screen.\nNow that we have all the components of our application ready, let\u0026rsquo;s see what it takes to initialize all the components without using Wire. Our main function would look like this:\nfunc main() { message := NewMessage() greeter := NewGreeter(message) event := NewEvent(greeter) event.Start() } First we create a message, then we create a greeter with that message, and finally we create an event with that greeter. With all the initialization done, we\u0026rsquo;re ready to start our event.\nWe are using the dependency injection design principle. In practice, that means we pass in whatever each component needs. This style of design lends itself to writing easily tested code and makes it easy to swap out one dependency with another.\nUsing Wire to Generate Code One downside to dependency injection is the need for so many initialization steps. Let\u0026rsquo;s see how we can use Wire to make the process of initializing our components smoother.\nLet\u0026rsquo;s start by changing our main function to look like this:\nfunc main() { e := InitializeEvent() e.Start() } Next, in a separate file called wire.go we will define InitializeEvent. This is where things get interesting:\n// wire.go  func InitializeEvent() Event { wire.Build(NewEvent, NewGreeter, NewMessage) return Event{} } Rather than go through the trouble of initializing each component in turn and passing it into the next one, we instead have a single call to wire.Build passing in the initializers we want to use. In Wire, initializers are known as \u0026ldquo;providers,\u0026rdquo; functions which provide a particular type. We add a zero value for Event as a return value to satisfy the compiler. Note that even if we add values to Event, Wire will ignore them. In fact, the injector\u0026rsquo;s purpose is to provide information about which providers to use to construct an Event and so we will exclude it from our final binary with a build constraint at the top of the file:\n//+build wireinject  Note, a build constraint requires a blank, trailing line.\nIn Wire parlance, InitializeEvent is an \u0026ldquo;injector.\u0026rdquo; Now that we have our injector complete, we are ready to use the wire command line tool.\nInstall the tool with:\ngo get github.com/google/wire/cmd/wire Then in the same directory with the above code, simply run wire. Wire will find the InitializeEvent injector and generate a function whose body is filled out with all the necessary initialization steps. The result will be written to a file named wire_gen.go.\nLet\u0026rsquo;s take a look at what Wire did for us:\n// wire_gen.go  func InitializeEvent() Event { message := NewMessage() greeter := NewGreeter(message) event := NewEvent(greeter) return event } It looks just like what we wrote above! Now this is a simple example with just three components, so writing the initializer by hand isn\u0026rsquo;t too painful. Imagine how useful Wire is for components that are much more complex. When working with Wire, we will commit both wire.go and wire_gen.go to source control.\nMaking Changes with Wire To show a small part of how Wire handles more complex setups, let\u0026rsquo;s refactor our initializer for Event to return an error and see what happens.\nfunc NewEvent(g Greeter) (Event, error) { if g.Grumpy { return Event{}, errors.New(\u0026#34;could not create event: event greeter is grumpy\u0026#34;) } return Event{Greeter: g}, nil } We\u0026rsquo;ll say that sometimes a Greeter might be grumpy and so we cannot create an Event. The NewGreeter initializer now looks like this:\nfunc NewGreeter(m Message) Greeter { var grumpy bool if time.Now().Unix()%2 == 0 { grumpy = true } return Greeter{Message: m, Grumpy: grumpy} } We have added a Grumpy field to Greeter struct and if the invocation time of the initializer is an even number of seconds since the Unix epoch, we will create a grumpy greeter instead of a friendly one.\nThe Greet method then becomes:\nfunc (g Greeter) Greet() Message { if g.Grumpy { return Message(\u0026#34;Go away!\u0026#34;) } return g.Message } Now you see how a grumpy Greeter is no good for an Event. So NewEvent may fail. Our main must now take into account that InitializeEvent may in fact fail:\nfunc main() { e, err := InitializeEvent() if err != nil { fmt.Printf(\u0026#34;failed to create event: %s\\n\u0026#34;, err) os.Exit(2) } e.Start() } We also need to update InitializeEvent to add an error type to the return value:\n// wire.go  func InitializeEvent() (Event, error) { wire.Build(NewEvent, NewGreeter, NewMessage) return Event{}, nil } With the setup complete, we are ready to invoke the wire command again. Note, that after running wire once to produce a wire_gen.go file, we may also use go generate. Having run the command, our wire_gen.go file looks like this:\n// wire_gen.go  func InitializeEvent() (Event, error) { message := NewMessage() greeter := NewGreeter(message) event, err := NewEvent(greeter) if err != nil { return Event{}, err } return event, nil } Wire has detected that the NewEvent provider may fail and has done the right thing inside the generated code: it checks the error and returns early if one is present.\nChanging the Injector Signature As another improvement, let\u0026rsquo;s look at how Wire generates code based on the signature of the injector. Presently, we have hard-coded the message inside NewMessage. In practice, it\u0026rsquo;s much nicer to allow callers to change that message however they see fit. So let\u0026rsquo;s change InitializeEvent to look like this:\nfunc InitializeEvent(phrase string) (Event, error) { wire.Build(NewEvent, NewGreeter, NewMessage) return Event{}, nil } Now InitializeEvent allows callers to pass in the phrase for a Greeter to use. We also add a phrase argument to NewMessage:\nfunc NewMessage(phrase string) Message { return Message(phrase) } After we run wire again, we will see that the tool has generated an initializer which passes the phrase value as a Message into Greeter. Neat!\n// wire_gen.go  func InitializeEvent(phrase string) (Event, error) { message := NewMessage(phrase) greeter := NewGreeter(message) event, err := NewEvent(greeter) if err != nil { return Event{}, err } return event, nil } Wire inspects the arguments to the injector, sees that we added a string to the list of arguments (e.g., phrase), and likewise sees that among all the providers, NewMessage takes a string, and so it passes phrase into NewMessage.\nCatching Mistakes with Helpful Errors Let\u0026rsquo;s also look at what happens when Wire detects mistakes in our code and see how Wire\u0026rsquo;s error messages help us correct any problems.\nFor example, when writing our injector InitializeEvent, let\u0026rsquo;s say we forget to add a provider for Greeter. Let\u0026rsquo;s see what happens:\nfunc InitializeEvent(phrase string) (Event, error) { wire.Build(NewEvent, NewMessage) // woops! We forgot to add a provider for Greeter  return Event{}, nil } Running wire, we see the following:\n# wrapping the error across lines for readability $GOPATH/src/github.com/google/wire/_tutorial/wire.go:24:1: inject InitializeEvent: no provider found for github.com/google/wire/_tutorial.Greeter (required by provider of github.com/google/wire/_tutorial.Event) wire: generate failed Wire is telling us some useful information: it cannot find a provider for Greeter. Note that the error message prints out the full path to the Greeter type. It\u0026rsquo;s also telling us the line number and injector name where the problem occurred: line 24 inside InitializeEvent. In addition, the error message tells us which provider needs a Greeter. It\u0026rsquo;s the Event type. Once we pass in a provider of Greeter, the problem will be solved.\nAlternatively, what happens if we provide one too many providers to wire.Build?\nfunc NewEventNumber() int { return 1 } func InitializeEvent(phrase string) (Event, error) { // woops! NewEventNumber is unused.  wire.Build(NewEvent, NewGreeter, NewMessage, NewEventNumber) return Event{}, nil } Wire helpfully tells us that we have an unused provider:\n$GOPATH/src/github.com/google/wire/_tutorial/wire.go:24:1: inject InitializeEvent: unused provider \u0026#34;NewEventNumber\u0026#34; wire: generate failed Deleting the unused provider from the call to wire.Build resolves the error.\nConclusion Let\u0026rsquo;s summarize what we have done here. First, we wrote a number of components with corresponding initializers, or providers. Next, we created an injector function, specifying which arguments it receives and which types it returns. Then, we filled in the injector function with a call to wire.Build supplying all necessary providers. Finally, we ran the wire command to generate code that wires up all the different initializers. When we added an argument to the injector and an error return value, running wire again made all the necessary updates to our generated code.\nThe example here is small, but it demonstrates some of the power of Wire, and how it takes much of the pain out of initializing code using dependency injection. Furthermore, using Wire produced code that looks much like what we would otherwise write. There are no bespoke types that commit a user to Wire. Instead it\u0026rsquo;s just generated code. We may do with it what we will. Finally, another point worth considering is how easy it is to add new dependencies to our component initialization. As long as we tell Wire how to provide (i.e., initialize) a component, we may add that component anywhere in the dependency graph and Wire will handle the rest.\nIn closing, it is worth mentioning that Wire supports a number of additional features not discussed here. Providers may be grouped in provider sets. There is support for binding interfaces, binding values, as well as support for cleanup functions. See the Advanced Features section for more.\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5wire%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/","summary":"官方教程，写的很好，我就不多说了\n Let\u0026rsquo;s learn to use Wire by example. The Wire guide provides thorough documentation of the tool\u0026rsquo;s usage. For readers eager to see Wire applied to a larger server, the guestbook sample in Go Cloud uses Wire to initialize its components. Here we are going to build a small greeter program to understand how to use Wire. The finished product may be found in the same directory as this README.","title":"依赖注入：wire包的使用"},{"content":"1. 栈（stack） 栈用于存储函数（包括主函数）内部使用的变量，是一个先进先出（LIFO）结构，每次声明一个新变量，都会将其推入栈中。当函数运行结束时，栈上上与该函数所有相关的变量（称为一个栈帧）将被释放。栈由CPU自动管理，用户不必关心如何分配和释放内存。栈内存分为栈帧，每次函数调用都会为其分配一个栈帧，在函数返回时释放。\n栈的大小通常有限，如果程序试图将过多的信息放入栈中，就会出现栈溢出。\n栈的先进后出并不是指栈中的变量是先进后出的，而是指\u0026quot;栈帧\u0026quot;的先进后出，这保证了函数的调用顺序。\n 栈内存由CPU管理 变量自动分配和释放 栈的大小有限制 当变量创建和销毁时，栈会增长和收缩  2. 堆（heap） 堆是一块大的内存，支持动态分配，由用户负责管理。可以通过malloc方法分配内存，通过free方法回收内存，若内存使用后没有回收，则会导致\u0026quot;内存泄漏\u0026quot;，即这块内存无法被其他进程所用。\n与栈不同，除物理内存大小的限制，堆的大小没有严格限制。在堆中创建的变量可在程序的任何地方访问（全局变量）。\n 堆内存由程序员管理 在C中，使用malloc和free来分配和释放堆内存 需要用指针访问堆  3. 考虑以下程序\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; int x; int main(void) { int y; char *str; y = 4; printf(\u0026#34;stack memory: %d\\n\u0026#34;, y); str = malloc(100 * sizeof(char)); str[0] = \u0026#39;m\u0026#39;; printf(\u0026#34;heap memory: %c\\n\u0026#34;, str[0]); free(str); return 0; } 在上面这段程序中，x是一个全局变量，y和str都是局部变量。malloc为str在堆上分配了100个字节的内存，free则释放了分配的这些内存。\n","permalink":"http://yangchnet.github.io/Dessert/posts/c++/%E5%A0%86%E5%92%8C%E6%A0%88%E7%9A%84%E5%8C%BA%E5%88%AB/","summary":"1. 栈（stack） 栈用于存储函数（包括主函数）内部使用的变量，是一个先进先出（LIFO）结构，每次声明一个新变量，都会将其推入栈中。当函数运行结束时，栈上上与该函数所有相关的变量（称为一个栈帧）将被释放。栈由CPU自动管理，用户不必关心如何分配和释放内存。栈内存分为栈帧，每次函数调用都会为其分配一个栈帧，在函数返回时释放。\n栈的大小通常有限，如果程序试图将过多的信息放入栈中，就会出现栈溢出。\n栈的先进后出并不是指栈中的变量是先进后出的，而是指\u0026quot;栈帧\u0026quot;的先进后出，这保证了函数的调用顺序。\n 栈内存由CPU管理 变量自动分配和释放 栈的大小有限制 当变量创建和销毁时，栈会增长和收缩  2. 堆（heap） 堆是一块大的内存，支持动态分配，由用户负责管理。可以通过malloc方法分配内存，通过free方法回收内存，若内存使用后没有回收，则会导致\u0026quot;内存泄漏\u0026quot;，即这块内存无法被其他进程所用。\n与栈不同，除物理内存大小的限制，堆的大小没有严格限制。在堆中创建的变量可在程序的任何地方访问（全局变量）。\n 堆内存由程序员管理 在C中，使用malloc和free来分配和释放堆内存 需要用指针访问堆  3. 考虑以下程序\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; int x; int main(void) { int y; char *str; y = 4; printf(\u0026#34;stack memory: %d\\n\u0026#34;, y); str = malloc(100 * sizeof(char)); str[0] = \u0026#39;m\u0026#39;; printf(\u0026#34;heap memory: %c\\n\u0026#34;, str[0]); free(str); return 0; } 在上面这段程序中，x是一个全局变量，y和str都是局部变量。malloc为str在堆上分配了100个字节的内存，free则释放了分配的这些内存。","title":"堆和栈的区别"},{"content":"sudo usermod -aG docker $USER \u0026amp;\u0026amp; newgrp docker # 将当前用户添加到docker用户组 退出重新登陆即可\n","permalink":"http://yangchnet.github.io/Dessert/posts/cloud/docker-sudo/","summary":"sudo usermod -aG docker $USER \u0026amp;\u0026amp; newgrp docker # 将当前用户添加到docker用户组 退出重新登陆即可","title":"docker sudo"},{"content":"1. wazsmwazsm/mortar（★74） 简单介绍\n创建一个容量为 N 的池, 在池容量未满时, 每塞入一个任务（生产任务）, 任务池开启一个 worker (建立协程) 去处理任务（消费任务）。 当任务池容量赛满，每塞入一个任务（生产任务）, 任务会被已有的 N 个 worker 抢占执行（消费任务），达到协程限制的功能。但worker创建后不会回收，除非将整个pool撤销。\n结构\ntype Task struct { Handler func(v ...interface{}) // 函数签名 \tParams []interface{} // 参数 } // Pool task pool type Pool struct { capacity uint64 // 池的容量，自行制定 \trunningWorkers uint64 // 正在运行的worker \tstatus int64 // 池的状态 \tchTask chan *Task // 任务队列，worker从中获取任务 \tPanicHandler func(interface{}) // 自定义的PanicHandler，防止因某个goroutine发生panic而导致服务崩溃。 \tsync.Mutex // 全局锁 } 核心代码\n// Put put a task to pool func (p *Pool) Put(task *Task) error { p.Lock() // 加锁，防止数据竞争 \tdefer p.Unlock() if p.status == STOPED { // 检查池是否还在运行 \treturn ErrPoolAlreadyClosed } // run worker \tif p.GetRunningWorkers() \u0026lt; p.GetCap() { // worker数量尚未达到容量限制，新建一个worker \tp.run() // 这里存在一个问题，若池中有空闲的worker，也会创建一个新的worker，这个worker创建后也会空闲，存在资源浪费 \t} // send task \tif p.status == RUNNING { p.chTask \u0026lt;- task // 向任务队列发送任务 \t} return nil } func (p *Pool) run() { p.incRunning() // worker计数+1  go func() { defer func() { // 防止因某个goroutine发生panic而导致服务崩溃 \tp.decRunning() // worker计数减1 \tif r := recover(); r != nil { if p.PanicHandler != nil { p.PanicHandler(r) // 自定义了panic后操作 \t} else { log.Printf(\u0026#34;Worker panic: %s\\n\u0026#34;, r) // panic后默认操作 \t} } p.checkWorker() // 检查是否还有worker在运行，保持最少一个worker \t}() for { // worker 保持运行 \tselect { case task, ok := \u0026lt;-p.chTask: // 接收task \tif !ok { // 当任务队列被关闭是goroutine退出 \treturn } task.Handler(task.Params...) // 执行任务 \t} } }() } 在Put函数中，每当放入一个新任务之前，都会直接创建一个新的worker，这显然不甚合理，因为此时池中可能存在空闲的worker，应设法利用这些空闲的worker而不是创建新的worker。\n 尝试优化\n 一种优化思路是：在Pool对象中增加chIdle chan struct{}字段，作为空闲标志。放入任务之前首先检查chIdle是否存在空闲标志，若存在，则说明此时存在空闲worker，不需创建新的worker，只需将task放入即可。 而若chIdle中不存在空闲标志，则检查worker数量是否达到限制，然后创建新的worker。 优化后的核心代码如下：\ntype Pool struct { capacity uint64 runningWorkers uint64 status int64 chTask chan *Task chIdle chan struct{} PanicHandler func(interface{}) sync.Mutex } func (p *Pool) Put(task *Task) error { p.Lock() defer p.Unlock() if p.status == STOPED { return ErrPoolAlreadyClosed } select { case \u0026lt;-p.chIdle: default: if p.GetRunningWorkers() \u0026lt; p.GetCap() { p.run() } } // send task \tif p.status == RUNNING { p.chTask \u0026lt;- task } return nil } func (p *Pool) run() { p.incRunning() p.chIdle \u0026lt;- struct{}{} go func() { defer func() { p.decRunning() if r := recover(); r != nil { if p.PanicHandler != nil { p.PanicHandler(r) } else { log.Printf(\u0026#34;Worker panic: %s\\n\u0026#34;, r) } } p.checkWorker() // check worker avoid no worker running \t}() for { select { case task, ok := \u0026lt;-p.chTask: if !ok { return } task.Handler(task.Params...) p.chIdle \u0026lt;- struct{}{} } } }() } 优化前后banchmark测试数据对比如下：\n优化前：BenchmarkPut-4 815876\t1460 ns/op\t0 B/op\t0 allocs/op 优化后：BenchmarkPut-4 948046\t1323 ns/op\t0 B/op\t0 allocs/op 优化前: BenchmarkPutTimelife-4 1000000\t1287 ns/op\t0 B/op\t0 allocs/op 优化后: BenchmarkPutTimelife-4 930015\t1356 ns/op\t0 B/op\t0 allocs/op 优化前: BenchmarkPoolPutSetTimes-4 1\t1433118500 ns/op\t12104 B/op\t52 allocs/op 优化后: BenchmarkPoolPutSetTimes-4 1\t1474117300 ns/op\t8504 B/op\t42 allocs/op 优化前: BenchmarkPoolTimeLifeSetTimes-4 1\t1277515600 ns/op\t10208 B/op\t47 allocs/op 优化后: BenchmarkPoolTimeLifeSetTimes-4 1\t1279696300 ns/op\t7928 B/op\t39 allocs/op 基本使用\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;github.com/wazsmwazsm/mortar\u0026#34; ) func main() { // 创建容量为 10 的任务池 \tpool, err := mortar.NewPool(10) if err != nil { panic(err) } wg := new(sync.WaitGroup) for i := 0; i \u0026lt; 1000; i++ { wg.Add(1) // 创建任务 \ttask := \u0026amp;mortar.Task{ Handler: func(v ...interface{}) { wg.Done() fmt.Println(v) }, } // 添加任务函数的参数 \ttask.Params = []interface{}{i, i * 2, \u0026#34;hello\u0026#34;} // 将任务放入任务池 \tpool.Put(task) } wg.Add(1) // 再创建一个任务 \tpool.Put(\u0026amp;mortar.Task{ Handler: func(v ...interface{}) { wg.Done() fmt.Println(v) }, Params: []interface{}{\u0026#34;hi!\u0026#34;}, // 也可以在创建任务时设置参数 \t}) wg.Wait() // 安全关闭任务池（保证已加入池中的任务被消费完） \tpool.Close() // 如果任务池已经关闭, Put() 方法会返回 ErrPoolAlreadyClosed 错误 \terr = pool.Put(\u0026amp;mortar.Task{ Handler: func(v ...interface{}) {}, }) if err != nil { fmt.Println(err) // print: pool already closed \t} } 总结 goroutine的复用很好的减小了大批量异步任务中的内存分配与垃圾回收压力，但不能回收的goroutine可能在任务波峰过去后成为内存浪费（不撤销池的情况下），适合用于长期大规模执行并发任务的情况下。\n2. go-playground/pool（★614） 简单介绍 提供了LimitedPool,UnLimitedPool两种池，分别可以创建(worker数量)有限的worker和无限的worker；提供了Unit task和batch Task两种任务模式，分别适用于单次偶发任务及多次重复任务。LimitedPool,UnLimitedPool都实现了Pool接口。可长期保持运行。\n系统结构\ntype Pool interface { Queue(fn WorkFunc) WorkUnit // 传入要执行的任务，立即开始执行 \tReset() // 重新初始化一个池 \tCancel() // 取消所有未在运行的任务 \tClose() // 清除所有池数据并取消所有未提交的任务 \tBatch() Batch // 创建批量任务 } 核心代码\n// passing work and cancel channels to newWorker() to avoid any potential race condition // betweeen p.work read \u0026amp; write func (p *limitedPool) newWorker(work chan *workUnit, cancel chan struct{}) { go func(p *limitedPool) { var wu *workUnit defer func(p *limitedPool) { if err := recover(); err != nil { // ... \t} }(p) var value interface{} var err error for { select { case wu = \u0026lt;-work: // possible for one more nilled out value to make it \t// through when channel closed, don\u0026#39;t quite understand the why \tif wu == nil { continue } // support for individual WorkUnit cancellation \t// and batch job cancellation \tif wu.cancelled.Load() == nil { value, err = wu.fn(wu) wu.writing.Store(struct{}{}) // need to check again in case the WorkFunc cancelled this unit of work \t// otherwise we\u0026#39;ll have a race condition \tif wu.cancelled.Load() == nil \u0026amp;\u0026amp; wu.cancelling.Load() == nil { wu.value, wu.err = value, err // who knows where the Done channel is being listened to on the other end \t// don\u0026#39;t want this to block just because caller is waiting on another unit \t// of work to be done first so we use close \tclose(wu.done) } } case \u0026lt;-cancel: return } } }(p) } // Queue queues the work to be run, and starts processing immediately func (p *limitedPool) Queue(fn WorkFunc) WorkUnit { w := \u0026amp;workUnit{ done: make(chan struct{}), fn: fn, } go func() { p.m.RLock() if p.closed { w.err = \u0026amp;ErrPoolClosed{s: errClosed} if w.cancelled.Load() == nil { close(w.done) } p.m.RUnlock() return } p.work \u0026lt;- w p.m.RUnlock() }() return w } 基本使用\n Per Unit Work\n package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;gopkg.in/go-playground/pool.v3\u0026#34; ) func main() { p := pool.NewLimited(10) defer p.Close() user := p.Queue(getUser(13)) other := p.Queue(getOtherInfo(13)) user.Wait() if err := user.Error(); err != nil { // handle error \t} // do stuff with user \tusername := user.Value().(string) fmt.Println(username) other.Wait() if err := other.Error(); err != nil { // handle error \t} // do stuff with other \totherInfo := other.Value().(string) fmt.Println(otherInfo) } func getUser(id int) pool.WorkFunc { return func(wu pool.WorkUnit) (interface{}, error) { // simulate waiting for something, like TCP connection to be established \t// or connection from pool grabbed \ttime.Sleep(time.Second * 1) if wu.IsCancelled() { // return values not used \treturn nil, nil } // ready for processing...  return \u0026#34;Joeybloggs\u0026#34;, nil } } func getOtherInfo(id int) pool.WorkFunc { return func(wu pool.WorkUnit) (interface{}, error) { // simulate waiting for something, like TCP connection to be established \t// or connection from pool grabbed \ttime.Sleep(time.Second * 1) if wu.IsCancelled() { // return values not used \treturn nil, nil } // ready for processing...  return \u0026#34;Other Info\u0026#34;, nil } }  Batch Work\n package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;gopkg.in/go-playground/pool.v3\u0026#34; ) func main() { p := pool.NewLimited(10) defer p.Close() batch := p.Batch() // for max speed Queue in another goroutine \t// but it is not required, just can\u0026#39;t start reading results \t// until all items are Queued.  go func() { for i := 0; i \u0026lt; 10; i++ { batch.Queue(sendEmail(\u0026#34;email content\u0026#34;)) } // DO NOT FORGET THIS OR GOROUTINES WILL DEADLOCK \t// if calling Cancel() it calles QueueComplete() internally \tbatch.QueueComplete() }() for email := range batch.Results() { if err := email.Error(); err != nil { // handle error \t// maybe call batch.Cancel() \t} // use return value \tfmt.Println(email.Value().(bool)) } } func sendEmail(email string) pool.WorkFunc { return func(wu pool.WorkUnit) (interface{}, error) { // simulate waiting for something, like TCP connection to be established \t// or connection from pool grabbed \ttime.Sleep(time.Second * 1) if wu.IsCancelled() { // return values not used \treturn nil, nil } // ready for processing...  return true, nil // everything ok, send nil, error if not \t} } 总结 功能齐全，但好像用起来不太舒服。\n3. ivpusic/grpool（★634） 简单介绍 用户可以提交task。 Dispatcher接受task，并将其发送给第一个可用的worker。 当worker完成处理工作时，将返回到workers pool。 worker数量和task队列大小是可配置的。\n整个程序以一个调度器为核心，Pool通过调度器来调度worker，worker可复用，但任务结束时worker不会自动回收，不适合长期运行。只能传递无参任务。\n系统结构\n// Gorouting instance which can accept client jobs type worker struct { workerPool chan *worker // 工人池 \tjobChannel chan Job // 这个worker独有的任务队列 \tstop chan struct{} } type dispatcher struct { workerPool chan *worker // 与worker是同一个workerPool \tjobQueue chan Job // 与池共有的任务队列 \tstop chan struct{} } // Represents user request, function which should be executed in some worker. type Job func() type Pool struct { JobQueue chan Job // 与dispatcher共有的任务队列 \tdispatcher *dispatcher wg sync.WaitGroup } 核心代码\nfunc (w *worker) start() { go func() { // 创建了一个worker \tvar job Job for { // worker free, add it to pool \tw.workerPool \u0026lt;- w // 把worker放入worker池  select { case job = \u0026lt;-w.jobChannel: job() case \u0026lt;-w.stop: w.stop \u0026lt;- struct{}{} return } } }() } func (d *dispatcher) dispatch() { for { select { case job := \u0026lt;-d.jobQueue: worker := \u0026lt;-d.workerPool worker.jobChannel \u0026lt;- job case \u0026lt;-d.stop: // 停止所有worker \tfor i := 0; i \u0026lt; cap(d.workerPool); i++ { worker := \u0026lt;-d.workerPool worker.stop \u0026lt;- struct{}{} \u0026lt;-worker.stop } d.stop \u0026lt;- struct{}{} return } } } 其程序运行图大致如下： 程序中存在一个workerPool，所有的worker都存在在这个池中，在程序的开始，会创建指定数目的worker，一次性放入池中（此后不再增加或减少）。每个worker有一个独有的jobChannel，这个jobChannel向worker传递其要执行的任务。\n用户通过直接使用Pool的JobQueue来提交任务，提交的任务会由dispatcher接收，然后分配给某个空闲的worker，即放入其jobChannel中。\n基本使用\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/ivpusic/grpool\u0026#34; ) func main() { // number of workers, and size of job queue  pool := grpool.NewPool(100, 50) // release resources used by pool  defer pool.Release() // submit one or more jobs to pool  for i := 0; i \u0026lt; 10; i++ { count := i pool.JobQueue \u0026lt;- func() { fmt.Printf(\u0026#34;I am worker! Number %d\\n\u0026#34;, count) } } // dummy wait until jobs are finished  time.Sleep(1 * time.Second) } 性能测试\n总结 设计简单，代码简洁清晰，但无panic恢复，不能动态扩容，任务传递简陋，感觉像是一个未完成版本的任务池\n4. Jeffail/tunny（★2.4k） 简单介绍 有两种模式，第一种模式在池创建时即指定任务，池创建完成后任务不可更改，但可传入不同的参数，有返回值。第二种模式在创建时不给定任务，可在池创建后自行指配任务，但只可执行无参数任务且无返回值。支持超时机制，如果任务超过时间期限还没完成，则会终止并返回错误。\n系统结构\ntype Pool struct { queuedJobs int64 // 当前挂起的任务数  ctor func() Worker // 初始任务 \tworkers []*workerWrapper reqChan chan workRequest workerMut sync.Mutex } type workerWrapper struct { // 负责管理worker和goroutine的生命周期 \tworker Worker interruptChan chan struct{} // 中止信号通道 \treqChan chan\u0026lt;- workRequest // 用于发送任务请求信号 \tcloseChan chan struct{} // 退出信号通道 \tclosedChan chan struct{} // 退出标志通道 } type workRequest struct { // 任务请求信号 \tjobChan chan\u0026lt;- interface{} // 用于接收任务 \tretChan \u0026lt;-chan interface{} // 用于返回结果 \tinterruptFunc func() } 核心代码\nfunc (p *Pool) Process(payload interface{}) interface{} { // 省略了部分错误处理代码 \trequest, open := \u0026lt;-p.reqChan request.jobChan \u0026lt;- payload payload, open = \u0026lt;-request.retChan // 同步等待任务完成 \treturn payload } func (w *workerWrapper) run() { jobChan, retChan := make(chan interface{}), make(chan interface{}) defer func() { w.worker.Terminate() close(retChan) close(w.closedChan) }() for { // NOTE: Blocking here will prevent the worker from closing down. \tw.worker.BlockUntilReady() select { case w.reqChan \u0026lt;- workRequest{ // tunny.go:156 \tjobChan: jobChan, retChan: retChan, interruptFunc: w.interrupt, }: select { case payload := \u0026lt;-jobChan: // tunny.go: 161 \tresult := w.worker.Process(payload) select { case retChan \u0026lt;- result: // tunny.go: 163 \tcase \u0026lt;-w.interruptChan: w.interruptChan = make(chan struct{}) } case _, _ = \u0026lt;-w.interruptChan: w.interruptChan = make(chan struct{}) } case \u0026lt;-w.closeChan: return } } } 根据以上代码，其主要执行过程如下：\n 某个goroutine空闲后，通过其reqChan通道发送workRequest。workRequest中包括一个jobChan，用于生产者发送任务；一个retChan，用于返回执行结果；一个interrupt Func，用于任务执行超时时强行停止worker。需要注意的是，reqChan是池和workerWrapper共有的，因此发送的workRequest可直接被池接收到。 每当池中有新任务时，池尝试从reqChan中获取一个workRequest，若获取到，则将任务通过workRequest持有的jobChan(即某个worker持有的jobChan)发送到worker 发送完毕后，同步的从retChan中读取结果。 若设置了超时时间，则在时间超时后会强制停止  基本使用\npackage main import ( \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;github.com/Jeffail/tunny\u0026#34; ) func main() { numCPUs := runtime.NumCPU() pool := tunny.NewFunc(numCPUs, func(payload interface{}) interface{} { var result []byte // TODO: Something CPU heavy with payload  return result }) defer pool.Close() http.HandleFunc(\u0026#34;/work\u0026#34;, func(w http.ResponseWriter, r *http.Request) { input, err := ioutil.ReadAll(r.Body) if err != nil { http.Error(w, \u0026#34;Internal error\u0026#34;, http.StatusInternalServerError) } defer r.Body.Close() // Funnel this work into our pool. This call is synchronous and will \t// block until the job is completed. \tresult := pool.Process(input) w.Write(result.([]byte)) }) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) }  超时演示：\n func main() { p := tunny.NewFunc(4, func(payload interface{}) interface{} { n := payload.(int) result := fib(n) time.Sleep(5 * time.Second) return result }) defer p.Close() var wg sync.WaitGroup wg.Add(4) for i := 0; i \u0026lt; 4; i++ { go func(i int) { n := rand.Intn(30) result, err := p.ProcessTimed(n, time.Second) nowStr := time.Now().Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) if err != nil { fmt.Printf(\u0026#34;[%s]task(%d) failed:%v\\n\u0026#34;, nowStr, i, err) } else { fmt.Printf(\u0026#34;[%s]fib(%d) = %d\\n\u0026#34;, nowStr, n, result) } wg.Done() }(i) } wg.Wait() } func fib(n int) int { if n \u0026lt;= 1 { return 1 } return fib(n-1) + fib(n-2) } 5. panjf2000/ants（★5.8k） 简单介绍 启动服务之时先初始化一个 Goroutine Pool 池，这个 Pool 维护了一个类似栈的 LIFO 队列 ，里面存放负责处理任务的 Worker，然后在 client 端提交 task 到 Pool 中之后，在 Pool 内部，接收 task 之后的核心操作是：\n 检查当前 Worker 队列中是否有可用的 Worker，如果有，取出执行当前的 task； 没有可用的 Worker，判断当前在运行的 Worker 是否已超过该 Pool 的容量：{是 —\u0026gt; 再判断工作池是否为非阻塞模式：[是 ——\u0026gt; 直接返回 nil，否 ——\u0026gt; 阻塞等待直至有 Worker 被放回 Pool]，否 —\u0026gt; 新开一个 Worker（goroutine）处理}； 每个 Worker 执行完任务之后，放回 Pool 的队列中等待。  大致流程如下： 这个库的亮点之一是其为每一个worker设置了过期时间，若worker空闲了一定时间就会被回收，很好的节约了资源。\n系统结构\ntype Pool struct { capacity int32 // 池的容量  running int32 // 正在运行的worker  workers workerArray // 存储可获取的worker  state int32 // 池的状态  lock sync.Locker cond *sync.Cond // 条件变量，用于在条件满足时唤醒阻塞的程序  workerCache sync.Pool blockingNum int // 被阻塞的数量  options *Options } type workerArray interface { len() int isEmpty() bool insert(worker *goWorker) error detach() *goWorker retrieveExpiry(duration time.Duration) []*goWorker reset() } type goWorker struct { pool *Pool task chan func() recycleTime time.Time // recycleTime will be update when putting a worker back into queue } 核心代码\nfunc NewPool(size int, options ...Option) (*Pool, error) { if expiry := opts.ExpiryDuration; expiry \u0026lt; 0 { return nil, ErrInvalidPoolExpiry } else if expiry == 0 { opts.ExpiryDuration = DefaultCleanIntervalTime } p := \u0026amp;Pool{ capacity: int32(size), lock: internal.NewSpinLock(), options: opts, } p.workerCache.New = func() interface{} { return \u0026amp;goWorker{ pool: p, task: make(chan func(), workerChanCap), } } if p.options.PreAlloc { if size == -1 { return nil, ErrInvalidPreAllocSize } p.workers = newWorkerArray(loopQueueType, size) } else { p.workers = newWorkerArray(stackType, 0) } p.cond = sync.NewCond(p.lock) // Start a goroutine to clean up expired workers periodically. \tgo p.purgePeriodically() // 用一个独立的goroutine来回收过期的worker  return p, nil } func (w *goWorker) run() { w.pool.incRunning() go func() { defer func() { w.pool.decRunning() w.pool.workerCache.Put(w) if p := recover(); p != nil { // ... \t} // Call Signal() here in case there are goroutine waiting for available workers. \tw.pool.cond.Signal() // 发出信号唤醒某些阻塞等待worker的线程 \t}() for f := range w.task { if f == nil { return } f() if ok := w.pool.revertWorker(w); !ok { // 将worker重新放入池中循环利用 \treturn } } }() } // 返回一个可用的worker func (p *Pool) retrieveWorker() (w *goWorker) { spawnWorker := func() { // 返回一个可用的worker \tw = p.workerCache.Get().(*goWorker) w.run() } p.lock.Lock() w = p.workers.detach() // 尝试从worker队列中取worker \tif w != nil { // 从队列中获得了worker，直接返回 \tp.lock.Unlock() } else if capacity := p.Cap(); capacity == -1 || capacity \u0026gt; p.Running() { // 队列为空其worker数量未达到限制，则生成新的worker \tp.lock.Unlock() spawnWorker() } else { // 队列为空且数量达到限制，则阻塞等待其他worker空闲 \tif p.options.Nonblocking { p.lock.Unlock() return } retry: if p.options.MaxBlockingTasks != 0 \u0026amp;\u0026amp; p.blockingNum \u0026gt;= p.options.MaxBlockingTasks { p.lock.Unlock() return } p.blockingNum++ p.cond.Wait() // 阻塞等待可用的worker \tp.blockingNum-- var nw int if nw = p.Running(); nw == 0 { // awakened by the scavenger \tp.lock.Unlock() if !p.IsClosed() { spawnWorker() } return } if w = p.workers.detach(); w == nil { if nw \u0026lt; capacity { p.lock.Unlock() spawnWorker() return } goto retry } p.lock.Unlock() } return } // 定时清理过期worker // purgePeriodically clears expired workers periodically which runs in an individual goroutine, as a scavenger. func (p *Pool) purgePeriodically() { heartbeat := time.NewTicker(p.options.ExpiryDuration) defer heartbeat.Stop() for range heartbeat.C { if p.IsClosed() { break } p.lock.Lock() expiredWorkers := p.workers.retrieveExpiry(p.options.ExpiryDuration) p.lock.Unlock() // Notify obsolete workers to stop. \t// This notification must be outside the p.lock, since w.task \t// may be blocking and may consume a lot of time if many workers \t// are located on non-local CPUs. \tfor i := range expiredWorkers { expiredWorkers[i].task \u0026lt;- nil expiredWorkers[i] = nil } // There might be a situation that all workers have been cleaned up(no any worker is running) \t// while some invokers still get stuck in \u0026#34;p.cond.Wait()\u0026#34;, \t// then it ought to wake all those invokers. \tif p.Running() == 0 { p.cond.Broadcast() } } } func (wq *workerStack) retrieveExpiry(duration time.Duration) []*goWorker { n := wq.len() if n == 0 { return nil } expiryTime := time.Now().Add(-duration) index := wq.binarySearch(0, n-1, expiryTime) wq.expiry = wq.expiry[:0] if index != -1 { wq.expiry = append(wq.expiry, wq.items[:index+1]...) m := copy(wq.items, wq.items[index+1:]) for i := m; i \u0026lt; n; i++ { wq.items[i] = nil } wq.items = wq.items[:m] } return wq.expiry } 基本使用\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/panjf2000/ants/v2\u0026#34; ) var sum int32 func myFunc(i interface{}) { n := i.(int32) atomic.AddInt32(\u0026amp;sum, n) fmt.Printf(\u0026#34;run with %d\\n\u0026#34;, n) } func demoFunc() { time.Sleep(10 * time.Millisecond) fmt.Println(\u0026#34;Hello World!\u0026#34;) } func main() { defer ants.Release() runTimes := 1000 // Use the common pool. \tvar wg sync.WaitGroup syncCalculateSum := func() { demoFunc() wg.Done() } for i := 0; i \u0026lt; runTimes; i++ { wg.Add(1) _ = ants.Submit(syncCalculateSum) } wg.Wait() fmt.Printf(\u0026#34;running goroutines: %d\\n\u0026#34;, ants.Running()) fmt.Printf(\u0026#34;finish all tasks.\\n\u0026#34;) // Use the pool with a function, \t// set 10 to the capacity of goroutine pool and 1 second for expired duration. \tp, _ := ants.NewPoolWithFunc(10, func(i interface{}) { myFunc(i) wg.Done() }) defer p.Release() // Submit tasks one by one. \tfor i := 0; i \u0026lt; runTimes; i++ { wg.Add(1) _ = p.Invoke(int32(i)) } wg.Wait() fmt.Printf(\u0026#34;running goroutines: %d\\n\u0026#34;, p.Running()) fmt.Printf(\u0026#34;finish all tasks, result is %d\\n\u0026#34;, sum) } ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/5%E7%A7%8Dgoroutine%E6%B1%A0%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%AF%B9%E6%AF%94/","summary":"1. wazsmwazsm/mortar（★74） 简单介绍\n创建一个容量为 N 的池, 在池容量未满时, 每塞入一个任务（生产任务）, 任务池开启一个 worker (建立协程) 去处理任务（消费任务）。 当任务池容量赛满，每塞入一个任务（生产任务）, 任务会被已有的 N 个 worker 抢占执行（消费任务），达到协程限制的功能。但worker创建后不会回收，除非将整个pool撤销。\n结构\ntype Task struct { Handler func(v ...interface{}) // 函数签名 \tParams []interface{} // 参数 } // Pool task pool type Pool struct { capacity uint64 // 池的容量，自行制定 \trunningWorkers uint64 // 正在运行的worker \tstatus int64 // 池的状态 \tchTask chan *Task // 任务队列，worker从中获取任务 \tPanicHandler func(interface{}) // 自定义的PanicHandler，防止因某个goroutine发生panic而导致服务崩溃。 \tsync.Mutex // 全局锁 } 核心代码","title":"5种goroutine池的实现之对比"},{"content":"1. evm虚拟机 交易的执行是区块链节点上的一个重要的功能。是把交易中的智能合约二进制代码取出来，用执行器（Executor）执行。在交易的执行过程中，会对区块链的状态（State）进行修改，形成新区块的状态储存下来（Storage）。执行器在这个过程中，类似于一个黑盒，输入是智能合约代码，输出是状态的改变.\n以太坊虚拟机（environment virtual machine，简称EVM），作用是将智能合约代码编译成可在以太坊上执行的机器码，并提供智能合约的运行环境。它是一个对外完全隔离的沙盒环境，在运行期间不能访问网络、文件，即使不同合约之间也有有限的访问权限。以太坊虚拟机提供了面向合约的高级编程语言solidity，这使得开发者可以专注于应用本身，更方便、快捷的开发去中心化应用程序，同时也大大降低了开发难度。\nEVM是一种基于栈的虚拟机（区别于基于寄存器的虚拟机），用于执行智能合约，同时EVM是图灵完备的，EVM操作数栈调用深度为1024,EVM机器码长度一个字节，最多可以有256个操作码，目前已经定义了144个操作码，还有100多个操作码可以扩展，每个操作码都根据其弹栈数、压栈数定义了相应的gas消耗数量。泰岳链应用了以太坊EVM机制来实现智能合约，并增加了对国密算法的支持(SM3)。\n2. solidity语言 Solidity 是一门面向合约的、为实现智能合约而创建的高级编程语言。这门语言受到了 C++，Python 和 Javascript 语言的影响，设计的目的是能在 以太坊虚拟机（EVM） 上运行。\nSolidity 是静态类型语言，支持继承、库和复杂的用户定义类型等特性。\n直接看这里：Solidity最新中文文档\n3. 使用Ganache与truffle进行合约开发 Ganache\nganache可以快速的在本机上启动一条以太坊链，用户可以方便的在上面部署合约，调用合约，完成各种与合约之间的交互。\nganache提供了Windows、Linux以及Mac三种系统的版本，直接到其官网或GitHub页面下载安装即可。\n安装完成后，即可以快速部署一条链 使用QUICKSTART模式部署的链只会在本次会话中存在，关闭当前会话或注销当前用户都会导致链的撤销，如果只是写个小demo的话，那么使用这种方式即可。\nNEW WORKSPACE则会创建一条持久化的链，不会因会话结束或用户注销而撤销链。\n 使用QUICKSTART模式启动\n ganache会自动创建10个测试账号，每个账号分配了100个原生币，交易需要消耗这些原生币。 在页面的上方，还有其他一些选项卡，可以方便的查看当前区块、交易、事件、日志等。需要注意的是在这些选项卡的下方，还标注了本链的一些信息，如它的端口，网络ID等。 truffle\ntruffle提供了合约开发、测试、部署等一系列工具，通过与Ganache配合可以十分方便的测试你的合约。\n安装truffle\nnpm install -g truffle 新建一个truffle项目\nmkdir MyContract truffle init truffle会创建如下的目录结构：\n├── contracts │ └── Migrations.sol ├── migrations │ └── 1_initial_migration.js ├── test └── truffle-config.js contract目录中存放我们的合约;migrations目录中存放migrate文件，功能类似数据库migrate文件，简单来说，就是让你的应用从一个状态迁移到另一个状态;test目录中存放测试文件（还未创建）;truffle-config.js是配置文件，其中配置了链的地址等信息。\n根据提示我们来创建一个简单的合约模板：\ntruffle create contract Counter truffle创建了Counter.sol文件，再次查看目录结构：\n├── contracts │ ├── Counter.sol │ └── Migrations.sol ├── migrations │ └── 1_initial_migration.js ├── test └── truffle-config.js 打开Counter.sol\n// SPDX-License-Identifier: MIT pragma solidity \u0026gt;=0.4.22 \u0026lt;0.9.0; contract Counter { constructor() public { } } 为我们提供了一个合约模板，修改合约：\n// SPDX-License-Identifier: MIT pragma solidity \u0026gt;=0.4.22 \u0026lt;0.9.0; contract counter { address owner; mapping (string =\u0026gt; uint256) values; constructor() public { owner = msg.sender; } function increase(string memory key) public payable { values[key] = values[key] + 1; } function get(string memory key) view public returns (uint) { return values[key]; } function getOwner()view public returns (address) { return owner; } } 以上合约是一个非常简单的计数器合约，提供了increase, get, getOwner三个方法，分别用来增加计数、获取计数值、获取合约所有者。\n编译合约\ntruffle compile 编辑migrate文件\nvim migrations/2_deploy_contracts.js const Counter = artifacts.require(\u0026#34;Counter\u0026#34;) module.exports = function(deployer) { deployer.deploy(Counter); } 现在准备工作已经完成了，开始让我们的合约上链。\n配置truffle-config.js文件，主要是要配置network：\nnetworks: { development: { host: \u0026#34;127.0.0.1\u0026#34;, // Localhost (default: none)  port: 7545, // Standard Ethereum port (default: none)  network_id: \u0026#34;*\u0026#34;, // Any network (default: none)  }, }, networks有很详细的注释，这里的关键是需要与Ganache中显示的端口号等一致。 配置完成，终于到了激动人心的一步，合约上链。\ntruffle migrate Compiling your contracts... =========================== \u0026gt; Everything is up to date, there is nothing to compile. Starting migrations... ====================== \u0026gt; Network name: 'development' \u0026gt; Network id: 5777 \u0026gt; Block gas limit: 6721975 (0x6691b7) 1_initial_migration.js ====================== Deploying 'Migrations' ---------------------- \u0026gt; transaction hash: 0x75050e06c2f6f1097257de17ea1370a86225c35909e8319ed62407424b21587e \u0026gt; Blocks: 0 Seconds: 0 \u0026gt; contract address: 0xd366fCAF0F7A2b1A2Ebde67E89f1bbC2ED708c55 \u0026gt; block number: 112 \u0026gt; block timestamp: 1623909962 \u0026gt; account: 0x128F3853c98671ac43e8358e3A043f3A7bD0Ca18 \u0026gt; balance: 99.72713514 \u0026gt; gas used: 191943 (0x2edc7) \u0026gt; gas price: 20 gwei \u0026gt; value sent: 0 ETH \u0026gt; total cost: 0.00383886 ETH 2_deploy_contracts.js ===================== Deploying 'Counter' ------------------- \u0026gt; transaction hash: 0x692c27b1b9a7f04301c10a2b3ece8c5f7737581d4de9f01e88653a907eb4b711 \u0026gt; Blocks: 0 Seconds: 0 \u0026gt; contract address: 0x76607048A6628A43d981811a35a629Ff049c2B11 \u0026gt; block number: 114 \u0026gt; block timestamp: 1623910129 \u0026gt; account: 0x128F3853c98671ac43e8358e3A043f3A7bD0Ca18 \u0026gt; balance: 99.72055518 \u0026gt; gas used: 286660 (0x45fc4) \u0026gt; gas price: 20 gwei \u0026gt; value sent: 0 ETH \u0026gt; total cost: 0.0057332 ETH \u0026gt; Saving migration to chain. \u0026gt; Saving artifacts ------------------------------------- \u0026gt; Total cost: 0.0057332 ETH Summary ======= \u0026gt; Total deployments: 2 \u0026gt; Final cost: 0.00957206 ETH 从输出的日志信息中可以看到部署的合约及其消耗的资源。\n查看Ganache可看到，链增长了4个区块 消耗的原生币也显示了出来 执行的所有交易 现在我们完成了合约编写，合约上链等步骤，但到这里只能证明合约语法的正确性，我们还需要进行一系列的测试来保证我们的合约逻辑是无误的，可以按照我们预定的逻辑执行。\n生成测试代码(可选)\ntruffle create test Counter cat test/counter.js const Counter = artifacts.require(\u0026#34;Counter\u0026#34;); /* * uncomment accounts to access the test accounts made available by the * Ethereum client * See docs: https://www.trufflesuite.com/docs/truffle/testing/writing-tests-in-javascript */ contract(\u0026#34;Counter\u0026#34;, function (/* accounts */) { it(\u0026#34;should assert true\u0026#34;, async function () { await Counter.deployed(); return assert.isTrue(true); }); }); 如果你不想自动生成代码，那么可以手动创建自己的测试文件\nvim test/counter.test.js 内容同上\n开始测试\n$ truffle test Using network \u0026#39;development\u0026#39;. Compiling your contracts... =========================== \u0026gt; Compiling ./contracts/Counter.sol \u0026gt; Artifacts written to /tmp/test--10676-pZQ9laXW37D3 \u0026gt; Compiled successfully using: - solc: 0.5.16+commit.9c3226ce.Emscripten.clang Contract: Counter ✓ should assert true 1 passing (74ms) 以上的测试文件中，断言永远为真，所以只要这个测试可以跑起来，就肯定不会fail，这只是官方给我们生成的测试代码模板，真正的测试代码还需要我们自己来编写。 修改测试代码：\nconst Counter = artifacts.require(\u0026#34;Counter\u0026#34;); contract(\u0026#34;counter\u0026#34;, async account =\u0026gt; { contract(\u0026#34;counter 1st test\u0026#34;, async account =\u0026gt; { let c; before(\u0026#34;deploy contract\u0026#34;, async () =\u0026gt; { c = await Counter.deployed(); }) it(\u0026#34;test getOwner\u0026#34;, async () =\u0026gt; { const owner = await c.getOwner(); expectedOwner = \u0026#34;0x128F3853c98671ac43e8358e3A043f3A7bD0Ca18\u0026#34; assert.equal(owner, expectedOwner, \u0026#34;owner is wrong\u0026#34;) }) it(\u0026#34;test increase\u0026#34;, async () =\u0026gt; { await c.increase(\u0026#34;0x128F3853c98671ac43e8358e3A043f3A7bD0Ca18\u0026#34;) const num = await c.get(\u0026#34;0x128F3853c98671ac43e8358e3A043f3A7bD0Ca18\u0026#34;) let expectedNum = 1; assert.equal(expectedNum, num, \u0026#34;error occur\u0026#34;); }) }) }); 重新运行测试：\ntruffle test Using network 'development'. Compiling your contracts... =========================== \u0026gt; Compiling ./contracts/Counter.sol \u0026gt; Artifacts written to /tmp/test--10825-NWIGDIdS1RrG \u0026gt; Compiled successfully using: - solc: 0.5.16+commit.9c3226ce.Emscripten.clang Contract: counter Contract: counter 1st test ✓ test getOwner ✓ test increase (81ms) 2 passing (184ms) 测试通过，完美。\n更多资料，请参考官网文档：TRUFFLE SUITE\n 时间处理： https://ethereum.stackexchange.com/questions/34110/compare-dates-in-solidity\n ","permalink":"http://yangchnet.github.io/Dessert/posts/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%8D%81%E5%88%86%E9%92%9F%E5%AD%A6%E4%BC%9A%E5%8C%BA%E5%9D%97%E9%93%BE%E5%90%88%E7%BA%A6%E5%BC%80%E5%8F%91/","summary":"1. evm虚拟机 交易的执行是区块链节点上的一个重要的功能。是把交易中的智能合约二进制代码取出来，用执行器（Executor）执行。在交易的执行过程中，会对区块链的状态（State）进行修改，形成新区块的状态储存下来（Storage）。执行器在这个过程中，类似于一个黑盒，输入是智能合约代码，输出是状态的改变.\n以太坊虚拟机（environment virtual machine，简称EVM），作用是将智能合约代码编译成可在以太坊上执行的机器码，并提供智能合约的运行环境。它是一个对外完全隔离的沙盒环境，在运行期间不能访问网络、文件，即使不同合约之间也有有限的访问权限。以太坊虚拟机提供了面向合约的高级编程语言solidity，这使得开发者可以专注于应用本身，更方便、快捷的开发去中心化应用程序，同时也大大降低了开发难度。\nEVM是一种基于栈的虚拟机（区别于基于寄存器的虚拟机），用于执行智能合约，同时EVM是图灵完备的，EVM操作数栈调用深度为1024,EVM机器码长度一个字节，最多可以有256个操作码，目前已经定义了144个操作码，还有100多个操作码可以扩展，每个操作码都根据其弹栈数、压栈数定义了相应的gas消耗数量。泰岳链应用了以太坊EVM机制来实现智能合约，并增加了对国密算法的支持(SM3)。\n2. solidity语言 Solidity 是一门面向合约的、为实现智能合约而创建的高级编程语言。这门语言受到了 C++，Python 和 Javascript 语言的影响，设计的目的是能在 以太坊虚拟机（EVM） 上运行。\nSolidity 是静态类型语言，支持继承、库和复杂的用户定义类型等特性。\n直接看这里：Solidity最新中文文档\n3. 使用Ganache与truffle进行合约开发 Ganache\nganache可以快速的在本机上启动一条以太坊链，用户可以方便的在上面部署合约，调用合约，完成各种与合约之间的交互。\nganache提供了Windows、Linux以及Mac三种系统的版本，直接到其官网或GitHub页面下载安装即可。\n安装完成后，即可以快速部署一条链 使用QUICKSTART模式部署的链只会在本次会话中存在，关闭当前会话或注销当前用户都会导致链的撤销，如果只是写个小demo的话，那么使用这种方式即可。\nNEW WORKSPACE则会创建一条持久化的链，不会因会话结束或用户注销而撤销链。\n 使用QUICKSTART模式启动\n ganache会自动创建10个测试账号，每个账号分配了100个原生币，交易需要消耗这些原生币。 在页面的上方，还有其他一些选项卡，可以方便的查看当前区块、交易、事件、日志等。需要注意的是在这些选项卡的下方，还标注了本链的一些信息，如它的端口，网络ID等。 truffle\ntruffle提供了合约开发、测试、部署等一系列工具，通过与Ganache配合可以十分方便的测试你的合约。\n安装truffle\nnpm install -g truffle 新建一个truffle项目\nmkdir MyContract truffle init truffle会创建如下的目录结构：\n├── contracts │ └── Migrations.sol ├── migrations │ └── 1_initial_migration.js ├── test └── truffle-config.js contract目录中存放我们的合约;migrations目录中存放migrate文件，功能类似数据库migrate文件，简单来说，就是让你的应用从一个状态迁移到另一个状态;test目录中存放测试文件（还未创建）;truffle-config.js是配置文件，其中配置了链的地址等信息。\n根据提示我们来创建一个简单的合约模板：\ntruffle create contract Counter truffle创建了Counter.sol文件，再次查看目录结构：\n├── contracts │ ├── Counter.","title":"十分钟学会区块链合约开发"},{"content":"1. WSL的安装 1.1 升级Windows WSL需要高版本的windows，可使用微软官方的易升工具或直接从设置中升级，升级需要一定的时间。\n1.2 安装WSL  使用管理员模式打开power shell， 使用如下命令开启WSL功能  dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart 重启你的机器\n启用虚拟机功能 以管理员身份打开powershell，使用如下命令：  dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 重新启动电脑\n下载Linux内核更新包 适用于 x64 计算机的 WSL2 Linux 内核更新包  运行你下载的更新包。\n将WSL2设置为默认版本 以管理员身份打开powershell，使用如下命令：  wsl --set-default-version 2 选择你要安装的发行版  这里我选择了Ubuntu18.04，获取后安装\n启动安装的发行版即可  2. 使用WSL图形界面(可选)  设置环境变量  export DISPLAY=$(awk \u0026#39;/nameserver / {print $2; exit}\u0026#39; /etc/resolv.conf 2\u0026gt;/dev/null):0 export LIBGL_ALWAYS_INDIRECT=1  安装Xserver，这里选择的软件是vcxsrv， 可在sourceforge中下载安装。\n  安装完成后直接启动即可\n   每次重新启动电脑都要重新打开一下，略烦。微软官方的WSLg已经发布，不过需要Windows Insider,先等等吧\n 在wsl中安装gedit  sudo apt install gedit 尝试启动gedit：\ngedit 完成！！！\n 可以从wsl中启动图形界面意味着你可以将vscode、idea、goland等开发工具直接装在wsl中。但我强烈建议你使用vscode的Remote-WSL插件来进行开发，而只在一些必要的时候才从wsl中直接启动图形界面。\n 3. WSL中文(如未安装GUI，则不需要配置) 3.1 WSL中文显示  安装语言包  sudo apt-get -y install locales xfonts-intl-chinese fonts-wqy-microhei 语言环境设置  sudo dpkg-reconfigure locales 使用空格键选中en_US.UTF-8 UTF-8、zh_CN.UTF-8 UTF-8，使用Tab键切换至OK，再将en_US.UTF-8选为默认。 重启系统  exit ... wsl --shutdown wsl 3.2 WSL中文输入法配置(如未安装GUI，则不需要配置)  安装fcitx及输入法\n安装fcitx核心及CJK字体  sudo apt install fcitx fonts-noto-cjk fonts-noto-color-emoji dbus-x11 安装搜狗输入法，直接到搜狗输入法官网网站下载并按照说明安装即可\n配置输入环境  首先使用root账号生成dbus机器码\ndbus-uuidgen \u0026gt; /var/lib/dbus/machine-id 用root账号创建/etc/profile.d/fcitx.sh文件，内容如下：\n#!/bin/bash export QT_IM_MODULE=fcitx export GTK_IM_MODULE=fcitx export XMODIFIERS=@im=fcitx export DefaultIMModule=fcitx #可选，fcitx 自启 fcitx-autostart \u0026amp;\u0026gt;/dev/null 其他配置 将以下内容添加到你的bashrc配置文件中  vim ~/.bashrc export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=@im=fcitx export DefaultIMModule=fcitx fcitx-autostart \u0026amp;\u0026gt;/dev/null source ~/.bashrc 运行fcitx-config-gtk3,会出现如图的界面：\n按照提示将键盘布局放在第一位，输入法放在第二位。\n为防止和Windows上的输入法切换快捷键冲突，这里将快捷键切换更改为ctrl+shift(Windows上为shift)\n到这里，配置就完成了。\n开始输入\n  4. WSL 开发环境备份 可以使用wsl --export \u0026lt;DistributionName\u0026gt; \u0026lt;fileName\u0026gt;， 例如，我们使用Ubuntu18.04的Linux版本，那么使用如下命令导出：\n# --export \u0026lt;分发版\u0026gt; \u0026lt;文件名\u0026gt; wsl --export Ubuntu18.04 ubuntuLinux 那么就会在当前文件夹下生成一个名为ubuntuLinux的文件，这就是我们的WSL开发环境了。\n如何导入？\n# --import \u0026lt;分发版\u0026gt; \u0026lt;安装位置\u0026gt; \u0026lt;文件名\u0026gt;  wsl --import Ubuntu18.04 . ubuntuLinux  导入的WSL默认用户会被设置为root，但是有时候使用root会比较麻烦，比如我们使用普通账户开发的项目，如果用root打开的话，可能有些文件的所有人和所有组会变成root，这时候如果再使用普通账户读写就会产生问题。因此要设置默认用户为普通用户。\n sudo vim /etc/wsl.conf 文件内容如下：\n[user] default=lc FAQ   打开的图形界面字体很模糊 找到自己的VcXsrv安装位置，找到vsxsrc.exe和xlaunch.exe两个应用程序文件，右键属性\u0026gt;兼容性\u0026gt;更改高DPI设置\u0026gt;勾选替代高DPI缩放行为（应用程序）\u0026gt; 确定。\n  如何从windows访问wsl2的网络\n在命令行中使用如下命令显示wsl2的ip地址\n  wsl -- ifconfig eth0 启动vsxsrc后，依然不能打开图形界面 首先检查vsxsrc的配置是否正确，环境变量是否配置，若检查无误，则可能是防火墙阻止了连接。 尝试关闭全部防火墙（事后再重新打开），再次尝试启动，若可以启动成功，则证明为防火墙问题。 修复方法：确保防火墙入站规则中私有和公有规则都允许连接   Reference 在WSL上配置输入法\nwsl下Ubuntu中文显示方法\nWSL 下 Ubuntu 20.04 中文显示设置\nWindows Subsystem for Linux Installation Guide for Windows 10\n","permalink":"http://yangchnet.github.io/Dessert/posts/env/wsl2-%E4%B8%8D%E8%BE%93mac%E7%9A%84%E5%BC%80%E5%8F%91%E4%BD%93%E9%AA%8C%E4%B8%80wsl2%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/","summary":"1. WSL的安装 1.1 升级Windows WSL需要高版本的windows，可使用微软官方的易升工具或直接从设置中升级，升级需要一定的时间。\n1.2 安装WSL  使用管理员模式打开power shell， 使用如下命令开启WSL功能  dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart 重启你的机器\n启用虚拟机功能 以管理员身份打开powershell，使用如下命令：  dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart 重新启动电脑\n下载Linux内核更新包 适用于 x64 计算机的 WSL2 Linux 内核更新包  运行你下载的更新包。\n将WSL2设置为默认版本 以管理员身份打开powershell，使用如下命令：  wsl --set-default-version 2 选择你要安装的发行版  这里我选择了Ubuntu18.04，获取后安装\n启动安装的发行版即可  2. 使用WSL图形界面(可选)  设置环境变量  export DISPLAY=$(awk \u0026#39;/nameserver / {print $2; exit}\u0026#39; /etc/resolv.conf 2\u0026gt;/dev/null):0 export LIBGL_ALWAYS_INDIRECT=1  安装Xserver，这里选择的软件是vcxsrv， 可在sourceforge中下载安装。\n  安装完成后直接启动即可","title":"WSL2-不输Mac的开发体验（一）：WSL2的安装及基本配置"},{"content":"1. go generate go generate命令运行时，将找到源代码中所有包含//go:generate的特殊注释，提取并执行//go:generate后附加的命令。\n基本语法：\n//go:generate [-run regexp] [-n] [-v] [-x] [build flags] [file.go... | packages] 需要注意的几点：\n 该特殊注释必须在.go源码文件中。 每个源码文件可以包含多个generate特殊注释。 go generate只在运行go generate命令时运行，go build, go get, go test等其他命令不会运行它。 命令串行执行的，如果出错，就终止后面的执行。 特殊注释必须以\u0026quot;//go:generate\u0026quot;开头，双斜线后面没有空格。  简单的例子：\npackage main import \u0026#34;fmt\u0026#34; //go:generate echo \u0026#34;world\u0026#34; func main() { fmt.Println(\u0026#34;hello\u0026#34;) } 运行结果：\n在go generate命令中，还可以使用一些环境变量：\n $GOARCH The execution architecture (arm, amd64, etc.) $GOOS The execution operating system (linux, windows, etc.) $GOFILE The base name of the file. $GOLINE The line number of the directive in the source file. $GOPACKAGE The name of the package of the file containing the directive. $DOLLAR A dollar sign. go generate的参数\n-run 正则表达式匹配命令行，仅执行匹配的命令 -v 输出被处理的包名和源文件名 -n 只显示要执行的命令，但不执行 -x 显示要执行的命令并执行 2. 使用go generate自动生成mock接口 在我们对代码进行单元测试时，某段代码可能有一些依赖项，一般情况下我们可以手动去构建这些依赖项。但当依赖项过多时，手动去构建每一个依赖项就是一项复杂、艰巨且无聊的工作。这时候，就到了mock大显身手的时候。mock会为你提供一些虚拟的依赖项，并规定它们的行为，从而你可以方便的在自己的测试中使用它。\ngomock针对接口进行mock操作。\n例如：我们有一个Client接口，这个接口的Address方法返回其地址，Serve方法中需要用到这个Client的Address。假设现在我们还没有配置完成Client，但需要先对Serve的行为进行测试。这时我们可以\u0026quot;mock\u0026quot;出一个Client。\ntype Client interface { Address() string } func Serve(c Client) { // ...  address := client.Address() // ... } 接下来我们应该怎么做呢？\n  安装gomock\n  生成mock文件\n  在单元测试中规定Client的操作以检查Serve的行为\n  安装gomock\n  $ go install github.com/golang/mock/mockgen@v1.5.0 生成mock文件 在你的项目目录中运行如下命令：  $ mockgen -destination=mock_client.go -package=mock . Client 其中-destination指定了需要生成的文件名， -package指定了生成的mock文件的包名，若不指定，则默认为mock_后跟输入文件的包名, 点.指定了源目录，最后的Client指需要mock该接口。\n运行上面命令后，就可以在本地生成mock_client.go文件。\n在单元测试中规定Client的操作以检查Serve的行为  func Test_Serve(t *testing.T) { ctrl := gomock.NewController(t) defer ctrl.Finish() c := NewMockClient(ctrl) // 构建出mock的Client  c.EXCEPT().Address().RETURN(\u0026#34;127.0.0.1\u0026#34;).AnyTimes() // 当使用Client.Address()方法时返回”127.0.0.1“，可使用任意次  Server(c) // ... }  每次使用c.EXCEPT().Address().RETURN()仅能为一次Address()指定返回，可多次使用该方法以返回不同的值。\n 最后，mock和go generate有啥关系？\n我们可以将go generate特殊注释写在接口的头部，然后直接使用go generate命令来方便快捷的生成mock文件。如下：\n//go:generate mockgen -destination=mock_client.go -package=mock . Clien type Client interface { Address() string } 3. 使用go generate生成错误码 stringer是用于自动创建满足fmt.Stringer方法的工具。给定一个变量/常量名T, stringer可以生成类似下面方法的代码：\nfunc (t T) String string  以下内容主要参考了： 深入理解Go之generate\n 在我们的服务中，经常会使用一些错误码，这时就需要我们去定义errCode和ErrMsg, 这里介绍一种优雅的方法解决这个问题。\n定义错误码的传统方式\n定义错误码：\npackage errcode import \u0026#34;fmt\u0026#34; // 定义错误码 const ( ERR_CODE_OK = 0 // OK  ERR_CODE_INVALID_PARAMS = 1 // 无效参数  ERR_CODE_TIMEOUT = 2 // 超时  // ... ) // 定义错误码与描述信息的映射 var mapErrDesc = map[int]string { ERR_CODE_OK: \u0026#34;OK\u0026#34;, ERR_CODE_INVALID_PARAMS: \u0026#34;无效参数\u0026#34;, ERR_CODE_TIMEOUT: \u0026#34;超时\u0026#34;, // ... } // 根据错误码返回描述信息 func GetDescription(errCode int) string { if desc, exist := mapErrDesc[errCode]; exist { return desc } return fmt.Sprintf(\u0026#34;error code: %d\u0026#34;, errCode) } 使用错误码：\npackage main import ( \u0026#34;github.com/darjun/errcode\u0026#34; ) func main() { code := errcode.ERR_CODE_INVALID_PARAMS fmt.Println(code, errcode.GetDescription(errCode)) // 输出: 1 无效参数 } 为了使用方便，我们可以为错误码定义一个新的类型，然后为该类型定义String()方法，这样就不用手动调用GetDescription函数了。修改如下：\ntype ErrCode int const ( ERR_CODE_OK ErrCode = 0 // OK  ERR_CODE_INVALID_PARAMS ErrCode = 1 // 无效参数  ERR_CODE_TIMEOUT ErrCode = 2 // 超时 ) func (e ErrCode) String() string { return GetDescription(e) } 这种方式有什么问题呢？ 每次增加错误码时，都需要修改错误码到错误信息的map，有时候可能会忘， 另外，错误信息在注释和map中都出现了，有一定的冗余。 那能不能只写注释，然后自动生成代码呢？\n使用\nstringer有两种模式，默认是根据变量/常量名来生成字符串描述。我们在常量定义上增加注释：\n//go:generate stringer -type ErrCode 选项-type指定stringer命令作用的类型名。 然后在同一个目录下执行：\n$ go generate 这会在同一个目录下生成一个文件errcode_string.go。文件名格式是类型名小写_string.go。也可以通过-output选项指定输出文件名，例如下面就是指定输出文件名为code_string.go：\n//go:generate stringer -type ErrCode -output code_string.go 我们来看看这个文件的内容：\n// Code generated by \u0026#34;stringer -type ErrCode -output errcode_string.go\u0026#34;; DO NOT EDIT.  package errcode import \u0026#34;strconv\u0026#34; const _ErrCode_name = \u0026#34;ERR_CODE_OKERR_CODE_INVALID_PARAMSERR_CODE_TIMEOUT\u0026#34; var _ErrCode_index = [...]uint8{0, 11, 34, 50} func (i ErrCode) String() string { if i \u0026lt; 0 || i \u0026gt;= ErrCode(len(_ErrCode_index)-1) { return \u0026#34;ErrCode(\u0026#34; + strconv.FormatInt(int64(i), 10) + \u0026#34;)\u0026#34; } return _ErrCode_name[_ErrCode_index[i]:_ErrCode_index[i+1]] } 复制代码生成的代码做了一些优化，减少了字符串对象的数量。\n这时ERR_CODE_INVALID_PARAMS.String()返回的描述信息是ERR_CODE_INVALID_PARAMS。在一些上下文中甚至不需要自己调用String()方法，如fmt.Println。因为ErrCode实现了fmt.Stringer，一些上下文中会自动调用。\n这样errcode.go文件中mapErrDesc全局变量和getDescription函数都可以去掉了。\n但是我们更希望的是能返回后面的注释作为错误描述。\n这就需要使用stringer的-linecomment选项。修改go:generate如下：\n//go:generate stringer -type ErrCode -linecomment -output code_string.go 复制代码然后，执行go generate命令。生成的code_string.go与之前的有所不同，如下：\nconst _ErrCode_name = \u0026#34;OK无效参数超时\u0026#34; var _ErrCode_index = [...]uint8{0, 2, 14, 20} 复制代码可以看到确实通过注释生成了错误消息。\nReference 深入理解Go之generate\nGoMock\nstringer\nPackage generate\ngo generate介绍\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/go-generate%E5%B7%A5%E5%85%B7/","summary":"1. go generate go generate命令运行时，将找到源代码中所有包含//go:generate的特殊注释，提取并执行//go:generate后附加的命令。\n基本语法：\n//go:generate [-run regexp] [-n] [-v] [-x] [build flags] [file.go... | packages] 需要注意的几点：\n 该特殊注释必须在.go源码文件中。 每个源码文件可以包含多个generate特殊注释。 go generate只在运行go generate命令时运行，go build, go get, go test等其他命令不会运行它。 命令串行执行的，如果出错，就终止后面的执行。 特殊注释必须以\u0026quot;//go:generate\u0026quot;开头，双斜线后面没有空格。  简单的例子：\npackage main import \u0026#34;fmt\u0026#34; //go:generate echo \u0026#34;world\u0026#34; func main() { fmt.Println(\u0026#34;hello\u0026#34;) } 运行结果：\n在go generate命令中，还可以使用一些环境变量：\n $GOARCH The execution architecture (arm, amd64, etc.) $GOOS The execution operating system (linux, windows, etc.) $GOFILE The base name of the file.","title":"go generate工具"},{"content":" context包定义了Context类型，这个类型在API边界即进程中传递截止日期、同步信号，请求值等相关信息。\n 1. 对context包的介绍 在服务器的传入请求中应包含context，而对服务器的传出调用应接收一个context。它们之间的调用链必须包含context，或是衍生的WithCancel, WithDeadline, WithTimeout, WithValue。当一个WithCancel Context被“cancel”，那么当前context所派生的所有context也都将被取消。\nWithCancel, WithDeadline, WithTimeout接收一个Context对象（父对象），并返回其父对象的一个携带有cancel/deadline/timeout的一个拷贝（子对象）。调用CancelFunc会取消其子对象及子对象的子对象等，删除父对象对子对象的引用，并停止所有关联的计时器。未能调用CancelFunc将造成父对象结束前或计时器被触发前子对象的泄露。使用go vet工具可以检查所有控制流路径上是否都使用了CancelFunc\n使用context的程序应遵循以下规则，以使各个包之间的接口保持一致，并启用静态分析工具来检查上下文传播：\n 不要将context存储在结构类型中，而是将context明确传递给需要它的每个函数。Context应该是第一个函数，通常命名为ctx。  func DoSomething(ctx context.Context, arg Arg) error { // ...use ctx... } 不要传递一个值为nil的context，即使一个函数允许这样做。如果你不确定Context的作用那就请传递context.TODO。 只在进程和API间传递请求范围数据时使用context值，不要用于将可选参数传递给函数。 同样的Context可以传递给运行在不同goroutine中的函数，Context是线程安全的。  2. Context接口 type Context interface { Done() \u0026lt;-chan struct{} Err() error Deadline() (deadline time.Time, ok bool) Value(key interface{}) interface{} } Context是一个接口，其定义非常的简单，只包含4个方法：\n  Done() \u0026lt;-chan struct{} Done()方法将一个channel作为取消信号返回给持有context的函数，当该channel被关闭（即Done()被调用），这些函数应该立即停止其工作并返回。\n  Err() error Err()返回一个Error，说明为什么取消context。如果Done()没有被调用，那么Err返回nil。\n  Deadline() (deadline time.Time, ok bool) Deadline()方法返回持有这个context的函数的预期结束时间。如果并没有设置deadline，那么返回的ok将被设置为false。\n  Value(key interface{}) interface{} Value()方法返回与此context关联的key，如果没有与key对应的值那么返回nil。 key可以是任何支持比较的类型，为了避免冲突，应将key定义为不可导出的。 示例：\npackage user import \u0026#34;context\u0026#34; type User struct{...} type key int var userKey key func NewContext(ctx context.Context, u *User) context.Context { return context.WithValue(ctx, userKey, u) } func FromContext(ctx context.Context)(*User, bool){ u, ok := ctx.Value(userKey).(*User) return u, ok }   3. Context构造 构造一个context对象有两种方法。\nfunc Background() Context func TODO() Context 上面两个方法都会返回一个非nil，非空的Context对象。Background()一般用于构造出最初的Context，所有的Context都派生自它。TODO()用当传入的方法不确定是哪种类型的Context时，为了避免Context参数为nil而初始化的Context。\n构造出Context对象后，我们就可以使用WithCancel, WithDeadline, WithTimeout, WithValue来进一步的设置Context，构造出的Context都派生自Background或是TODO 4. context.With\u0026hellip; 4.1 context.WithCancel() func WithCancel(parent Context) (ctx Context, cancel CancelFunc) WithCancel接收一个父context并返回该父context的一个持有Done channel的子context和一个cancel方法，当cancel方法被调用时或是父context的Done channel被关闭时，当前context的Done channel将被关闭。\nWithCancel常被用于通知goroutine退出。\nfunc fibonacci(c chan int, ctx context.Context) { x, y := 0, 1 for{ select { case c \u0026lt;- x: x, y = y, x+y case \u0026lt;-ctx.Done(): fmt.Println(\u0026#34;quit\u0026#34;) return } } } func main() { defer func() { time.Sleep(time.Second) fmt.Println(\u0026#34;the number of goroutines: \u0026#34;, runtime.NumGoroutine()) }() ctx, cancel := context.WithCancel(context.Background()) defer cancel() c := make(chan int) go fibonacci(c, ctx) for i := 0; i \u0026lt; 10; i++{ fmt.Println(\u0026lt;- c) } } 4.2 context.WithValue() func WithValue(parent Context, key, val interface{}) Context WithValue方法接收一个父context，以及一个键值对，返回一个包含这个键值对的子context。可使用context.Value(key)方法取出其中保存的值。\nfunc main() { ctx, cancel := context.WithCancel(context.Background()) valueCtx := context.WithValue(ctx, key, \u0026#34;add value\u0026#34;) go watch(valueCtx) time.Sleep(10 * time.Second) cancel() time.Sleep(5 * time.Second) } func watch(ctx context.Context) { for { select { case \u0026lt;-ctx.Done(): //get value \tfmt.Println(ctx.Value(key), \u0026#34;is cancel\u0026#34;) return default: //get value \tfmt.Println(ctx.Value(key), \u0026#34;int goroutine\u0026#34;) time.Sleep(2 * time.Second) } } } 4.3 context.WithDeadline() func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) WithDeadline方法接收一个父context和一个截止时间d，返回子context并为其设置一个不晚于d的截止时间。如果父context的截止时间已经早于d, 那么WithDeadline在语义上与父context相同。当以下三种情况发生时：1.截止时间到来，2.cancelFunc被调用，3.父context的Done channel被关闭，当前context的Done channel将被关闭。\nfunc main() { d := time.Now().Add(5 * time.Second) // 5秒后到期 \tctx, cancel := context.WithDeadline(context.Background(), d) defer cancel() // 一个好的习惯是调用cancel()以防止goroutine泄露  go doSomething(ctx) time.Sleep(10 * time.Second) } func doSomething(ctx context.Context) { for { select { case \u0026lt;-time.After(time.Second): fmt.Println(\u0026#34;I\u0026#39;m doing some funny things\u0026#34;) case \u0026lt;-ctx.Done(): // 5秒时到期 \tfmt.Println(ctx.Err()) return } } } I'm doing some funny things I'm doing some funny things I'm doing some funny things I'm doing some funny things context deadline exceeded 4.4 context.WithTimeout() func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } 从其实现就可以看出，WithTimeout只是对WithDeadline的进一步封装，这层封装为context设置了一个截止时间，也就是规定了其超时时间。\nfunc main() { ctx, cancel := context.WithTimeout(context.Background(), time.Duration(2*time.Second)) // 超过两秒就退出，不再continue \tdefer cancel() // 一个好的习惯是调用cancel()以防止goroutine泄露  go seek(ctx) time.Sleep(5 * time.Second) } func seek(ctx context.Context) { for { select { case \u0026lt;-time.After(time.Second): fmt.Println(\u0026#34;I\u0026#39;m looking for something\u0026#34;) case \u0026lt;-ctx.Done(): fmt.Println(ctx.Err()) return } } } 5. context包的其他函数 5.1 context.String() 实现fmt.Stringer接口，用于打印context\nfunc main() { backgroundCtx := context.Background() fmt.Println(backgroundCtx) withValueCtx := context.WithValue(backgroundCtx, \u0026#34;one\u0026#34;, 1) fmt.Println(withValueCtx) withCancelCtx, cancel := context.WithCancel(withValueCtx) defer cancel() fmt.Println(withCancelCtx) d := time.Now().Add(2 * time.Second) withDeadlineCtx, cancel := context.WithDeadline(withCancelCtx, d) defer cancel() fmt.Println(withDeadlineCtx) withTimeoutCtx, cancel := context.WithTimeout(withDeadlineCtx, time.Duration(2 * time.Second)) defer cancel() fmt.Println(withTimeoutCtx) } context.Background context.Background.WithValue(type string, val \u0026lt;not Stringer\u0026gt;) context.Background.WithValue(type string, val \u0026lt;not Stringer\u0026gt;).WithCancel context.Background.WithValue(type string, val \u0026lt;not Stringer\u0026gt;).WithCancel.WithDeadline(2021-05-23 15:49:31.513587636 +0800 CST m=+2.000147913 [1.999897372s]) context.Background.WithValue(type string, val \u0026lt;not Stringer\u0026gt;).WithCancel.WithDeadline(2021-05-23 15:49:31.513587636 +0800 CST m=+2.000147913 [1.999881255s]).WithCancel 5.2 context.Value() 用于从WithValue context中根据key取值\nfunc main() { ctx := context.WithValue(context.Background(), \u0026#34;one\u0026#34;, 1) fmt.Println(ctx.Value(\u0026#34;one\u0026#34;)) } 值取出后context不会删除它，可重复取值，对一个不存在的key取值会返回nil。\nReference Package context\nGo Concurrency Patterns: Context\nGolang Context深入理解\nGolang Context 原理与实战\nGo 语言设计与实现\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/golang%E4%B8%ADcontext%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/","summary":"context包定义了Context类型，这个类型在API边界即进程中传递截止日期、同步信号，请求值等相关信息。\n 1. 对context包的介绍 在服务器的传入请求中应包含context，而对服务器的传出调用应接收一个context。它们之间的调用链必须包含context，或是衍生的WithCancel, WithDeadline, WithTimeout, WithValue。当一个WithCancel Context被“cancel”，那么当前context所派生的所有context也都将被取消。\nWithCancel, WithDeadline, WithTimeout接收一个Context对象（父对象），并返回其父对象的一个携带有cancel/deadline/timeout的一个拷贝（子对象）。调用CancelFunc会取消其子对象及子对象的子对象等，删除父对象对子对象的引用，并停止所有关联的计时器。未能调用CancelFunc将造成父对象结束前或计时器被触发前子对象的泄露。使用go vet工具可以检查所有控制流路径上是否都使用了CancelFunc\n使用context的程序应遵循以下规则，以使各个包之间的接口保持一致，并启用静态分析工具来检查上下文传播：\n 不要将context存储在结构类型中，而是将context明确传递给需要它的每个函数。Context应该是第一个函数，通常命名为ctx。  func DoSomething(ctx context.Context, arg Arg) error { // ...use ctx... } 不要传递一个值为nil的context，即使一个函数允许这样做。如果你不确定Context的作用那就请传递context.TODO。 只在进程和API间传递请求范围数据时使用context值，不要用于将可选参数传递给函数。 同样的Context可以传递给运行在不同goroutine中的函数，Context是线程安全的。  2. Context接口 type Context interface { Done() \u0026lt;-chan struct{} Err() error Deadline() (deadline time.Time, ok bool) Value(key interface{}) interface{} } Context是一个接口，其定义非常的简单，只包含4个方法：\n  Done() \u0026lt;-chan struct{} Done()方法将一个channel作为取消信号返回给持有context的函数，当该channel被关闭（即Done()被调用），这些函数应该立即停止其工作并返回。\n  Err() error Err()返回一个Error，说明为什么取消context。如果Done()没有被调用，那么Err返回nil。\n  Deadline() (deadline time.Time, ok bool) Deadline()方法返回持有这个context的函数的预期结束时间。如果并没有设置deadline，那么返回的ok将被设置为false。","title":"golang中context包的使用"},{"content":" 在计算机领域，反射是指一类应用，它们能够自描述和自控制。也即是说，这类应用通过采用某种机制来实现对自己行为的描述和监测，并能根据自身行为的状态和结果，调整或修改应用所描述行为的状态和相关的语义。 反射（reflect）让我们能在运行期探知对象的类型信息和内存结构，这从一定程度上弥(mi)补了静态语言在动态行为上的不足。 反射（reflect）是在计算机程序运行时，访问，检查，修改它自身的一种能力，是元编程的一种形式。 Go语音提供了一种机制在运行时更新变量和检查它们的值、调用它们的方法和它们支持的内在操作，但是在编译时并不知道这些变量的具体类型。这种机制被称为反射。反射也可以让我们将类型本身作为第一类的值类型处理。\n 1. 为何我们需要反射？ fmt.Fprintf函数提供字符串格式化处理逻辑，它可以对任意类型的值格式化并打印，甚至支持用户自定义的类型。 让我们也来尝试实现一个类似功能的函数。为了简单起见，我们的函数只接收一个参数，然后返回和fmt.Sprint类似的格式化后的字符串。我们实现的函数名也叫Sprint。 这里我们使用switch类型分支来对不同的类型进行处理。\nfunc Sprint(x interface{}) string { type stringer interface { String() string } switch x := x.(type) { case stringer: return x.String() case string: return x case int: return strconv.Itoa(x) // ...similar cases for int16, uint32, and so on...  case bool: if x { return \u0026#34;true\u0026#34; } return \u0026#34;false\u0026#34; default: // array, chan, func, map, pointer, slice, struct  return \u0026#34;???\u0026#34; } } 但是我们如何处理其它类似[]float64、map[string][]string等类型呢？我们当然可以添加更多的测试分支，但是这些组合类型的数目基本是无穷的。还有如何处理url.Values等命名的类型呢？虽然类型分支可以识别出底层的基础类型是map[string][]string，但是它并不匹配url.Values类型，因为它们是两种不同的类型，而且switch类型分支也不可能包含每个类似url.Values的类型，这会导致对这些库的循环依赖。 没有一种方法来检查未知类型的表示方式，我们被卡住了，这就是我们为何需要反射的原因。\n2. reflect.Type和reflect.Values 2.1 interface{}和反射 接口值 概念上讲一个接口的值，由两部分组成，一个具体的类型和那个类型的值, 它们被称为接口的动态类型和动态值。在Go的概念模型中，一些提供每个类型信息的值被称为类型描述符，比如类型的名称和方法。在一个接口值中，类型部分代表与之相关类型的描述符。 下面4个语句中，变量w得到了3个不同的值（第一个和最后一个是相同的）\nvar w io.Writer w = os.Stdout w = new(bytes.Buffer) w = nil 进一步观察在每一个语句后的w变量的值和动态行为。\n 第一个语句定义了变量w:  var w io.Writer 在Go语言中，变量总是被一个定义明确的值初始化，即使接口类型也不例外。对于一个接口的零值就是它的类型和值的部分都是nil.如下图： 一个接口值基于它的动态类型被描述为空或非空，所以这是一个空的接口值。\n第二个语句将一个*os.File类型的值赋给变量w  w = os.Stdout 这个赋值过程调用了一个具体类型到接口类型的隐式转换，这和显式的使用io.Writer(os.Stdout)是等价的.这个接口值的动态类型被设为*os.Stdout指针的类型描述符，它的动态值持有os.Stdout的拷贝；这是一个代表处理标准输出的os.File类型变量的指针 调用一个包含*os.File类型指针的接口值的Write方法，使得(*os.File).Write方法被调用。这个调用输出“hello”。\nw.Write([]byte(\u0026#34;hello\u0026#34;)) 通常在编译期，我们不知道接口值的动态类型是什么，所以一个接口上的调用必须使用动态分配(即运行时分配)。因为不是直接进行调用，所以编译器必须把代码生成在类型描述符的方法Write上，然后间接调用那个地址。这个调用的接收者是一个接口动态值的拷贝：os.Stdout(参照上图)。效果和下面这个直接调用一样：\nos.Stdout.Write([]byte(\u0026#34;hello\u0026#34;)) // \u0026#34;hello\u0026#34; 第三个语句给接口值赋了一个*bytes.Buffer类型的值  w = new(bytes.Buffer) 现在动态类型是*bytes.Buffer并且动态值是一个指向新分配的缓冲区的指针。 Write方法的调用也使用了和之前一样的机制。\nw.Write([]byte(\u0026#34;hello\u0026#34;)) // write \u0026#34;hello\u0026#34; to the bytes.Buffers 这次类型描述符是*bytes.Buffer，所以调用了(*bytes.Buffer).Write方法，并且接收者是该缓冲区的地址。这个调用把字符串“hello”添加到缓冲区中。\n最后，第四个语句将nil赋给了接口值  w = nil 这个重置将它所有的部分都设为nil值，把变量w恢复到和它之前定义时相同的状态图。 interface及其动态类型，动态值的存在，是Golang中实现反射的前提，理解了接口的动态类型的动态值，就更容易理解反射。反射就是用来检测存储在接口变量内部动态类型，动态值的一种机制。\n2.2 类型（Type） 一个Type表示一个Go类型，它是一个接口:reflect.Type()。 函数reflect.TypeOf接受任意的interface{}类型，并返回对应动态类型的reflect.Type:\nt := reflect.TypeOf(3) // a reflect.Type fmt.Println(t.String()) // int fmt.Println(t) // int TypeOf(3)调用将3作为interface{}类型参数传入。按[2.1节](### 2.1 interface{}和反射)所述，将一个具体的值转为接口类型会有一个隐式的接口转换操作，它会创建一个包含两个信息的接口值：操作数的动态类型（这里是int）和它的动态的值（这里是3）。\n因为reflect.TypeOf返回的是一个接口的动态类型值，它总是返回具体的类型，因此下面的代码将打印“*os.File”而不是“io.Writer”.\nvar w io.Writer = os.Stdout fmt.Println(reflect.TypeOf(w)) // \u0026#34;*os.File\u0026#34; reflect.Type接口是满足fmt.Stringer接口的。因为打印动态类型值对于调试和日志是很有帮助的，fmt.Printf提供了一个简短的%T标志参数，内部使用reflect.TypeOf的结果输出。\nfmt.Printf(\u0026#34;%T\\n\u0026#34;, 3) // \u0026#34;int\u0026#34; 2.3 值（Value） 一个reflect.Value可以持有一个任意类型的值，函数reflect.ValueOf接受任意的interface{}类型，并返回对应动态类型的reflect。Value。 与reflect.TypeOf类似，reflect.ValueOf返回的结果也是对于具体的类型，但是reflect.Value也可以持有一个接口值。\nv := reflect.ValueOf(3) // a reflect.Value fmt.Println(v) // \u0026#34;3\u0026#34; fmt.Printf(\u0026#34;%v\\n\u0026#34;, v) // \u0026#34;3\u0026#34; fmt.Println(v.String()) // NOTE: \u0026#34;\u0026lt;int Value\u0026gt; 和reflect.Type 类似, reflect.Value 也满足 fmt.Stringer 接口, 但是除非 Value 持有的是字符串,否则 String 只是返回具体的类型. 相同, 使用 fmt 包的 %v 标志参数, 将使用 reflect.Values 的结果格式化.\n调用Value的Type方法将返回具体类型所对应的reflect.Type\nt := v.Type() fmt.Println(t.String()) // int 逆操作是调用 reflect.ValueOf 对应的 reflect.Value.Interface 方法. 它返回一个 interface{} 类型 表示 reflect.Value 对应类型的具体值:\nv := reflect.ValueOf(3) // a reflect.Value x := v.Interface() // an interface{} i := x.(int) // an int fmt.Printf(\u0026#34;%d\\n\u0026#34;, i) // \u0026#34;3\u0026#34; 一个 reflect.Value 和 interface{} 都能保存任意的值. 所不同的是, 一个空的接口隐藏了值对应的表示方式和所有的公开的方法, 因此只有我们知道具体的动态类型才能使用类型断言来访问内部的值(就像上面那样), 对于内部值并没有特别可做的事情. 相比之下, 一个 Value 则有很多方法来检查其内容, 无论它的具体类型是什么.\n2.4 再次尝试format.Any 我们使用 reflect.Value 的 Kind 方法来替代之前的类型switch. 虽然还是有无穷多的类型, 但是它们的kinds类型却是有限的: Bool, String 和 所有数字类型的基础类型; Array 和 Struct 对应的聚合类型; Chan, Func, Ptr, Slice, 和 Map 对应的引用类似; 接口类型; 还有表示空值的无效类型. (空的 reflect.Value 对应 Invalid 无效类型. reflect.Value.Kind()返回reflect.Value的基类型。即对应类型的底层表示。\nvar f func(string)int v := reflect.TypeOf(f) fmt.Println(v, v.Kind()) // \u0026#34;func(string)int\u0026#34; \u0026#34;int\u0026#34; v.Kind() == reflect.Func // true package format import ( \u0026#34;reflect\u0026#34; \u0026#34;strconv\u0026#34; ) // Any formats any value as a string. func Any(value interface{}) string { return formatAtom(reflect.ValueOf(value)) } // formatAtom formats a value without inspecting its internal structure. func formatAtom(v reflect.Value) string { switch v.Kind() { case reflect.Invalid: return \u0026#34;invalid\u0026#34; case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: return strconv.FormatInt(v.Int(), 10) case reflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64, reflect.Uintptr: return strconv.FormatUint(v.Uint(), 10) // ...floating-point and complex cases omitted for brevity... \tcase reflect.Bool: return strconv.FormatBool(v.Bool()) case reflect.String: return strconv.Quote(v.String()) case reflect.Chan, reflect.Func, reflect.Ptr, reflect.Slice, reflect.Map: return v.Type().String() + \u0026#34; 0x\u0026#34; + strconv.FormatUint(uint64(v.Pointer()), 16) default: // reflect.Array, reflect.Struct, reflect.Interface \treturn v.Type().String() + \u0026#34; value\u0026#34; } } 3. 三大法则 3.1 从interface{}变量可以反射出反射对象 当我们执行 reflect.ValueOf(1) 时，虽然看起来是获取了基本类型 int 对应的反射类型，但是由于 reflect.TypeOf、reflect.ValueOf 两个方法的入参都是 interface{} 类型，所以在方法执行的过程中发生了类型转换。\n因为Go 语言的函数调用都是值传递的，所以变量会在函数调用时进行类型转换。基本类型 int 会转换成 interface{} 类型，这也就是为什么第一条法则是从接口到反射对象。\n上面提到的 reflect.TypeOf 和 reflect.ValueOf 函数就能完成这里的转换，如果我们认为 Go 语言的类型和反射类型处于两个不同的世界，那么这两个函数就是连接这两个世界的桥梁。 请看以下例子：\nauthor := \u0026#34;Bob\u0026#34; fmt.Println(\u0026#34;TypeOf author:\u0026#34;, reflect.TypeOf(author)) // TypeOf author: string  fmt.Println(\u0026#34;ValueOf author:\u0026#34;, reflect.ValueOf(author)) // ValueOf author: Bob 有了变量的类型之后，我们可以通过 Method 方法获得类型实现的方法，通过 Field 获取类型包含的全部字段。对于不同的类型，我们也可以调用不同的方法获取相关信息：\n 结构体：获取字段的数量并通过下标和字段名获取字段 StructField； 哈希表：获取哈希表的 Key 类型； 函数或方法：获取入参和返回值的类型； … 总而言之，使用 reflect.TypeOf和 reflect.ValueOf 能够获取 Go 语言中的变量对应的反射对象。一旦获取了反射对象，我们就能得到跟当前类型相关数据和操作，并可以使用这些运行时获取的结构执行方法。  3.2 从反射对象可以获取interface{}变量 既然能够将接口类型的变量转换成反射对象，那么一定需要其他方法将反射对象还原成接口类型的变量，reflect包中的reflect.Value.Interface就能完成这项工作：\nv := reflect.ValueOf(3) // a reflect.Value x := v.Interface() // an interface{} i := x.(int) // an int fmt.Printf(\u0026#34;%d\\n\u0026#34;, i) // \u0026#34;3\u0026#34; reflect.ValueOf、reflect.TypeOf与Interface方法可以说是一个互逆的过程。reflect.ValueOf、reflect.TypeOf将interface{}转化为reflect对象，而Interface将一个reflect对象重新转化为一个接口值。 3.3 要修改反射对象，其值必须可设置 假如我们想要更新一个reflect.Value,那么它持有的值一定是可以被更新的，假设有如下代码：\ni := 1 v := reflect.ValueOf(i) v.SetInt(10) fmt.Println(i) // panic: reflect: reflect.flag.mustBeAssignable using unaddressable value 运行上述代码会导致程序崩溃并报出 “reflect: reflect.flag.mustBe Assignableusing unaddressable value” 错误，仔细思考一下就能够发现出错的原因：由于 Go 语言的函数调用都是传值的，所以我们得到的反射对象跟最开始的变量没有任何关系，那么直接修改反射对象无法改变原始变量，程序为了防止错误就会崩溃。\n想要修改原变量只能使用如下方法：\ni := 1 v := reflect.ValueOf(\u0026amp;i) v.Elem().SetInt(10) fmt.Println(i) reflect.Value.Elem()方法获取指针指向的变量\n4. reflect场景实践  动态调用函数(无参数)  type T struct {} func main() { name := \u0026#34;Do\u0026#34; t := \u0026amp;T{} reflect.ValueOf(t).MethodByName(name).Call(nil) } func (t *T) Do() { fmt.Println(\u0026#34;hello\u0026#34;) } 动态调用函数(有参数)  type T struct{} func main() { name := \u0026#34;Do\u0026#34; t := \u0026amp;T{} a := reflect.ValueOf(1111) b := reflect.ValueOf(\u0026#34;world\u0026#34;) in := []reflect.Value{a, b} reflect.ValueOf(t).MethodByName(name).Call(in) } func (t *T) Do(a int, b string) { fmt.Println(\u0026#34;hello\u0026#34; + b, a) } 处理返回值中的错误  返回值也是 Value 类型，对于错误，可以转为 interface 之后断言\ntype T struct{} func main() { name := \u0026#34;Do\u0026#34; t := \u0026amp;T{} ret := reflect.ValueOf(t).MethodByName(name).Call(nil) fmt.Printf(\u0026#34;strValue: %[1]v\\nerrValue: %[2]v\\nstrType: %[1]T\\nerrType: %[2]T\u0026#34;, ret[0], ret[1].Interface().(error)) } func (t *T) Do() (string, error) { return \u0026#34;hello\u0026#34;, errors.New(\u0026#34;new error\u0026#34;) } struct tag 解析  type T struct { A int `json:\u0026#34;aaa\u0026#34; test:\u0026#34;testaaa\u0026#34;` B string `json:\u0026#34;bbb\u0026#34; test:\u0026#34;testbbb\u0026#34;` } func main() { t := T{ A: 123, B: \u0026#34;hello\u0026#34;, } tt := reflect.TypeOf(t) for i := 0; i \u0026lt; tt.NumField(); i++ { field := tt.Field(i) if json, ok := field.Tag.Lookup(\u0026#34;json\u0026#34;); ok { fmt.Println(json) } test := field.Tag.Get(\u0026#34;test\u0026#34;) fmt.Println(test) } } 类型转换和赋值  type T struct { A int `newT:\u0026#34;AA\u0026#34;` B string `newT:\u0026#34;BB\u0026#34;` } type newT struct { AA int BB string } func main() { t := T{ A: 123, B: \u0026#34;hello\u0026#34;, } tt := reflect.TypeOf(t) tv := reflect.ValueOf(t) newT := \u0026amp;newT{} newTValue := reflect.ValueOf(newT) for i := 0; i \u0026lt; tt.NumField(); i++ { field := tt.Field(i) newTTag := field.Tag.Get(\u0026#34;newT\u0026#34;) tValue := tv.Field(i) newTValue.Elem().FieldByName(newTTag).Set(tValue) } fmt.Println(newT) } 通过 kind（）处理不同分支  func main() { a := 1 t := reflect.TypeOf(a) switch t.Kind() { case reflect.Int: fmt.Println(\u0026#34;int\u0026#34;) case reflect.String: fmt.Println(\u0026#34;string\u0026#34;) } } 判断实例是否实现了某接口  type IT interface { test1() } type T struct { A string } func (t *T) test1() {} func main() { t := \u0026amp;T{} ITF := reflect.TypeOf((*IT)(nil)).Elem() tv := reflect.TypeOf(t) fmt.Println(tv.Implements(ITF)) } Reference 《The Go Programmer Language》 Golang的反射reflect深入理解和示例\nGo 语言设计与实现\n《Go学习笔记 . 雨痕》反射\nGo Reflect 高级实践\nPackage reflect\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/golang%E4%B8%AD%E5%8F%8D%E5%B0%84reflect%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","summary":"在计算机领域，反射是指一类应用，它们能够自描述和自控制。也即是说，这类应用通过采用某种机制来实现对自己行为的描述和监测，并能根据自身行为的状态和结果，调整或修改应用所描述行为的状态和相关的语义。 反射（reflect）让我们能在运行期探知对象的类型信息和内存结构，这从一定程度上弥(mi)补了静态语言在动态行为上的不足。 反射（reflect）是在计算机程序运行时，访问，检查，修改它自身的一种能力，是元编程的一种形式。 Go语音提供了一种机制在运行时更新变量和检查它们的值、调用它们的方法和它们支持的内在操作，但是在编译时并不知道这些变量的具体类型。这种机制被称为反射。反射也可以让我们将类型本身作为第一类的值类型处理。\n 1. 为何我们需要反射？ fmt.Fprintf函数提供字符串格式化处理逻辑，它可以对任意类型的值格式化并打印，甚至支持用户自定义的类型。 让我们也来尝试实现一个类似功能的函数。为了简单起见，我们的函数只接收一个参数，然后返回和fmt.Sprint类似的格式化后的字符串。我们实现的函数名也叫Sprint。 这里我们使用switch类型分支来对不同的类型进行处理。\nfunc Sprint(x interface{}) string { type stringer interface { String() string } switch x := x.(type) { case stringer: return x.String() case string: return x case int: return strconv.Itoa(x) // ...similar cases for int16, uint32, and so on...  case bool: if x { return \u0026#34;true\u0026#34; } return \u0026#34;false\u0026#34; default: // array, chan, func, map, pointer, slice, struct  return \u0026#34;?","title":"Golang中反射reflect的基本使用"},{"content":"1. 按照Oh my zsh $ sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; 2. 配置Oh my zsh   将zsh设置为默认Shell (脚本的最后一般会问你是否切换)\nchsh -s /bin/zsh # 不需要使用root权限   更换主题\nvim ~/.zshrc 找到ZSH_THEME='robbyrussell', 更换为你想要使用的主题，可以在这里找到你想要的主题\n  安装插件\nvim ~/.zshrc 找到plugins=(), 添加插件名称，我这里添加的插件有：\nplugins=(git zsh-autosuggestions zsh-syntax-highlighting) git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions   完成\nsource ~/.zshrc # 启动zsh   3. 使用主题powerlevel10k 下载主题\ngit clone --depth=1 https://gitee.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k 打开你的~/.zshrc,将主题换为：powerlevel10k/powerlevel10k\n更改保存并使用主题\nsource ~/.zshrc 这时powerlevel10k会自动启动，询问你想要的配置 按照提示配置你想要的风格即可\n","permalink":"http://yangchnet.github.io/Dessert/posts/env/zsh%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE/","summary":"1. 按照Oh my zsh $ sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; 2. 配置Oh my zsh   将zsh设置为默认Shell (脚本的最后一般会问你是否切换)\nchsh -s /bin/zsh # 不需要使用root权限   更换主题\nvim ~/.zshrc 找到ZSH_THEME='robbyrussell', 更换为你想要使用的主题，可以在这里找到你想要的主题\n  安装插件\nvim ~/.zshrc 找到plugins=(), 添加插件名称，我这里添加的插件有：\nplugins=(git zsh-autosuggestions zsh-syntax-highlighting) git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions   完成\nsource ~/.zshrc # 启动zsh   3. 使用主题powerlevel10k 下载主题\ngit clone --depth=1 https://gitee.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k 打开你的~/.zshrc,将主题换为：powerlevel10k/powerlevel10k\n更改保存并使用主题\nsource ~/.zshrc 这时powerlevel10k会自动启动，询问你想要的配置 按照提示配置你想要的风格即可","title":"zsh的基本配置"},{"content":"1. Anaconda下载地址  官方下载地址：https://www.anaconda.com/distribution/#linux 清华大学镜像：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ # 选择你想安装的版本下载  2. 安装 $ bash Anaconda3-2021.05-Linux-x86_64.sh # 中间会有一些选择，按照自己的意愿选择即可  关闭并重新打开你的终端来激活conda\n3. 使用  更新自己  # 更新conda conda update conda conda update anaconda  更新时出现了ValueError: check_hostname requires server_hostname错误，经查发现是代理的问题，可尝试关闭或开启代理再次尝试\n  对包的操作\n 更新包  conda update --all # 更新所有包  安装包  conda install [包名]   对环境的操作\n 创建环境  conda create --name [环境名字] # 使用默认的Python版本  激活环境  conda activate [环境名字]  退出环境  conda deactivate  查看环境名字  conda env list # conda info -e  删除环境中某个包  conda remove [环境名] [包名]  修改环境名字  conda create -n [新环境名] --clone [旧环境名] # 克隆旧的 conda remove -n [旧环境名] # 删除旧的  删除环境  conda remove -n [环境名] --all   4. 配置 #1、换源,添加清华源 方法一 vim ~/.condarc #添加以下内容 channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaults show_channel_urls: true #2、换源,添加清华源 方法二 onda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes #换回默认源 conda config --remove-key channels ","permalink":"http://yangchnet.github.io/Dessert/posts/python/deepin15%E5%AE%89%E8%A3%85anaconda/","summary":"1. Anaconda下载地址  官方下载地址：https://www.anaconda.com/distribution/#linux 清华大学镜像：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ # 选择你想安装的版本下载  2. 安装 $ bash Anaconda3-2021.05-Linux-x86_64.sh # 中间会有一些选择，按照自己的意愿选择即可  关闭并重新打开你的终端来激活conda\n3. 使用  更新自己  # 更新conda conda update conda conda update anaconda  更新时出现了ValueError: check_hostname requires server_hostname错误，经查发现是代理的问题，可尝试关闭或开启代理再次尝试\n  对包的操作\n 更新包  conda update --all # 更新所有包  安装包  conda install [包名]   对环境的操作\n 创建环境  conda create --name [环境名字] # 使用默认的Python版本  激活环境  conda activate [环境名字]  退出环境  conda deactivate  查看环境名字  conda env list # conda info -e  删除环境中某个包  conda remove [环境名] [包名]  修改环境名字  conda create -n [新环境名] --clone [旧环境名] # 克隆旧的 conda remove -n [旧环境名] # 删除旧的  删除环境  conda remove -n [环境名] --all   4.","title":"Deepin15安装Anaconda"},{"content":"一、野生goroutine的问题  引言： 毋庸置疑，golang原生的goroutine极大降低了程序员使用并发编程的代价，程序员们不需要再去关心如何实现接口、如何继承Thread类等多余的操作，只需要简简单单的go, 就可以开启一个并发的协程。但这种简单的使用方式同时也带来一些问题，这些goroutine不再受我们控制了，它们在运行时可能会发生任何错误或意外，而我们无法得知或去处理这些意外。我们将启动后不再受主进程控制的goroutine称为野生goroutine，本节将介绍野生goroutine存在的一些问题并介绍一些简单的解决方法。\n 1. goroutine中panic无法恢复 正常的函数中panic的recover\nimport ( \u0026#34;fmt\u0026#34; ) func main(){ defer func(){ if err := recover(); err != nil{ fmt.Println(err) } }() var bar = []int{1} fmt.Println(bar[1]) } reflect: slice index out of range goroutine中panic的恢复\nfunc main(){ defer func(){ if err := recover(); err != nil { // 在这里使用recover(),不能捕获panic \tfmt.Println(\u0026#34;catch you, bad guy\u0026#34;) } }() go func(){ fmt.Println(\u0026#34;I\u0026#39;m in a goroutine\u0026#34;) panic(\u0026#34;come to catch me\u0026#34;) }() time.Sleep(10 * time.Second) // 主进程做其他事 } I'm in a goroutine panic: come to catch me goroutine 6 [running]: main.foo.func2() /home/lc/Workspace/lab/main/main.go:22 +0x95 created by main.foo /home/lc/Workspace/lab/main/main.go:15 +0x5b Process finished with the exit code 2 上面的程序在goroutine中触发了一个panic，现在假设foo()主进程是我们的服务，我们在服务的main()中使用recover()尝试捕获程序中发生的panic，但程序运行后发现，goroutine中触发的panic没有被捕获，程序直接退出，代表我们的服务挂掉了。显然在正式环境中，我们不可能容忍这样的事情发生，因此需要对goroutine中触发的panic进行捕获处理。\n显然最简单的方法是在goroutine中添加recover(), 进行异常捕获，如下：\nfunc main(){ go func(){ defer func(){ if err := recover(); err != nil { // 在这里使用recover(),不能捕获panic  fmt.Println(\u0026#34;catch you, bad guy\u0026#34;) } }() fmt.Println(\u0026#34;I\u0026#39;m in a goroutine\u0026#34;) panic(\u0026#34;come to catch me\u0026#34;) }() time.Sleep(10 * time.Second) // 主进程做其他事 } I'm in a goroutine catch you, bad guy Process finished with the exit code 0 这时程序正常退出，服务没有挂掉。但每次使用goroutine都要加recover()稍显麻烦。我们可以使用函数式编程的思想来对goroutine进行一下封装：\nfunc Go(f func()){ go func(){ defer func(){ if err := recover(); err != nil{ fmt.Println(\u0026#34;catch you, bad guy\u0026#34;) } }() f() }() } func main(){ Go(func(){ fmt.Printf(\u0026#34;I\u0026#39;m in a goroutine\u0026#34;) panic(\u0026#34;come to catch me\u0026#34;) }) time.Sleep(10 * time.Second) // 主进程做其他事 } I'm in a goroutine catch you, bad guy Process finished with the exit code 0 显然，这样封装过后我们捕获到了panic，但实际上我们仍然是在goroutine内部捕获的panic，A goroutine cannot recover from a panic in another goroutine.\n2. goroutine的泄露问题 2.1 什么是goroutine泄露 如果你启动了一个goroutine，但并没有符合预期的退出，而是直到程序结束，此goroutine才退出，这种情况就是goroutine泄露。当goroutine泄露发生时，该goroutine所占用的内存不能得到释放，可用内存持续减少，最终将导致系统崩溃。\npackage main import \u0026#34;fmt\u0026#34; func main(){ for{ go sayHello() } } func sayHello(){ for{ fmt.Println(\u0026#34;hello\u0026#34;) } } 以上代码是一个最简单的goroutine泄露场景，所有的goroutine都没有退出，而是一直在后台执行。在我的电脑上执行以上代码后，内存占用达到98%。\n2.2 goroutine泄露发生的原因 2.2.1 channel导致的泄露 1. 只读不发\nfunc leak() { ch := make(chan int) go func() { val := \u0026lt;-ch fmt.Println(\u0026#34;We received a value:\u0026#34;, val) }() } 在这个函数中，leak创建了一个goroutine，其在从ch中读取一个值之前一直阻塞，但leak并没有往ch中发送值，因此goroutine将一直阻塞，在函数结束前不能得到释放。\n解决方案\n记得发送\nfunc unleak() { ch := make(chan int) go func() { val := \u0026lt;-ch fmt.Println(\u0026#34;We received a value:\u0026#34;, val) }() ch \u0026lt;- 1 } 2. 只发不读\nfunc gen(nums ...int) \u0026lt;-chan int { out := make(chan int) go func() { for _, n := range nums { out \u0026lt;- n } close(out) }() return out } func main() { defer func() { time.Sleep(time.Second) fmt.Println(\u0026#34;the number of goroutines: \u0026#34;, runtime.NumGoroutine()) }() _ = gen(2, 3) } the number of goroutines: 2 在这段代码中，gen向channel中发送数据，但由于没有对channel做接收，因此这个channel将会被阻塞，从而goroutine也不能正常退出，发生了goroutine泄露。\n解决方案\n利用channel向接收者发送停止消息,goroutine中使用select多路复用接收退出信号。但如果我们想要退出多个goroutine怎么办呢。这时一种可行的方法是使用channel的广播机制向所有goroutine广播消息。\n当一个被关闭的channel中已经发送的数据都被成功接收后，后续的接收操作将不再阻塞，它们会立即返回一个零值。将这个机制拓展一下，不要向channel发送值，而是用关闭一个channel来进行广播。\nfunc gen(done chan struct{}, nums ...int) \u0026lt;-chan int { out := make(chan int) go func() { defer close(out) for _, n := range nums { select{ case out \u0026lt;- n: case \u0026lt;- done: return } } }() return out } func main() { defer func() { time.Sleep(time.Second) fmt.Println(\u0026#34;the number of goroutines: \u0026#34;, runtime.NumGoroutine()) }() done := make(chan struct{}) defer close(done) gen(done, 2, 3) gen(done, 2, 3) // 调用两次，代表创建多个goroutine } the number of goroutines: 1 3. select操作在所有case上阻塞\nfunc fibonacci(c, quit chan int) { x, y := 0, 1 for{ select { case c \u0026lt;- x: x, y = y, x+y case \u0026lt;-quit: fmt.Println(\u0026#34;quit\u0026#34;) return } } } func main() { defer func() { time.Sleep(time.Second) fmt.Println(\u0026#34;the number of goroutines: \u0026#34;, runtime.NumGoroutine()) }() c := make(chan int) quit := make(chan int) go fibonacci(c, quit) for i := 0; i \u0026lt; 10; i++{ fmt.Println(\u0026lt;- c) } // close(quit) } the number of goroutines: 2 在上面的代码中，我们用一个独立的goroutine作为斐波那契数列的生成器，但是若在主函数的最后忘记向quit发送或关闭quit，那么显然fibonacci将一直运行到main退出，造成goroutine泄露。\n解决方案\n 使用defer或在程序的最后往quit中发送或关闭quit 使用context包  func fibonacci(c chan int, ctx context.Context) { x, y := 0, 1 for{ select { case c \u0026lt;- x: x, y = y, x+y case \u0026lt;-ctx.Done(): fmt.Println(\u0026#34;quit\u0026#34;) return } } } func main() { defer func() { time.Sleep(time.Second) fmt.Println(\u0026#34;the number of goroutines: \u0026#34;, runtime.NumGoroutine()) }() ctx, cancel := context.WithCancel(context.Background()) defer cancel() c := make(chan int) go fibonacci(c, ctx) for i := 0; i \u0026lt; 10; i++{ fmt.Println(\u0026lt;- c) } } the number of goroutines: 1 context.WithCancel接收一个context.Context作为参数，返回一个附带Done channel的concext.Context的拷贝和一个cancel函数，当cancel被调用时，附带的Done channel就会被关闭。goroutine中通过检查ctx.Done()，就可以得知是否应该退出。\n4. nil channel 向nil channel发送和接收数据都将会导致阻塞。这种情况可能在我们定义 channel 时忘记初始化的时候发生。\nfunc main() { defer func() { time.Sleep(time.Second) fmt.Println(\u0026#34;the number of goroutines: \u0026#34;, runtime.NumGoroutine()) }() var ch chan int go func() { \u0026lt;-ch // ch \u0026lt;- 1 \t}() } the number of goroutines: 2 无论是向nil channel中发送还是接受，都会导致阻塞\n解决方案\n不要忘记初始化\n5. 死循环 同2.1小节中的情况\n2.2.2 同步机制导致的泄露 1. 锁导致的goroutine泄露\nfunc main() { total := 0 defer func() { time.Sleep(time.Second) fmt.Println(\u0026#34;the number of goroutines: \u0026#34;, runtime.NumGoroutine()) }() var mutex sync.Mutex for i := 0; i \u0026lt; 2; i++ { go func() { mutex.Lock() total += 1 }() } } 在这段代码中，创建了两个goroutine，两个goroutine都要对total进行独占访问，但由于第一个goroutine没有解锁，导致第二个goroutine一直阻塞。\n解决方案\nLock()之后马上defer Unlock()\n2. WaitGroup导致的内存泄露\nfunc handle() { var wg sync.WaitGroup wg.Add(4) go func() { wg.Done() }() go func() { wg.Done() }() go func() { wg.Done() }() wg.Wait() } func main() { defer func() { time.Sleep(time.Second * 2) fmt.Println(\u0026#34;the number of goroutines: \u0026#34;, runtime.NumGoroutine()) }() go handle() } the number of goroutines: 2 在以上代码中，设置并发任务数为4，但其实只有3个任务，所以wg.Wait永远不可能满足。handle将一直被阻塞。\n解决方案\n建议不要一次性设置任务数量，尽量在任务启动时通过wg.Add(1)的方式来增加任务数。\n  使用原生goroutine还存在一个问题，就是在大规模使用时会导致频繁的资源创建与回收，导致性能下降。鉴于以上种种问题，我们应该有某种goroutine管理的机制，能够使得goroutine在panic时不会影响到主进程，在发生长时间阻塞或死锁时能够被动退出，在要用到大量并发协程时能够做到资源复用\u0026hellip; 由此我们引出了——池。\n 二、池的思想  引言：机器学习中的池化，指的是将某一大小矩阵中的数据经过一定的运算后变成一个更小的矩阵，甚至直接变成一个值，这种池化可有效减小数据量并保留数据特征。而在程序设计中的池化，则是另外一种截然不同的思想，是指通过对资源的有效管理、复用，达到节约资源、提升性能的目的。本小节介绍了池化的基本思想、阐述了一些基本概念。\n 1. 线程池基础 1.1 野生线程 在我们平常的开发中，经常会有用到多线程的场景，合理利用多线程可有效利用CPU的多核结构，提高程序的执行效率。有这样一种线程：我们利用其完成一些工作，但只是将工作交给这个线程，该线程并不保证完成任务，也不保证正常退出，并且在线程开始运行后我们无法对其进行控制。这种状态可称为：野生线程，意为其已经不受控制，在内存中自由运行。\n这种线程可能带来一系列问题：\n 频繁申请/销毁线程，可能带来巨大的额外消耗 当内存中存在较多的野生线程，会导致过分调度，降低系统性能 不能正常退出的线程会导致内存泄露 系统无法合理管理内部的资源分布，会降低系统的稳定性 ……  鉴于以上野生线程带来的问题，我们需要一种方式将其管理起来，使其从野生的线程变成\u0026quot;家养\u0026quot;的线程。\n1.2 什么是线程池  池化：池化是一种将资源统一进行管理，从而最大化收益并最小化风险的思想。\n 线程池维护若干个线程，在总体上控制线程的数量，具体上控制线程的创建、销毁等生命周期，系统可通过申请线程池中的线程异步的完成某个任务。线程池通过对线程的管理实现对资源的有效利用，避免系统资源浪费或内存泄露等问题。\n1.3 使用线程池的好处  线程池中的线程可反复利用，减少了线程创建和销毁的开销 任务无需等待线程创建即可开始运行，提高了系统响应速度 通过设置合理的线程池线程数，可有效避免资源使用不当，资源浪费 对线程运行进行有效的监视与管理  通俗易懂的讲，如果将线程比作完成任务的人，那么线程池就像一个专门管理这些人的部门。当我有任务到来时，直接把任务交给该部门，而不用自己再去找人来完成任务。\n2. 线程池的工作机制 2.1 线程池模型 线程池的内部实际上可以看做是生产者消费者模型，二者并不直接关联，通过任务队列进行交互，从而可以有效的缓冲任务，复用线程。\n在线程池模型中，扮演生产者角色的是任务管理部分，其接受提交的任务，并判断任务应如何处理：\n 直接申请线程执行该任务 缓冲到队列中等待线程执行 直接拒绝该任务  线程管理部分是消费者，线程被统一维护在线程池中，当有任务请求到来时，某一线程被分配去执行这个任务，执行完成后继续或许新的任务来执行，最终当线程获取不到任务时，线程就被回收以节省系统资源。\n2.2 线程池的状态 线程池一方面维护自身的状态，另一方面管理线程和任务，使二者良好的结合从而执行并行任务。 线程池的状态有5种：\n   运行状态 状态描述     RUNNING 能接受新提交的任务，并且也能处理阻塞队列中的任务   SHUTDOWN 关闭状态，不再接受新提交的任务，但可以继续处理阻塞队列中已保存的任务   STOP 不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程   TIDYING 所有的任务都已终止，有效线程数为0   TERMINATED 在terminated()方法执行后进入该状态   其生命周期转换如下图所示：     sequenceDiagram RUNNING-\u0026gt;\u0026gt;SHUTDOWN:shutdown() RUNNING-\u0026gt;\u0026gt;STOP: shutdownNow() SHUTDOWN-\u0026gt;\u0026gt;TIDYING:所有任务都已完成，阻塞队列为空，工作线程数为0 STOP-\u0026gt;\u0026gt;TIDYING:线程池中工作线程数为0 TIDYING-\u0026gt;\u0026gt;TERMINATED: terminated() 2.3 任务执行机制 2.3.1 任务调度 任务调度是线程池的主要入口，用户提交任务后，这部分将决定任务如何执行。 任务调度的流程图如下：  提交任务后，首先检测运行池状态，若不是RUNNING，则直接拒绝，线程池要保证在RUNNING状态下执行任务。 如果线程数小于核心数，说明系统还没有被充分利用，则可添加线程并执行。若线程数大于核心数，此时再一味增加线程数只会带来调度开销，则将任务放入阻塞队列。 若阻塞队列未满，则将任务放入阻塞队列等待执行，否则进行下一步。 若线程数小于最大线程数，可添加工作线程执行，若线程数已大于等于最大线程数，此时直接拒绝任务，不予执行。  2.3.2 任务缓冲 线程池的本质是对任务和线程的管理，要让二者分别独立运行。如果不使用线程池，一个任务即一个线程，对任务与线程的管理都将陷入混乱。要分别对任务和线程进行管理，则需要将二者进行解耦，不让二者直接关联。线程池以消费者-生产者模型，通过一个阻塞队列实现二者的解耦。阻塞队列缓存任务，工作线程从阻塞队列中获取任务。\n阻塞队列(BlockingQueue)是一个支持两个附加操作的队列。这两个附加的操作是：\n 在队列为空时，获取元素的线程会等待队列变为非空。 当队列满时，存储元素的线程会等待队列可用。 阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 其工作模式如下：   2.3.3 任务申请 从上文任务调度部分可知，任务的执行有两种可能：\n 任务直接由新创建的线程执行 线程从任务队列中获取任务然后执行，执行完任务的空闲进行再次去队列中申请任务执行 第一种情况仅在线程初始创建时出现，多数情况下是第二种情况  线程要从任务缓存模块中不断获取任务执行，定义getTask方法来帮助线程从阻塞队列中获取任务，实现线程管理模块和任务管理模块的通信。其执行流程如下： 这里进行了多次判断，目的在于控制线程的数量，使其符合线程池的状态。如果线程池现在不应该持有那么多线程，则会返回null值。工作线程Worker会不断接收新任务去执行，而当工作线程Worker接收不到任务的时候，就会开始被回收。\n2.3.4 任务拒绝 任务拒绝模块是线程池的保护部分。当线程池的任务缓存队列已满，并且线程数达到设定的最大值，就需要拒绝到来的任务，保护线程池。\n2.4 Worker线程管理 2.4.1 Worker线程 线程池为了掌握线程的状态并维护线程的生命周期，设计了线程池内的工作线程Worker。\nWorker持有一个线程thread和一个初始化的任务firstTask。thread是在Worker创建时创建的线程，可用来执行任务；firstTask保存传入的第一个任务，这个任务可以有也可以为空。如果此值非空，那么Worker会先执行这个任务，再去获取任务执行；如果此值为空，那么Worker直接去阻塞队列中获取任务执行。 线程池需要管理线程的生命周期，需要在线程长时间不运行的时候进行回收。线程池使用一张Hash表去持有线程的引用，这样可以通过添加引用、移除引用这样的操作来控制线程的生命周期。这个时候重要的就是如何判断线程是否在运行。\n​Worker是通过继承AQS，使用AQS来实现独占锁这个功能。没有使用可重入锁ReentrantLock，而是使用AQS，为的就是实现不可重入的特性去反应线程现在的执行状态。\n1.lock方法一旦获取了独占锁，表示当前线程正在执行任务中。 2.如果正在执行任务，则不应该中断线程。 3.如果该线程现在不是独占锁的状态，也就是空闲的状态，说明它没有在处理任务，这时可以对该线程进行中断。 4.线程池在执行shutdown方法或tryTerminate方法时会调用interruptIdleWorkers方法来中断空闲的线程，interruptIdleWorkers方法会使用tryLock方法来判断线程池中的线程是否是空闲状态；如果线程是空闲状态则可以安全回收。\n在线程回收过程中就使用到了这种特性，回收过程如下图所示：\n2.4.2 Worker线程增加 增加线程是通过线程池中的addWorker方法，该方法的功能就是增加一个线程，该方法不考虑线程池是在哪个阶段增加的该线程，这个分配线程的策略是在上个步骤完成的，该步骤仅仅完成增加线程，并使它运行，最后返回是否成功这个结果。addWorker方法有两个参数：firstTask、core。firstTask参数用于指定新增的线程执行的第一个任务，该参数可以为空；core参数为true表示在新增线程时会判断当前活动线程数是否少于corePoolSize，false表示新增线程前需要判断当前活动线程数是否少于maximumPoolSize，其执行流程如下图所示： 2.4.3 Worker线程回收 线程池中线程的销毁依赖JVM自动的回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程被JVM回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可。Worker被创建出来后，就会不断地进行轮询，然后获取任务去执行，核心线程可以无限等待获取任务，非核心线程要限时获取任务。当Worker无法获取到任务，也就是获取的任务为空时，循环会结束，Worker会主动消除自身在线程池内的引用。\n事实上，将线程引用移出线程池就已经结束了线程销毁的部分。但由于引起线程销毁的可能性有很多，线程池还要判断是什么引发了这次销毁，是否要改变线程池的现阶段状态，是否要根据新状态，重新分配线程。\n2.4.4 Worker线程执行任务 在Worker类中的run方法调用了runWorker方法来执行任务，runWorker方法的执行过程如下：\n1.while循环不断地通过getTask()方法获取任务。 2.getTask()方法从阻塞队列中取任务。 3.如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态。 4.执行任务。 5.如果getTask结果为null则跳出循环，执行processWorkerExit()方法，销毁线程。 三、现有goroutine池的解决方案  引言： 本小节简单分析了GitHub上五个goroutine池实现的基本思想，其中部分代码做了简化。\n 1. wazsmwazsm/mortar（★74） 简单介绍\n创建一个容量为 N 的池, 在池容量未满时, 每塞入一个任务（生产任务）, 任务池开启一个 worker (建立协程) 去处理任务（消费任务）。 当任务池容量赛满，每塞入一个任务（生产任务）, 任务会被已有的 N 个 worker 抢占执行（消费任务），达到协程限制的功能。但worker创建后不会回收，除非将整个pool撤销。\n结构\ntype Task struct { Handler func(v ...interface{}) // 函数签名 \tParams []interface{} // 参数 } // Pool task pool type Pool struct { capacity uint64 // 池的容量，自行制定 \trunningWorkers uint64 // 正在运行的worker \tstatus int64 // 池的状态 \tchTask chan *Task // 任务队列，worker从中获取任务 \tPanicHandler func(interface{}) // 自定义的PanicHandler，防止因某个goroutine发生panic而导致服务崩溃。 \tsync.Mutex // 全局锁 } 核心代码\n// Put put a task to pool func (p *Pool) Put(task *Task) error { p.Lock() // 加锁，防止数据竞争 \tdefer p.Unlock() if p.status == STOPED { // 检查池是否还在运行 \treturn ErrPoolAlreadyClosed } // run worker \tif p.GetRunningWorkers() \u0026lt; p.GetCap() { // worker数量尚未达到容量限制，新建一个worker \tp.run() // 这里存在一个问题，若池中有空闲的worker，也会创建一个新的worker，这个worker创建后也会空闲，存在资源浪费 \t} // send task \tif p.status == RUNNING { p.chTask \u0026lt;- task // 向任务队列发送任务 \t} return nil } func (p *Pool) run() { p.incRunning() // worker计数+1  go func() { defer func() { // 防止因某个goroutine发生panic而导致服务崩溃 \tp.decRunning() // worker计数减1 \tif r := recover(); r != nil { if p.PanicHandler != nil { p.PanicHandler(r) // 自定义了panic后操作 \t} else { log.Printf(\u0026#34;Worker panic: %s\\n\u0026#34;, r) // panic后默认操作 \t} } p.checkWorker() // 检查是否还有worker在运行，保持最少一个worker \t}() for { // worker 保持运行 \tselect { case task, ok := \u0026lt;-p.chTask: // 接收task \tif !ok { // 当任务队列被关闭是goroutine退出 \treturn } task.Handler(task.Params...) // 执行任务 \t} } }() } 在Put函数中，每当放入一个新任务之前，都会直接创建一个新的worker，这显然不甚合理，因为此时池中可能存在空闲的worker，应设法利用这些空闲的worker而不是创建新的worker。\n尝试优化\n一种优化思路是：在Pool对象中增加chIdle chan struct{}字段，作为空闲标志。放入任务之前首先检查chIdle是否存在空闲标志，若存在，则说明此时存在空闲worker，不需创建新的worker，只需将task放入即可。 而若chIdle中不存在空闲标志，则检查worker数量是否达到限制，然后创建新的worker。 优化后的核心代码如下：\ntype Pool struct { capacity uint64 runningWorkers uint64 status int64 chTask chan *Task chIdle chan struct{} PanicHandler func(interface{}) sync.Mutex } func (p *Pool) Put(task *Task) error { p.Lock() defer p.Unlock() if p.status == STOPED { return ErrPoolAlreadyClosed } select { case \u0026lt;-p.chIdle: default: if p.GetRunningWorkers() \u0026lt; p.GetCap() { p.run() } } // send task \tif p.status == RUNNING { p.chTask \u0026lt;- task } return nil } func (p *Pool) run() { p.incRunning() p.chIdle \u0026lt;- struct{}{} go func() { defer func() { p.decRunning() if r := recover(); r != nil { if p.PanicHandler != nil { p.PanicHandler(r) } else { log.Printf(\u0026#34;Worker panic: %s\\n\u0026#34;, r) } } p.checkWorker() // check worker avoid no worker running \t}() for { select { case task, ok := \u0026lt;-p.chTask: if !ok { return } task.Handler(task.Params...) p.chIdle \u0026lt;- struct{}{} } } }() } 优化前后banchmark测试数据对比如下：\n优化前：BenchmarkPut-4 815876\t1460 ns/op\t0 B/op\t0 allocs/op 优化后：BenchmarkPut-4 948046\t1323 ns/op\t0 B/op\t0 allocs/op 优化前: BenchmarkPutTimelife-4 1000000\t1287 ns/op\t0 B/op\t0 allocs/op 优化后: BenchmarkPutTimelife-4 930015\t1356 ns/op\t0 B/op\t0 allocs/op 优化前: BenchmarkPoolPutSetTimes-4 1\t1433118500 ns/op\t12104 B/op\t52 allocs/op 优化后: BenchmarkPoolPutSetTimes-4 1\t1474117300 ns/op\t8504 B/op\t42 allocs/op 优化前: BenchmarkPoolTimeLifeSetTimes-4 1\t1277515600 ns/op\t10208 B/op\t47 allocs/op 优化后: BenchmarkPoolTimeLifeSetTimes-4 1\t1279696300 ns/op\t7928 B/op\t39 allocs/op 基本使用\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;github.com/wazsmwazsm/mortar\u0026#34; ) func main() { // 创建容量为 10 的任务池 \tpool, err := mortar.NewPool(10) if err != nil { panic(err) } wg := new(sync.WaitGroup) for i := 0; i \u0026lt; 1000; i++ { wg.Add(1) // 创建任务 \ttask := \u0026amp;mortar.Task{ Handler: func(v ...interface{}) { wg.Done() fmt.Println(v) }, } // 添加任务函数的参数 \ttask.Params = []interface{}{i, i * 2, \u0026#34;hello\u0026#34;} // 将任务放入任务池 \tpool.Put(task) } wg.Add(1) // 再创建一个任务 \tpool.Put(\u0026amp;mortar.Task{ Handler: func(v ...interface{}) { wg.Done() fmt.Println(v) }, Params: []interface{}{\u0026#34;hi!\u0026#34;}, // 也可以在创建任务时设置参数 \t}) wg.Wait() // 安全关闭任务池（保证已加入池中的任务被消费完） \tpool.Close() // 如果任务池已经关闭, Put() 方法会返回 ErrPoolAlreadyClosed 错误 \terr = pool.Put(\u0026amp;mortar.Task{ Handler: func(v ...interface{}) {}, }) if err != nil { fmt.Println(err) // print: pool already closed \t} } 总结 goroutine的复用很好的减小了大批量异步任务中的内存分配与垃圾回收压力，但不能回收的goroutine可能在任务波峰过去后成为内存浪费（不撤销池的情况下），适合用于长期大规模执行并发任务的情况下。\n2. go-playground/pool（★614） 简单介绍 提供了LimitedPool,UnLimitedPool两种池，分别可以创建(worker数量)有限的worker和无限的worker；提供了Unit task和batch Task两种任务模式，分别适用于单次偶发任务及多次重复任务。LimitedPool,UnLimitedPool都实现了Pool接口。可长期保持运行。\n系统结构\ntype Pool interface { Queue(fn WorkFunc) WorkUnit // 传入要执行的任务，立即开始执行 \tReset() // 重新初始化一个池 \tCancel() // 取消所有未在运行的任务 \tClose() // 清除所有池数据并取消所有未提交的任务 \tBatch() Batch // 创建批量任务 } 核心代码\n// passing work and cancel channels to newWorker() to avoid any potential race condition // betweeen p.work read \u0026amp; write func (p *limitedPool) newWorker(work chan *workUnit, cancel chan struct{}) { go func(p *limitedPool) { var wu *workUnit defer func(p *limitedPool) { if err := recover(); err != nil { // ... \t} }(p) var value interface{} var err error for { select { case wu = \u0026lt;-work: // possible for one more nilled out value to make it \t// through when channel closed, don\u0026#39;t quite understand the why \tif wu == nil { continue } // support for individual WorkUnit cancellation \t// and batch job cancellation \tif wu.cancelled.Load() == nil { value, err = wu.fn(wu) wu.writing.Store(struct{}{}) // need to check again in case the WorkFunc cancelled this unit of work \t// otherwise we\u0026#39;ll have a race condition \tif wu.cancelled.Load() == nil \u0026amp;\u0026amp; wu.cancelling.Load() == nil { wu.value, wu.err = value, err // who knows where the Done channel is being listened to on the other end \t// don\u0026#39;t want this to block just because caller is waiting on another unit \t// of work to be done first so we use close \tclose(wu.done) } } case \u0026lt;-cancel: return } } }(p) } // Queue queues the work to be run, and starts processing immediately func (p *limitedPool) Queue(fn WorkFunc) WorkUnit { w := \u0026amp;workUnit{ done: make(chan struct{}), fn: fn, } go func() { p.m.RLock() if p.closed { w.err = \u0026amp;ErrPoolClosed{s: errClosed} if w.cancelled.Load() == nil { close(w.done) } p.m.RUnlock() return } p.work \u0026lt;- w p.m.RUnlock() }() return w } 基本使用\n Per Unit Work\n package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;gopkg.in/go-playground/pool.v3\u0026#34; ) func main() { p := pool.NewLimited(10) defer p.Close() user := p.Queue(getUser(13)) other := p.Queue(getOtherInfo(13)) user.Wait() if err := user.Error(); err != nil { // handle error \t} // do stuff with user \tusername := user.Value().(string) fmt.Println(username) other.Wait() if err := other.Error(); err != nil { // handle error \t} // do stuff with other \totherInfo := other.Value().(string) fmt.Println(otherInfo) } func getUser(id int) pool.WorkFunc { return func(wu pool.WorkUnit) (interface{}, error) { // simulate waiting for something, like TCP connection to be established \t// or connection from pool grabbed \ttime.Sleep(time.Second * 1) if wu.IsCancelled() { // return values not used \treturn nil, nil } // ready for processing...  return \u0026#34;Joeybloggs\u0026#34;, nil } } func getOtherInfo(id int) pool.WorkFunc { return func(wu pool.WorkUnit) (interface{}, error) { // simulate waiting for something, like TCP connection to be established \t// or connection from pool grabbed \ttime.Sleep(time.Second * 1) if wu.IsCancelled() { // return values not used \treturn nil, nil } // ready for processing...  return \u0026#34;Other Info\u0026#34;, nil } }  Batch Work\n package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;gopkg.in/go-playground/pool.v3\u0026#34; ) func main() { p := pool.NewLimited(10) defer p.Close() batch := p.Batch() // for max speed Queue in another goroutine \t// but it is not required, just can\u0026#39;t start reading results \t// until all items are Queued.  go func() { for i := 0; i \u0026lt; 10; i++ { batch.Queue(sendEmail(\u0026#34;email content\u0026#34;)) } // DO NOT FORGET THIS OR GOROUTINES WILL DEADLOCK \t// if calling Cancel() it calles QueueComplete() internally \tbatch.QueueComplete() }() for email := range batch.Results() { if err := email.Error(); err != nil { // handle error \t// maybe call batch.Cancel() \t} // use return value \tfmt.Println(email.Value().(bool)) } } func sendEmail(email string) pool.WorkFunc { return func(wu pool.WorkUnit) (interface{}, error) { // simulate waiting for something, like TCP connection to be established \t// or connection from pool grabbed \ttime.Sleep(time.Second * 1) if wu.IsCancelled() { // return values not used \treturn nil, nil } // ready for processing...  return true, nil // everything ok, send nil, error if not \t} } 总结 功能齐全，但好像用起来不太舒服。\n3. ivpusic/grpool（★634） 简单介绍 用户可以提交task。 Dispatcher接受task，并将其发送给第一个可用的worker。 当worker完成处理工作时，将返回到workers pool。 worker数量和task队列大小是可配置的。\n整个程序以一个调度器为核心，Pool通过调度器来调度worker，worker可复用，但任务结束时worker不会自动回收，不适合长期运行。只能传递无参任务。\n系统结构\n// Gorouting instance which can accept client jobs type worker struct { workerPool chan *worker // 工人池 \tjobChannel chan Job // 这个worker独有的任务队列 \tstop chan struct{} } type dispatcher struct { workerPool chan *worker // 与worker是同一个workerPool \tjobQueue chan Job // 与池共有的任务队列 \tstop chan struct{} } // Represents user request, function which should be executed in some worker. type Job func() type Pool struct { JobQueue chan Job // 与dispatcher共有的任务队列 \tdispatcher *dispatcher wg sync.WaitGroup } 核心代码\nfunc (w *worker) start() { go func() { // 创建了一个worker \tvar job Job for { // worker free, add it to pool \tw.workerPool \u0026lt;- w // 把worker放入worker池  select { case job = \u0026lt;-w.jobChannel: job() case \u0026lt;-w.stop: w.stop \u0026lt;- struct{}{} return } } }() } func (d *dispatcher) dispatch() { for { select { case job := \u0026lt;-d.jobQueue: worker := \u0026lt;-d.workerPool worker.jobChannel \u0026lt;- job case \u0026lt;-d.stop: // 停止所有worker \tfor i := 0; i \u0026lt; cap(d.workerPool); i++ { worker := \u0026lt;-d.workerPool worker.stop \u0026lt;- struct{}{} \u0026lt;-worker.stop } d.stop \u0026lt;- struct{}{} return } } } 其程序运行图大致如下： 程序中存在一个workerPool，所有的worker都存在在这个池中，在程序的开始，会创建指定数目的worker，一次性放入池中（此后不再增加或减少）。每个worker有一个独有的jobChannel，这个jobChannel向worker传递其要执行的任务。\n用户通过直接使用Pool的JobQueue来提交任务，提交的任务会由dispatcher接收，然后分配给某个空闲的worker，即放入其jobChannel中。\n基本使用\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/ivpusic/grpool\u0026#34; ) func main() { // number of workers, and size of job queue  pool := grpool.NewPool(100, 50) // release resources used by pool  defer pool.Release() // submit one or more jobs to pool  for i := 0; i \u0026lt; 10; i++ { count := i pool.JobQueue \u0026lt;- func() { fmt.Printf(\u0026#34;I am worker! Number %d\\n\u0026#34;, count) } } // dummy wait until jobs are finished  time.Sleep(1 * time.Second) } 性能测试\n总结 设计简单，代码简洁清晰，但无panic恢复，不能动态扩容，任务传递简陋，感觉像是一个未完成版本的任务池\n4. Jeffail/tunny（★2.4k） 简单介绍 有两种模式，第一种模式在池创建时即指定任务，池创建完成后任务不可更改，但可传入不同的参数，有返回值。第二种模式在创建时不给定任务，可在池创建后自行指配任务，但只可执行无参数任务且无返回值。支持超时机制，如果任务超过时间期限还没完成，则会终止并返回错误。\n系统结构\ntype Pool struct { queuedJobs int64 // 当前挂起的任务数  ctor func() Worker // 初始任务 \tworkers []*workerWrapper reqChan chan workRequest workerMut sync.Mutex } type workerWrapper struct { // 负责管理worker和goroutine的生命周期 \tworker Worker interruptChan chan struct{} // 中止信号通道 \treqChan chan\u0026lt;- workRequest // 用于发送任务请求信号 \tcloseChan chan struct{} // 退出信号通道 \tclosedChan chan struct{} // 退出标志通道 } type workRequest struct { // 任务请求信号 \tjobChan chan\u0026lt;- interface{} // 用于接收任务 \tretChan \u0026lt;-chan interface{} // 用于返回结果 \tinterruptFunc func() } 核心代码\nfunc (p *Pool) Process(payload interface{}) interface{} { // 省略了部分错误处理代码 \trequest, open := \u0026lt;-p.reqChan request.jobChan \u0026lt;- payload payload, open = \u0026lt;-request.retChan // 同步等待任务完成 \treturn payload } func (w *workerWrapper) run() { jobChan, retChan := make(chan interface{}), make(chan interface{}) defer func() { w.worker.Terminate() close(retChan) close(w.closedChan) }() for { // NOTE: Blocking here will prevent the worker from closing down. \tw.worker.BlockUntilReady() select { case w.reqChan \u0026lt;- workRequest{ // tunny.go:156 \tjobChan: jobChan, retChan: retChan, interruptFunc: w.interrupt, }: select { case payload := \u0026lt;-jobChan: // tunny.go: 161 \tresult := w.worker.Process(payload) select { case retChan \u0026lt;- result: // tunny.go: 163 \tcase \u0026lt;-w.interruptChan: w.interruptChan = make(chan struct{}) } case _, _ = \u0026lt;-w.interruptChan: w.interruptChan = make(chan struct{}) } case \u0026lt;-w.closeChan: return } } } 根据以上代码，其主要执行过程如下：\n 某个goroutine空闲后，通过其reqChan通道发送workRequest。workRequest中包括一个jobChan，用于生产者发送任务；一个retChan，用于返回执行结果；一个interrupt Func，用于任务执行超时时强行停止worker。需要注意的是，reqChan是池和workerWrapper共有的，因此发送的workRequest可直接被池接收到。 每当池中有新任务时，池尝试从reqChan中获取一个workRequest，若获取到，则将任务通过workRequest持有的jobChan(即某个worker持有的jobChan)发送到worker 发送完毕后，同步的从retChan中读取结果。 若设置了超时时间，则在时间超时后会强制停止  基本使用\npackage main import ( \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;github.com/Jeffail/tunny\u0026#34; ) func main() { numCPUs := runtime.NumCPU() pool := tunny.NewFunc(numCPUs, func(payload interface{}) interface{} { var result []byte // TODO: Something CPU heavy with payload  return result }) defer pool.Close() http.HandleFunc(\u0026#34;/work\u0026#34;, func(w http.ResponseWriter, r *http.Request) { input, err := ioutil.ReadAll(r.Body) if err != nil { http.Error(w, \u0026#34;Internal error\u0026#34;, http.StatusInternalServerError) } defer r.Body.Close() // Funnel this work into our pool. This call is synchronous and will \t// block until the job is completed. \tresult := pool.Process(input) w.Write(result.([]byte)) }) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) }  超时演示：\n func main() { p := tunny.NewFunc(4, func(payload interface{}) interface{} { n := payload.(int) result := fib(n) time.Sleep(5 * time.Second) return result }) defer p.Close() var wg sync.WaitGroup wg.Add(4) for i := 0; i \u0026lt; 4; i++ { go func(i int) { n := rand.Intn(30) result, err := p.ProcessTimed(n, time.Second) nowStr := time.Now().Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) if err != nil { fmt.Printf(\u0026#34;[%s]task(%d) failed:%v\\n\u0026#34;, nowStr, i, err) } else { fmt.Printf(\u0026#34;[%s]fib(%d) = %d\\n\u0026#34;, nowStr, n, result) } wg.Done() }(i) } wg.Wait() } func fib(n int) int { if n \u0026lt;= 1 { return 1 } return fib(n-1) + fib(n-2) } 5. panjf2000/ants（★5.8k） 简单介绍 启动服务之时先初始化一个 Goroutine Pool 池，这个 Pool 维护了一个类似栈的 LIFO 队列 ，里面存放负责处理任务的 Worker，然后在 client 端提交 task 到 Pool 中之后，在 Pool 内部，接收 task 之后的核心操作是：\n 检查当前 Worker 队列中是否有可用的 Worker，如果有，取出执行当前的 task； 没有可用的 Worker，判断当前在运行的 Worker 是否已超过该 Pool 的容量：{是 —\u0026gt; 再判断工作池是否为非阻塞模式：[是 ——\u0026gt; 直接返回 nil，否 ——\u0026gt; 阻塞等待直至有 Worker 被放回 Pool]，否 —\u0026gt; 新开一个 Worker（goroutine）处理}； 每个 Worker 执行完任务之后，放回 Pool 的队列中等待。  大致流程如下： 这个库的亮点之一是其为每一个worker设置了过期时间，若worker空闲了一定时间就会被回收，很好的节约了资源。\n系统结构\ntype Pool struct { capacity int32 // 池的容量  running int32 // 正在运行的worker  workers workerArray // 存储可获取的worker  state int32 // 池的状态  lock sync.Locker cond *sync.Cond // 条件变量，用于在条件满足时唤醒阻塞的程序  workerCache sync.Pool blockingNum int // 被阻塞的数量  options *Options } type workerArray interface { len() int isEmpty() bool insert(worker *goWorker) error detach() *goWorker retrieveExpiry(duration time.Duration) []*goWorker reset() } type goWorker struct { pool *Pool task chan func() recycleTime time.Time // recycleTime will be update when putting a worker back into queue } 核心代码\nfunc NewPool(size int, options ...Option) (*Pool, error) { if expiry := opts.ExpiryDuration; expiry \u0026lt; 0 { return nil, ErrInvalidPoolExpiry } else if expiry == 0 { opts.ExpiryDuration = DefaultCleanIntervalTime } p := \u0026amp;Pool{ capacity: int32(size), lock: internal.NewSpinLock(), options: opts, } p.workerCache.New = func() interface{} { return \u0026amp;goWorker{ pool: p, task: make(chan func(), workerChanCap), } } if p.options.PreAlloc { if size == -1 { return nil, ErrInvalidPreAllocSize } p.workers = newWorkerArray(loopQueueType, size) } else { p.workers = newWorkerArray(stackType, 0) } p.cond = sync.NewCond(p.lock) // Start a goroutine to clean up expired workers periodically. \tgo p.purgePeriodically() // 用一个独立的goroutine来回收过期的worker  return p, nil } func (w *goWorker) run() { w.pool.incRunning() go func() { defer func() { w.pool.decRunning() w.pool.workerCache.Put(w) if p := recover(); p != nil { // ... \t} // Call Signal() here in case there are goroutine waiting for available workers. \tw.pool.cond.Signal() // 发出信号唤醒某些阻塞等待worker的线程 \t}() for f := range w.task { if f == nil { return } f() if ok := w.pool.revertWorker(w); !ok { // 将worker重新放入池中循环利用 \treturn } } }() } // 返回一个可用的worker func (p *Pool) retrieveWorker() (w *goWorker) { spawnWorker := func() { // 返回一个可用的worker \tw = p.workerCache.Get().(*goWorker) w.run() } p.lock.Lock() w = p.workers.detach() // 尝试从worker队列中取worker \tif w != nil { // 从队列中获得了worker，直接返回 \tp.lock.Unlock() } else if capacity := p.Cap(); capacity == -1 || capacity \u0026gt; p.Running() { // 队列为空其worker数量未达到限制，则生成新的worker \tp.lock.Unlock() spawnWorker() } else { // 队列为空且数量达到限制，则阻塞等待其他worker空闲 \tif p.options.Nonblocking { p.lock.Unlock() return } retry: if p.options.MaxBlockingTasks != 0 \u0026amp;\u0026amp; p.blockingNum \u0026gt;= p.options.MaxBlockingTasks { p.lock.Unlock() return } p.blockingNum++ p.cond.Wait() // 阻塞等待可用的worker \tp.blockingNum-- var nw int if nw = p.Running(); nw == 0 { // awakened by the scavenger \tp.lock.Unlock() if !p.IsClosed() { spawnWorker() } return } if w = p.workers.detach(); w == nil { if nw \u0026lt; capacity { p.lock.Unlock() spawnWorker() return } goto retry } p.lock.Unlock() } return } // 定时清理过期worker // purgePeriodically clears expired workers periodically which runs in an individual goroutine, as a scavenger. func (p *Pool) purgePeriodically() { heartbeat := time.NewTicker(p.options.ExpiryDuration) defer heartbeat.Stop() for range heartbeat.C { if p.IsClosed() { break } p.lock.Lock() expiredWorkers := p.workers.retrieveExpiry(p.options.ExpiryDuration) p.lock.Unlock() // Notify obsolete workers to stop. \t// This notification must be outside the p.lock, since w.task \t// may be blocking and may consume a lot of time if many workers \t// are located on non-local CPUs. \tfor i := range expiredWorkers { expiredWorkers[i].task \u0026lt;- nil expiredWorkers[i] = nil } // There might be a situation that all workers have been cleaned up(no any worker is running) \t// while some invokers still get stuck in \u0026#34;p.cond.Wait()\u0026#34;, \t// then it ought to wake all those invokers. \tif p.Running() == 0 { p.cond.Broadcast() } } } func (wq *workerStack) retrieveExpiry(duration time.Duration) []*goWorker { n := wq.len() if n == 0 { return nil } expiryTime := time.Now().Add(-duration) index := wq.binarySearch(0, n-1, expiryTime) wq.expiry = wq.expiry[:0] if index != -1 { wq.expiry = append(wq.expiry, wq.items[:index+1]...) m := copy(wq.items, wq.items[index+1:]) for i := m; i \u0026lt; n; i++ { wq.items[i] = nil } wq.items = wq.items[:m] } return wq.expiry } 基本使用\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/panjf2000/ants/v2\u0026#34; ) var sum int32 func myFunc(i interface{}) { n := i.(int32) atomic.AddInt32(\u0026amp;sum, n) fmt.Printf(\u0026#34;run with %d\\n\u0026#34;, n) } func demoFunc() { time.Sleep(10 * time.Millisecond) fmt.Println(\u0026#34;Hello World!\u0026#34;) } func main() { defer ants.Release() runTimes := 1000 // Use the common pool. \tvar wg sync.WaitGroup syncCalculateSum := func() { demoFunc() wg.Done() } for i := 0; i \u0026lt; runTimes; i++ { wg.Add(1) _ = ants.Submit(syncCalculateSum) } wg.Wait() fmt.Printf(\u0026#34;running goroutines: %d\\n\u0026#34;, ants.Running()) fmt.Printf(\u0026#34;finish all tasks.\\n\u0026#34;) // Use the pool with a function, \t// set 10 to the capacity of goroutine pool and 1 second for expired duration. \tp, _ := ants.NewPoolWithFunc(10, func(i interface{}) { myFunc(i) wg.Done() }) defer p.Release() // Submit tasks one by one. \tfor i := 0; i \u0026lt; runTimes; i++ { wg.Add(1) _ = p.Invoke(int32(i)) } wg.Wait() fmt.Printf(\u0026#34;running goroutines: %d\\n\u0026#34;, p.Running()) fmt.Printf(\u0026#34;finish all tasks, result is %d\\n\u0026#34;, sum) } References Java线程池实现原理及其在美团业务中的实践 新手一看就懂的线程池！ 线程池学习看这篇就够了,万字总结线程池!!! Golang中的goroutine泄漏问题 跟读者聊 Goroutine 泄露的 N 种方法，真刺激！ 如何防止 goroutine 泄露 技术解析系列 | PouchContainer Goroutine Leak 检测实践 goroutine泄露：原理、场景、检测和防范 Go语言野生goroutine的处理 goroutine 到底该怎么用 如何查看golang程序中有哪些goroutine 正在执行 Go 语言踩坑记——panic 与 recover Handling panics in go routines INSTRUMENTING A GO APPLICATION FOR PROMETHEUS Package pprof Handling 1 Million Requests per Minute with Go Go by Example: Worker Pools Golang : Creating a concurrent worker pool Work Queue Systems The Case For A Go Worker Pool Buffered Channels and Worker Pools\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E9%87%8E%E7%94%9Fgoroutine%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","summary":"一、野生goroutine的问题  引言： 毋庸置疑，golang原生的goroutine极大降低了程序员使用并发编程的代价，程序员们不需要再去关心如何实现接口、如何继承Thread类等多余的操作，只需要简简单单的go, 就可以开启一个并发的协程。但这种简单的使用方式同时也带来一些问题，这些goroutine不再受我们控制了，它们在运行时可能会发生任何错误或意外，而我们无法得知或去处理这些意外。我们将启动后不再受主进程控制的goroutine称为野生goroutine，本节将介绍野生goroutine存在的一些问题并介绍一些简单的解决方法。\n 1. goroutine中panic无法恢复 正常的函数中panic的recover\nimport ( \u0026#34;fmt\u0026#34; ) func main(){ defer func(){ if err := recover(); err != nil{ fmt.Println(err) } }() var bar = []int{1} fmt.Println(bar[1]) } reflect: slice index out of range goroutine中panic的恢复\nfunc main(){ defer func(){ if err := recover(); err != nil { // 在这里使用recover(),不能捕获panic \tfmt.Println(\u0026#34;catch you, bad guy\u0026#34;) } }() go func(){ fmt.Println(\u0026#34;I\u0026#39;m in a goroutine\u0026#34;) panic(\u0026#34;come to catch me\u0026#34;) }() time.","title":"野生Goroutine带来的问题及对应解决方案"},{"content":"1. 前后台切换  在Linux终端运行命令的时候，在命令末尾加上 \u0026amp; 符号，就可以让程序在后台运行  $ ./main \u0026amp;  如果程序正在前台运行，可以使用 Ctrl+z 选项把程序暂停，然后用 bg %[number] 命令把这个程序放到后台运行，摁Ctrl+z，然后在最后一行加上bg %number\n  对于所有运行的程序，我们可以用jobs –l 指令查看\n  $ jobs -l 也可以用 fg %[number] 指令把一个程序掉到前台  $ fg %1 也可以直接终止后台运行的程序，使用 kill 命令  $ kill %1 2. fg、bg、jobs、\u0026amp;、nohup、ctrl+z、ctrl+c 命令  \u0026amp; 加在一个命令的最后，可以把这个命令放到后台执行，如  watch -n 10 sh test.sh \u0026amp; #每10s在后台执行一次test.sh脚本  ctrl + z 可以将一个正在前台执行的命令放到后台，并且处于暂停状态。\n  jobs 查看当前有多少在后台运行的命令 jobs -l选项可显示所有任务的PID，jobs的状态可以是running, stopped, Terminated。但是如果任务被终止了（kill），shell 从当前的shell环境已知的列表中删除任务的进程标识。\n  fg 将后台中的命令调至前台继续运行。如果后台中有多个命令，可以用fg %jobnumber（是命令编号，不是进程号）将选中的命令调出。\n  bg 将一个在后台暂停的命令，变成在后台继续执行。如果后台中有多个命令，可以用bg %jobnumber将选中的命令调出。\n  kill\n   通过jobs命令查看job号（假设为num），然后执行kill %num 通过ps命令查看job的进程号（PID，假设为pid），然后执行kill pid 前台进程的终止：Ctrl+c  nohup 如果让程序始终在后台执行，即使关闭当前的终端也执行（之前的\u0026amp;做不到），这时候需要nohup。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。关闭中断后，在另一个终端jobs已经无法看到后台跑得程序了，此时利用ps（进程查看命令）  $ ps -aux | grep “test.sh” #a:显示所有程序 u:以用户为主的格式来显示 x:显示所有程序，不以终端机来区分 查看nohup.out的日志  $ tail -fn 50 nohup.out  转自：Linux程序前台后台切换\n ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/linux%E7%A8%8B%E5%BA%8F%E5%89%8D%E5%8F%B0%E5%90%8E%E5%8F%B0%E5%88%87%E6%8D%A2/","summary":"1. 前后台切换  在Linux终端运行命令的时候，在命令末尾加上 \u0026amp; 符号，就可以让程序在后台运行  $ ./main \u0026amp;  如果程序正在前台运行，可以使用 Ctrl+z 选项把程序暂停，然后用 bg %[number] 命令把这个程序放到后台运行，摁Ctrl+z，然后在最后一行加上bg %number\n  对于所有运行的程序，我们可以用jobs –l 指令查看\n  $ jobs -l 也可以用 fg %[number] 指令把一个程序掉到前台  $ fg %1 也可以直接终止后台运行的程序，使用 kill 命令  $ kill %1 2. fg、bg、jobs、\u0026amp;、nohup、ctrl+z、ctrl+c 命令  \u0026amp; 加在一个命令的最后，可以把这个命令放到后台执行，如  watch -n 10 sh test.sh \u0026amp; #每10s在后台执行一次test.sh脚本  ctrl + z 可以将一个正在前台执行的命令放到后台，并且处于暂停状态。\n  jobs 查看当前有多少在后台运行的命令 jobs -l选项可显示所有任务的PID，jobs的状态可以是running, stopped, Terminated。但是如果任务被终止了（kill），shell 从当前的shell环境已知的列表中删除任务的进程标识。","title":"Linux程序前台后台切换"},{"content":"1. 线程池基础 1.1 野生线程 在我们平常的开发中，经常会有用到多线程的场景，合理利用多线程可有效利用CPU的多核结构，提高程序的执行效率。有这样一种线程：我们利用其完成一些工作，但只是将工作交给这个线程，该线程并不保证完成任务，也不保证正常退出，并且在线程开始运行后我们无法对其进行控制。这种状态可称为：野生线程，意为其已经不受控制，在内存中自由运行。\n这种线程可能带来一系列问题：\n 频繁申请/销毁线程，可能带来巨大的额外消耗 当内存中存在较多的野生线程，会导致过分调度，降低系统性能 不能正常退出的线程会导致内存泄露 系统无法合理管理内部的资源分布，会降低系统的稳定性 ……  鉴于以上野生线程带来的问题，我们需要一种方式将其管理起来，使其从野生的线程变成\u0026quot;家养\u0026quot;的线程。\n1.2 什么是线程池  池化：池化是一种将资源统一进行管理，从而最大化收益并最小化风险的思想。\n 线程池维护若干个线程，在总体上控制线程的数量，具体上控制线程的创建、销毁等生命周期，系统可通过申请线程池中的线程异步的完成某个任务。线程池通过对线程的管理实现对资源的有效利用，避免系统资源浪费或内存泄露等问题。\n1.3 使用线程池的好处  线程池中的线程可反复利用，减少了线程创建和销毁的开销 任务无需等待线程创建即可开始运行，提高了系统响应速度 通过设置合理的线程池线程数，可有效避免资源使用不当，资源浪费 对线程运行进行有效的监视与管理  通俗易懂的讲，如果将线程比作完成任务的人，那么线程池就像一个专门管理这些人的部门。当我有任务到来时，直接把任务交给该部门，而不用自己再去找人来完成任务。\n2. 线程池的工作机制 2.1 线程池模型 线程池的内部实际上可以看做是生产者消费者模型，二者并不直接关联，通过任务队列进行交互，从而可以有效的缓冲任务，复用线程。\n在线程池模型中，扮演生产者角色的是任务管理部分，其接受提交的任务，并判断任务应如何处理：\n 直接申请线程执行该任务 缓冲到队列中等待线程执行 直接拒绝该任务  线程管理部分是消费者，线程被统一维护在线程池中，当有任务请求到来时，某一线程被分配去执行这个任务，执行完成后继续或许新的任务来执行，最终当线程获取不到任务时，线程就被回收以节省系统资源。\n2.2 线程池的状态 线程池一方面维护自身的状态，另一方面管理线程和任务，使二者良好的结合从而执行并行任务。 线程池的状态有5种：\n   运行状态 状态描述     RUNNING 能接受新提交的任务，并且也能处理阻塞队列中的任务   SHUTDOWN 关闭状态，不再接受新提交的任务，但可以继续处理阻塞队列中已保存的任务   STOP 不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程   TIDYING 所有的任务都已终止，有效线程数为0   TERMINATED 在terminated()方法执行后进入该状态   其生命周期转换如下图所示：     sequenceDiagram RUNNING-\u0026gt;\u0026gt;SHUTDOWN:shutdown() RUNNING-\u0026gt;\u0026gt;STOP: shutdownNow() SHUTDOWN-\u0026gt;\u0026gt;TIDYING:所有任务都已完成，阻塞队列为空，工作线程数为0 STOP-\u0026gt;\u0026gt;TIDYING:线程池中工作线程数为0 TIDYING-\u0026gt;\u0026gt;TERMINATED: terminated() 2.3 任务执行机制 2.3.1 任务调度 任务调度是线程池的主要入口，用户提交任务后，这部分将决定任务如何执行。 任务调度的流程图如下：  提交任务后，首先检测运行池状态，若不是RUNNING，则直接拒绝，线程池要保证在RUNNING状态下执行任务。 如果线程数小于核心数，说明系统还没有被充分利用，则可添加线程并执行。若线程数大于核心数，此时再一味增加线程数只会带来调度开销，则将任务放入阻塞队列。 若阻塞队列未满，则将任务放入阻塞队列等待执行，否则进行下一步。 若线程数小于最大线程数，可添加工作线程执行，若线程数已大于等于最大线程数，此时直接拒绝任务，不予执行。  2.3.2 任务缓冲 线程池的本质是对任务和线程的管理，要让二者分别独立运行。如果不使用线程池，一个任务即一个线程，对任务与线程的管理都将陷入混乱。要分别对任务和线程进行管理，则需要将二者进行解耦，不让二者直接关联。线程池以消费者-生产者模型，通过一个阻塞队列实现二者的解耦。阻塞队列缓存任务，工作线程从阻塞队列中获取任务。\n阻塞队列(BlockingQueue)是一个支持两个附加操作的队列。这两个附加的操作是：\n 在队列为空时，获取元素的线程会等待队列变为非空。 当队列满时，存储元素的线程会等待队列可用。 阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。 其工作模式如下：   2.3.3 任务申请 从上文任务调度部分可知，任务的执行有两种可能：\n 任务直接由新创建的线程执行 线程从任务队列中获取任务然后执行，执行完任务的空闲进行再次去队列中申请任务执行 第一种情况仅在线程初始创建时出现，多数情况下是第二种情况  线程要从任务缓存模块中不断获取任务执行，定义getTask方法来帮助线程从阻塞队列中获取任务，实现线程管理模块和任务管理模块的通信。其执行流程如下： 这里进行了多次判断，目的在于控制线程的数量，使其符合线程池的状态。如果线程池现在不应该持有那么多线程，则会返回null值。工作线程Worker会不断接收新任务去执行，而当工作线程Worker接收不到任务的时候，就会开始被回收。\n2.3.4 任务拒绝 任务拒绝模块是线程池的保护部分。当线程池的任务缓存队列已满，并且线程数达到设定的最大值，就需要拒绝到来的任务，保护线程池。\n2.4 Worker线程管理 2.4.1 Worker线程 线程池为了掌握线程的状态并维护线程的生命周期，设计了线程池内的工作线程Worker。\nWorker持有一个线程thread和一个初始化的任务firstTask。thread是在Worker创建时创建的线程，可用来执行任务；firstTask保存传入的第一个任务，这个任务可以有也可以为空。如果此值非空，那么Worker会先执行这个任务，再去获取任务执行；如果此值为空，那么Worker直接去阻塞队列中获取任务执行。 线程池需要管理线程的生命周期，需要在线程长时间不运行的时候进行回收。线程池使用一张Hash表去持有线程的引用，这样可以通过添加引用、移除引用这样的操作来控制线程的生命周期。这个时候重要的就是如何判断线程是否在运行。\n​Worker是通过继承AQS，使用AQS来实现独占锁这个功能。没有使用可重入锁ReentrantLock，而是使用AQS，为的就是实现不可重入的特性去反应线程现在的执行状态。\n1.lock方法一旦获取了独占锁，表示当前线程正在执行任务中。 2.如果正在执行任务，则不应该中断线程。 3.如果该线程现在不是独占锁的状态，也就是空闲的状态，说明它没有在处理任务，这时可以对该线程进行中断。 4.线程池在执行shutdown方法或tryTerminate方法时会调用interruptIdleWorkers方法来中断空闲的线程，interruptIdleWorkers方法会使用tryLock方法来判断线程池中的线程是否是空闲状态；如果线程是空闲状态则可以安全回收。\n在线程回收过程中就使用到了这种特性，回收过程如下图所示：\n2.4.2 Worker线程增加 增加线程是通过线程池中的addWorker方法，该方法的功能就是增加一个线程，该方法不考虑线程池是在哪个阶段增加的该线程，这个分配线程的策略是在上个步骤完成的，该步骤仅仅完成增加线程，并使它运行，最后返回是否成功这个结果。addWorker方法有两个参数：firstTask、core。firstTask参数用于指定新增的线程执行的第一个任务，该参数可以为空；core参数为true表示在新增线程时会判断当前活动线程数是否少于corePoolSize，false表示新增线程前需要判断当前活动线程数是否少于maximumPoolSize，其执行流程如下图所示： 2.4.3 Worker线程回收 线程池中线程的销毁依赖JVM自动的回收，线程池做的工作是根据当前线程池的状态维护一定数量的线程引用，防止这部分线程被JVM回收，当线程池决定哪些线程需要回收时，只需要将其引用消除即可。Worker被创建出来后，就会不断地进行轮询，然后获取任务去执行，核心线程可以无限等待获取任务，非核心线程要限时获取任务。当Worker无法获取到任务，也就是获取的任务为空时，循环会结束，Worker会主动消除自身在线程池内的引用。\n事实上，将线程引用移出线程池就已经结束了线程销毁的部分。但由于引起线程销毁的可能性有很多，线程池还要判断是什么引发了这次销毁，是否要改变线程池的现阶段状态，是否要根据新状态，重新分配线程。\n2.4.4 Worker线程执行任务 在Worker类中的run方法调用了runWorker方法来执行任务，runWorker方法的执行过程如下：\n1.while循环不断地通过getTask()方法获取任务。 2.getTask()方法从阻塞队列中取任务。 3.如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态。 4.执行任务。 5.如果getTask结果为null则跳出循环，执行processWorkerExit()方法，销毁线程。 ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/%E7%BA%BF%E7%A8%8B%E6%B1%A0/","summary":"1. 线程池基础 1.1 野生线程 在我们平常的开发中，经常会有用到多线程的场景，合理利用多线程可有效利用CPU的多核结构，提高程序的执行效率。有这样一种线程：我们利用其完成一些工作，但只是将工作交给这个线程，该线程并不保证完成任务，也不保证正常退出，并且在线程开始运行后我们无法对其进行控制。这种状态可称为：野生线程，意为其已经不受控制，在内存中自由运行。\n这种线程可能带来一系列问题：\n 频繁申请/销毁线程，可能带来巨大的额外消耗 当内存中存在较多的野生线程，会导致过分调度，降低系统性能 不能正常退出的线程会导致内存泄露 系统无法合理管理内部的资源分布，会降低系统的稳定性 ……  鉴于以上野生线程带来的问题，我们需要一种方式将其管理起来，使其从野生的线程变成\u0026quot;家养\u0026quot;的线程。\n1.2 什么是线程池  池化：池化是一种将资源统一进行管理，从而最大化收益并最小化风险的思想。\n 线程池维护若干个线程，在总体上控制线程的数量，具体上控制线程的创建、销毁等生命周期，系统可通过申请线程池中的线程异步的完成某个任务。线程池通过对线程的管理实现对资源的有效利用，避免系统资源浪费或内存泄露等问题。\n1.3 使用线程池的好处  线程池中的线程可反复利用，减少了线程创建和销毁的开销 任务无需等待线程创建即可开始运行，提高了系统响应速度 通过设置合理的线程池线程数，可有效避免资源使用不当，资源浪费 对线程运行进行有效的监视与管理  通俗易懂的讲，如果将线程比作完成任务的人，那么线程池就像一个专门管理这些人的部门。当我有任务到来时，直接把任务交给该部门，而不用自己再去找人来完成任务。\n2. 线程池的工作机制 2.1 线程池模型 线程池的内部实际上可以看做是生产者消费者模型，二者并不直接关联，通过任务队列进行交互，从而可以有效的缓冲任务，复用线程。\n在线程池模型中，扮演生产者角色的是任务管理部分，其接受提交的任务，并判断任务应如何处理：\n 直接申请线程执行该任务 缓冲到队列中等待线程执行 直接拒绝该任务  线程管理部分是消费者，线程被统一维护在线程池中，当有任务请求到来时，某一线程被分配去执行这个任务，执行完成后继续或许新的任务来执行，最终当线程获取不到任务时，线程就被回收以节省系统资源。\n2.2 线程池的状态 线程池一方面维护自身的状态，另一方面管理线程和任务，使二者良好的结合从而执行并行任务。 线程池的状态有5种：\n   运行状态 状态描述     RUNNING 能接受新提交的任务，并且也能处理阻塞队列中的任务   SHUTDOWN 关闭状态，不再接受新提交的任务，但可以继续处理阻塞队列中已保存的任务   STOP 不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程   TIDYING 所有的任务都已终止，有效线程数为0   TERMINATED 在terminated()方法执行后进入该状态   其生命周期转换如下图所示：     sequenceDiagram RUNNING-\u0026gt;\u0026gt;SHUTDOWN:shutdown() RUNNING-\u0026gt;\u0026gt;STOP: shutdownNow() SHUTDOWN-\u0026gt;\u0026gt;TIDYING:所有任务都已完成，阻塞队列为空，工作线程数为0 STOP-\u0026gt;\u0026gt;TIDYING:线程池中工作线程数为0 TIDYING-\u0026gt;\u0026gt;TERMINATED: terminated() 2.","title":"线程池"},{"content":"1. 安装高版本的Python  这里要说明，不能删除原来的python2以及python3.5，因为系统是依赖于这两个python版本的，当然你也可以试试，后果自负\u0026hellip;\n  去官网下载最新的Python 我这里下载的是源码，因为没有对应的安装包。（Python3.9） 下载完成后解压到本地 sudo tar -xvf Python-3.9.5.tar.xz -C /opt/python  编译安装 cd /opt/python mv Python-3.9.5 python3.9 sudo ./configure --enable-optimizations # 默认安装到/usr/local/bin, 可用--prefix指定安装目录 make -j8 \u0026amp;\u0026amp; sudo make altinstall sudo make clean  验证安装成功 /usr/local/bin/python3.9  Python 3.9.5 (default, May 13 2021, 09:51:10) [GCC 6.3.0 20170516] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; exit()   2. 设置默认Python版本 2.1 用户级修改 vim ~/.bashrc 增加一行：alias python='/usr/local/bin/python3.9\nsource ~/.bashrc 2.2 系统级修改 sudo rm /usr/bin/python # 删除原来默认的Python软链接 sudo ln -s /usr/local/bin/python3.9 /usr/bin/python # 设置新的软连接 从python3.9指向python 2.3 基于update-alternatives 列出所有可用的python版本替代信息\nupdate-alternatives --list python 若出现update-alternatives: 错误: 无 python 的候选项，则说明update-alternatives没有添加Python的替代版本，需要手动添加\nsudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.5 2 sudo update-alternatives --install /usr/bin/python python /usr/local/bin/python3.9 3 update-alternatives的--install参数后跟4个参数，分别是link name path priority， link is the generic name for the master link, name is the name of its symlink in the alternatives directory, and path is the alternative being introduced for the master link。最后一个是优先级，数字越高优先级越高 再次使用update-alternatives --list python查看可用的版本替代信息\n开始切换：\nupdate-alternatives --config python 有 3 个候选项可用于替换 python (提供 /usr/bin/python)。 选择 路径 优先级 状态 ------------------------------------------------------------ * 0 /usr/local/bin/python3.9 3 自动模式 1 /usr/bin/python2.7 1 手动模式 2 /usr/bin/python3.5 2 手动模式 3 /usr/local/bin/python3.9 3 手动模式 要维持当前值[*]请按\u0026lt;回车键\u0026gt;，或者键入选择的编号： 输入3，选择Python3.9为默认值\n3. pip错误 更换版本后，pip可能会出现错误。 在我的电脑上出现了subprocess.CalledProcessError: Command '('lsb_release', '-a')' returned non-zero exit status 1.错误， 解决方法为：删除/usr/bin/lsb_release\nsudo rm /usr/bin/lsb_release ","permalink":"http://yangchnet.github.io/Dessert/posts/python/deepin%E4%B8%8A%E5%8D%87%E7%BA%A7python/","summary":"1. 安装高版本的Python  这里要说明，不能删除原来的python2以及python3.5，因为系统是依赖于这两个python版本的，当然你也可以试试，后果自负\u0026hellip;\n  去官网下载最新的Python 我这里下载的是源码，因为没有对应的安装包。（Python3.9） 下载完成后解压到本地 sudo tar -xvf Python-3.9.5.tar.xz -C /opt/python  编译安装 cd /opt/python mv Python-3.9.5 python3.9 sudo ./configure --enable-optimizations # 默认安装到/usr/local/bin, 可用--prefix指定安装目录 make -j8 \u0026amp;\u0026amp; sudo make altinstall sudo make clean  验证安装成功 /usr/local/bin/python3.9  Python 3.9.5 (default, May 13 2021, 09:51:10) [GCC 6.3.0 20170516] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; exit()   2. 设置默认Python版本 2.1 用户级修改 vim ~/.","title":"deepin上升级Python"},{"content":"在使用git时，有时某分支已在远程服务器删除，但本地不会同步删除，这个分支依然存在。\n如下命令可以删除本地版本库上那些失效的远程追踪分支，具体用法是，假如你的远程版本库名是 origin,则使用如下命令先查看哪些分支需要清理：\n$ git remote prune origin --dry-run 修剪 origin URL：git@yuhu.github.com:yuhu-tech/grampus-contracts.git * [将删除] origin/cq-2021-0227_refactor * [将删除] origin/lichagn-feat-impl-grc20 * [将删除] origin/lichagn-feat-impl-grc20-em20-em721 * [将删除] origin/lichang-feat-add-some-support-for-ANT * [将删除] origin/lichang-feat-impl-20-and-720 * [将删除] origin/lichang-feat-mdy-20 * [将删除] origin/lichang-feat-mdy-20-and-721 * [将删除] origin/litao-dev * [将删除] origin/runjam-v0.0.2 可以看到， 以上分支已经失效，将被删除，执行\n$ git remote prune origin 修剪 origin URL：git@yuhu.github.com:yuhu-tech/grampus-contracts.git * [已删除] origin/cq-2021-0227_refactor * [已删除] origin/lichagn-feat-impl-grc20 * [已删除] origin/lichagn-feat-impl-grc20-em20-em721 * [已删除] origin/lichang-feat-add-some-support-for-ANT * [已删除] origin/lichang-feat-impl-20-and-720 * [已删除] origin/lichang-feat-mdy-20 * [已删除] origin/lichang-feat-mdy-20-and-721 * [已删除] origin/litao-dev * [已删除] origin/runjam-v0.0.2 ","permalink":"http://yangchnet.github.io/Dessert/posts/git/%E6%B8%85%E7%90%86%E6%9C%AC%E5%9C%B0%E5%88%86%E6%94%AF/","summary":"在使用git时，有时某分支已在远程服务器删除，但本地不会同步删除，这个分支依然存在。\n如下命令可以删除本地版本库上那些失效的远程追踪分支，具体用法是，假如你的远程版本库名是 origin,则使用如下命令先查看哪些分支需要清理：\n$ git remote prune origin --dry-run 修剪 origin URL：git@yuhu.github.com:yuhu-tech/grampus-contracts.git * [将删除] origin/cq-2021-0227_refactor * [将删除] origin/lichagn-feat-impl-grc20 * [将删除] origin/lichagn-feat-impl-grc20-em20-em721 * [将删除] origin/lichang-feat-add-some-support-for-ANT * [将删除] origin/lichang-feat-impl-20-and-720 * [将删除] origin/lichang-feat-mdy-20 * [将删除] origin/lichang-feat-mdy-20-and-721 * [将删除] origin/litao-dev * [将删除] origin/runjam-v0.0.2 可以看到， 以上分支已经失效，将被删除，执行\n$ git remote prune origin 修剪 origin URL：git@yuhu.github.com:yuhu-tech/grampus-contracts.git * [已删除] origin/cq-2021-0227_refactor * [已删除] origin/lichagn-feat-impl-grc20 * [已删除] origin/lichagn-feat-impl-grc20-em20-em721 * [已删除] origin/lichang-feat-add-some-support-for-ANT * [已删除] origin/lichang-feat-impl-20-and-720 * [已删除] origin/lichang-feat-mdy-20 * [已删除] origin/lichang-feat-mdy-20-and-721 * [已删除] origin/litao-dev * [已删除] origin/runjam-v0.","title":"清理本地分支"},{"content":"1. 什么是同质化代币（FT） 同质化代币是一种能够相互替换，具有统一性，可接近无穷拆分的代币。在同质化代币的交易中，只需要关注代币交接的数量即可，其价值可能会根据交换的时间间隔而改变，但其本质没有发生变化。 举例来说，美元，人民币都是同质化代币，虽然每一张美元或人民币的序号不同，但在面额相同的情况下，不同序号的币对持有者来说没有区别。\n2. 什么是非同质化代币(NFT) Non-Fungible Tokens\n非同质化代币包含了记录在其智能合约中的识别信息。这些信息使每种代币具有其独特性，因此不能被另一种代币直接取代。它们不能以一换一，因为没有两个 NFT 是相同的。 此外，非同质化代币也不可分割，就像不能送给别人演唱会门票的一部分一样，门票的一部分并不值钱也不能兑换。 非同质化的独特属性使得它通常与特定资产挂钩，可以用来证明数字物品（如游戏皮肤）的所有权，甚至实物资产的所有权，主要应用于游戏和加密收藏品领域。 FT 和 NFT 的一大区别在于使用了不同的合约接口，前者为 ERC-20，后者为 ERC-721。\n3. 什么是ERC-20? ERC-20 协议是以太坊区块链较早的、比较流行的代币规格协议。若以太坊平台上两种代币都以 ERC-20 发行，则两者之间可以进行自由置换。ERC20 是标准代币接口，规定了其基本功能 , 方便第三方使用。系统开源使得 ERC20 的标准已经简单到可以 5 分钟发行一个 ERC-20 代币。ERC-20 代币听命于同一组代币合约的命令，也就意味着所有 ERC-20 协议中的代币都可轻松实现转移、请求、批准等功能，但其功能因此也具有局限性。\n4. 什么是ERC-721？ 相比于 ERC-20，ERC-721 协议功能更多且技术更先进。该协议是以太坊的针对不可置换代币的 NFT 数字资产的第一个标准，应用于 CryptoKitties、Decentraland 等项目。ERC721 标准正是由 CryptoKitties 的 CTO Dieter Shirley 所创建和发布的，Dieter Shirley 是 NFT 的奠基人之一。\n虽然 ERC-721 较 ERC-20 用例较少，功能还处于探索之用，但 721 协议下的资产——画作、债券、房子或是汽车——的优势在于能保证所有权的安全性、所有权转移的便捷性以及所有权历史的不可更改性和透明性。另外，ERC721 还可以促进追踪、交易和管理真实资 产的交易和管理等等。随着游戏虚拟资产不断流行起来，5G 和 VR 不断普及，搭载区块链技术，721 协议前景一片光明。\n","permalink":"http://yangchnet.github.io/Dessert/posts/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%90%8C%E8%B4%A8%E5%8C%96%E4%BB%A3%E5%B8%81%E5%92%8C%E9%9D%9E%E5%90%8C%E8%B4%A8%E5%8C%96%E4%BB%A3%E5%B8%81/","summary":"1. 什么是同质化代币（FT） 同质化代币是一种能够相互替换，具有统一性，可接近无穷拆分的代币。在同质化代币的交易中，只需要关注代币交接的数量即可，其价值可能会根据交换的时间间隔而改变，但其本质没有发生变化。 举例来说，美元，人民币都是同质化代币，虽然每一张美元或人民币的序号不同，但在面额相同的情况下，不同序号的币对持有者来说没有区别。\n2. 什么是非同质化代币(NFT) Non-Fungible Tokens\n非同质化代币包含了记录在其智能合约中的识别信息。这些信息使每种代币具有其独特性，因此不能被另一种代币直接取代。它们不能以一换一，因为没有两个 NFT 是相同的。 此外，非同质化代币也不可分割，就像不能送给别人演唱会门票的一部分一样，门票的一部分并不值钱也不能兑换。 非同质化的独特属性使得它通常与特定资产挂钩，可以用来证明数字物品（如游戏皮肤）的所有权，甚至实物资产的所有权，主要应用于游戏和加密收藏品领域。 FT 和 NFT 的一大区别在于使用了不同的合约接口，前者为 ERC-20，后者为 ERC-721。\n3. 什么是ERC-20? ERC-20 协议是以太坊区块链较早的、比较流行的代币规格协议。若以太坊平台上两种代币都以 ERC-20 发行，则两者之间可以进行自由置换。ERC20 是标准代币接口，规定了其基本功能 , 方便第三方使用。系统开源使得 ERC20 的标准已经简单到可以 5 分钟发行一个 ERC-20 代币。ERC-20 代币听命于同一组代币合约的命令，也就意味着所有 ERC-20 协议中的代币都可轻松实现转移、请求、批准等功能，但其功能因此也具有局限性。\n4. 什么是ERC-721？ 相比于 ERC-20，ERC-721 协议功能更多且技术更先进。该协议是以太坊的针对不可置换代币的 NFT 数字资产的第一个标准，应用于 CryptoKitties、Decentraland 等项目。ERC721 标准正是由 CryptoKitties 的 CTO Dieter Shirley 所创建和发布的，Dieter Shirley 是 NFT 的奠基人之一。\n虽然 ERC-721 较 ERC-20 用例较少，功能还处于探索之用，但 721 协议下的资产——画作、债券、房子或是汽车——的优势在于能保证所有权的安全性、所有权转移的便捷性以及所有权历史的不可更改性和透明性。另外，ERC721 还可以促进追踪、交易和管理真实资 产的交易和管理等等。随着游戏虚拟资产不断流行起来，5G 和 VR 不断普及，搭载区块链技术，721 协议前景一片光明。","title":"同质化代币和非同质化代币"},{"content":" ubuntu环境\n 0. 拉取github仓库的两种方式 在拉取github仓库时，我们常用\ngit clone https://github.com/username/repoName.git 的方式，这种方式使用https协议 还可以使用ssh协议，以如下方式拉取仓库\ngit clone git@github.com:username/repoName.git 以下介绍的设置方法，基于ssh协议。\n1. 使用SSH连接到GitHub 使用 SSH 协议可以连接远程服务器和服务并向它们验证。 利用 SSH 密钥可以连接 GitHub，而无需在每次访问时都提供用户名和个人访问令牌。\n检查现有SSH秘钥 在生成 SSH 密钥之前，您可以检查是否有任何现有的 SSH 密钥。\n$ ls -al ~/.ssh # Lists the files in your .ssh directory, if they exist 如果你的主机上已有SSH公钥，则其可能是如下：\nid_rsa.pub id_ecdsa.pub id_ed25519.pub 如果你没有现有的公钥和私钥对，或者不想使用现有的秘钥连接到github，则可以生成新的SSH秘钥。\n生成新SSH秘钥 输入如下命令：\nssh-keygen -t rsa -C \u0026#34;your_email@example.com\u0026#34; 会有如下输出：\nGenerating public/private rsa key pair. Enter file in which to save the key (/home/lc/.ssh/id_rsa): # 可以直接回车使用默认位置 Enter passphrase (empty for no passphrase): # 也可直接回车 然后就可以在你的~/.ssh目录下生成新的秘钥\n添加秘钥到ssh-agent 启动ssh-agent\neval \u0026#34;$(ssh-agent -s)\u0026#34; # 输出： Agent pid 59566 添加\nssh-add ~/.ssh/id_rsa 新增 SSH 密钥到 GitHub 帐户 复制你的ssh秘钥（id_rsa.pub文件内容），打开github 点击new ssh key,将你的秘钥复制到对应的位置 测试你的秘钥是否生效 ssh -T git@github.com # Hi yenian! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. 2. 设置两个github账户 取消已有的全局配置 git config --global --unset user.name git config --global --unset user.email 生成两个新的ssh秘钥 输入如下命令：\nssh-keygen -t rsa -C \u0026#34;your_email@example.com\u0026#34; 会有如下输出：\nGenerating public/private rsa key pair. Enter file in which to save the key (/home/lc/.ssh/id_rsa): # 这里要为两个秘钥定义不同的文件名 Enter passphrase (empty for no passphrase): # 也可直接回车 这里要注意的是设置文件名时不能直接跳过，要为两个秘钥定义不同的文件名，比如所一个为id_rsa, 另一个为id_rsa_work ，其中work可以设置你的工作git用户名。分别用于你个人和工作用的github账号。\n将两个秘钥分别加入对应的github账号 \u0026hellip; 如上 \u0026hellip;\n配置 ~/.ssh/config 文件 打开~/.ssh/config 文件（没有则创建）\nvim ~/.ssh/config 文件内容如下：\n# default github account Host github.com HostName github.com User git IdentityFile ~/.ssh/id_rsa_one Host work.github.com HostName github.com User git IdentityFile ~/.ssh/id_rsa_two 测试是否设置成功 ssh -T git@work.github.com # Hi $yourWorkAccountName$! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. ssh -T git@github.com # Hi $yourPersonAccountName$! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. 还需要做什么 如果你要以工作账户的身份去拉取某个库，那么你需要将原来用的\ngit clone git@github.com:username/repository-name.git 替换为\ngit clone git@work.github.com:username/repository-name.git 和我们的config相对应\n而如果直接使用原命令，即\ngit clone git@github.com:username/repository-name.git 那么默认使用的是你个人账户。\n如果你想要为你一个已有的仓库指定账户，可使用\ngit remote origin set-url git@work.github.com:username/repository-name.git # 指定工作账户 ","permalink":"http://yangchnet.github.io/Dessert/posts/git/%E4%B8%BB%E6%9C%BA%E4%B8%8A%E8%AE%BE%E7%BD%AE%E4%B8%A4%E4%B8%AAgit%E8%B4%A6%E5%8F%B7/","summary":"ubuntu环境\n 0. 拉取github仓库的两种方式 在拉取github仓库时，我们常用\ngit clone https://github.com/username/repoName.git 的方式，这种方式使用https协议 还可以使用ssh协议，以如下方式拉取仓库\ngit clone git@github.com:username/repoName.git 以下介绍的设置方法，基于ssh协议。\n1. 使用SSH连接到GitHub 使用 SSH 协议可以连接远程服务器和服务并向它们验证。 利用 SSH 密钥可以连接 GitHub，而无需在每次访问时都提供用户名和个人访问令牌。\n检查现有SSH秘钥 在生成 SSH 密钥之前，您可以检查是否有任何现有的 SSH 密钥。\n$ ls -al ~/.ssh # Lists the files in your .ssh directory, if they exist 如果你的主机上已有SSH公钥，则其可能是如下：\nid_rsa.pub id_ecdsa.pub id_ed25519.pub 如果你没有现有的公钥和私钥对，或者不想使用现有的秘钥连接到github，则可以生成新的SSH秘钥。\n生成新SSH秘钥 输入如下命令：\nssh-keygen -t rsa -C \u0026#34;your_email@example.com\u0026#34; 会有如下输出：\nGenerating public/private rsa key pair. Enter file in which to save the key (/home/lc/.","title":"主机上设置两个git账号"},{"content":"使用git log命令查看git日志文件，假设为如下内容\ncommit cc7b5fc7bd2ae6f8d88144cd61c8ffad15d44e41 Author: yangchnet \u0026lt;1048887414@qq.com\u0026gt; Date: Sun Apr 25 19:40:03 2021 +0800 4-25 commit fbd7265095b4c8989fba830393eb32ef29cd9ee1 Merge: 3ae3c19 6a25204 Author: yangchnet \u0026lt;1048887414@qq.com\u0026gt; Date: Sun Apr 25 15:04:38 2021 +0800 Merge branch 'master' of https://github.com/yangchnet/Tem commit 6a25204187602449bfe4ca8c862c9677e65fed04 Author: yangchnet \u0026lt;30308940+yangchnet@users.noreply.github.com\u0026gt; Date: Thu Apr 22 21:36:05 2021 +0800 Delete CNAME ... 现在想合并最后两个提交，则进行以下步骤：\n 复制倒数第三个提交的哈希值，即：6a25204187602449bfe4ca8c862c9677e65fed04 使用如下命令进行合并：  git rebase -i 6a25204187602449bfe4ca8c862c9677e65fed04 # 这个哈希值就是你刚才复制的 若有如下提示，请进行第4步，否则直接进行第5步  不能变基：您有未暂存的变更。 请提交或为它们保存进度。 使用git stash暂存修改  $ git stash 保存工作目录和索引状态 WIP on master: cc7b5fc 4-25 HEAD 现在位于 cc7b5fc 4-25 使用git rebase后，会出现如下内容  pick 3ae3c19 增加graphql介绍 pick cc7b5fc 4-25 # 变基 6a25204..cc7b5fc 到 6a25204（2 个提交） # # 命令: # p, pick = 使用提交 # r, reword = 使用提交，但修改提交说明 # e, edit = 使用提交，但停止以便进行提交修补 # s, squash = 使用提交，但和前一个版本融合 # f, fixup = 类似于 \u0026quot;squash\u0026quot;，但丢弃提交说明日志 # x, exec = 使用 shell 运行命令（此行剩余部分） # d, drop = 删除提交 # # 这些行可以被重新排序；它们会被从上至下地执行。 # # # 如果您在这里删除一行，对应的提交将会丢失。 # # 然而，如果您删除全部内容，变基操作将会终止。 # # 注意空提交已被注释掉 前两行为你想要合并的commit，按照注释内容，要保留的commit开头不变，要合并到另一个的开头设为s，意为：使用提交，但和前一个版本融合 （可以按照你的需求改变），更改完成后保存。\n保存完成后会出现如下内容  # 这是一个 2 个提交的组合。 # 这是第一个提交说明： 增加graphql介绍 # 这是提交说明 #2： 4-25 # 请为您的变更输入提交说明。以 '#' 开始的行将被忽略，而一个空的提交 # 说明将会终止提交。 # ... 这是你的commit的说明，将你想保留的commit说明保留，不想要的直接删除。保存之。\n再次查看git log, 可以看到刚才的更改已经生效。  commit 2994a193577f4b4175b1fe7015db955df9143b89 Author: yangchnet \u0026lt;1048887414@qq.com\u0026gt; Date: Thu Apr 22 21:30:48 2021 +0800 增加graphql介绍 commit 6a25204187602449bfe4ca8c862c9677e65fed04 Author: yangchnet \u0026lt;30308940+yangchnet@users.noreply.github.com\u0026gt; Date: Thu Apr 22 21:36:05 2021 +0800 Delete CNAME 若进行了第4步，则需进行本步骤  git stash pop ","permalink":"http://yangchnet.github.io/Dessert/posts/git/%E5%B0%86%E4%B8%A4%E4%B8%AAcommit%E5%90%88%E5%B9%B6%E4%B8%BA%E4%B8%80%E4%B8%AA/","summary":"使用git log命令查看git日志文件，假设为如下内容\ncommit cc7b5fc7bd2ae6f8d88144cd61c8ffad15d44e41 Author: yangchnet \u0026lt;1048887414@qq.com\u0026gt; Date: Sun Apr 25 19:40:03 2021 +0800 4-25 commit fbd7265095b4c8989fba830393eb32ef29cd9ee1 Merge: 3ae3c19 6a25204 Author: yangchnet \u0026lt;1048887414@qq.com\u0026gt; Date: Sun Apr 25 15:04:38 2021 +0800 Merge branch 'master' of https://github.com/yangchnet/Tem commit 6a25204187602449bfe4ca8c862c9677e65fed04 Author: yangchnet \u0026lt;30308940+yangchnet@users.noreply.github.com\u0026gt; Date: Thu Apr 22 21:36:05 2021 +0800 Delete CNAME ... 现在想合并最后两个提交，则进行以下步骤：\n 复制倒数第三个提交的哈希值，即：6a25204187602449bfe4ca8c862c9677e65fed04 使用如下命令进行合并：  git rebase -i 6a25204187602449bfe4ca8c862c9677e65fed04 # 这个哈希值就是你刚才复制的 若有如下提示，请进行第4步，否则直接进行第5步  不能变基：您有未暂存的变更。 请提交或为它们保存进度。 使用git stash暂存修改  $ git stash 保存工作目录和索引状态 WIP on master: cc7b5fc 4-25 HEAD 现在位于 cc7b5fc 4-25 使用git rebase后，会出现如下内容  pick 3ae3c19 增加graphql介绍 pick cc7b5fc 4-25 # 变基 6a25204.","title":"将两个commit合并为一个"},{"content":"1. 什么是Graphql GraphQL 既是一种用于 API 的查询语言也是一个满足你数据查询的runtime。 GraphQL对你的API中的数据提供了一套易于理解的完整描述，使得客户端能够准确地获得它需要的数据，而且没有任何冗余，也让API更容易地随着时间推移而演进，还能用于构建强大的开发者工具。 一个 GraphQL 服务是通过定义类型和类型上的字段来创建的，然后给每个类型上的每个字段提供解析函数。\n简单的说，GraphQL为我们定义数据库提供了更为便捷的方式，你不需要写任何SQL语句，即可完成数据库的创建及迁移等工作。\n2. 概览 例如，一个 GraphQL 服务告诉我们当前登录用户是 me，这个用户的名称可能像这样：\ntype Query { me: User } type User { id: ID name: String } 一并的还有每个类型上字段的解析函数：\nfunction Query_me(request) { return request.auth.user; } function User_name(user) { return user.getName(); } 一旦一个 GraphQL 服务运行起来（通常在 web 服务的一个 URL 上），它就能接收 GraphQL 查询，并验证和执行。接收到的查询首先会被检查确保它只引用了已定义的类型和字段，然后运行指定的解析函数来生成结果。\n例如这个查询：\n{ me { name } } 会产生这样的JSON结果：\n{ \u0026#34;me\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Luke Skywalker\u0026#34; } } 3. Schema 和类型 GraphQL 服务可以用任何语言编写，但并不依赖于任何特定语言的句法句式（譬如 JavaScript）来与 GraphQL schema 沟通，Graphql定义了自己的简单语言，称之为 “GraphQL schema language”。\n3.1 对象类型和字段 一个 GraphQL schema 中的最基本的组件是对象类型，它就表示你可以从服务上获取到什么类型的对象，以及这个对象有什么字段。使用 GraphQL schema language，我们可以这样表示它：\ntype Character { name: String! appearsIn: [Episode!]! }  说明\n  Character 是一个 GraphQL 对象类型，表示其是一个拥有一些字段的类型。你的 schema 中的大多数类型都会是对象类型。 name 和 appearsIn 是 Character 类型上的字段。这意味着在一个操作 Character 类型的 GraphQL 查询中的任何部分，都只能出现 name 和 appearsIn 字段。 String 是内置的标量类型之一 —— 标量类型是解析到单个标量对象的类型，无法在查询中对它进行次级选择。后面我们将细述标量类型。 String! 表示这个字段是非空的，GraphQL 服务保证当你查询这个字段后总会给你返回一个值。在类型语言里面，我们用一个感叹号来表示这个特性。 [Episode!]! 表示一个 Episode 数组。因为它也是非空的，所以当你查询 appearsIn 字段的时候，你也总能得到一个数组（零个或者多个元素）。且由于 Episode! 也是非空的，你总是可以预期到数组中的每个项目都是一个 Episode 对象  3.1.5 查询和变更类型（The Query and Mutation Types） 你的 schema 中大部分的类型都是普通对象类型，但是一个 schema 内有两个特殊类型：\nschema { query: Query mutation: Mutation } 每一个 GraphQL 服务都有一个 query 类型，可能有一个 mutation 类型。这两个类型和常规对象类型无差，但是它们之所以特殊，是因为它们定义了每一个 GraphQL 查询的入口。因此如果你看到一个像这样的查询：\n# Request query { hero { name } droid(id: \u0026#34;2000\u0026#34;) { name } } # Response { \u0026#34;data\u0026#34;: { \u0026#34;hero\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;R2-D2\u0026#34; }, \u0026#34;droid\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;C-3PO\u0026#34; } } } 那表示这个 GraphQL 服务需要一个 Query 类型，且其上有 hero 和 droid 字段：\ntype Query { hero(episode: Episode): Character droid(id: ID!): Droid } Mutation也是类似的工作方式 —— 你在 Mutation 类型上定义一些字段，然后这些字段将作为 mutation 根字段使用，接着你就能在你的查询中调用.\n有必要记住的是，除了作为 schema 的入口，Query 和 Mutation 类型与其它 GraphQL 对象类型别无二致，它们的字段也是一样的工作方式。\n3.2 参数 GraphQL 对象类型上的每一个字段都可能有零个或者多个参数，例如下面的 length 字段:\ntype Starship { id: ID! name: String! length(unit: LengthUnit = METER): Float } 所有参数都是具名的。在本例中，length字段定义了一个参数：unit。参数可能是必选或者可选的，当一个参数是可选的，我们可以定义一个默认值 —— 如果 unit 参数没有传递，那么它将会被默认设置为 METER。\n3.3 标量类型（Scalar Types） 一个对象类型有自己的名字和字段，而某些时候，这些字段必然会解析到具体数据。这就是标量类型的来源：它们表示对应 GraphQL 查询的叶子节点。\n下列查询中，name 和 appearsIn 字段将解析到标量类型：\n{ hero { name appearsIn } } { \u0026#34;data\u0026#34;: { \u0026#34;hero\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;R2-D2\u0026#34;, \u0026#34;appearsIn\u0026#34;: [ \u0026#34;NEWHOPE\u0026#34;, \u0026#34;EMPIRE\u0026#34;, \u0026#34;JEDI\u0026#34; ] } } } 标量类型没有任何次级字段，它们是一次查询的叶子节点。 Graphql中预定义的标量类型有：\n Int：有符号 32 位整数。 Float：有符号双精度浮点值。 String：UTF‐8 字符序列。 Boolean：true 或者 false。 ID：ID 标量类型表示一个唯一标识符，通常用以重新获取对象或者作为缓存中的键。ID 类型使用和 String 一样的方式序列化；然而将其定义为 ID 意味着并不需要人类可读型。  还可以自定义标量类型：\nscalar Date 在自己的实现中定义如何将其序列化、反序列化和验证。例如，你可以指定 Date 类型应该总是被序列化成整型时间戳，而客户端应该知道去要求任何 date 字段都是这个格式。\n3.4 枚举类型（Enumseration Types） 枚举类型是一种特殊的标量，它限制在一个特殊的可选值集合内。这让你能够：\n 验证这个类型的任何参数是可选值的的某一个 与类型系统沟通，一个字段总是一个有限值集合的其中一个值。  下面是一个用 GraphQL schema 语言表示的 enum 定义：\nenum Episode { NEWHOPE EMPIRE JEDI } 这表示无论我们在 schema 的哪处使用了 Episode，都可以肯定它返回的是 NEWHOPE、EMPIRE 和 JEDI 之一。\n3.5 列表和非空 对象类型、标量以及枚举是 GraphQL 中你唯一可以定义的类型种类。但是当你在 schema 的其他部分使用这些类型时，或者在你的查询变量声明处使用时，你可以给它们应用额外的类型修饰符来影响这些值的验证。我们先来看一个例子：\ntype Character { name: String! appearsIn: [Episode]! } String类型后的感叹号！表示此类型非空。服务器在返回这个字段时，总是会返回一个非空值，如果结果得到了一个空值，那么事实上将会触发一个 GraphQL 执行错误，以让客户端知道发生了错误。\n在 GraphQL schema 语言中，我们通过将类型包在方括号（[ 和 ]）中的方式来标记列表。列表对于参数也是一样的运作方式，验证的步骤会要求对应值为数组。\n非空和列表修饰符可以组合使用。\nmyField: [String!] # 这表示数组本身可以为空,但是其不能有任何空值成员. # myField: null // 有效 # myField: [] // 有效 # myField: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] // 有效 # myField: [\u0026#39;a\u0026#39;, null, \u0026#39;b\u0026#39;] // 错误 myField: [String]! # 数组本身不能为空，但是其可以包含空值成员 # myField: null // 错误 # myField: [] // 有效 # myField: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] // 有效 # myField: [\u0026#39;a\u0026#39;, null, \u0026#39;b\u0026#39;] // 有效 你可以根据需求嵌套任意层非空和列表修饰符。\n3.6 接口（interface） 跟许多类型系统一样，GraphQL 支持接口。一个接口是一个抽象类型，它包含某些字段，而对象类型必须包含这些字段，才能算实现了这个接口。\n例如，你可以用一个 Character 接口用以表示《星球大战》三部曲中的任何角色：\ninterface Character { id: ID! name: String! friends: [Character] appearsIn: [Episode]! } 这意味着任何实现 Character 的类型都要具有这些字段，并有对应参数和返回类型。\n例如，这里有一些可能实现了 Character 的类型：\ntype Human implements Character { id: ID! name: String! friends: [Character] appearsIn: [Episode]! starships: [Starship] totalCredits: Int } type Droid implements Character { id: ID! name: String! friends: [Character] appearsIn: [Episode]! primaryFunction: String } 可见这两个类型都具备 Character 接口的所有字段，但也引入了其他的字段 totalCredits、starships 和 primaryFunction，这都属于特定的类型的角色。\n3.7 联合类型 联合类型和接口十分相似，但是它并不指定类型之间的任何共同字段。\nunion SearchResult = Human | Droid | Starship 在我们的schema中，任何返回一个 SearchResult 类型的地方，都可能得到一个 Human、Droid 或者 Starship。注意，联合类型的成员需要是具体对象类型；你不能使用接口或者其他联合类型来创造一个联合类型。\n3.7 输入类型 目前为止，我们只讨论过将例如枚举和字符串等标量值作为参数传递给字段，但是你也能很容易地传递复杂对象。这在变更（mutation）中特别有用，因为有时候你需要传递一整个对象作为新建对象。在 GraphQL schema language 中，输入对象看上去和常规对象一模一样，除了关键字是 input 而不是 type：\ninput ReviewInput { stars: Int! commentary: String } ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/graphql/","summary":"1. 什么是Graphql GraphQL 既是一种用于 API 的查询语言也是一个满足你数据查询的runtime。 GraphQL对你的API中的数据提供了一套易于理解的完整描述，使得客户端能够准确地获得它需要的数据，而且没有任何冗余，也让API更容易地随着时间推移而演进，还能用于构建强大的开发者工具。 一个 GraphQL 服务是通过定义类型和类型上的字段来创建的，然后给每个类型上的每个字段提供解析函数。\n简单的说，GraphQL为我们定义数据库提供了更为便捷的方式，你不需要写任何SQL语句，即可完成数据库的创建及迁移等工作。\n2. 概览 例如，一个 GraphQL 服务告诉我们当前登录用户是 me，这个用户的名称可能像这样：\ntype Query { me: User } type User { id: ID name: String } 一并的还有每个类型上字段的解析函数：\nfunction Query_me(request) { return request.auth.user; } function User_name(user) { return user.getName(); } 一旦一个 GraphQL 服务运行起来（通常在 web 服务的一个 URL 上），它就能接收 GraphQL 查询，并验证和执行。接收到的查询首先会被检查确保它只引用了已定义的类型和字段，然后运行指定的解析函数来生成结果。\n例如这个查询：\n{ me { name } } 会产生这样的JSON结果：\n{ \u0026#34;me\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Luke Skywalker\u0026#34; } } 3. Schema 和类型 GraphQL 服务可以用任何语言编写，但并不依赖于任何特定语言的句法句式（譬如 JavaScript）来与 GraphQL schema 沟通，Graphql定义了自己的简单语言，称之为 “GraphQL schema language”。","title":"Graphql基本概念"},{"content":"1. 区块链定义 区块链技术本质上是一个去中心化的数据库，它是比特币的核心技术与基础架构，是分布式数据存储、点对点传输、共识机制、加密算法等计算机技术的新型应用模式。狭义来讲，区块链是一种按照时间顺序将数据区块以顺序相连的方式组合成的一种链式数据结构，并以密码学方式保证的不可篡改、不可伪造的分布式账本。广义来讲，区块链技术是利用块链式数据结构来验证与存储数据、利用分布式节点共识算法来生成和更新数据、利用密码学方式保证数据传输和访问的安全、利用由自动化脚本代码组成的智能合约来编程和操作数据的一种全新的分布式基础架构与计算范式。\n1.1 区块链的技术特征 区块链上存储的数据需由全网节点共同维护，可以在缺乏信任的节点之间有效地传递价值。相比现有的数据库技术，区块链具有以下技术特征。\n 块链式数据结构\n区块链利用块链式数据结构来验证和存储数据，通过上文对区块链基本概念的介绍可以知道，每个区块打包记录了一段时间内发生的交易是对当前账本的一次共识，并且通过记录上一个区块的哈希值进行关联，从而形成块链式的数据结构。 分布式共识算法\n区块链系统利用分布式共识算法来生成和更新数据，从技术层面杜绝了非法篡改数据的可能性，从而取代了传统应用中保证信任和交易安全的第三方中介机构，降低了为维护信用而造成的时间成本、人力成本和资源耗用 密码学方式\n区块链系统利用密码学的方式保证数据传输和访问的安全。存储在区块链上的交易信息是公开的，但账户的身份信息是高度加密的。区块链系统集成了对称加密、非对称加密及哈希算法的优点，并使用数字签名技术来保证交易的安全。  1.2 区块链的功能特征 区块链系统的以上技术特征决定了其应用具有如下功能特征。\n  多中心 不同于传统应用的中心化数据管理，区块链网络中有多个机构进行相互监督并实时对账，从而避免了单一记账人造假的可能性，提高了数据的安全性。\n  自动化 区块链系统中的智能合约是可以自动化执行一些预先定义好的规则和条款的一段计算机程序代码，它大大提高了经济活动与契约的自动化程度。\n  可信任 存储在区块链上的交易记录和其他数据是不可篡改并且可溯源的，所以能够很好地解决各方不信任的问题，无需第三方可信中介。\n  2. 区块链的相关概念 区块链以密码学的方式维护一份不可篡改和不可伪造的分布式账本，并通过基于协商一致的规范和协议（共识机制）解决了去中心化的记账系统的一致性问题，其相关概念主要包括以下三个。\n 交易（Transaction）\n区块链上每一次导致区块状态变化的操作都称为交易，每一次交易对应唯一的交易哈希值，一段时间后便会对交易进行打包。 区块（Block）\n打包记录一段时间内发生的交易和状态结果，是对当前账本的一次共识。每个区块以一个相对平稳的时间间隔加入到链上，在企业级区块链平台中，共识时间可以动态设置。 链（Chain）\n区块按照时间顺序串联起来，通过每个区块记录上一个区块的哈希值关联，是整个状态改变的日志记录。   区块链的主要结构  如何解决交易中的信任和安全问题 区块链技术体系不是通过一个权威的中心化机构来保证交易的可信和安全，而是通过加密和分布式共识机制来解决信任和安全问题，其主要技术创新有以下4点。\n  分布式账本 交易是由分布式系统中的多个节点共同记录的。每一个节点都记录完整的交易记录，因此它们都可以参与监督交易合法性并验证交易的有效性。不同于传统的中心化技术方案，区块链中没有任何一个节点有权限单独记录交易，从而避免了因单一记账人或节点被控制而造假的可能性。另一方面，由于全网节点参与记录，理论上讲，除非所有的节点都被破坏，否则交易记录就不会丢失，从而保证了数据的安全性。\n  加密技术和授权技术 区块链技术很好地集成了当前对称加密、非对称加密和哈希算法的许多优点，并使用了数字签名技术来保证交易的安全性，其中最具代表性的是使用椭圆曲线加密算法生成用户的公私钥对和使用椭圆曲线数字签名算法来保证交易安全。打包在区块上的交易信息对于参与共识的所有节点是公开的，但是账户的身份信息是经过严格加密的。\n  共识机制 共识机制是区块链系统中各个节点达成一致的策略和方法。区块链的共识机制替代了传统应用中保证信任和交易安全的第三方中心机构，能够降低由于各方不信任而产生的第三方信用成本、时间成本和资本耗用。常用的共识机制主要有PoW、PoS、DPoS、Paxos、PBFT等，共识机制既是数据写入的方式，也是防止篡改的手段。\n  智能合约 智能合约是可以自动化执行预先定义规则的一段计算机程序代码，它自己就是一个系统参与者。它能够实现价值的存储、传递、控制和管理，为基于区块链的应用提供了创新性的解决方案。\n  3. 区块链分类 按照节点参与方式的不同，区块链技术可以分为：公有链（Public Blockchain）、联盟链（Consortium Blockchain）和私有链（Private Blockchain）。按照权限的不同，区块链技术可以分为：许可链（Permissioned Blockchain）和非许可链（Permissionless Blockchain）。前述的三大类区块链技术中，联盟链和私有链属于许可链，公有链属于非许可链。\n  公有链\n公有链，顾名思义，就是公开的区块链。公有链是全公开的，所有人都可以作为网络中的一个节点，而不需要任何人给予权限或授权。在公有链中，每个节点都可以自由加入或者退出网络，参与链上数据的读写、执行交易，还可以参与网络中共识达成的过程，即决定哪个区块可以添加到主链上并记录当前的网络状态。公有链是完全意义上的去中心化区块链，它借助密码学中的加密算法保证链上交易的安全。在采取共识算法达成共识时，公有链主要采取工作量证明（PoW，Proof of Work）机制或权益证明（PoS，Proof of Stake）机制等共识算法，将经济奖励和加密数字验证结合起来，来达到去中心化和全网达成共识的目的。在这些算法共识形成的过程中，每个节点都可以为共识过程做出贡献，也是我们俗称的“挖矿”，来获取与贡献成正比的经济奖励，也就是系统中发行的数字代币。\n公有链通常也被称为公共链，它属于一种非许可链，不需要许可就可以自由参加退出。当前最典型的代表应用有比特币、以太坊（Ethereum）等。因其完全去中心化和面向大众的特性，公有链通常适用于“虚拟加密货币”和面向大众的一些金融服务以及电子商务等。\n  联盟链 联盟链不是完全去中心化的，而是一种多中心化或者部分去中心化的区块链。在区块链系统运行时，它的共识过程可能会受某些指定节点的控制。例如，在一个有15个金融机构接入的区块链系统中，每个机构都作为链上的一个节点，每确认一笔交易，都需要至少对10个节点进行确认（2/3确认），这笔交易或者这个区块才能被认可。联盟链账本上的数据与公有链的完全公开是不同的，只有联盟成员节点才可以访问，并且链上的读写权限、参与记账规则等操作也需要由联盟成员节点共同决定。由于联盟链场景中的参与者组成一个联盟， 参与共识的节点相对公有链而言会少很多，并且一般是针对某个商业场景，所以共识协议一般不采用与工作量证明类似的挖矿机制，同时也不一定需要代币作为激励机制，而是采用PBFT、RAFT这类适用于多中心化且效率较高的共识算法。同时，联盟链对交易的时间、状态、每秒交易数等与公有链有很大区别，所以它比公有链有更高的安全和性能要求。\n联盟链属于一种许可链，意味着不是任何人都能自由加入网络中，而是需要一定的权限许可，才可以作为一个新的节点加入。当前联盟链典型的代表有Linux基金会支持的超级账本（Hyperledger）项目、R3区块链联盟开发的Corda，以及趣链科技推出的Hyperchain平台等。\n  私有链 私有链，是指整个区块链上的所有写入权限仅仅掌握在一个组织手里，而读取权限可以根据情况对外开放或者任意进行限制。所以，私有链的应用场景一般是单一的企业内部总公司对分公司的管理方面，如数据库管理和审计等。相比于公有链和联盟链，私有链的价值主要体现在它可以提供一个安全、可追溯、不可篡改的平台，并且可以同时防止来自内部和外部的安全攻击。目前对于私有链确实存在着一些争议，有人认为私有链的意义不大，因为它需要依赖于第三方的区块链平台机构，所有的权限都被控制在一个节点中，已经违背了区块链技术的初衷，不能算一种区块链技术，而是已经存在的分布式账本技术。但是也有人认为私有链拥有很大的潜在价值，因为它可以给当前存在的许多问题提供一个很好的解决方案，比如企业内部规章制度的遵守、金融机构的反洗钱行为以及政府部门的预算和执行，等等。\n与联盟链一样，私有链也属于一种许可链，不过它的许可权掌握在单一节点中，在有些场景中，私有链还被称为专有链。当下私有链的应用不是很多，开创者都在努力探索之中。当前已经存在的应用主要有英国币科学公司（Coin Sciences Ltd.）推出的多链（Multichain）平台，这个平台的宗旨是希望能帮助各企业快速地部署私链环境，提供良好的隐私保护和权限控制。\n  自诞生至今，区块链技术经历了三次大的技术演进，其典型代表平台为2009年的比特币、2013年的以太坊和2015年的Fabric和Hyperchain，其组织形态从资源消耗严重、交易性能低下、缺乏灵活控制机制的公有区块链，向高效共识、智能可编程、可保护隐私的联盟区块链转变。当前，Hyperchian平台的TPS（每秒事务处理量）已达到千甚至万量级，可以满足大部分商业场景的需要。将来，随着技术的进一步发展，基于联盟链的区块链商业应用将成为区块链应用的主要形态。\n4. 区块链关键技术 4.1 基础模型 区块链基本架构可以分为数据层、网络层、共识层、激励层、合约层和应用层：\n 数据层封装了区块链的链式结构、区块数据以及非对称加密等区块链核心技术； 网络层提供点对点的数据通信传播以及验证机制； 共识层主要是网络节点间达成共识的各种共识算法； 激励层将经济因素引入到区块链技术体系之中，主要包括经济因素的发行机制和分配机制； 合约层展示了区块链系统的可编程性，封装了各类脚本、智能合约和算法； 应用层则封装了区块链技术的应用场景和案例。  在该架构中，基于时间戳的链式结构、分布式节点间的共识机制和可编程的智能合约是区块链技术最具代表性的创新点。一般可以在合约层编写智能合约或者进行脚本编程，来构建基于区块链的去中心化应用。下面将对本架构中每一层所涉及的技术展开具体介绍。 4.2 数据层 数据层是区块链的核心部分，区块链本质上是一种数据库技术和分布式共享账本，是由包含交易信息的区块从后向前有序连接起来的一种数据结构。该层涉及的技术主要包括：区块结构、Merkle树、非对称加密、时间戳、数字签名和哈希函数。时间戳和哈希函数相对比较简单，这里重点介绍一下区块结构、Merkle树、非对称加密和数字签名。\n区块结构 每个区块一般都由区块头和区块体两部分组成。如图所示，区块头部分包含了父区块哈希值、时间戳、Merkle根等信息，而区块体部分则包含着此区块中所有的交易信息。除此之外，每一个区块还对应着两个值来识别区块：区块头哈希值和区块高度。 每一个区块都会有一个区块头哈希值，这是一个通过SHA256算法对区块头进行二次哈希计算而得到的32字节的数字指纹。例如，比特币的第一个区块的头哈希值为000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f。区块头哈希值可以唯一标识一个区块链上的区块，并且任何节点通过对区块头进行简单的哈希计算都可以得到该区块头的哈希值。区块头哈希也包含在区块的整体数据结构中，但是区块头的数据和区块体的数据并不一定一起存储，为了检索效率起见，在实现中可以将二者分开存储。\n除了通过头哈希值来识别区块，还可以通过区块高度来对区块进行识别。例如高度为0和前面000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f所索引的区块都是第一个区块。但是与头哈希值不同的是，区块高度并不能唯一地标识一个区块。由于区块链存在着分叉情况，所以可能存在2个或以上区块的区块高度是一样的\nMerkle树 前面介绍了区块头哈希值、区块高度和区块头的结构，接着来看看区块体。区块体存储着交易信息，在区块中它们是以一棵Merkle树的数据结构进行存储的，而Merkle树是一种用来有效地总结区块中所有交易的数据结构。Merkle树是一棵哈希二叉树，树的每个叶子节点都是一笔交易的哈希值。以比特币为例，在比特币网络中，Merkle树被用来归纳一个区块中的所有交易，同时生成整个交易集合的数字指纹即Merkle树根，且提供了一种校验区块是否存在某交易的高效途径。生成一棵Merkle树需要递归地对每两个哈希节点进行哈希得到一个新的哈希值，并将新的哈希值存入Merkle树中，直到两两结合最终只有一个哈希值时，这个哈希值就是这一区块所有交易的Merkle根，存储到上面介绍的区块头结构中。 非对称加密与数字签名 非对称加密是区块链技术中用于安全性需求和所有权认证时采用的加密技术，常见的非对称加密算法有RSA、Elgamal、背包算法、Rabin、D-H、ECC（椭圆曲线加密算法）和ECDSA（椭圆曲线数字签名算法），等等。与对称加密算法不同的是，非对称加密算法需要两个密钥：公开密钥（public key）和私有密钥（private key）。基于非对称加密算法可使通信双方在不安全的媒体上交换信息，安全地达成信息的一致。公开密钥是对外公开的，而私有密钥是保密的，其他人不能通过公钥推算出对应的私钥。每一个公开密钥都有其相对应的私有密钥，如果我们使用公开密钥对信息进行了加密，那么则必须有对应的私有密钥才能对加密后的信息进行解密；而如果是用私有密钥加密信息，则只有对应的公开密钥才可以进行解密。在区块链中，非对称加密主要用于信息加密、数字签名等场景。\n在信息加密场景中，如图所示，信息发送者A需要发送一个信息给信息接收者B，需要先使用B的公钥对信息进行加密，B收到后，使用自己的私钥就可以对这一信息进行解密，而其他人没有私钥，是没办法对这个加密信息进行解密的。 而在数字签名场景中，如图所示，发送者A先用哈希函数对原文生成一个摘要（Digest），然后使用私钥对摘要进行加密，生成数字签名（Signature），之后将数字签名与原文一起发送给接收者B；B收到信息后使用A的公钥对数字签名进行解密得到摘要，由此确保信息是A发出的，然后再对收到的原文使用哈希函数产生摘要，并与解密得到的摘要进行对比，如果相同，则说明收到的信息在传输过程中没有被修改过。 4.3 网络层 网络层是区块链平台信息传输的基础，通过P2P的组网方式、特定的信息传播协议和数据验证机制，使得区块链网络中的每个节点都可以平等地参与共识与记账。下面将详细介绍区块链平台网络层中的P2P网络架构、信息传输机制和数据验证机制。\nP2P网络架构\n区块链网络架构一般采用的是基于互联网的P2P（peer-to-peer）架构，在P2P网络中，每台计算机每个节点都是对等的，它们共同为全网提供服务。而且，没有任何中心化的服务端，每台主机都可以作为服务端响应请求，也可以作为客户端使用其他节点所提供的服务。P2P通信不需要从其他实体或CA获取地址验证，因此有效地消除了篡改的可能性和第三方欺骗。所以P2P网络是去中心化和开放的，这也正符合区块链技术的理念。\n在区块链网络中，所有的节点地位均等且以扁平式拓扑结构相互连通和交互，每个节点都需要承担网络路由、验证区块数据、传播区块数据等功能。在比特币网络中，存在着两类节点，一类是全节点，它保存着区块链上所有的完整数据信息，并需要实时地参与区块链数据的校验和记录来更新区块链主链。另一类是轻节点，它只保存着区块链中的部分信息，通过简易支付验证（SPV）方式向其他相邻的节点请求数据以便完成数据的验证。\n传输机制\n在新的区块数据生成后，生成该数据的节点会将其广播到全网的其他节点以供验证。目前的区块链底层平台一般都会根据自身的实际应用需求，在比特币传输机制的基础上重新设计或者改进出新的传输机制，如以太坊区块链集成了所谓的“幽灵协议”，以解决因区块数据确认速度快而导致的高区块作废率和随之而来的安全性风险。这里我们以中本聪设计的比特币系统为例，列出其传输协议的步骤如下：\n  比特币交易节点将新生成的交易数据向全网所有节点进行广播；\n  每个节点都将收集到的交易数据存储到一个区块中；\n  每个节点基于自身算力在区块中找到一个具有足够难度的工作量证明；\n  当节点找到区块的工作量证明后，就向全网所有节点广播此区块；\n  只有包含在区块中的所有交易都有效且之前未存在过，其他节点才认同该区块的有效性；\n  其他节点接收该数据区块，并在该区块的末尾制造新的区块以延长链，而将被接收的区块的随机哈希值视为新区块的前序区块哈希值。\n  如果交易的相关节点是一个未与其他节点相连接的新节点，比特币系统通常会将一组长期稳定运行的“种子节点”推荐给新节点以建立连接，或者推荐至少一个节点连接新节点。此外，进行广播的交易数据并不需要全部节点都接收到，只要有足够多的节点做出响应，交易数据便可整合到区块链账本中。而未接收到完整交易数据的节点可以向临近节点请求下载缺失的交易数据。\n验证机制 在区块链网络中，所有的节点都会时刻监听网络中广播的交易数据和新产生的区块。在接收到相邻节点发来的数据后，会首先验证该数据的有效性，若数据有效则按接收顺序为新数据建立存储池来暂存这些数据，并且继续向临近节点转发；若数据无效则立即废弃该数据，从而保证无效数据不会在区块链网络中继续传播。验证有效性的方法是根据预定义好的标准，从数据结构、语法规范性、输入输出和数字签名等各方面进行校验。对于新区块的校验同理，某节点产生出新区块后，其他节点按照预定义的标准对新区块的工作量证明、时间戳等方面进行校验，若确认有效，则将该区块链接到主区块链上，并开始争取下一个区块的记账权。\n4.4 共识层 Leslie Lamport于1982年提出著名的拜占庭将军问题，引发了无数研究者探索解决方案。如何在分布式系统中高效地达成共识是分布式计算领域的一个重要研究课题。区块链的共识层的作用就是在不同的应用场景下通过使用不同的共识算法，在决策权高度分散的去中心化系统中使得各个节点高效地达成共识。\n最初，比特币区块链选用了一种依赖节点算力的工作量证明共识（Proof of Work，PoW）机制来保证比特币网络分布式记账的一致性。之后随着区块链技术的不断演进和改进，研究者陆续提出了一些不过度依赖算力而能达到全网一致的算法，比如权益证明共识（Proof of Stake，PoS）机制、授权股份证明共识（Delegated Proof of Stake，DPoS）机制、实用拜占庭容错（Practical Byzantine Fault Tolerance，PBFT）算法，等等。下面我们对这几种共识算法进行简单介绍。\nPoW（工作量证明机制）\nPoW机制诞生于1997年 Adam Back 设计的Hashcash系统，它最初被创造出来用于预防邮件系统中漫天遍地的垃圾邮件。2009年，中本聪将PoW机制运用于比特币区块链网络中，作为达成全网一致性的共识机制。从严格意义上讲，比特币中所采用的是一种可重复使用的Hashcash工作证明，使得生成工作证明量可以是一个概率意义上的随机过程。在该机制中，网络上的每一个节点都在使用SHA256哈希算法运算一个不断变化的区块头的哈希值。共识要求算出的值必须等于或者小于某个给定的值。在分布式网络中，所有的参与者都需要使用不同的随机数来持续计算该哈希值，直到达到目标为止。当一个节点得出了确切的值，其他所有的节点必须相互确认该值的正确性。之后，新区块中的交易将被验证以防欺诈。然后，用于计算的交易信息的集合会被确认为认证结果，用区块链中的新区块表示。在比特币中，运算哈希值的节点被称作“矿工”，而PoW的过程被称为“挖矿”。由于认证的计算是一个耗时的过程，所以也提出了相应的激励机制（例如向矿工授予一小部分比特币）。总的来说，工作量证明就是对于工作量的证明，每个区块加入到链上，必须得到网络参与者的同意验证，矿工对它完成了相对应的工作量。PoW的优点是完全的去中心化和分布式账簿。缺点也很明显，即消耗资源：挖矿行为造成了大量的资源浪费，同时PoW达成共识的周期也比较长，比特币网络会自动调整目标值来确保区块生成过程大约需要10分钟，因此它不是很适合商业运用。\nPoS（股权证明机制）\nPoS的想法源于尼克·萨博（Nick Szabo），是PoW的一种节能替代选择，它不需要用户在不受限制的空间中找到一个随机数，而是要求人们证明货币数量的所有权，因为其相信拥有货币数量多的人攻击网络的可能性更低。由于基于账户余额的选择是非常不公平的，因为单一最富有的人势必在网络中占主导地位，所以提出了许多解决方案，结合股权来决定谁来创建下一个块。其中，Blackcoin使用随机选择来预测下一个创建者，而Peercoin则倾向于基于币龄来选择。Peercoin首次开创性地实现了真正的股权证明，它采用工作量证明机制发行新币，采用股权证明机制维护网络安全，这也是“虚拟货币”历史上的一次创举。同比特币网络要求证明人执行一定量的工作不同，该机制只需要证明人提供一定数量“数字货币”的所有权即可。在股权证明机制中，每当创建一个区块时，矿工需要创建一个称为“币权”的交易，这个交易会按照一定的比例预先将一些币发给矿工。然后股权证明机制根据每个节点持有代币的比例和时间，依据算法等比例地降低节点的挖矿难度，以加快节点寻找随机数的速度，缩短达成共识所需的时间。与PoW相比，PoS可以节省更多的能源，更有效率。但是，由于挖矿成本接近于零，因此可能会遭受攻击。且PoS在本质上仍然需要网络中的节点进行挖矿运算，所以它同样难以应用于商业领域。\nDPoS（股份授权证明机制）\nDPoS由比特股（Bitshares）项目组发明。股权拥有者选举他们的代表来进行区块的生成和验证。DPoS类似于现代企业董事会制度，比特股系统将代币持有者称为股东，由股东投票选出101名代表，然后由这些代表负责生成和验证区块。持币者若想成为一名代表，需先用自己的公钥去区块链注册，获得一个长度为32位的特有身份标识符，股东可以对这个标识符以交易的形式进行投票，得票数前101位被选为代表。代表们轮流产生区块，收益（交易手续费）平分。如果有的代表不老实生产区块，很容易被其他代表和股东发现，他将立即被踢出“董事会”，空缺位置由票数排名102的代表自动填补。DPoS的优点在于大幅减少了参与区块验证和记账的节点数量，从而缩短了共识验证所需要的时间，大幅提高了交易效率。从某种角度来说，DPoS可以理解为多中心系统，兼具去中心化和中心化优势\nPBFT（实用拜占庭容错算法）\n这个算法最初出现在MIT的Miguel和Barbara Liskov的学术论文中[33]，初衷是为一个低延迟存储系统所设计，降低算法的复杂度，该算法可以应用于吞吐量不大但需要处理大量事件的数字资产平台。它允许每个节点发布公钥，任何通过节点的消息都由节点签名，以验证其格式。验证过程分为三个阶段：预备、准备、落实。如果已经收到超过$\\frac{1}{3}$不同节点的批准，服务操作将是有效的。使用PBFT，区块链网络$N$个节点中可以包含$f$个拜占庭恶意节点，其中$f=\\frac{N-1}{3}$。 换句话说，PBFT确保至少$2f+1$个节点在将信息添加到分布式共享账簿之前达到共识。目前，HyperLedger联盟、中国ChinaLedger 联盟等诸多区块链联盟都在研究和验证这个算法的实际部署和应用。\n4.5 激励层 激励层作为将经济因素引入区块链技术的一个层次，其存在的必要性取决于建立在区块链技术上的具体应用需求。这里以比特币系统为例，对其激励层进行介绍。\n在比特币系统中，大量的节点算力资源通过共识过程得以汇聚，从而实现区块链账本的数据验证和记账工作，因而其本质上是一种共识节点间的任务众包过程。在去中心化系统中，共识节点本身是自利的，其参与数据验证和记账工作的根本目的是最大化自身收益。所以，必须设计合理的激励机制，使得共识节点最大化自身收益的个体行为与区块链系统的安全性和有效性相契合，从而使大规模的节点对区块链历史形成稳定的共识。\n比特币采用PoW共识机制，在该共识中其经济激励由两部分组成：一是新发行的比特币；二是交易流通过程中的手续费。两者组合在一起，奖励给PoW共识过程中成功计算出符合要求的随机数并生成新区块的节点。因此，只有当各节点达成共识，共同合作来构建和维护区块链历史记录及其系统的有效性，当作奖励的比特币才会有价值。\n 发行机制  在比特币系统中，新区块产生发行比特币的数量是随着时间阶梯型递减的。从创世区块起，每个新区块将发行50个比特币奖励给该区块的记账者，此后每隔约4年（21万个区块），每个新区块发行的比特币数量减少一半，以此类推，一直到比特币的数量稳定在上限2100万为止。前文提到过，给记账者的另一部分奖励是比特币交易过程中产生的手续费，目前默认的手续费是1/10000个比特币。两部分费用会被封装在新区块的第一个交易（称为Coinbase交易）中。虽然现在每个新区块的总手续费与新发行的比特币相比要少得多，但随着时间推移，未来比特币的发行数量会越来越少，甚至停止发行，到那时手续费便会成为共识节点记账的主要动力。此外，手续费还可以起到保障安全性的作用，防止大量微额交易对比特币系统发起“粉尘攻击”。\n分配机制  随着比特币挖矿生态圈的成熟，“矿池”出现在人们的视野中。大量的小算力节点通过加入矿池而联合起来，相互合作汇集算力来提高获得记账权的概率，并共享生成新区块得到的新发行比特币和交易手续费奖励。据Bitcoinminning.com统计，目前已经存在13种不同的分配机制。现今主流矿池通常采用PPLNS（Pay Per Last N Shares）、PPS（Pay Per Share）和PROP（PRO Portionately）等机制。在矿池中，根据各个节点贡献的算力，按比例划分为不同的股份。PPLNS机制在产生新的区块后，各合作节点根据其在最后N个股份内贡献的实际股份比例来分配奖励；PPS则直接根据股份比例为各节点估算和支付一个固定的理论收益，采用此方式的矿池将会适度收取手续费来弥补其为各个节点承担的收益不确定性风险；PROP机制则根据节点贡献的股份按比例地分配奖励。\n4.6 合约层 合约层封装了各类脚本、算法和智能合约，是区块链可编程性的体现。比特币本身就具有简单脚本的编写功能，而以太坊极大地强化了编程语言协议，理论上可以编写实现任何功能的应用。如果把比特币看成是全球账本的话，以太坊可以看作一台“全球计算机”，任何人都可以上传和执行任意的应用程序，并且程序的有效执行能得到保证。如果说数据、网络和共识三个层次作为区块链底层“虚拟机”，分别承担数据表示、数据传播和数据验证功能，合约层则是建立在区块链虚拟机之上的商业逻辑和算法，是实现区块链系统灵活编程和操作数据的基础。包括比特币在内的“数字加密货币”大多采用非图灵完备的简单脚本代码来编程控制交易过程，这也是智能合约的雏形。随着技术的发展，目前已经出现以太坊等图灵完备的可实现更为复杂和灵活的智能合约的脚本语言，使得区块链能够支持宏观金融和社会系统的诸多应用。\n智能合约的概念可以追溯到1995年，是由学者尼克·萨博提出并进行如下定义的：“一个智能合约是一套以数字形式定义的承诺，包括合约参与方可以在上面执行这些承诺的协议。”其设计初衷是希望通过将智能合约内置到物理实体来创造各种灵活可控的智能资产。但由于计算手段的落后和应用场景的缺失，智能合约在当时并未受到研究者的广泛关注。\n区块链技术的出现对智能合约进行了新的定义并使其成为了可能。智能合约作为区块链技术的关键特性之一，是运行在区块链上的模块化、可重用、自动执行的脚本，能够实现数据处理、价值转移、资产管理等一系列功能。合约部署的时候被虚拟机编译成操作码存储在区块链上，对应地会有一个存储地址。当预定的条件发生时，就会发送一笔交易（transaction）到该合约地址，全网节点都会执行合约脚本编译生成的操作码，最后将执行结果写入区块链[。作为一种嵌入式程序化合约，智能合约可以内置在任何区块链数据、交易或资产中，形成可由程序自行控制的系统、市场或资产。智能合约不仅为金融行业提供了创新性的解决方案，同时也能在社会系统中的信息、资产、合同、监管等事务管理中发挥重要作用。\n基于区块链技术的智能合约不仅可以发挥智能合约在成本效率方面的优势，还可以避免恶意行为对合约正常执行的干扰。智能合约可以应用到任何一种数据驱动的业务逻辑中，以太坊首先看到了区块链和智能合约的契合，发布了白皮书《以太坊：下一代智能合约和去中心化应用平台》，构建了内置有图灵完备编程语言的公有区块链，使得任何人都能够创建合约和去中心化应用。\n智能合约与区块链的结合，丰富了区块链本身的价值内涵，其特性有以下3点：\n 用程序逻辑中的丰富合约规则表达能力实现了不信任方之间的公平交换，避免了恶意方中断协议等可能性； 最小化交易方之间的交互，避免了计划外的监控和跟踪的可能性； 丰富了交易与外界状态的交互，比如可信数据源提供的股票信息、天气预报等。  ","permalink":"http://yangchnet.github.io/Dessert/posts/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8/","summary":"1. 区块链定义 区块链技术本质上是一个去中心化的数据库，它是比特币的核心技术与基础架构，是分布式数据存储、点对点传输、共识机制、加密算法等计算机技术的新型应用模式。狭义来讲，区块链是一种按照时间顺序将数据区块以顺序相连的方式组合成的一种链式数据结构，并以密码学方式保证的不可篡改、不可伪造的分布式账本。广义来讲，区块链技术是利用块链式数据结构来验证与存储数据、利用分布式节点共识算法来生成和更新数据、利用密码学方式保证数据传输和访问的安全、利用由自动化脚本代码组成的智能合约来编程和操作数据的一种全新的分布式基础架构与计算范式。\n1.1 区块链的技术特征 区块链上存储的数据需由全网节点共同维护，可以在缺乏信任的节点之间有效地传递价值。相比现有的数据库技术，区块链具有以下技术特征。\n 块链式数据结构\n区块链利用块链式数据结构来验证和存储数据，通过上文对区块链基本概念的介绍可以知道，每个区块打包记录了一段时间内发生的交易是对当前账本的一次共识，并且通过记录上一个区块的哈希值进行关联，从而形成块链式的数据结构。 分布式共识算法\n区块链系统利用分布式共识算法来生成和更新数据，从技术层面杜绝了非法篡改数据的可能性，从而取代了传统应用中保证信任和交易安全的第三方中介机构，降低了为维护信用而造成的时间成本、人力成本和资源耗用 密码学方式\n区块链系统利用密码学的方式保证数据传输和访问的安全。存储在区块链上的交易信息是公开的，但账户的身份信息是高度加密的。区块链系统集成了对称加密、非对称加密及哈希算法的优点，并使用数字签名技术来保证交易的安全。  1.2 区块链的功能特征 区块链系统的以上技术特征决定了其应用具有如下功能特征。\n  多中心 不同于传统应用的中心化数据管理，区块链网络中有多个机构进行相互监督并实时对账，从而避免了单一记账人造假的可能性，提高了数据的安全性。\n  自动化 区块链系统中的智能合约是可以自动化执行一些预先定义好的规则和条款的一段计算机程序代码，它大大提高了经济活动与契约的自动化程度。\n  可信任 存储在区块链上的交易记录和其他数据是不可篡改并且可溯源的，所以能够很好地解决各方不信任的问题，无需第三方可信中介。\n  2. 区块链的相关概念 区块链以密码学的方式维护一份不可篡改和不可伪造的分布式账本，并通过基于协商一致的规范和协议（共识机制）解决了去中心化的记账系统的一致性问题，其相关概念主要包括以下三个。\n 交易（Transaction）\n区块链上每一次导致区块状态变化的操作都称为交易，每一次交易对应唯一的交易哈希值，一段时间后便会对交易进行打包。 区块（Block）\n打包记录一段时间内发生的交易和状态结果，是对当前账本的一次共识。每个区块以一个相对平稳的时间间隔加入到链上，在企业级区块链平台中，共识时间可以动态设置。 链（Chain）\n区块按照时间顺序串联起来，通过每个区块记录上一个区块的哈希值关联，是整个状态改变的日志记录。   区块链的主要结构  如何解决交易中的信任和安全问题 区块链技术体系不是通过一个权威的中心化机构来保证交易的可信和安全，而是通过加密和分布式共识机制来解决信任和安全问题，其主要技术创新有以下4点。\n  分布式账本 交易是由分布式系统中的多个节点共同记录的。每一个节点都记录完整的交易记录，因此它们都可以参与监督交易合法性并验证交易的有效性。不同于传统的中心化技术方案，区块链中没有任何一个节点有权限单独记录交易，从而避免了因单一记账人或节点被控制而造假的可能性。另一方面，由于全网节点参与记录，理论上讲，除非所有的节点都被破坏，否则交易记录就不会丢失，从而保证了数据的安全性。\n  加密技术和授权技术 区块链技术很好地集成了当前对称加密、非对称加密和哈希算法的许多优点，并使用了数字签名技术来保证交易的安全性，其中最具代表性的是使用椭圆曲线加密算法生成用户的公私钥对和使用椭圆曲线数字签名算法来保证交易安全。打包在区块上的交易信息对于参与共识的所有节点是公开的，但是账户的身份信息是经过严格加密的。\n  共识机制 共识机制是区块链系统中各个节点达成一致的策略和方法。区块链的共识机制替代了传统应用中保证信任和交易安全的第三方中心机构，能够降低由于各方不信任而产生的第三方信用成本、时间成本和资本耗用。常用的共识机制主要有PoW、PoS、DPoS、Paxos、PBFT等，共识机制既是数据写入的方式，也是防止篡改的手段。\n  智能合约 智能合约是可以自动化执行预先定义规则的一段计算机程序代码，它自己就是一个系统参与者。它能够实现价值的存储、传递、控制和管理，为基于区块链的应用提供了创新性的解决方案。\n  3. 区块链分类 按照节点参与方式的不同，区块链技术可以分为：公有链（Public Blockchain）、联盟链（Consortium Blockchain）和私有链（Private Blockchain）。按照权限的不同，区块链技术可以分为：许可链（Permissioned Blockchain）和非许可链（Permissionless Blockchain）。前述的三大类区块链技术中，联盟链和私有链属于许可链，公有链属于非许可链。","title":"区块链基础入门"},{"content":"1. sync.Mutex互斥锁 不同goroutine之间对公共资源进行访问需要使用互斥锁。例如在对银行账户的操作中，如果我们有两种操作，一个是查询余额，一个是存款。其操作如下：\npackage bank // 存款余额 var balance int // 存款 func Deposit(amount int) { balance = balance + amount } // 查询 func Balance() int { return balance } // Alice: go func() { bank.Deposit(200) // A1  fmt.Println(\u0026#34;=\u0026#34;, bank.Balance()) // A2 }() // Bob: go bank.Deposit(100) // B 这其中，若把A1分为两个操作，A1r：把余额从内存中读出来；A2w：把修改后的余额写入内存。\n若执行顺序为A1r → B → A1w → A2， 正常情况下，Alice和Bob分别存入了$200，$100，因此最后的存款应该是300，但最后输出结果为200。因为A在计算时是按照A1r读出的数值进行计算，忽略了B的操作，A与B之间发生了数据竞争。\n 数据竞争：无论任何时候，只要有两个goroutine并发访问同一变量，且至少其中的一个是写操作的时候就会发生数据竞争。\n 解决此问题的办法之一是使用互斥锁。\nimport \u0026#34;sync\u0026#34; var ( mu sync.Mutex // guards balance  balance int ) func Deposit(amount int) { mu.Lock() defer mu.Unlock() balance = balance + amount } func Balance() int { mu.Lock() defer mu.Unlock() return balance } 每次一个goroutine访问bank变量时(这里只有balance余额变量)，它都会调用mutex的Lock方法来获取一个互斥锁。如果其它的goroutine已经获得了这个锁的话，这个操作会被阻塞直到其它goroutine调用了Unlock使该锁变回可用状态.\n2. sync.RWMutex读写锁 由于Balance函数只需要读取变量的状态，所以我们同时让多个Balance调用并发运行事实上 是安全的，只要在运行的时候没有存款或者取款操作就行。在这种场景下我们需要一种特殊 类型的锁，其允许多个只读操作并行执行，但写操作会完全互斥。这种锁叫作“多读单写”锁 (multiple readers, single writer lock)，Go语言提供的这样的锁是sync.RWMutex：\nvar mu sync.RWMutex var balance int func Balance() int { mu.RLock() // readers lock  defer mu.RUnlock() return balance } 读写锁的规则是\n 读锁不能阻塞读锁 读锁需要阻塞写锁，直到所有读锁都释放 写锁需要阻塞读锁，直到所有写锁都释放 写锁需要阻塞写锁  ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/go%E4%B8%AD%E7%9A%84%E9%94%81/","summary":"1. sync.Mutex互斥锁 不同goroutine之间对公共资源进行访问需要使用互斥锁。例如在对银行账户的操作中，如果我们有两种操作，一个是查询余额，一个是存款。其操作如下：\npackage bank // 存款余额 var balance int // 存款 func Deposit(amount int) { balance = balance + amount } // 查询 func Balance() int { return balance } // Alice: go func() { bank.Deposit(200) // A1  fmt.Println(\u0026#34;=\u0026#34;, bank.Balance()) // A2 }() // Bob: go bank.Deposit(100) // B 这其中，若把A1分为两个操作，A1r：把余额从内存中读出来；A2w：把修改后的余额写入内存。\n若执行顺序为A1r → B → A1w → A2， 正常情况下，Alice和Bob分别存入了$200，$100，因此最后的存款应该是300，但最后输出结果为200。因为A在计算时是按照A1r读出的数值进行计算，忽略了B的操作，A与B之间发生了数据竞争。\n 数据竞争：无论任何时候，只要有两个goroutine并发访问同一变量，且至少其中的一个是写操作的时候就会发生数据竞争。\n 解决此问题的办法之一是使用互斥锁。\nimport \u0026#34;sync\u0026#34; var ( mu sync.Mutex // guards balance  balance int ) func Deposit(amount int) { mu.","title":"Go中的锁"},{"content":"三个goroutine分别输出张三、李四、王五，使其按上述顺序输出5遍。 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var w sync.WaitGroup func main() { w.Add(15) chan1 := make(chan struct{}, 0) chan2 := make(chan struct{}, 0) for i := 0; i \u0026lt; 5; i++ { go func() { defer w.Done() fmt.Println(\u0026#34;张三\u0026#34;) chan1 \u0026lt;- struct{}{} }() go func() { defer w.Done() \u0026lt;- chan1 fmt.Println(\u0026#34;李四\u0026#34;) chan2 \u0026lt;- struct{}{} }() go func() { defer w.Done() \u0026lt;- chan2 fmt.Println(\u0026#34;王五\u0026#34;) }() } w.Wait() } 编写程序输出某目录下的所有文件（包括子目录） package main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) func main() { dir := os.Args[1] listAll(dir, 0) } func listAll(path string, curHier int) { fileInfos, err := ioutil.ReadDir(path) if err != nil { fmt.Println(err) return } for _, info := range fileInfos { if info.IsDir(){ for tmpHier := curHier; tmpHier \u0026gt; 0; tmpHier-- { fmt.Printf(\u0026#34;|\\t\u0026#34;) } fmt.Println(info.Name(), \u0026#34;\\\\\u0026#34;) listAll(path + \u0026#34;/\u0026#34; + info.Name(), curHier + 1) } else { for tmpHier := curHier; tmpHier \u0026gt; 0; tmpHier-- { fmt.Printf(\u0026#34;|\\t\u0026#34;) } fmt.Println(info.Name()) } } } ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E9%9D%A2%E8%AF%95%E9%A2%98/","summary":"三个goroutine分别输出张三、李四、王五，使其按上述顺序输出5遍。 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var w sync.WaitGroup func main() { w.Add(15) chan1 := make(chan struct{}, 0) chan2 := make(chan struct{}, 0) for i := 0; i \u0026lt; 5; i++ { go func() { defer w.Done() fmt.Println(\u0026#34;张三\u0026#34;) chan1 \u0026lt;- struct{}{} }() go func() { defer w.Done() \u0026lt;- chan1 fmt.Println(\u0026#34;李四\u0026#34;) chan2 \u0026lt;- struct{}{} }() go func() { defer w.Done() \u0026lt;- chan2 fmt.Println(\u0026#34;王五\u0026#34;) }() } w.Wait() } 编写程序输出某目录下的所有文件（包括子目录） package main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;os\u0026#34; ) func main() { dir := os.","title":"面试题golang"},{"content":" 转载自：https://segmentfault.com/a/1190000011365430\n 1. WSGI介绍 1.1 什么是WSGI 首先介绍几个关于WSGI相关的概念 WSGI：全称是Web Server Gateway Interface，WSGI不是服务器，python 模块，框架，API或者任何软件，只是一种规范，描述web server如何与web application通信的规范。server和application的规范在PEP 3333中有具体描述。要实现WSGI协议，必须同时实现web server和web application，当前运行在WSGI协议之上的web框架有Torando,Flask,Django\nuwsgi:与WSGI一样是一种通信协议，是uWSGI服务器的独占协议，用于定义传输信息的类型(type of information)，每一个uwsgi packet前4byte为传输信息类型的描述，与WSGI协议是两种东西，据说该协议是fcgi协议的10倍快。\nuWSGI：是一个web服务器，实现了WSGI协议、uwsgi协议、http协议等。\nWSGI协议主要包括server和application两部分：\n WSGI server负责从客户端接收请求，将request转发给application，将application返回的response返回给客户端； WSGI application接收由server转发的request，处理请求，并将处理结果返回给server。application中可以包括多个栈式的中间件(middlewares)，这些中间件需要同时实现server与application，因此可以在WSGI服务器与WSGI应用之间起调节作用：对服务器来说，中间件扮演应用程序，对应用程序来说，中间件扮演服务器。\n WSGI协议其实是定义了一种server与application解耦的规范，即可以有多个实现WSGI server的服务器，也可以有多个实现WSGI application的框架，那么就可以选择任意的server和applicatiodn组合实现自己的web应用。例如uWSGI和Gunicorn都是实现了WSGI server协议的服务器，Django，Flask是实现了WSGI application协议的web框架，可以根据项目实际情况搭配使用。\n以上介绍了相关的常识，接下来我们来看看如何简单实现WSGI协议。\n1.2 怎么实现WSGI 上文说过，实现WSGI协议必须要有wsgi server和application，因此，我们就来实现这两个东西。\n我们来看看官方WSGI使用WSGI的wsgiref模块实现的小demo\ndef demo_app(environ,start_response): from StringIO import StringIO stdout = StringIO() print \u0026gt;\u0026gt;stdout, \u0026#34;Hello world!\u0026#34; print \u0026gt;\u0026gt;stdout h = environ.items(); h.sort() for k,v in h: print \u0026gt;\u0026gt;stdout, k,\u0026#39;=\u0026#39;, repr(v) start_response(\u0026#34;200 OK\u0026#34;, [(\u0026#39;Content-Type\u0026#39;,\u0026#39;text/plain\u0026#39;)]) return [stdout.getvalue()] httpd = make_server(\u0026#39;localhost\u0026#39;, 8002, demo_app) httpd.serve_forever() # 使用select  实现了一个application，来获取客户端的环境和回调函数两个参数，以及httpd服务端的实现，我们来看看make_server的源代码\ndef make_server( host, port, app, server_class=WSGIServer, handler_class=WSGIRequestHandler): \u0026#34;\u0026#34;\u0026#34;Create a new WSGI server listening on `host` and `port` for `app`\u0026#34;\u0026#34;\u0026#34; server = server_class((host, port), handler_class) server.set_app(app) return server 下面我们自己来实现一遍： WSGI 规定每个 python 程序（Application）必须是一个可调用的对象（实现了__call__ 函数的方法或者类），接受两个参数 environ（WSGI 的环境信息） 和 start_response（开始响应请求的函数），并且返回 iterable。几点说明：\nenviron 和 start_response 由 http server 提供并实现 environ 变量是包含了环境信息的字典 Application 内部在返回前调用 start_response start_response也是一个 callable，接受两个必须的参数，status（HTTP状态）和 response_headers（响应消息的头） 可调用对象要返回一个值，这个值是可迭代的。  application\n # 1. 可调用对象是一个函数 def application(environ, start_response): response_body = \u0026#39;The request method was %s\u0026#39; % environ[\u0026#39;REQUEST_METHOD\u0026#39;] # HTTP response code and message status = \u0026#39;200 OK\u0026#39; # 应答的头部是一个列表，每对键值都必须是一个 tuple。 response_headers = [(\u0026#39;Content-Type\u0026#39;, \u0026#39;text/plain\u0026#39;), (\u0026#39;Content-Length\u0026#39;, str(len(response_body)))] # 调用服务器程序提供的 start_response，填入两个参数 start_response(status, response_headers) # 返回必须是 iterable return [response_body] # 2. 可调用对象是一个类 class AppClass: \u0026#34;\u0026#34;\u0026#34;这里的可调用对象就是 AppClass 这个类，调用它就能生成可以迭代的结果。 使用方法类似于： for result in AppClass(env, start_response): do_somthing(result) \u0026#34;\u0026#34;\u0026#34; def __init__(self, environ, start_response): self.environ = environ self.start = start_response def __iter__(self): status = \u0026#39;200 OK\u0026#39; response_headers = [(\u0026#39;Content-type\u0026#39;, \u0026#39;text/plain\u0026#39;)] self.start(status, response_headers) yield \u0026#34;Hello world!\\n\u0026#34; # 3. 可调用对象是一个实例  class AppClass: \u0026#34;\u0026#34;\u0026#34;这里的可调用对象就是 AppClass 的实例，使用方法类似于： app = AppClass() for result in app(environ, start_response): do_somthing(result) \u0026#34;\u0026#34;\u0026#34; def __init__(self): pass def __call__(self, environ, start_response): status = \u0026#39;200 OK\u0026#39; response_headers = [(\u0026#39;Content-type\u0026#39;, \u0026#39;text/plain\u0026#39;)] self.start(status, response_headers) yield \u0026#34;Hello world!\\n\u0026#34;  server\n 上面已经说过，标准要能够确切地实行，必须要求程序端和服务器端共同遵守。上面提到， envrion 和 start_response 都是服务器端提供的。下面就看看，服务器端要履行的义务。\n准备 environ 参数 定义 start_response 函数 调用程序端的可调用对象 import os, sys def run_with_cgi(application): # application 是程序端的可调用对象 # 准备 environ 参数，这是一个字典，里面的内容是一次 HTTP 请求的环境变量 environ = dict(os.environ.items()) environ[\u0026#39;wsgi.input\u0026#39;] = sys.stdin environ[\u0026#39;wsgi.errors\u0026#39;] = sys.stderr environ[\u0026#39;wsgi.version\u0026#39;] = (1, 0) environ[\u0026#39;wsgi.multithread\u0026#39;] = False environ[\u0026#39;wsgi.multiprocess\u0026#39;] = True environ[\u0026#39;wsgi.run_once\u0026#39;] = True environ[\u0026#39;wsgi.url_scheme\u0026#39;] = \u0026#39;http\u0026#39; headers_set = [] headers_sent = [] # 把应答的结果输出到终端 def write(data): sys.stdout.write(data) sys.stdout.flush() # 实现 start_response 函数，根据程序端传过来的 status 和 response_headers 参数， # 设置状态和头部 def start_response(status, response_headers, exc_info=None): headers_set[:] = [status, response_headers] return write # 调用客户端的可调用对象，把准备好的参数传递过去 result = application(environ, start_response) # 处理得到的结果，这里简单地把结果输出到标准输出。 try: for data in result: if data: # don\u0026#39;t send headers until body appears write(data) finally: if hasattr(result, \u0026#39;close\u0026#39;): result.close() 2. 由Django框架分析WSGI 下面我们以django为例，分析一下wsgi的整个流程\n2.1 django WSGI application WSGI application应该实现为一个可调用iter对象，例如函数、方法、类(包含call方法)。需要接收两个参数：一个字典，该字典可以包含了客户端请求的信息以及其他信息，可以认为是请求上下文，一般叫做environment（编码中多简写为environ、env），一个用于发送HTTP响应状态（HTTP status）、响应头（HTTP headers）的回调函数,也就是start_response()。通过回调函数将响应状态和响应头返回给server，同时返回响应正文(response body)，响应正文是可迭代的、并包含了多个字符串。 下面是Django中application的具体实现部分：\n# 继承, 但只实现了 __call__ 方法, 方便使用 class WSGIHandler(base.BaseHandler): initLock = Lock() # 可以将其视为一个代表 http 请求的类 request_class = WSGIRequest # WSGIHandler 也可以作为函数来调用 def __call__(self, environ, start_response): # Set up middleware if needed. We couldn\u0026#39;t do this earlier, because # settings weren\u0026#39;t available. # 这里的检测: 因为 self._request_middleware 是最后才设定的, 所以如果为空, # 很可能是因为 self.load_middleware() 没有调用成功. if self._request_middleware is None: with self.initLock: try: # Check that middleware is still uninitialised. if self._request_middleware is None: 因为 load_middleware() 可能没有调用, 调用一次. self.load_middleware() except: # Unload whatever middleware we got self._request_middleware = None raise set_script_prefix(base.get_script_name(environ)) signls.request_started.send(sender=self.__class__) # __class__ 代表自己的类 try: # 实例化 request_class = WSGIRequest, 将在日后文章中展开, 可以将其视为一个代表 http 请求的类 request = self.request_class(environ) except UnicodeDecodeError: logger.warning(\u0026#39;Bad Request (UnicodeDecodeError)\u0026#39;, exc_info=sys.exc_info(), extra={ \u0026#39;status_code\u0026#39;: 400, } ) response = http.HttpResponseBadRequest() else: # 调用 self.get_response(), 将会返回一个相应对象 response\u0026lt;br\u0026gt; ############# 关键的操作, self.response() 可以获取响应数据.  response = self.get_response(request) # 将 self 挂钩到 response 对象 response._handler_class = self.__class__ try: status_text = STATUS_CODE_TEXT[response.status_code] except KeyError: status_text = \u0026#39;UNKNOWN STATUS CODE\u0026#39; # 状态码 status = \u0026#39;%s%s\u0026#39; % (response.status_code, status_text) response_headers = [(str(k), str(v)) for k, v in response.items()] # 对于每个一个 cookie, 都在 header 中设置: Set-cookie xxx=yyy for c in response.cookies.values(): response_headers.append((str(\u0026#39;Set-Cookie\u0026#39;), str(c.output(header=\u0026#39;\u0026#39;)))) # start_response() 操作已经在上节中介绍了 start_response(force_str(status), response_headers) # 成功返回相应对象 return response 可以看出application的流程包括:加载所有中间件，以及执行框架相关的操作，设置当前线程脚本前缀，发送请求开始信号；处理请求，调用get_response()方法处理当前请求，该方法的的主要逻辑是通过urlconf找到对应的view和callback，按顺序执行各种middleware和callback。调用由server传入的start_response()方法将响应header与status返回给server。返回响应正文\n2.2 django WSGI Server 负责获取http请求，将请求传递给WSGI application，由application处理请求后返回response。以Django内建server为例看一下具体实现。通过runserver运行django 项目，在启动时都会调用下面的run方法，创建一个WSGIServer的实例，之后再调用其serve_forever()方法启动服务。\ndef run(addr, port, wsgi_handler, ipv6=False, threading=False): server_address = (addr, port) if threading: httpd_cls = type(str(\u0026#39;WSGIServer\u0026#39;), (socketserver.ThreadingMixIn, WSGIServer), {}) else: httpd_cls = WSGIServer # 这里的wsgi_handler就是WSGIApplication  httpd = httpd_cls(server_address, WSGIRequestHandler, ipv6=ipv6) if threading: httpd.daemon_threads = True httpd.set_app(wsgi_handler) httpd.serve_forever() 下面表示WSGI server服务器处理流程中关键的类和方法。\n**WSGIServerrun()**方法会创建WSGIServer实例，主要作用是接收客户端请求，将请求传递给application，然后将application返回的response返回给客户端。 创建实例时会指定HTTP请求的handler：WSGIRequestHandler类 通过set_app和get_app方法设置和获取WSGIApplication实例wsgi_handler 处理http请求时，调用handler_request方法，会创建WSGIRequestHandler 实例处理http请求。 WSGIServer中get_request方法通过socket接受请求数据\nWSGIRequestHandler 由WSGIServer在调用handle_request时创建实例，传入request、cient_address、WSGIServer三个参数，__init__方法在实例化同时还会调用自身的handle方法handle方法会创建ServerHandler实例，然后调用其run方法处理请求\nServerHandler WSGIRequestHandler在其handle方法中调用run方法，传入self.server.get_app()参数，获取WSGIApplication，然后调用实例(call)，获取response，其中会传入start_response回调，用来处理返回的header和status。通过application获取response以后，通过finish_response返回response\nWSGIHandler WSGI协议中的application，接收两个参数，environ字典包含了客户端请求的信息以及其他信息，可以认为是请求上下文，start_response用于发送返回status和header的回调函数\n虽然上面一个WSGI server涉及到多个类实现以及相互引用，但其实原理还是调用WSGIHandler，传入请求参数以及回调方法start_response()，并将响应返回给客户端\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/wsgi/","summary":"转载自：https://segmentfault.com/a/1190000011365430\n 1. WSGI介绍 1.1 什么是WSGI 首先介绍几个关于WSGI相关的概念 WSGI：全称是Web Server Gateway Interface，WSGI不是服务器，python 模块，框架，API或者任何软件，只是一种规范，描述web server如何与web application通信的规范。server和application的规范在PEP 3333中有具体描述。要实现WSGI协议，必须同时实现web server和web application，当前运行在WSGI协议之上的web框架有Torando,Flask,Django\nuwsgi:与WSGI一样是一种通信协议，是uWSGI服务器的独占协议，用于定义传输信息的类型(type of information)，每一个uwsgi packet前4byte为传输信息类型的描述，与WSGI协议是两种东西，据说该协议是fcgi协议的10倍快。\nuWSGI：是一个web服务器，实现了WSGI协议、uwsgi协议、http协议等。\nWSGI协议主要包括server和application两部分：\n WSGI server负责从客户端接收请求，将request转发给application，将application返回的response返回给客户端； WSGI application接收由server转发的request，处理请求，并将处理结果返回给server。application中可以包括多个栈式的中间件(middlewares)，这些中间件需要同时实现server与application，因此可以在WSGI服务器与WSGI应用之间起调节作用：对服务器来说，中间件扮演应用程序，对应用程序来说，中间件扮演服务器。\n WSGI协议其实是定义了一种server与application解耦的规范，即可以有多个实现WSGI server的服务器，也可以有多个实现WSGI application的框架，那么就可以选择任意的server和applicatiodn组合实现自己的web应用。例如uWSGI和Gunicorn都是实现了WSGI server协议的服务器，Django，Flask是实现了WSGI application协议的web框架，可以根据项目实际情况搭配使用。\n以上介绍了相关的常识，接下来我们来看看如何简单实现WSGI协议。\n1.2 怎么实现WSGI 上文说过，实现WSGI协议必须要有wsgi server和application，因此，我们就来实现这两个东西。\n我们来看看官方WSGI使用WSGI的wsgiref模块实现的小demo\ndef demo_app(environ,start_response): from StringIO import StringIO stdout = StringIO() print \u0026gt;\u0026gt;stdout, \u0026#34;Hello world!\u0026#34; print \u0026gt;\u0026gt;stdout h = environ.items(); h.sort() for k,v in h: print \u0026gt;\u0026gt;stdout, k,\u0026#39;=\u0026#39;, repr(v) start_response(\u0026#34;200 OK\u0026#34;, [(\u0026#39;Content-Type\u0026#39;,\u0026#39;text/plain\u0026#39;)]) return [stdout.","title":"wsgi"},{"content":"1. ORM是什么 面向对象编程把所有实体看成对象（object），关系型数据库则是采用实体之间的关系（relation）连接数据。很早就有人提出，关系也可以用对象表达，这样的话，就能使用面向对象编程，来操作关系型数据库。 简单的说，ORM 就是通过实例对象的语法，完成关系型数据库的操作的技术，是\u0026quot;对象-关系映射\u0026quot;（Object/Relational Mapping） 的缩写。\nORM把数据库映射为对象\n 数据库的表（table） \u0026ndash;\u0026gt; 类（class） 记录（record，行数据）\u0026ndash;\u0026gt; 对象（object） 字段（field）\u0026ndash;\u0026gt; 对象的属性（attribute）\n 举例来说，下面是一行SQL语句\nSELECT id, first_name, last_name, phone, birth_date, sex FROM persons WHERE id = 10 程序直接运行SQL，操作数据库的写法如下：\nres = db.execSql(sql) name = res[0][\u0026#34;FIRST_NAME\u0026#34;] 改成ORM的写法如下：\np = Person.get(10) name = p.first_name 一比较就可以发现，ORM 使用对象，封装了数据库操作，因此可以不碰 SQL 语言。开发者只使用面向对象编程，与数据对象直接交互，不用关心底层数据库。\n总结起来，ORM有如下优点：\n 数据模型都在一个地方定义，更容易更新和维护，也利于重用代码。 ORM 有现成的工具，很多功能都可以自动完成，比如数据消毒、预处理、事务等等。 它迫使你使用 MVC 架构，ORM 就是天然的 Model，最终使代码更清晰。 基于 ORM 的业务代码比较简单，代码量少，语义性好，容易理解。 你不必编写性能不佳的 SQL。 但是ORM也有很突出的缺点： ORM 库不是轻量级工具，需要花很多精力学习和设置。 对于复杂的查询，ORM 要么是无法表达，要么是性能不如原生的 SQL。 ORM 抽象掉了数据库层，开发者无法了解底层的数据库操作，也无法定制一些特殊的 SQL。  2. 实例 Django是一个非常著名的python web框架，其提供了简单易用的ORM。\n安装django\npip install django 完成安装后，使用如下命令创建一个新的django项目\ndjango-admin startproject newSite 尝试运行\ncd newSite python manage.py runserver 访问https://127.0.0.1:8000查看是否运行成功。\n2.1 Model 完成了项目的创建后，下一步是要定义一个Model，一般来说，每个Model都映射一张数据库表。\n 每个模型都是一个 Python 的类，这些类继承 django.db.models.Model 模型类的每个属性都相当于一个数据库的字段。 利用这些，Django 提供了一个自动生成访问数据库的 API。  为django项目添加一个新的app\npython manage.py startapp ormTest 切换到ormTest目录下，添加如下代码\n# models.py  from django.db import models class Person(models.Model): first_name = models.CharField(max_length=30) last_name = models.CharField(max_length=30) 以上代码定义了一个 Person 模型，拥有 first_name 和 last_name:first_name 和 last_name 是模型的字段。每个字段都被指定为一个类属性，并且每个属性映射为一个数据库列。 在/newSite/newSite/setting.py中的INSTALLED_APPS添加如下代码，来告诉django你准备使用这个模型。\nINSTALLED_APPS = [ #... \u0026#39;ormTest.apps.OrmtestConfig\u0026#39;, #... ] 使用如下命令生成对应的数据库表\npython manage.py makemigrations ormTest python manage.py migrate 上面的Person模型会创建一个如下的数据库表：\nCREATE TABLE ormTest_person ( \u0026#34;id\u0026#34; serial NOT NULL PRIMARY KEY, \u0026#34;first_name\u0026#34; varchar(30) NOT NULL, \u0026#34;last_name\u0026#34; varchar(30) NOT NULL ); 2.2 CRUD 进入python命令行\npython manage.py shell In [1]: from ormTest.models import Person # Create  In [4]: Person.objects.create(first_name=\u0026#39;li\u0026#39;, last_name=\u0026#39;chang\u0026#39;) Out[4]: \u0026lt;Person: Person object (1)\u0026gt; # Read In [5]: Person.objects.all() Out[5]: \u0026lt;QuerySet [\u0026lt;Person: Person object (1)\u0026gt;]\u0026gt; # Update In [6]: person1 = Person.objects.get(id=1) In [8]: person1.first_name Out[8]: \u0026#39;li\u0026#39; In [9]: person1.last_name Out[9]: \u0026#39;chang\u0026#39; In [10]: person1.last_name = \u0026#39;YANG\u0026#39; In [11]: person1.save() In [14]: person2 = Person.objects.get(id=1) In [15]: person2.last_name Out[15]: \u0026#39;YANG\u0026#39; # Delete In [16]: person2.delete() Out[16]: (1, {\u0026#39;ormTest.Person\u0026#39;: 1}) In [17]: Person.objects.all() Out[17]: \u0026lt;QuerySet []\u0026gt; ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/orm%E7%AE%80%E4%BB%8B/","summary":"1. ORM是什么 面向对象编程把所有实体看成对象（object），关系型数据库则是采用实体之间的关系（relation）连接数据。很早就有人提出，关系也可以用对象表达，这样的话，就能使用面向对象编程，来操作关系型数据库。 简单的说，ORM 就是通过实例对象的语法，完成关系型数据库的操作的技术，是\u0026quot;对象-关系映射\u0026quot;（Object/Relational Mapping） 的缩写。\nORM把数据库映射为对象\n 数据库的表（table） \u0026ndash;\u0026gt; 类（class） 记录（record，行数据）\u0026ndash;\u0026gt; 对象（object） 字段（field）\u0026ndash;\u0026gt; 对象的属性（attribute）\n 举例来说，下面是一行SQL语句\nSELECT id, first_name, last_name, phone, birth_date, sex FROM persons WHERE id = 10 程序直接运行SQL，操作数据库的写法如下：\nres = db.execSql(sql) name = res[0][\u0026#34;FIRST_NAME\u0026#34;] 改成ORM的写法如下：\np = Person.get(10) name = p.first_name 一比较就可以发现，ORM 使用对象，封装了数据库操作，因此可以不碰 SQL 语言。开发者只使用面向对象编程，与数据对象直接交互，不用关心底层数据库。\n总结起来，ORM有如下优点：\n 数据模型都在一个地方定义，更容易更新和维护，也利于重用代码。 ORM 有现成的工具，很多功能都可以自动完成，比如数据消毒、预处理、事务等等。 它迫使你使用 MVC 架构，ORM 就是天然的 Model，最终使代码更清晰。 基于 ORM 的业务代码比较简单，代码量少，语义性好，容易理解。 你不必编写性能不佳的 SQL。 但是ORM也有很突出的缺点： ORM 库不是轻量级工具，需要花很多精力学习和设置。 对于复杂的查询，ORM 要么是无法表达，要么是性能不如原生的 SQL。 ORM 抽象掉了数据库层，开发者无法了解底层的数据库操作，也无法定制一些特殊的 SQL。  2.","title":"ORM简介"},{"content":"1. 安装并对redis进行配置 更新源并安装redis\nsudo apt-get update sudo apt-get install redis-server 将redis设置为systemctl\nsudo vim /etc/redis/redis.conf 找到supervised选项，设置为systemd\n# If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \u0026#34;process is ready.\u0026#34; # They do not enable continuous liveness pings back to your supervisor. supervised systemd 开启redis服务\nsudo service redis-server start 2. 测试redis 查看redis运行状态\nsudo service redis-server status 打开redis命令行\nredis-cli 127.0.0.1:6379\u0026gt; ping PONG 127.0.0.1:6379\u0026gt; set test \u0026#34;It\u0026#39;s a working\u0026#34; OK 127.0.0.1:6379\u0026gt; get test \u0026#34;It\u0026#39;s a working\u0026#34; 127.0.0.1:6379\u0026gt;exit 退出后重新连接redis，再次访问test键\n127.0.0.1:6379\u0026gt; get test \u0026#34;It\u0026#39;s a working\u0026#34; 127.0.0.1:6379\u0026gt; 3. 绑定到本地 一般来说，不推荐配置redis远程可访问，因此，这里只将其配置为本地可访问。\n打开Redis configuration file\nsudo vim /etc/redis/redis.conf 定位到bind,并将其配置如下\n# By default, if no \u0026#34;bind\u0026#34; configuration directive is specified, Redis listens # for connections from all the network interfaces available on the server. # It is possible to listen to just one or multiple selected interfaces using # the \u0026#34;bind\u0026#34; configuration directive, followed by one or more IP addresses. # # Examples: # # bind 192.168.1.100 10.0.0.1 bind 127.0.0.1 ::1 # 这里本来被注释掉，去掉注释即可 保存退出，重启redis\nsudo service redis-server restart 查看此项配置是否生效\n$ sudo netstat -lnp | grep redis tcp 0 0 127.0.0.1:6379 0.0.0.0:* LISTEN 2086/redis-server 1 tcp6 0 0 ::1:6379 :::* LISTEN 2086/redis-server 1 4. 为redis配置密码 打开redis配置文件，定位到# requirepass foobared\nsudo vim /etc/redis/redis.conf 将# requirepass foobared注释去掉，把foobared改成你想要的密码。\n重启redis服务\nsudo service redis-server restart 再次进入redis，尝试get\n$ redis-cli 127.0.0.1:6379\u0026gt; get test (error) NOAUTH Authentication required. 127.0.0.1:6379\u0026gt; get失败，因为没有进行认证.\n127.0.0.1:6379\u0026gt; auth your_redis_password OK 127.0.0.1:6379\u0026gt; get test \u0026#34;It\u0026#39;s a working\u0026#34; 127.0.0.1:6379\u0026gt;  参考：How To Install and Secure Redis on Ubuntu 18.04\n ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/ubuntu18%E5%AE%89%E8%A3%85redis/","summary":"1. 安装并对redis进行配置 更新源并安装redis\nsudo apt-get update sudo apt-get install redis-server 将redis设置为systemctl\nsudo vim /etc/redis/redis.conf 找到supervised选项，设置为systemd\n# If you run Redis from upstart or systemd, Redis can interact with your # supervision tree. Options: # supervised no - no supervision interaction # supervised upstart - signal upstart by putting Redis into SIGSTOP mode # supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET # supervised auto - detect upstart or systemd method based on # UPSTART_JOB or NOTIFY_SOCKET environment variables # Note: these supervision methods only signal \u0026#34;process is ready.","title":"Ubuntu18（WSL2）安装redis"},{"content":"1. 问题现象 莫名其妙，多出来几个显示器。 从设备管理器中看，也是存在多个通用非即插即用显示器2. 解决办法 重新安装Intel显卡驱动，可以从电脑厂家官网下载。但需要注意的一点是：在重新安装显卡驱动后，需要禁用显卡驱动程序的自动更新，否则还有可能出现这个问题。 打开组策略gpedit.msc，选择计算机配置”-\u0026gt;“管理模板”-\u0026gt;“系统”-\u0026gt;“设备安装”-\u0026gt;“设备安装限制”，找到Intel显卡的类Guid添加到阻止使用与下列设备安装程序类相匹配的驱动程序安装设备中\n","permalink":"http://yangchnet.github.io/Dessert/posts/windows/%E5%A4%9A%E5%87%BA%E5%87%A0%E4%B8%AA%E9%80%9A%E7%94%A8%E9%9D%9E%E5%8D%B3%E6%8F%92%E5%8D%B3%E7%94%A8%E6%98%BE%E7%A4%BA%E5%99%A8/","summary":"1. 问题现象 莫名其妙，多出来几个显示器。 从设备管理器中看，也是存在多个通用非即插即用显示器2. 解决办法 重新安装Intel显卡驱动，可以从电脑厂家官网下载。但需要注意的一点是：在重新安装显卡驱动后，需要禁用显卡驱动程序的自动更新，否则还有可能出现这个问题。 打开组策略gpedit.msc，选择计算机配置”-\u0026gt;“管理模板”-\u0026gt;“系统”-\u0026gt;“设备安装”-\u0026gt;“设备安装限制”，找到Intel显卡的类Guid添加到阻止使用与下列设备安装程序类相匹配的驱动程序安装设备中","title":"多出几个通用非即插即用显示器"},{"content":"1. 长度  数组\n 对于数组来说，它的长度是固定的，并且数组的长度是其类型的一部分，即对于以下两个数组来说，他们是不同的类型。\nvar a [5]int var b [6]int fmt.Printf(\u0026#34;%v\u0026#34;, reflect.TypeOf(a) == reflect.TypeOf(b)) // 输出： false 数组的长度必须是常量表达式，因为数组的长度需要在编译阶段确定。 对于数组来说，由于其长度是固定的，因此不能添加或删除元素。\n 切片\n 而对于切片，其长度是不固定的，不同长度的切片，只要其元素类型相同，则它们就是相同的切片类型。\na := make([]int, 5) b := make([]int, 6) fmt.Printf(\u0026#34;%v\\n\u0026#34;, reflect.TypeOf(a) == reflect.TypeOf(b)) // 输出： true 如果切片操作超出cap(s)的上限将导致一个panic异常，但是超出len(s)则是意味着扩展了 slice，因为新slice的长度会变大：\nmonths := [...]string{1: \u0026#34;January\u0026#34;, /* ... */, 12: \u0026#34;December\u0026#34;} summer := months[6:9] fmt.Println(summer[:20]) // panic: out of range endlessSummer := summer[:5] // extend a slice (within capacity) fmt.Println(endlessSummer) // \u0026#34;[June July August September October]\u0026#34; 2. 比较  数组\n 如果一个数组的元素类型是可以相互比较的，那么数组类型也是可以相互比较的，这时候我们可以直接通过==比较运算符来比较两个数组。只有当两个数组的所有元素都是相等的时候数组才是相等的。不相等比较运算符!=遵循同样的规则.\na := [2]int{1, 2} b := [...]int{1, 2} c := [2]int{1, 3} fmt.Println(a==b, a==c, b==c) // \u0026#34;true, false, false\u0026#34; d := [3]int{1, 2} fmt.Println(a==d) // compile error: cannot compare [2]int == [3]int  切片\n 切片之间不能比较，因此我们不能使用==操作符来判断两个slice是否含有全部相等元素。一般来说，要想判断两个slice是否相等，我们必须自己展开每个元素进行比较。\n为什么slice不直接支持比较运算符呢？1. 一个slice的元素是间接引用的，一个slice甚至可以包含自身。虽然有很多办法可以处理这种情形，但没有一个是简单有效的。2. 因为slice是间接引用的，一个固定值的slice在不同的时间可能包含不同的元素，因为底层数组的元素可能会被修改. slice唯一合法的比较操作是和nil进行比较，例如：\nif summer == nil { /* ... */ } 3. 作为函数参数  数组\n 当数组作为函数的参数传递时，实际上传递的是一个数组的复制。因此，任何在函数内部对于数组的修改都是在数组的复制上修改的，并不能直接修改调用时原始的数组变量。\n//先定义一个数组 var a = [5]int{1, 2, 3, 4, 5} //定义一个函数，将数组中的第一个值设为0 func change(a [5]int){ a[0] = 0 fmt.Println(a) } change(a) fmt.Println(a) /* 输出： [0 2 3 4 5] [1 2 3 4 5] */ // 只修改了复制的数组，原始数组没有得到修改。 可以显式的传入一个数组指针，这样的话函数通过指针对数组的任何修改都可以直接反馈到调用者。\nfunc main() { var a = [5]int{1, 2, 3, 4, 5} fmt.Println(a) change(\u0026amp;a) fmt.Println(a) } func change(ptr *[5]int) { ptr[0] = 0 fmt.Println(*ptr) } /* 输出: [1 2 3 4 5] [0 2 3 4 5] [0 2 3 4 5] */  slice\n slice是一个引用类型，slice值包含指向第一个slice元素的指针，因此向函数传递slice将允许在函数内部修改底 层数组的元素。换句话说，复制一个slice只是对底层的数组创建了一个新的slice别名。\nfunc reverse(s []int) { for i, j := 0, len(s)-1; i \u0026lt; j; i, j = i+1, j-1 { s[i], s[j] = s[j], s[i] } } a := [...]int{0, 1, 2, 3, 4, 5} reverse(a[:]) fmt.Println(a) // \u0026#34;[5 4 3 2 1 0]\u0026#34; 这里存在一个坑，就是我们会误以为对于传入的切片作出的任何更改都会反映到原来的切片上，其实不然，请看下面程序：\npackage main import \u0026#34;fmt\u0026#34; func myAppend(s []int) []int { // 这里 s 虽然改变了，但并不会影响外层函数的 s  s = append(s, 100) return s } func myAppendPtr(s *[]int) { // 会改变外层 s 本身  *s = append(*s, 100) return } func main() { s := []int{1, 1, 1} newS := myAppend(s) fmt.Println(s) fmt.Println(newS) s = newS myAppendPtr(\u0026amp;s) fmt.Println(s) } 运行结果为：\n[1 1 1] [1 1 1 100] [1 1 1 100 100] 很明显，对于切片作出的改变未反映到原切片上，这是为什么呢？ 仔细观察，可以看出，对于反映到原切片上的改动，都是直接访问了切片元素，而没有反映到原切片上的改动，则没有直接访问元素。这是因为对于切片元素的访问，实际上访问的是切片内部array指针，因此才能反映到原数组中。 而对于切片的直接修改，没有访问array指针(可能是改变了array或len、cap)，因此无法反映到原切片上. 但当我们传入切片指针时，作出的任何改变就都可以反映到原切片上了。\ntype slice struct { array unsafe.Pointer len int cap int } 示意图如下，传入函数内部的slice是对外部slice的一个复制，因此二者具有相同的array、len、cap等。对于函数内部的slice.array进行访问修改，实际上就是在对外部slice的array进行修改。但如果是直接对array修改（不访问指针），那么无论作出任何修改，都仅仅是在外部slice的一个复制上作出的改动。 一个小坑 请看下面程序\na := []int{1, 2, 3} b := a[1:2] // b: [2] b[0] = 4 // b: [4] b = append(b, 5) // b: [4, 5]; a: [1, 4, 5] b = append(b, 6) // b: [4, 5, 6]; a: [1, 4, 5] b[0] = 7 // b: [7, 5, 6]; a: [1, 4, 5] fmt.Println(a) // 1, 4, 5 fmt.Println(b) // 7, 5, 6 这里b是截取a得到的切片，也就是说b与a共享同一个底层数组。那么对b的修改应反映到a上。但在倒数第三行中，对b的第一个元素的修改卻并没有被反映到a中。这是为什么呢？\n首先，a的容量为3,b截取了a的第1个元素，此时b的容量为1，且b的第0个元素与a的第1个元素为同一个底层int值。对b进行一次append，b的容量变为2,且由于b的长度未超出a的容量，所以是直接在a与b共享的底层数组上进行的修改。但再次对b进行append时，a的容量已经不足，这时就要重新为b分配内存，分配完毕后将b的值复制到新切片上，然后再进行append。也就是说此时a与b已经不是一个底层数组了，因此对b的修改将不再与a有关。\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/slice%E5%92%8C%E6%95%B0%E7%BB%84%E7%9A%84%E5%8C%BA%E5%88%AB/","summary":"1. 长度  数组\n 对于数组来说，它的长度是固定的，并且数组的长度是其类型的一部分，即对于以下两个数组来说，他们是不同的类型。\nvar a [5]int var b [6]int fmt.Printf(\u0026#34;%v\u0026#34;, reflect.TypeOf(a) == reflect.TypeOf(b)) // 输出： false 数组的长度必须是常量表达式，因为数组的长度需要在编译阶段确定。 对于数组来说，由于其长度是固定的，因此不能添加或删除元素。\n 切片\n 而对于切片，其长度是不固定的，不同长度的切片，只要其元素类型相同，则它们就是相同的切片类型。\na := make([]int, 5) b := make([]int, 6) fmt.Printf(\u0026#34;%v\\n\u0026#34;, reflect.TypeOf(a) == reflect.TypeOf(b)) // 输出： true 如果切片操作超出cap(s)的上限将导致一个panic异常，但是超出len(s)则是意味着扩展了 slice，因为新slice的长度会变大：\nmonths := [...]string{1: \u0026#34;January\u0026#34;, /* ... */, 12: \u0026#34;December\u0026#34;} summer := months[6:9] fmt.Println(summer[:20]) // panic: out of range endlessSummer := summer[:5] // extend a slice (within capacity) fmt.Println(endlessSummer) // \u0026#34;[June July August September October]\u0026#34; 2.","title":"slice和数组的区别"},{"content":"1. Reader接口 type Reader interface { Read(p []byte) (n int, err error) }  接口说明\n Read 将 len(p) 个字节读取到 p 中。它返回读取的字节数 n（0 \u0026lt;= n \u0026lt;= len(p)） 以及任何遇到的错误。即使 Read 返回的 n \u0026lt; len(p)，它也会在调用过程中占用 len(p) 个字节作为暂存空间。若可读取的数据不到 len(p) 个字节，Read 会返回可用数据，而不是等待更多数据。\n当 Read 在成功读取 n \u0026gt; 0 个字节后遇到一个错误或 EOF (end-of-file)，它会返回读取的字节数。它可能会同时在本次的调用中返回一个non-nil错误,或在下一次的调用中返回这个错误（且 n 为 0）。 一般情况下, Reader会返回一个非0字节数n, 若 n = len(p) 个字节从输入源的结尾处由 Read 返回，Read可能返回 err == EOF 或者 err == nil。并且之后的 Read() 都应该返回 (n:0, err:EOF)。\n调用者在考虑错误之前应当首先处理返回的数据。这样做可以正确地处理在读取一些字节后产生的 I/O 错误，同时允许EOF的出现\n 例子\n func ReadFrom(reader io.Reader, num int) ([]byte, error) { p := make([]byte, num) n, err := reader.Read(p) if n \u0026gt; 0 { return p[:n], nil } return p, err } ReadFrom函数将io.Reader作为参数，也就是说，ReadFrom可以从任何实现了io.Reader接口的地方读取数据。\n// 从标准输入读取数据 data, err := ReadFrom(os.Stdin, 11) // 从普通文件中读取， file = new(os.File) data, err := Read From(file, 9) // 从字符串读取 data, err := ReadFrom(strings.NewReader(\u0026#34;from string\u0026#34;), 12) io.Reader将当前io.Reader实例中的值读取到参数p中 $$io.Reader \\stackrel{data}{\\rightarrow} [\\ ]byte$$\n2. Writer接口 type Writer interface { Write(p []byte) (n int, err error) }  功能说明\n Write 将 len(p) 个字节从 p 中写入到基本数据流中。它返回从 p 中被写入的字节数 n（0 \u0026lt;= n \u0026lt;= len(p)）以及任何遇到的引起写入提前停止的错误。若 Write 返回的 n \u0026lt; len(p)，它就必须返回一个 非nil 的错误。\n 例子\n 在fmt标准库中，有一组函数:Fprint/Fprintf/Fprintln,它们接收一个io.Wirter类型参数，也就是说它们将数据格式化输出到io.Writer中。那么，调用这组函数时，该如何实现这个参数呢？ 以fmt.Fprintln()为例：\nfunc Println(a ...interface{}) (n int, err error) { return Fprintln(os.Stdout, a...) } io.Writer将p中的数据写入到io.Wirter实例中 $$[\\ ]byte \\stackrel{data}{\\rightarrow} io.Writer$$\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/reader%E5%92%8Cwriter%E6%8E%A5%E5%8F%A3/","summary":"1. Reader接口 type Reader interface { Read(p []byte) (n int, err error) }  接口说明\n Read 将 len(p) 个字节读取到 p 中。它返回读取的字节数 n（0 \u0026lt;= n \u0026lt;= len(p)） 以及任何遇到的错误。即使 Read 返回的 n \u0026lt; len(p)，它也会在调用过程中占用 len(p) 个字节作为暂存空间。若可读取的数据不到 len(p) 个字节，Read 会返回可用数据，而不是等待更多数据。\n当 Read 在成功读取 n \u0026gt; 0 个字节后遇到一个错误或 EOF (end-of-file)，它会返回读取的字节数。它可能会同时在本次的调用中返回一个non-nil错误,或在下一次的调用中返回这个错误（且 n 为 0）。 一般情况下, Reader会返回一个非0字节数n, 若 n = len(p) 个字节从输入源的结尾处由 Read 返回，Read可能返回 err == EOF 或者 err == nil。并且之后的 Read() 都应该返回 (n:0, err:EOF)。","title":"Reader和Writer接口"},{"content":"1. 线程 在操作系统中，进程是分配资源的基本单位，但当进程作为调度的基本单位时，会造成较大的开销，频繁的进程调度将消耗大量时间。因此引出了线程：线程是处理器调度的基本单位，线程只拥有很小的运行时必要的资源。一个进程可拥有多个线程，同一个进程中的所有线程共享进程获得的主存空间和资源。 线程的实现\n有些系统同时支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程 和内核级线程的连接方式：多对一模型、一对一模型、多对多模型。\n2. goroutine 在Go语言中，每一个并发的执行单元叫作一个goroutine，是一种轻量级的线程。\n3. 线程与goroutine的区别   运行时栈的大小\n 每个系统级线程都会有一个固定大小的栈（一般为2MB），主要用于保存函数递归调用时参数和局部变量。这造成了两个问题：  对于某些需要很小的栈空间的线程来说是一个巨大的浪费 对于少数需要巨大栈空间的线程来说又面临栈溢出的风险   goroutine会以一个很小的栈启动（2KB或4KB），当遇到深度递归时导致当前栈空间不足，会根据需要动态的伸缩栈的大小。    调度\n go的运行时还包括了其自己的调度器，可以在n个操作系统线程上多工调度m个goroutine（类似于多线程模型中的多对多模型）。 go调度器的工作和内核的调度时相似的，但是这个调度器只关注单独的go程序中的goroutine。 goroutinie采用的是半抢占式的协作调度，只有当当前goroutine发生阻塞时才会导致调度。 这种调度发生在用户态，调度器会根据具体函数只保存必要的寄存器，切换的代价比系统线程要低得多。    创建和销毁\n Thread 创建和销毀都会有巨大的消耗，因为要和操作系统打交道，是内核级的，通常解决的办法就是线程池。- goroutine 因为是由 Go runtime 负责管理的，创建和销毁的消耗非常小，是用户级。    ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/goroutine%E5%92%8C%E7%BA%BF%E7%A8%8B/","summary":"1. 线程 在操作系统中，进程是分配资源的基本单位，但当进程作为调度的基本单位时，会造成较大的开销，频繁的进程调度将消耗大量时间。因此引出了线程：线程是处理器调度的基本单位，线程只拥有很小的运行时必要的资源。一个进程可拥有多个线程，同一个进程中的所有线程共享进程获得的主存空间和资源。 线程的实现\n有些系统同时支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程 和内核级线程的连接方式：多对一模型、一对一模型、多对多模型。\n2. goroutine 在Go语言中，每一个并发的执行单元叫作一个goroutine，是一种轻量级的线程。\n3. 线程与goroutine的区别   运行时栈的大小\n 每个系统级线程都会有一个固定大小的栈（一般为2MB），主要用于保存函数递归调用时参数和局部变量。这造成了两个问题：  对于某些需要很小的栈空间的线程来说是一个巨大的浪费 对于少数需要巨大栈空间的线程来说又面临栈溢出的风险   goroutine会以一个很小的栈启动（2KB或4KB），当遇到深度递归时导致当前栈空间不足，会根据需要动态的伸缩栈的大小。    调度\n go的运行时还包括了其自己的调度器，可以在n个操作系统线程上多工调度m个goroutine（类似于多线程模型中的多对多模型）。 go调度器的工作和内核的调度时相似的，但是这个调度器只关注单独的go程序中的goroutine。 goroutinie采用的是半抢占式的协作调度，只有当当前goroutine发生阻塞时才会导致调度。 这种调度发生在用户态，调度器会根据具体函数只保存必要的寄存器，切换的代价比系统线程要低得多。    创建和销毁\n Thread 创建和销毀都会有巨大的消耗，因为要和操作系统打交道，是内核级的，通常解决的办法就是线程池。- goroutine 因为是由 Go runtime 负责管理的，创建和销毁的消耗非常小，是用户级。    ","title":"goroutine和线程"},{"content":"1. Panic异常 func panic(v interface{}) 在通常情况下，函数向其调用方报告错误都是返回一个error类型，但有时会遇到致命（即会让程序崩溃）的错误时，显然无法通过返回error进行处理。这时我们使用panic函数来报告致命错误。\n当panic异常发生时，程序会中断运行，并立即执行在该goroutine中被defer的函数。随后，程序崩溃并输出日志信息（panic value和函数调用的堆栈信息）。在Go的panic机制中，延迟函数的调用在释放堆栈信息之前.\npanic的来源： 1. 运行时panic异常 2. 直接调用内置的panic函数\n例子：\nfunc main(){ fmt.Println(\u0026#34;main start\u0026#34;) outerFunc() fmt.Println(\u0026#34;main end\u0026#34;) } func outerFunc(){ fmt.Println(\u0026#34;out start\u0026#34;) innerFunc() fmt.Println(\u0026#34;out end\u0026#34;) } func innerFunc(){ panic(errors.New(\u0026#34;an intended fatal error\u0026#34;)) }  输出\n // 只有start，而没有end，因为程序崩溃了 main start out start panic: an intended fatal error 在这个程序中，当调用innerFunc中的panic时，innerFunc会立即停止执行，紧接着，outerFunc也会被停止，运行时panic沿着调用栈一直反方向进行传播，直至到达当前goroutine的调用栈最顶层。\n2. recover func recover() interface{} 通常来说，不应该对panic异常做任何处理，但有时我们可能需要在程序崩溃前做一些操作。这时，我们可以“从异常中恢复”。\n如果在defer函数中调用了内置函数recover，并且定义该defer语句的函数发生了panic异常，recover会使程序从panic中恢复，并返回panic value。导致panic异常的函数不会继续运行，但能正常返回。在未发生panic时调用recover，recover会返回nil。\n// ...  // 将innerFunc修改为如下 func innerFunc(){ defer func(){ if p := recover(); p != nil { fmt.Printf(\u0026#34;internal error:%v \\n\u0026#34;,p) } }() panic(errors.New(\u0026#34;an intended fatal error\u0026#34;)) }  输出\n // 程序正常结束 main start out start internal error:an intended fatal error out end main end ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/panic%E5%92%8Crecover/","summary":"1. Panic异常 func panic(v interface{}) 在通常情况下，函数向其调用方报告错误都是返回一个error类型，但有时会遇到致命（即会让程序崩溃）的错误时，显然无法通过返回error进行处理。这时我们使用panic函数来报告致命错误。\n当panic异常发生时，程序会中断运行，并立即执行在该goroutine中被defer的函数。随后，程序崩溃并输出日志信息（panic value和函数调用的堆栈信息）。在Go的panic机制中，延迟函数的调用在释放堆栈信息之前.\npanic的来源： 1. 运行时panic异常 2. 直接调用内置的panic函数\n例子：\nfunc main(){ fmt.Println(\u0026#34;main start\u0026#34;) outerFunc() fmt.Println(\u0026#34;main end\u0026#34;) } func outerFunc(){ fmt.Println(\u0026#34;out start\u0026#34;) innerFunc() fmt.Println(\u0026#34;out end\u0026#34;) } func innerFunc(){ panic(errors.New(\u0026#34;an intended fatal error\u0026#34;)) }  输出\n // 只有start，而没有end，因为程序崩溃了 main start out start panic: an intended fatal error 在这个程序中，当调用innerFunc中的panic时，innerFunc会立即停止执行，紧接着，outerFunc也会被停止，运行时panic沿着调用栈一直反方向进行传播，直至到达当前goroutine的调用栈最顶层。\n2. recover func recover() interface{} 通常来说，不应该对panic异常做任何处理，但有时我们可能需要在程序崩溃前做一些操作。这时，我们可以“从异常中恢复”。\n如果在defer函数中调用了内置函数recover，并且定义该defer语句的函数发生了panic异常，recover会使程序从panic中恢复，并返回panic value。导致panic异常的函数不会继续运行，但能正常返回。在未发生panic时调用recover，recover会返回nil。\n// ...  // 将innerFunc修改为如下 func innerFunc(){ defer func(){ if p := recover(); p !","title":"panic和recover"},{"content":"hugo默认不支持latex公式，为了在我们的博客上显示数学公式，我们需要使用katex.\n使用方法 对于hugo来说，我们只需要为每个页面加上\n\u0026lt;!-- KaTeX --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0-rc.1/katex.min.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0-rc.1/katex.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0-rc.1/contrib/auto-render.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; document.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function() { renderMathInElement(document.body, { delimiters: [ {left: \u0026#34;$$\u0026#34;, right: \u0026#34;$$\u0026#34;, display: true}, {left: \u0026#34;$\u0026#34;, right: \u0026#34;$\u0026#34;, display: false} ] }); }); \u0026lt;/script\u0026gt; 就行了。\n可以通过在themes/{themeName}/layouts/partials/footer.html中添加来使katex包含到每个页面中。\n书写公式 行内公式可以使用$f(x)= \\cos x$来编辑,效果为$f(x)= \\cos x$ 行间公式可使用如下格式：\n$$\\frac{ x^{2} }{ k+1 }\\qquad$$ 效果为： $$\\frac{ x^{2} }{ k+1 }\\qquad$$\n","permalink":"http://yangchnet.github.io/Dessert/posts/env/hugo%E4%B8%AD%E7%9A%84%E5%85%AC%E5%BC%8F%E9%97%AE%E9%A2%98/","summary":"hugo默认不支持latex公式，为了在我们的博客上显示数学公式，我们需要使用katex.\n使用方法 对于hugo来说，我们只需要为每个页面加上\n\u0026lt;!-- KaTeX --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0-rc.1/katex.min.css\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0-rc.1/katex.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0-rc.1/contrib/auto-render.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; document.addEventListener(\u0026#34;DOMContentLoaded\u0026#34;, function() { renderMathInElement(document.body, { delimiters: [ {left: \u0026#34;$$\u0026#34;, right: \u0026#34;$$\u0026#34;, display: true}, {left: \u0026#34;$\u0026#34;, right: \u0026#34;$\u0026#34;, display: false} ] }); }); \u0026lt;/script\u0026gt; 就行了。\n可以通过在themes/{themeName}/layouts/partials/footer.html中添加来使katex包含到每个页面中。\n书写公式 行内公式可以使用$f(x)= \\cos x$来编辑,效果为$f(x)= \\cos x$ 行间公式可使用如下格式：\n$$\\frac{ x^{2} }{ k+1 }\\qquad$$ 效果为： $$\\frac{ x^{2} }{ k+1 }\\qquad$$","title":"hugo中的公式问题"},{"content":"1. 数据库系统的结构抽象 1.1 三级模式(三级视图)  External Schema \u0026mdash;-（External）View 某一用户能看到与处理的数据的结构描述 (Conceptual) Schema \u0026mdash;- Conceptual View 从全局角度理解/管理的数据的结构描述, 含相应的关联约束。体现在数据之间的内在本质联系 Internal Schema \u0026mdash;- Internal View 存储在介质上的数据的结构描述，含存储路径、存储方式 、索引方式等  1.2 两层映像  E-C Mapping：External Schema-Conceptual Schema Mapping 将外模式映射为概念模式，从而支持实现数据概念视图向外部视图的转换，便于用户观察和使用 C-I Mapping：Conceptual Schema-Internal Schema Mapping 将概念模式映射为内模式，从而支持实现数据概念视图向内部视图的转换，便于计算机进行存储和处理  1.3 两个独立性  逻辑数据独立性 当概念模式变化时，可以不改变外部模式(只需改变E-C Mapping)，从而无需 改变应用程序 物理数据独立性 当内部模式变化时，可以不改变概念模式(只需改变C-I Mapping) ，从而不改 变外部模式  1.4 数据模型  数据模型  规定模式统一描述方式的模型，包括：数据结构、操作和约束 数据模型是对模式本身结构的抽象，模式是对数据本身结构形式的抽象   三大经典数据模型  关系模型：表的形式组织数据 层次模型：树的形式组织数据 网状模型：图的形式组织数据    2. 关系模型的基本概念 2.1 关系模型的三个要素  基本结构  形象地说，一个关系(relation)就是一个Table Relation/Table\n  基本操作  基本的\n  并$\\cup$ 差：$-$ 广义积：$\\times$ 选择: $\\delta$ 投影：$\\pi$   扩展的\n  交: $\\cap$ 连接: $\\Join$ 除：$\\div$   完整性约束 实体完整性、参照完整性和用户自定义的完整性  2.2 什么是关系 关系 笛卡尔积中具有某一方面意义的那些元组被称作一个关系\n 笛卡尔积的数学描述： $$一组域D_1, D_2, \u0026hellip; , D_n的笛卡尔积为： D_1 \\times D_2 \\times \\cdots \\times D_n = \\lbrace (d_1, d_2, \\cdots, d_n)| d_i\\in D_i,\\ i=1, \\cdots , n \\rbrace$$\n 2.3 关系模式与关系  同一关系模式下，可有很多的关系 关系模式是关系的结构, 关系是关系模式在某一时刻的数据 关系模式是稳定的；而关系是某一时刻的值，是随时间可能变化的  2.4 关系与表的异同 大部分方面都是相同的，但关系中任意两个元组不能完全相同，而表可能并不完全遵守此特性\n2.5 关系的特性  关系的任意两个元组不能完全相同 属性不可再分特性:又被称为关系第一范式  2.6 关系上的一些重要概念 候选码/候选键 关系中的一个属性组，其值能唯一标识一个元组，若从该属性组中去掉任何一个属性，它就不具有这一性质了，这样的属性组称作候选码。\n主码/主键 当有多个候选码时，可以选定一个作为主码。\n主属性和非主属性 包含在任何一个候选码中的属性被称作主属性，而其他属性被称作非主属性\n 最简单的，候选码只包含一个属性 最极端的，所有属性构成这个关系的候选码，称为全码(All-Key)\n 外码/外键 关系R中的一个属性组，它不是R的候选码，但它与另一个关系S的候选 码相对应，则称这个属性组为R的外码或外键。\n 两个关系通常是靠外码连接起来的。\n 2.3 关系模型的完整性   实体完整性\n关系的主码中的属性值不能为空值； 空值：不知道或无意义的值\n意义：关系中的元组对应到现实世界相互之间可区分的一个个个 体，这些个体是通过主码来唯一标识的；若主码为空，则出现不可标识 的个体，这是不容许的\n  参照完整性（对外码而言） 如果关系R1的外码Fk与关系R2的主 码Pk相对应，则R1中的每一个元组的 Fk值或者等于R2 中某个元组的Pk 值， 或者为空值\n  用户自定义完整性\n用户针对具体的应用环境定义的完整性约束条件\n   实体完整性和参照完整性由DBMS系统自动支持 DBMS系统通常提供了如下机制：  (1)它使用户可以自行定义有关的完整性约束条件 (2)当有更新操作发生时，DBMS将自动按照完整性约束条件检验更新操作的正确性，即是否符合用户自定义的完整性    3. 关系代数 3.1 关系代数操作 集合操作和纯关系操作\n3.2 关系代数基本操作 3.2.1 关系代数运算的约束  并相容性\n关系R与关系S存在相容性，当且仅当：\n(1) 关系R和关系S的属性数目必须相同；\n(2) 对于任意i，关系R的第i个属性的域必须和关系S的第i个属性的域相同  3.2.2 关系代数的操作  并$\\vee$(要满足并相容性)\n数学描述： $$R\\vee S= \\lbrace t|t\\in R \\vee t \\in S \\rbrace$$ 差$-$(要满足并相容性)\n数学描述： $$R-S=\\lbrace t| t \\in R \\land t \\notin S \\rbrace$$ 广义笛卡尔积操作\n数学描述： $$关系R\u0026lt; a_1, a_2, \\cdots , a_n\u0026gt;, 关系S\u0026lt;b_1, b_2, \\cdots , b_n\u0026gt;, 则 \\ R \\times S = \\lbrace \u0026lt;a_1, a_2, \\cdots , a_n, b_1, b_2, \\cdots , b_n\u0026gt; | \u0026lt;a_1, a_2, \\cdots , a_n\u0026gt; \\in R \\land \u0026lt;b_1, b_2, \\cdots , b_n\u0026gt; \\in S \\rbrace$$ 选择操作\n数学描述： $$\\delta _{con}(R)=\\lbrace t | t \\in R \\land con(t) = true \\rbrace$$ 投影\n数学描述： $$\\Pi_{a_{i1}, a_{i2}, \\cdots, a_{ik}}(R) = \\lbrace \u0026lt;t[A_{i1}],t[A_{i2}], \\cdots , t[A_{ik}]\u0026gt; | t \\in R \\rbrace$$ **示例**\n  3.3 关系代数扩展操作   交\n数学描述：\n$$R \\cap S= \\lbrace t|t\\in R \\land t \\in S \\rbrace$$\n  $\\theta$-连接\n第一步：对两个表进行广义笛卡尔积\n第二步：从广义笛卡尔积中选取出符合条件的元组\n数学描述： $$R \\underset{A \\theta B} \\Join S = \\delta_{t[A] \\theta s[B]}(R\\times B)$$\n其中$\\theta$是比较运算符\n 例子\n 查询至少98030101号同学和98040202号同学学过的所有课程号\n数学描述： $$\\pi_{SC.C}(\\delta_{SC.S=\u0026ldquo;98030101\u0026rdquo;\\land SC1.S=\u0026ldquo;98040202\u0026rdquo;}(SC \\underset{SC.C=SC1.C} \\Join \\rho_{SC1} (SC)) $$\n其中$\\rho$代表更名操作\n  等值连接\n给定关系R和关系S, R与S的等值连接运算结果也是一个关系， 记作$R \\underset{A=B} \\Join S$，它由关系R和关系S的笛卡尔积中选取R中属性A与S中属性 B上值相等的元组所构成。 数学描述：\n$$R \\underset{A=B}\\Join S = \\delta_{t[A]=s[B]}(R\\times S)$$ 当$\\theta$-连接中运算符为“＝”时，就是等值连接，等值连接是$\\theta$-连接的一个特例；   自然连接\n给定关系R和关系S, R与S的自然连接运算结果也是一个关系，记作$R \\Join S$ ，它由关系R和关系S的笛卡尔积中选取相同属性组B上值相等的元 组所构成。\n数学描述： $$R \\Join S = \\delta_{t[B]=s[B]}(R\\times S)$$\n是一种特殊的等值连接\n要求关系R和S必须有相同的属性组B\n  3.4 关系代数的复杂操作   除操作$\\div$\n前提条件\n若$R(A_1, A_2, \\cdots,A_n)$为n度关系，关系$S(B_1, B_2, \\cdots, B_n)$为m度关系，只有当$\\lbrace B_1, B_2, \\cdots, B_n\\rbrace \\subseteq \\lbrace A_1, A_2, \\cdots,A_n \\rbrace $即B是A的真子集，$m\u0026lt;n$时才可进行$R\\div S$运算\n定义\n设关系$R\u0026lt;a_1, \\cdots, a_n\u0026gt;$和关系$S\u0026lt;b_1, \\cdots, b_m\u0026gt;$,那么$R\\div S$结果为关系为元组$\u0026lt;c_1, \\cdots, c_k\u0026gt;$的集合，元组$\u0026lt;c_1, \\cdots, c_k\u0026gt;$满足下述条件:它与S中每一个元组$\u0026lt;b_1, \\cdots, b_m\u0026gt;$组合形成的一个新元组都是R中的某一个元组$\u0026lt;a_1, \\cdots, a_n\u0026gt;$\n示例 外连接（Outer-Join） 定义\n两个关系R与S进行连接时，如果关系R(或S)中的元组在S(或R)中找不到相匹配的元组，则为了避免该元组信息丢失，从而将该元组与S(或R)中 假定存在的全为空值的元组形成连接，放置在结果关系中，这种连接称之为外连接(Outer Join)。\n外连接 = 自然连接 (或$\\theta$连接) + 失配的元组(与全空元组形成的连接)\n外连接的形式   左外连接 = 自然连接(或$\\theta$连接) + 左侧表中失配的元组。 记作$⟕$  右外连接 = 自然连接(或$\\theta$连接) + 右侧表中失配的元组。 记作 $⟖$\n 全外连接 = 自然连接(或$\\theta$连接) + 两侧表中失配的元组。 记作$⟗$\n    4. 关系演算  关系元组演算  以元组变量作为谓词变量的基本对象\n  关系域演算  以域变量作为谓词变量的基本对象\n   4.1 关系元组演算 基本形式\n$$\\lbrace t | P(t)\\rbrace$$\n其中，$P(t)$可以是如下三种形式之一的原子公式：\n $t\\in R$ $s[A] \\theta c$\n其中$\\theta$为比较运算符$\u0026lt;, \\le, \\ge, \u0026gt;, \\ne$\n例如$\\lbrace t| t\\in R \\land t[Sage]\\le 19 \\land t[Sname] = \u0026lsquo;Bob\u0026rsquo;\\rbrace$ $s[A] \\theta u[B]$\ns[A]与u[B]为元组分量，A和B分别是某些关系的属性，他们之间满足比较关系$\\theta$\n例如：检索除年龄不是最小的所有同学\n$\\lbrace t|t\\in Student \\land \\exist (u \\in Student)(t[Sage]\u0026gt;u[Sage])\\rbrace$\n  4.2 关系域演算 \u0026hellip;\n关系数据库中的范式  第一范式（1NF）\n关系的每一个分量都是不可分的数据项 第二范式（2NF）\n若$R\\in 1NF$,且每一个非主属性完全函数依赖于任何一个候选码，则$R\\in 2NF$ 第三范式（3NF）\n设关系模式$R\u0026lt;U,F\u0026gt;\\in 1NF$, 若R中不存在这样的码X，属性组Y及非主属性$Z(Z\\nsubseteq Y)$，使得$X\\rightarrow Y, Y \\rightarrow Z$成立，$Y\\nrightarrow X$, 则称$R\u0026lt;U,F\u0026gt;\\in 3NF$。用人话说就是，若$R\\in 3NF$,则每一个非主属性既不传递依赖于码，也不部分依赖于码。 BC范式 关系模式$R\u0026lt;U,F\u0026gt;\\in1NF$,若$X\\rightarrow Y$且$Y\\nsubseteq X$时必含有码，则$R\u0026lt;U,F\u0026gt;\\in BCNF$。也就是说，关系模式$R\u0026lt;U,F\u0026gt;$中，若每一个决定因素都包含码，则$R\u0026lt;U,F\u0026gt;\\in BCNF$\n一个满足BCNF的关系模式有：  所有非主属性对每一个码都是完全函数依赖 所有主属性对每一个不包含它的码也是完全函数依赖 没有任何属性完全函数依赖于非码的任何一组属性    ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%9D%82%E8%AE%B0/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/","summary":"1. 数据库系统的结构抽象 1.1 三级模式(三级视图)  External Schema \u0026mdash;-（External）View 某一用户能看到与处理的数据的结构描述 (Conceptual) Schema \u0026mdash;- Conceptual View 从全局角度理解/管理的数据的结构描述, 含相应的关联约束。体现在数据之间的内在本质联系 Internal Schema \u0026mdash;- Internal View 存储在介质上的数据的结构描述，含存储路径、存储方式 、索引方式等  1.2 两层映像  E-C Mapping：External Schema-Conceptual Schema Mapping 将外模式映射为概念模式，从而支持实现数据概念视图向外部视图的转换，便于用户观察和使用 C-I Mapping：Conceptual Schema-Internal Schema Mapping 将概念模式映射为内模式，从而支持实现数据概念视图向内部视图的转换，便于计算机进行存储和处理  1.3 两个独立性  逻辑数据独立性 当概念模式变化时，可以不改变外部模式(只需改变E-C Mapping)，从而无需 改变应用程序 物理数据独立性 当内部模式变化时，可以不改变概念模式(只需改变C-I Mapping) ，从而不改 变外部模式  1.4 数据模型  数据模型  规定模式统一描述方式的模型，包括：数据结构、操作和约束 数据模型是对模式本身结构的抽象，模式是对数据本身结构形式的抽象   三大经典数据模型  关系模型：表的形式组织数据 层次模型：树的形式组织数据 网状模型：图的形式组织数据    2.","title":"数据库原理"},{"content":"0. 复试英语考试形式  自我介绍+老师提问（popular） 听一段文章内容，听完后回答（少） 小组讨论（少）  1. 自我介绍 一定不要说：我的英语很糟糕, My English is very poor!\n 目的是： 表达我配上研究生\n  先说姓名（微笑脸），年龄，学校 大学取得的成绩（没有成绩可以编\u0026hellip; 编点无从考证的，比如家教啥的） 再说性格爱好(稍微两句话提提)，尽量和专业联系到一起。 That\u0026rsquo;s all, thanks you very much. If I were admitted, I will go all out to learn my professional knowledge.除了学习①专业知识，我一定会加强我的②实践能力，我还要学会③更好的和导师和同学之间进行合作。Please trusted me.(不要超过三分钟)  2. 老师常问的9个问题  你为什么考研，为什么选择这个专业？  一定不要说本科学校坏话， 不要说我要挣钱.. 也不要说为了中华之崛起.. 我真心喜欢我的专业😊(举个例子怎么感兴趣的) 我遗憾自己以前没有好好学习，在别人选择工作的时候我决定考研提升自己 终极答案是喜欢   你对未来有什么规划？  3年研究生的规划和研究生刚毕业的规划，太远不要说 研究生入校后，一定跟老师好好学习专业知识，协助导师，争取自己早日发表论文，有机会考博，培养自己的实践能力，合作能力，与人相处能力。毕业后找一份自己喜欢的工作，在自己的岗位上做出贡献。（一定要配合导师）   介绍你的家乡  首先要说我爱的家乡（怀有一颗感恩的心） 说点名人和特产（有很多可以such as，然后重点说一个） 最后再说，欢迎老师到我的家乡去旅游   介绍你的家庭  先说几口人 对我的人生产生最大的影响是谁（举个栗子(〃￣︶￣)人）   介绍你的本科学校  一个字，好 学校有历史、就业率好、有名气（老师你可能从来没听过这个名字，但是我非常热爱它） 虽然我的学校不是那么有名气，但是我依然结到了很多朋友，给我了很多温暖   对英语的态度，关于英语你怎么看  喜欢，感兴趣 以前对英语没那么感兴趣，只是一门课 后来发现英语真他娘重要，意识到学英语的重要性 在研究生期间更加专注对英语的学习 我的口语没有那么好，但是我希望在研究生期间可以有长足的进步   你对我们学校和专业了解多少  首先，学校名气大（毋庸置疑） 其次，对我们学院相当有了解，对教授了解（提一下教授的名字） 师资力量，著作   你的优点和缺点  优点可以和专业相关，但是缺点不行 我喜欢交朋友\u0026hellip; 编\u0026hellip;（给爷爬）≡(▔﹏▔)≡   为什么换专业（给跨考）  \u0026hellip;    3. 注意事项  听不懂怎么办  提前准备可以听懂 pardon一下。。   提前找师兄师姐  贫僧自东土大唐而来，往西天取经而去。。 带点特产，送送礼   穿什么  不用穿西装打领带 牛仔裤，T恤衫就行，里面的衣服不要大于外面的衣服 女生不要染头发、涂太艳的指甲油、不要用香水 不能穿太休闲 走路不要发出声音，不要驼背   微笑，不要怕眼神接触 专业问题回答不上来怎么办  非常遗憾我还回答不上来，但是如果我读研究生，我会好好学习的，请给我这个机会   不要打断老师说话 不要用一句话来回答老师的问题 不要晃腿  ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%9D%82%E8%AE%B0/%E5%A4%8D%E8%AF%95%E8%8B%B1%E8%AF%AD/","summary":"0. 复试英语考试形式  自我介绍+老师提问（popular） 听一段文章内容，听完后回答（少） 小组讨论（少）  1. 自我介绍 一定不要说：我的英语很糟糕, My English is very poor!\n 目的是： 表达我配上研究生\n  先说姓名（微笑脸），年龄，学校 大学取得的成绩（没有成绩可以编\u0026hellip; 编点无从考证的，比如家教啥的） 再说性格爱好(稍微两句话提提)，尽量和专业联系到一起。 That\u0026rsquo;s all, thanks you very much. If I were admitted, I will go all out to learn my professional knowledge.除了学习①专业知识，我一定会加强我的②实践能力，我还要学会③更好的和导师和同学之间进行合作。Please trusted me.(不要超过三分钟)  2. 老师常问的9个问题  你为什么考研，为什么选择这个专业？  一定不要说本科学校坏话， 不要说我要挣钱.. 也不要说为了中华之崛起.. 我真心喜欢我的专业😊(举个例子怎么感兴趣的) 我遗憾自己以前没有好好学习，在别人选择工作的时候我决定考研提升自己 终极答案是喜欢   你对未来有什么规划？  3年研究生的规划和研究生刚毕业的规划，太远不要说 研究生入校后，一定跟老师好好学习专业知识，协助导师，争取自己早日发表论文，有机会考博，培养自己的实践能力，合作能力，与人相处能力。毕业后找一份自己喜欢的工作，在自己的岗位上做出贡献。（一定要配合导师）   介绍你的家乡  首先要说我爱的家乡（怀有一颗感恩的心） 说点名人和特产（有很多可以such as，然后重点说一个） 最后再说，欢迎老师到我的家乡去旅游   介绍你的家庭  先说几口人 对我的人生产生最大的影响是谁（举个栗子(〃￣︶￣)人）   介绍你的本科学校  一个字，好 学校有历史、就业率好、有名气（老师你可能从来没听过这个名字，但是我非常热爱它） 虽然我的学校不是那么有名气，但是我依然结到了很多朋友，给我了很多温暖   对英语的态度，关于英语你怎么看  喜欢，感兴趣 以前对英语没那么感兴趣，只是一门课 后来发现英语真他娘重要，意识到学英语的重要性 在研究生期间更加专注对英语的学习 我的口语没有那么好，但是我希望在研究生期间可以有长足的进步   你对我们学校和专业了解多少  首先，学校名气大（毋庸置疑） 其次，对我们学院相当有了解，对教授了解（提一下教授的名字） 师资力量，著作   你的优点和缺点  优点可以和专业相关，但是缺点不行 我喜欢交朋友\u0026hellip; 编\u0026hellip;（给爷爬）≡(▔﹏▔)≡   为什么换专业（给跨考）  \u0026hellip;    3.","title":"复试英语"},{"content":"Github 出现 Failed to connect to github.com port 443: Timed out 1. 问题来由 可能是由于使用了全局代理的原因\n2. 解决 取消全局代理： git config --global --unset http.proxy git config --global --unset https.proxy  设置全局代理\n git config --global http.proxy http://127.0.0.1:1080 git config --global https.proxy http://127.0.0.1:1080 ","permalink":"http://yangchnet.github.io/Dessert/posts/git/failed-to-connect-to-github.com-port-443/","summary":"Github 出现 Failed to connect to github.com port 443: Timed out 1. 问题来由 可能是由于使用了全局代理的原因\n2. 解决 取消全局代理： git config --global --unset http.proxy git config --global --unset https.proxy  设置全局代理\n git config --global http.proxy http://127.0.0.1:1080 git config --global https.proxy http://127.0.0.1:1080 ","title":"Failed to connect to github.com port 443"},{"content":"VsCode Snippets的Snippets功能  snippets是代码片段, 在这里的意思是代码模板. 在使用vscode写代码时，有时需要使用代码模板，一个典型的例子是在写文件头注释时，需要一个固定格式的注释，来表明当前的时间、作者等。\n 1. 使用内置的snippets vscode中已经为我们内置了许多语言的代码模板，在安装了对应的语言插件后,可直接使用这些snippets. 2. 安装来自marketplace的snippets 按Ctrl+Shift+X打开marketplace, 输入@category:\u0026quot;snippets\u0026quot;,即可下载来自marketplace的snippets 3. 自定义snippets 如果你对内置的或来自marketplace的snippets均不满意,那么你可以自定义你的snippets.\n在File \u0026gt; Preferences \u0026gt; User Snippets选项下,选择你要定义snippets的文件类型\n在选择了文件类型之后,你就可以根据vscode提供的Example自定义snippets了.\nExample: \u0026quot;Print to console\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;log\u0026quot;, \u0026quot;body\u0026quot;: [ \u0026quot;console.log('$1');\u0026quot;, \u0026quot;$2\u0026quot; ], \u0026quot;description\u0026quot;: \u0026quot;Log output to console\u0026quot; \u0026quot;Print to console\u0026quot;是你自定义的snippets的名字,prefix为前缀,在输入了你定义的prefix后,body中的内容就会输出到当前光标的位置.\n在body中,你可以使用\u0026quot;variables\u0026ldquo;来描述你的snippets, 其格式为:\n  ${1:label}: 其中的1表示在body输出后光标会第一个停放在这个位置,而label是对当前variables的描述.\n  ${1|one, two, three|}: 这个语法格式将提醒你选择one, two, three中的一个值.\n  $name或${name:default}: 其中的name为预定义的变量名,可使用default指定其默认值.预定义的变量名有如下:\n 有关文件与目录的\n TM_SELECTED_TEXT当前选定的文本或空字符串 TM_CURRENT_LIN当前行的内容 TM_CURRENT_WORD光标下或空字符串下的单词内容 TM_LINE_INDEX基于零指数的行数 TM_LINE_NUMBER基于一个索引的行数 TM_FILENAME当前文档的文件名 TM_FILENAME_BASE没有扩展的当前文档的文件名 TM_DIRECTORY当前文档的目录 TM_FILEPATH当前文档的完整文件路径 CLIPBOARD剪贴板的内容 WORKSPACE_NAME打开的工作区或文件夹的名称 WORKSPACE_FOLDER打开的工作区或文件夹的路径    有关时间的\n CURRENT_YEAR本年度 CURRENT_YEAR_SHORT今年最后两位数 CURRENT_MONTH以两位数表示的月份（例如\u0026quot;02\u0026rdquo;） CURRENT_MONTH_NAME本月全名（例如\u0026quot;七月\u0026quot;） CURRENT_MONTH_NAME_SHORT本月的简称（例如\u0026quot;七月\u0026quot;） CURRENT_DATE每月的一天 CURRENT_DAY_NAME日名称（例如\u0026quot;星期一\u0026quot;） CURRENT_DAY_NAME_SHORT当天的简称（例如\u0026quot;星期一\u0026quot;） CURRENT_HOUR24 小时时钟格式的当前小时 CURRENT_MINUTE当前分钟 CURRENT_SECOND当前秒 CURRENT_SECONDS_UNIX自Unix epoch以来的秒数    有关注释的\n BLOCK_COMMENT_START Example output: in PHP or in HTML /*\u0026lt;!-- BLOCK_COMMENT_END Example output: in PHP or in HTML */--\u0026gt; LINE_COMMENT Example output: in PHP //     ","permalink":"http://yangchnet.github.io/Dessert/posts/tool/vscode-snippets%E5%8A%9F%E8%83%BD%E7%9A%84%E4%BD%BF%E7%94%A8/","summary":"VsCode Snippets的Snippets功能  snippets是代码片段, 在这里的意思是代码模板. 在使用vscode写代码时，有时需要使用代码模板，一个典型的例子是在写文件头注释时，需要一个固定格式的注释，来表明当前的时间、作者等。\n 1. 使用内置的snippets vscode中已经为我们内置了许多语言的代码模板，在安装了对应的语言插件后,可直接使用这些snippets. 2. 安装来自marketplace的snippets 按Ctrl+Shift+X打开marketplace, 输入@category:\u0026quot;snippets\u0026quot;,即可下载来自marketplace的snippets 3. 自定义snippets 如果你对内置的或来自marketplace的snippets均不满意,那么你可以自定义你的snippets.\n在File \u0026gt; Preferences \u0026gt; User Snippets选项下,选择你要定义snippets的文件类型\n在选择了文件类型之后,你就可以根据vscode提供的Example自定义snippets了.\nExample: \u0026quot;Print to console\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;log\u0026quot;, \u0026quot;body\u0026quot;: [ \u0026quot;console.log('$1');\u0026quot;, \u0026quot;$2\u0026quot; ], \u0026quot;description\u0026quot;: \u0026quot;Log output to console\u0026quot; \u0026quot;Print to console\u0026quot;是你自定义的snippets的名字,prefix为前缀,在输入了你定义的prefix后,body中的内容就会输出到当前光标的位置.\n在body中,你可以使用\u0026quot;variables\u0026ldquo;来描述你的snippets, 其格式为:\n  ${1:label}: 其中的1表示在body输出后光标会第一个停放在这个位置,而label是对当前variables的描述.\n  ${1|one, two, three|}: 这个语法格式将提醒你选择one, two, three中的一个值.\n  $name或${name:default}: 其中的name为预定义的变量名,可使用default指定其默认值.预定义的变量名有如下:\n 有关文件与目录的\n TM_SELECTED_TEXT当前选定的文本或空字符串 TM_CURRENT_LIN当前行的内容 TM_CURRENT_WORD光标下或空字符串下的单词内容 TM_LINE_INDEX基于零指数的行数 TM_LINE_NUMBER基于一个索引的行数 TM_FILENAME当前文档的文件名 TM_FILENAME_BASE没有扩展的当前文档的文件名 TM_DIRECTORY当前文档的目录 TM_FILEPATH当前文档的完整文件路径 CLIPBOARD剪贴板的内容 WORKSPACE_NAME打开的工作区或文件夹的名称 WORKSPACE_FOLDER打开的工作区或文件夹的路径    有关时间的","title":"VsCode Snippets功能的使用"},{"content":"GitHub图床+vscode+Picgo 0. 来由 用markdown写博客的时候，图片往哪里存地干活？图床里存···\n1. GitHub配置   创建图床仓库 为了不污染我原来的git账号，我决定新建一个git账号，专门用作图床账号。 新建账号之后，new一个repo，啥都不用点，直接create。\n  生成token 点击你GitHub页面右上角的头像，点击settings 在页面左侧找到Developer settings，选择之，再找到Personal access tokens，再选择之，然后generate new tokens 在新弹出的页面中填写note，并选择repo， 然后直接到最下面，Generate token 这样GitHub会为你生成一个token（只会出现这一次），复制它留用。   2. 配置VScode中的Picgo插件 在vscode的插件商店中直接搜索Picgo，然后点击安装 安装完成后，再来配置你的Picgo File\u0026gt;Preferences\u0026gt;settings\u0026gt;Entensions\u0026gt;Picgo找到配置picgo的位置，填写必要的信息 \u0026#34;picgo.picBed.current\u0026#34;: \u0026#34;github\u0026#34;, \u0026#34;picgo.picBed.github.branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;picgo.picBed.github.path\u0026#34;: \u0026#34;\u0026#34;, # 你想要图片存储的路径 \u0026#34;picgo.picBed.github.repo\u0026#34;: \u0026#34;\u0026#34;, # 你的用户名以及repo名，user/REPO_name \u0026#34;picgo.picBed.github.token\u0026#34;: \u0026#34;\u0026#34; # 刚才复制的token，粘贴到这里 3. 使用picgo上传图片 截个图并复制到剪贴板，在vscode里按下\u0026quot;CTRL+ALT+u\u0026quot;，图片就可以十分迅速的上传到你配置的GitHub仓库并为你返回图片链接 （￣︶￣）↗。\nEND\n","permalink":"http://yangchnet.github.io/Dessert/posts/env/github%E5%9B%BE%E5%BA%8A+vscode+picgo-/","summary":"GitHub图床+vscode+Picgo 0. 来由 用markdown写博客的时候，图片往哪里存地干活？图床里存···\n1. GitHub配置   创建图床仓库 为了不污染我原来的git账号，我决定新建一个git账号，专门用作图床账号。 新建账号之后，new一个repo，啥都不用点，直接create。\n  生成token 点击你GitHub页面右上角的头像，点击settings 在页面左侧找到Developer settings，选择之，再找到Personal access tokens，再选择之，然后generate new tokens 在新弹出的页面中填写note，并选择repo， 然后直接到最下面，Generate token 这样GitHub会为你生成一个token（只会出现这一次），复制它留用。   2. 配置VScode中的Picgo插件 在vscode的插件商店中直接搜索Picgo，然后点击安装 安装完成后，再来配置你的Picgo File\u0026gt;Preferences\u0026gt;settings\u0026gt;Entensions\u0026gt;Picgo找到配置picgo的位置，填写必要的信息 \u0026#34;picgo.picBed.current\u0026#34;: \u0026#34;github\u0026#34;, \u0026#34;picgo.picBed.github.branch\u0026#34;: \u0026#34;master\u0026#34;, \u0026#34;picgo.picBed.github.path\u0026#34;: \u0026#34;\u0026#34;, # 你想要图片存储的路径 \u0026#34;picgo.picBed.github.repo\u0026#34;: \u0026#34;\u0026#34;, # 你的用户名以及repo名，user/REPO_name \u0026#34;picgo.picBed.github.token\u0026#34;: \u0026#34;\u0026#34; # 刚才复制的token，粘贴到这里 3. 使用picgo上传图片 截个图并复制到剪贴板，在vscode里按下\u0026quot;CTRL+ALT+u\u0026quot;，图片就可以十分迅速的上传到你配置的GitHub仓库并为你返回图片链接 （￣︶￣）↗。\nEND","title":"GitHub图床+vscode+Picgo "},{"content":"安装与配置  参考了这位老哥的博客\n 0. 来由 阿里云与腾讯云git太慢了。。想快点\n1. 下载安装 地址在这里，找到对应自己系统的版本，可以先下载到自己本地主机后再用FileZilla上传到云服务器（虽然蛮麻烦，但是它快呀）\n2. 安装 将下载的上传到自己的服务器之后，解压之：\ngunzip clash-linux-amd64-v1.4.1.gz 解压结果就是一个可执行文件 重命名：\nmv clash-linux-amd64 clash 赋予运行权限：\nchmod +x clash 移动到bin目录下：\nmv clash /usr/local/bin/clash 3. 编辑config.yaml文件 vim ~/.config/clash/config.yaml # port of HTTP port: 7890 # port of SOCKS5 socks-port: 7891 …… # 这里输入你自己的配置文件 4. 将添加为系统服务 cd /etc/systemd/system/ vim clash.service clash.service的内容为：\n[Unit] Description= proxy After=network.target [Service] Type=simple ExecStart=/usr/local/bin/clash -f /home/YourUsername/.config/clash/config.yaml [Install] WantedBy=multi-user.target  要想深入了解systemctl服务，可前往阮一峰大佬的教程\n 编辑完成后，重载systemctl\nsystemctl daemon-reload 开启服务\nsystemctl start clash 设置开机启动\nsystemctl enable clash 5. 开启http代理和socks5代理  手动开启代理 export https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7891  保持启动，修改~/.bashrc echo \u0026#34;export https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7891\u0026#34; \u0026gt;\u0026gt; ~/.bashrc   6. 测试是否成功 curl -I https://www.google.com/ 返回：\nHTTP/1.1 200 Connection established HTTP/2 200 content-type: text/html; charset=ISO-8859-1 p3p: CP=\u0026quot;This is not a P3P policy! See g.co/p3phelp for more info.\u0026quot; date: Thu, 04 Mar 2021 09:08:48 GMT server: gws x-xss-protection: 0 x-frame-options: SAMEORIGIN expires: Thu, 04 Mar 2021 09:08:48 GMT cache-control: private set-cookie: 1P_JAR=2021-03-04-09; expires=Sat, 03-Apr-2021 09:08:48 GMT; path=/; domain=.google.com; Secure set-cookie: NID=210=wzyYoXKPgWcDE4ptLfGQOeOE1w5kehSHm4-gycNKB2bUaeueNyGXgDUm-p5KR6X0aTtmcXjxsamD0xM3YJVRqJnw74uXjiOwcJpRiwsQ-jdUc4y4JG8ObdBOGQJQcddFf_d8fyJ1nVEeEWWZanQUDJToFA6b0T05oCegNpeM70Q; expires=Fri, 03-Sep-2021 09:08:48 GMT; path=/; domain=.google.com; HttpOnly alt-svc: h3-29=\u0026quot;:443\u0026quot;; ma=2592000,h3-T051=\u0026quot;:443\u0026quot;; ma=2592000,h3-Q050=\u0026quot;:443\u0026quot;; ma=2592000,h3-Q046=\u0026quot;:443\u0026quot;; ma=2592000,h3-Q043=\u0026quot;:443\u0026quot;; ma=2592000,quic=\u0026quot;:443\u0026quot;; ma=2592000; v=\u0026quot;46,43\u0026quot; ","permalink":"http://yangchnet.github.io/Dessert/posts/env/%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AEclash/","summary":"安装与配置  参考了这位老哥的博客\n 0. 来由 阿里云与腾讯云git太慢了。。想快点\n1. 下载安装 地址在这里，找到对应自己系统的版本，可以先下载到自己本地主机后再用FileZilla上传到云服务器（虽然蛮麻烦，但是它快呀）\n2. 安装 将下载的上传到自己的服务器之后，解压之：\ngunzip clash-linux-amd64-v1.4.1.gz 解压结果就是一个可执行文件 重命名：\nmv clash-linux-amd64 clash 赋予运行权限：\nchmod +x clash 移动到bin目录下：\nmv clash /usr/local/bin/clash 3. 编辑config.yaml文件 vim ~/.config/clash/config.yaml # port of HTTP port: 7890 # port of SOCKS5 socks-port: 7891 …… # 这里输入你自己的配置文件 4. 将添加为系统服务 cd /etc/systemd/system/ vim clash.service clash.service的内容为：\n[Unit] Description= proxy After=network.target [Service] Type=simple ExecStart=/usr/local/bin/clash -f /home/YourUsername/.config/clash/config.yaml [Install] WantedBy=multi-user.target  要想深入了解systemctl服务，可前往阮一峰大佬的教程\n 编辑完成后，重载systemctl\nsystemctl daemon-reload 开启服务","title":"Linux上安装与配置clash"},{"content":"博客建设记 0. 前记 一直想要建设一个属于自己的博客，一开始用python写过一个简单的，可以做到富文本编辑、发布、更新、评论等功能，但那个不是一个单纯的博客，并且界面也不是太友好，因此后来废弃了。后来又用了一段时间的jupyter notebook，很强大，尤其让我喜欢的是可以直接运行代码，曾经有一段时间想过可否把jupyter notebook直接作为我的博客页面，或者是嵌入我的页面内，于是看了看其源代码。。。遂放弃。后来又用了为知笔记，印象笔记，Notion等，但感觉都没jupyter notebook好用。 在用jupyter notebook记了有了一定的数目之后，就想将其发布出来，考虑过CSDN，但感觉上面广告好多，不太喜欢，因此没有使用。后来买了域名和服务器，用wordpress搞了一个，但是不是太满意，也没发布。后来用go语言写了一个，因为某些原因，中间的一些数据通路没有搞通（主要是从jupyter到md再到网站的自动发布），再加上后来考研，所以这个项目也没活到\u0026quot;成站\u0026quot;。 终于，用hugo搞了一个。之所以用hugo，一是因为最近研究go语言，对go语言的项目具有一定的好感，第二是因为看了网上的一些介绍并且发现了一些使用hugo的个人博客。 2021/2/28，记之。\n1. 使用hugo开始自己的网站 1.1 开始 hugo的使用炒鸡简单，你只需要使用\nhugo new site MySite 即可新建一个名为MySite的网站\n1.2 为你的网站选择一个theme 进入到我们刚才建立的网站目录\ncd Mysite/ 从GitHub导入你想应用的主题\ngit init git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke 待下载完成后，还需要修改你的配置文件\necho \u0026#39;theme = \u0026#34;ananke\u0026#34; \u0026#39; \u0026gt;\u0026gt; config.toml 1.3 为你的网站添加一些内容 hugo使用我们上传的md文件来自动生成静态网页，而我们上传的md文件的位置在MySite/content/posts/*, 我们可以直接复制已经编辑好的md文件到这个目录，或者使用如下命令：\nhugo new posts/my-first-post.md 需要注意的一点：为了让hugo知道更多的信息，我们上传的md文件一般会有一个\u0026quot;standand header\u0026quot;, 如下：\n--- title: \u0026quot;My First Post\u0026quot; date: 2019-03-26T08:47:11+01:00 draft: true --- 这里的头部并不是一成不变的，你可以根据需要自行配置。\n1.4 开始让你的网站服务 使用如下命令让你的网站开始服务吧！\nhugo server -D 注意，这个命令只会让你的hugo服务器监听本地访问，也就是127.0.0.1 如果你想要你的hugo服务器为整个网络服务，可以这样：\nhugo server --bind=\u0026#39;0.0.0.0\u0026#39; 这样，你的网站就可以全网访问了。\n2. 使用webhook 简单的说，webhook是一个触发器，当我们做出某个特定的的动作后，webhook可以自动完成特定任务。\n2.1 服务端配置 首先，我们使用https://github.com/adnanh/webhook.git来配置我们的服务器端，使其可监听webhook请求。\n 安装adnanh/webhook go build github.com/adnanh/webhook  编辑hooks.json [ { \u0026#34;id\u0026#34;: \u0026#34;redeploy-webhook\u0026#34;, \u0026#34;execute-command\u0026#34;: \u0026#34;/var/scripts/redeploy.sh\u0026#34;, //脚本的位置 \u0026#34;command-working-directory\u0026#34;: \u0026#34;/var/webhook\u0026#34; // 脚本的工作目录 } ] 如果你想要你的脚本返回一些值作为提醒，可以这样设置：\n\u0026#34;response-message\u0026#34;:\u0026#34;hooks,hooks\u0026#34;  编辑你要运行的自动部署脚本 # 你的脚本，根据你所要执行的任务自行编写   ✦✦你需要注意你的脚本权限，确保其可以运行\n2.2 GitHub的webhook 在服务端配置完成后，开始配置GitHub。 Github为我们提供了webhook功能，可以方便的应用在我们的代码仓库中。 在github上配置你的webhook，content type选择application/json,Payload URL为你想要webhook监听的端口与路径，其中路径中的最后一部分与hooks.json中的id是对应的。 2.3 测试通路 测试你的webhook是否设置成功。\n 开启服务端监听  /path/to/webhook -hooks hooks.json -verbose # 默认端口9000 # 你可以使用-port参数指定监听端口 在GitHub上发送请求 GitHub webhook配置完成后会自动发送请求查看是否有回应，若Recent Deliveries中是红色的感叹号，代表测试失败。你可以点击Redeliver按钮重新测试。若出现绿色的对号，则代表测试成功。 ![redeliver]{}  2.4 将webhook设置为systemctl 为了让webhook能一直监听请求，我们需要使webhook保持运行。 新建/etc/systemed/system/webhook.service 其内容如下：\n[Unit] Description=Webhooks [Service] ExecStart=/usr/bin/webhook -port 12345 -hooks /path/to/hooks.json -hotreload [Install] WantedBy=multi-user.target 保存并关闭。 运行以下命令\nsudo systemctl daemon-reload # 重载配置文件 sudo systemctl start webhook.service # 启动webhook.service sudo systemctl status webhook.service # 查看webhook的状态 3. 使用nginx作为服务器 安装nginx：\nsudo apt-get install nginx 使用hugo生成静态文件\n# cd Mysite hugo hugo生成的静态文件存储在public文件夹中。 将public文件夹中的所有文件复制到nginx指定的root文件夹中。 访问你的服务器ip地址或域名。 页面正常！ ✦✦如果你的nginx显示404，那么请检查你的nginx的root文件夹配置以及你的文件位置；如果显示403，可能是你服务端权限配置错误。\n4. 数据通路 本地编辑md文件 → 上传GitHub → webhook发送请求 → 服务器接收请求，拉取GitHub文件，重新生成静态文件 → 复制静态文件到nginx指定的root目录。更新完成\n","permalink":"http://yangchnet.github.io/Dessert/posts/env/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E8%AE%BE/","summary":"博客建设记 0. 前记 一直想要建设一个属于自己的博客，一开始用python写过一个简单的，可以做到富文本编辑、发布、更新、评论等功能，但那个不是一个单纯的博客，并且界面也不是太友好，因此后来废弃了。后来又用了一段时间的jupyter notebook，很强大，尤其让我喜欢的是可以直接运行代码，曾经有一段时间想过可否把jupyter notebook直接作为我的博客页面，或者是嵌入我的页面内，于是看了看其源代码。。。遂放弃。后来又用了为知笔记，印象笔记，Notion等，但感觉都没jupyter notebook好用。 在用jupyter notebook记了有了一定的数目之后，就想将其发布出来，考虑过CSDN，但感觉上面广告好多，不太喜欢，因此没有使用。后来买了域名和服务器，用wordpress搞了一个，但是不是太满意，也没发布。后来用go语言写了一个，因为某些原因，中间的一些数据通路没有搞通（主要是从jupyter到md再到网站的自动发布），再加上后来考研，所以这个项目也没活到\u0026quot;成站\u0026quot;。 终于，用hugo搞了一个。之所以用hugo，一是因为最近研究go语言，对go语言的项目具有一定的好感，第二是因为看了网上的一些介绍并且发现了一些使用hugo的个人博客。 2021/2/28，记之。\n1. 使用hugo开始自己的网站 1.1 开始 hugo的使用炒鸡简单，你只需要使用\nhugo new site MySite 即可新建一个名为MySite的网站\n1.2 为你的网站选择一个theme 进入到我们刚才建立的网站目录\ncd Mysite/ 从GitHub导入你想应用的主题\ngit init git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke 待下载完成后，还需要修改你的配置文件\necho \u0026#39;theme = \u0026#34;ananke\u0026#34; \u0026#39; \u0026gt;\u0026gt; config.toml 1.3 为你的网站添加一些内容 hugo使用我们上传的md文件来自动生成静态网页，而我们上传的md文件的位置在MySite/content/posts/*, 我们可以直接复制已经编辑好的md文件到这个目录，或者使用如下命令：\nhugo new posts/my-first-post.md 需要注意的一点：为了让hugo知道更多的信息，我们上传的md文件一般会有一个\u0026quot;standand header\u0026quot;, 如下：\n--- title: \u0026quot;My First Post\u0026quot; date: 2019-03-26T08:47:11+01:00 draft: true --- 这里的头部并不是一成不变的，你可以根据需要自行配置。\n1.4 开始让你的网站服务 使用如下命令让你的网站开始服务吧！\nhugo server -D 注意，这个命令只会让你的hugo服务器监听本地访问，也就是127.0.0.1 如果你想要你的hugo服务器为整个网络服务，可以这样：","title":"博客建设"},{"content":"1、python与其他语言的对比（hello world）  C语言\n include\u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello world\u0026#34;); return 0; }  Java语言\n public class HelloWorld{ public static void main(String[] args) { System.out.println(\u0026#34;Hello World!\u0026#34;); } }  Python\n print(\u0026#39;hello world\u0026#39;) 2、python中的常用数据类型  Number String List Tuple Dictionary  # Number a = 1 b = True c = 3.15 d = 1.1+2.2j # 字符串 str1 = \u0026#39;hello\u0026#39; str1_1 = \u0026#34;hello\u0026#34; str2 = \u0026#34;world\u0026#34; print(str1==str1_1) # 字符串连接 str3 = str1 + str2 print(str3) # 转义字符 str4 = \u0026#39;hello \\nworld\u0026#39; print(str4) str5 = \u0026#39;hello \\\\n world\u0026#39; print(str5) # 格式化输出 print(\u0026#39;str1:%s.\u0026#39;%str1) # 切片 print(str1[1:4]) True helloworld hello world hello \\n world str1:hello. ell  # 列表 list1 = [\u0026#39;google\u0026#39;, \u0026#39;alibaba\u0026#39;, 2001, 3.14] # 通过下标访问 print(list1[0]) # 更新列表 list1[2] = \u0026#39;baidu\u0026#39; print(list1) # 删除元素 del list1[3] print(list1) # 拼接列表 list2 = [\u0026#39;microsoft\u0026#39;, \u0026#39;amazon\u0026#39;] list3 = list1 + list2 print(list3) # 增添列表项 list1.append(\u0026#39;jingdong\u0026#39;) print(list1) google ['google', 'alibaba', 'baidu', 3.14] ['google', 'alibaba', 'baidu'] ['google', 'alibaba', 'baidu', 'microsoft', 'amazon'] ['google', 'alibaba', 'baidu', 'jingdong']  # 元组：类似列表,是一系列元素的有序集合,但元组中的元素无法修改 tuple1 = (\u0026#39;google\u0026#39;, \u0026#39;alibaba\u0026#39;, \u0026#39;baidu\u0026#39;) tuple1[0] = \u0026#39;amazon\u0026#39; # 不能被改变 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-23-4ed3e334c834\u0026gt; in \u0026lt;module\u0026gt; 1 # 元组：类似列表,是一系列元素的有序集合,但元组中的元素无法修改 2 tuple1 = ('google', 'alibaba', 'baidu') ----\u0026gt; 3 tuple1[0] = 'amazon' TypeError: 'tuple' object does not support item assignment  # 字典 dict1 = { \u0026#39;color\u0026#39;: \u0026#39;green\u0026#39;, \u0026#39;points\u0026#39;: 5 } # 访问列表中的值 print(dict1[\u0026#39;color\u0026#39;]) # 增加字典中键值对 dict1[\u0026#39;x_pos\u0026#39;] = 0 dict1[\u0026#39;y_pos\u0026#39;] =4 print(dict1) green {'color': 'green', 'points': 5, 'x_pos': 0, 'y_pos': 4}  3、python中的结构语句 3.1、if条件语句 car = \u0026#39;bmw\u0026#39; if car == \u0026#39;bmw\u0026#39;: print(car.upper()) # 输出car的大写版本 else: print(car.title()) # 输出car的标题版本 Bmw  现实世界中,很多情况下需要考虑的情形都超过两个。例如,来看一个根据年龄段收费的 游乐场:\n 4岁以下免费; 4~18岁收费5美元; 18岁(含)以上收费10美元。   如果只使用一条 if 语句,如何确定门票价格呢?下面的代码确定一个人所属的年龄段,并打印一条包含门票价格的消息:\n age = 12 if age \u0026lt; 4: print(\u0026#34;Your admission cost is $0.\u0026#34;) elif age \u0026lt; 18: print(\u0026#34;Your admission cost is $5.\u0026#34;) else: print(\u0026#34;Your admission cost is $10.\u0026#34;) Your admission cost is $5.  3.2、for循环语句 fruits = [\u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;mango\u0026#39;] for fruit in fruits: print(\u0026#39;当前水果：%s\u0026#39;%fruit) 当前水果：banana 当前水果：apple 当前水果：mango  # range 步长为1 for i in range(0, 6): print(i) 1 2 3 4 5  # range 步长为2 for i in range(0, 6, 2): print(i) 0 2 4  # break 和 continue for i in range(0, 6): if i == 3: break print(i) for i in range(0, 6): if i == 3: continue print(i, end=\u0026#39;\u0026#39;) 0 1 2 01245  3.3、while循环语句 # while循环 current_number = 1 while current_number \u0026lt;= 5: print(current_number) current_number += 1 1 2 3 4 5  3.4、函数 def greet_user(username): \u0026#34;\u0026#34;\u0026#34; 显示简单的问候语 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Hello, \u0026#34; + username.title() + \u0026#34;!\u0026#34;) greet_user(\u0026#39;jesse\u0026#39;) greet_user(\u0026#39;jack\u0026#39;) Hello, Jesse! Hello, Jack!  # 有返回值的函数 def add(a, b): return a+b print(\u0026#39;第一个函数：%d\u0026#39;%add(2, 3)) # 列表作为参数的函数 def add_l(mylist): result = 0 for l in mylist: result += l return result print(\u0026#39;第二个函数：%d\u0026#39;%add_l([1, 2, 3, 4])) # 有多个返回值的函数 def muti_re(mylist): a = max(mylist) b = min(mylist) return a, b a, b = muti_re([1, 2, 3, 4]) print(\u0026#39;第三个函数, 最大值：%d, 最小值：%d\u0026#39;%(a,b)) 第一个函数：5 第二个函数：10 第三个函数, 最大值：4, 最小值：1  有时候,你预先不知道函数需要接受多少个实参, 但是Python允许函数从调用语句中收集任意数量的实参。\n 例如,来看一个制作比萨的函数,它需要接受很多配料,但你无法预先确定顾客要多少种配 料。下面的函数只有一个形参 *toppings ,但不管调用语句提供了多少实参,这个形参都将它们 统统收入囊中:\n def make_pizza(*toppings): print(toppings) make_pizza(\u0026#39;pepperoni\u0026#39;) make_pizza(\u0026#39;mushrooms\u0026#39;, \u0026#39;green peppers\u0026#39;, \u0026#39;extra cheese\u0026#39;) ('pepperoni',) ('mushrooms', 'green peppers', 'extra cheese')  3.5、类 面向对象编程是最有效的软件编写方法之一。在面向对象编程中, 你编写表示现实世界中的事物和情景的类,并基于这些类来创建对象。 编写类时,你定义一大类对象都有的通用行为。基于类创建对象时, 每个对象都自动具备这种通用行为,然后可根据需要赋予每个对象独 特的个性。使用面向对象编程可模拟现实情景,其逼真程度达到了令 你惊讶的地步。 根据类来创建对象被称为实例化,这让你能够使用类的实例。在 本章中,你将编写一些类并创建其实例。你将指定可在实例中存储什 么信息,定义可对这些实例执行哪些操作。你还将编写一些类来扩展 既有类的功能,让相似的类能够高效地共享代码。你将把自己编写的类存储在模块中,并在 自己的程序文件中导入其他程序员编写的类\n使用类几乎可以模拟任何东西。下面来编写一个表示小狗的简单类 Dog ——它表示的不是特 定的小狗,而是任何小狗。对于大多数宠物狗,我们都知道些什么呢?它们都有名字和年龄;我 们还知道,大多数小狗还会蹲下和打滚。由于大多数小狗都具备上述两项信息(名字和年龄)和 两种行为(蹲下和打滚),我们的 Dog 类将包含它们。这个类让Python知道如何创建表示小狗的对 象。编写这个类后,我们将使用它来创建表示特定小狗的实例。\n# 创建类 class Dog(): def __init__(self, name, age): self.name = name self.age = age def sit(self): print(self.name.title() + \u0026#34; is now sitting.\u0026#34;) def roll_over(self): print(self.name.title() + \u0026#34; rolled over!\u0026#34;)对字符串的大小写进行更改 类中的函数称为方法;你前面学到的有关函数的一切都适用于方法,就目前而言,唯一重要 的差别是调用方法的方式。方法 init() 是一个特殊的方法,每当你根据 Dog 类创建新实 例时,Python都会自动运行它。在这个方法的名称中,开头和末尾各有两个下划线,这是一种约 定,旨在避免Python默认方法与普通方法发生名称冲突。\n# 根据类创建实例 my_dog = Dog(\u0026#39;willie\u0026#39;, 6) print(\u0026#34;My dog\u0026#39;s name is \u0026#34; + my_dog.name.title() + \u0026#34;.\u0026#34;) print(\u0026#34;My dog is \u0026#34; + str(my_dog.age) + \u0026#34; years old.\u0026#34;) My dog's name is Willie. My dog is 6 years old.  # 访问类的属性 my_dog.name 'willie'  # 调用类的方法 my_dog.sit() my_dog.roll_over() Willie is now sitting. Willie rolled over!  4、对字符串的一些操作 4.1、对字符串的大小写进行更改 # 对字符串的大小写进行更改 name = \u0026#34;ada lovelace\u0026#34; print(name.title()) name = \u0026#34;Ada Lovelace\u0026#34; print(name.upper()) print(name.lower()) Ada Lovelace ADA LOVELACE ada lovelace  4.2、合并/拼接字符串 first_name = \u0026#34;ada\u0026#34; last_name = \u0026#34;lovelace\u0026#34; full_name = first_name + \u0026#34; \u0026#34; + last_name print(full_name) ada lovelace  4.3、使用制表符或换行符来添加空白 print(\u0026#34;\\tPython\u0026#34;) print(\u0026#34;Languages:\\nPython\\nC\\nJavaScript\u0026#34;)  Python Languages: Python C JavaScript  4.4、删除空白 favorite_language = \u0026#39; python \u0026#39; # 删除末尾的空白 favorite_language.rstrip() # 删除开头的空白 favorite_language.lstrip() 'python '  4.5、字符串分割 my_languages = \u0026#39;python,C,JAVA,PHP,MATLAB\u0026#39; languages_list = my_languages.split(\u0026#39;,\u0026#39;) print(languages_list) print(languages_list[0]) print(languages_list[0][3]) ['python', 'C', 'JAVA', 'PHP', 'MATLAB'] python h  习题 1、打印出99乘法表 2、判断是否是闰年 3、编写Python程序计算斐波那契数列，要求：输入序号n，输出数列的第n项，数列的计算用函数实现 4、使用面向对象的思想， 创建一个名为 User 的类,其中包含属性 first_name 和 last_name ,还有用户简介通常会存储的其他几个属性。在类 User 中定义一个名为 describe_user() 的方法,它打印用户信息摘要;再定义一个名为 greet_user() 的方法,它向用户发出个性化的问候。 创建多个表示不同用户的实例,并对每个实例都调用上述两个方法。 ","permalink":"http://yangchnet.github.io/Dessert/posts/python/python%E5%9F%BA%E7%A1%80/","summary":"1、python与其他语言的对比（hello world）  C语言\n include\u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello world\u0026#34;); return 0; }  Java语言\n public class HelloWorld{ public static void main(String[] args) { System.out.println(\u0026#34;Hello World!\u0026#34;); } }  Python\n print(\u0026#39;hello world\u0026#39;) 2、python中的常用数据类型  Number String List Tuple Dictionary  # Number a = 1 b = True c = 3.15 d = 1.1+2.2j # 字符串 str1 = \u0026#39;hello\u0026#39; str1_1 = \u0026#34;hello\u0026#34; str2 = \u0026#34;world\u0026#34; print(str1==str1_1) # 字符串连接 str3 = str1 + str2 print(str3) # 转义字符 str4 = \u0026#39;hello \\nworld\u0026#39; print(str4) str5 = \u0026#39;hello \\\\n world\u0026#39; print(str5) # 格式化输出 print(\u0026#39;str1:%s.","title":"1、python与其他语言的对比（helloworld）"},{"content":"2016统计学专业数据挖掘实验课程考核  此考核预计用时120m, 满分100分\n  姓名： 学号：  1. Wordcount（30分） 计算出下文中每个英文字母出现的次数（不区分大小写, 去除空格与标点符号），并进行输出。\nOur entire class is quaking in its boots. The reason, of course, is the upcoming meeting in which the teachers decide who\u0026rsquo;ll be promoted to the next grade and who\u0026rsquo;ll be kept back. Half the class is making bets. G.Z. and I laugh ourselves sick at the two boys behind us, C.N. and Jacques Kocernoot, who have staked their entire vacation savings on their bet. From morning to night, it\u0026rsquo;s \u0026ldquo;You\u0026rsquo;re going to pass, No, I\u0026rsquo;m not,\u0026rdquo; \u0026ldquo;Yes, you are,\u0026rdquo; \u0026ldquo;No, I\u0026rsquo;m not.\u0026rdquo; Even G.\u0026rsquo;s pleading glances and my angry outbursts can\u0026rsquo;t calm them down. If you ask me, there are so many dummies that about a quarter of the class should be kept back, but teachers are the most unpredictable creatures on earth. Maybe this time they\u0026rsquo;ll be unpredictable in the right direction for a change. I\u0026rsquo;m not so worried about my girlfriends and myself.\n（节选自《安妮日记》）\nTips: 使用字典存储每个字母出现次数，例如：\nmy_str = \u0026#39;our\u0026#39; result = {} for s in my_str: result.setdefault(s, 0) result[s] += 1 print(result) {'o': 1, 'u': 1, 'r': 1}  你的答案：  2. 分词处理（30分） 使用jieba分词工具对下文进行分词，要求去除标点符号，并输出最后分词结果。\n我晓得他们的方法，直捷杀了，是不肯的，而且也不敢，怕有祸祟。所以他们大家连络，布满了罗网，逼我自戕。试看前几天街上男女的样子，和这几天我大哥的作为，便足可悟出八九分了。最好是解下腰带，挂在梁上，自己紧紧勒死；他们没有杀人的罪名，又偿了心愿，自然都欢天喜地的发出一种呜呜咽咽的笑声。否则惊吓忧愁死了，虽则略瘦，也还可以首肯几下。\n他们是只会吃死肉的！记得什么书上说，有一种东西，叫“海乙那”的，眼光和样子都很难看；时常吃死肉，连极大的骨头，都细细嚼烂，咽下肚子去，想起来也教人害怕。“海乙那”是狼的亲眷，狼是狗的本家。前天赵家的狗，看我几眼，可见他也同谋，早已接洽。老头子眼看着地，岂能瞒得我过。\n最可怜的是我的大哥，他也是人，何以毫不害怕；而且合伙吃我呢？还是历来惯了，不以为非呢？还是丧了良心，明知故犯呢？\n我诅咒吃人的人，先从他起头；要劝转吃人的人，也先从他下手。\n（节选自鲁迅《狂人日记》）\n 你的答案：  3. 朴素贝叶斯分类（40分） 鸢尾花数据集简介:\nIris数据集是常用的分类实验数据集，由Fisher, 1936收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性。iris以鸢尾花的特征作为数据来源，常用在分类操作中。该数据集由3种不同类型的鸢尾花的50个样本数据构成。其中的一个种类与另外两个种类是线性可分离的，后两个种类是非线性可分离的。数据集包括4个属性，分别为花萼的长、花萼的宽、花瓣的长和花瓣的宽。\n#加载数据集, 每一列代表一种属性，具体数据可查看iris.txt from sklearn import datasets iris = datasets.load_iris() X = iris.data Y = iris.target print(X[1:3]) print(Y[1:3]) [[4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2]] [0 0]   （1） 分别求出三种类型鸢尾花的花萼长度的平均值 你的答案： （2） 画出花萼长和花萼宽两种属性的散点分布图，使用“red”, \u0026ldquo;blue\u0026rdquo;, \u0026ldquo;green\u0026rdquo; 三种颜色代表三种类型的花。 你的答案： （3） 使用MultinomialNB分类器，对数据集进行训练，并计算最终分类准确率 你的答案：  恭喜你完成了本学期的所有数据挖掘实验课程！ 你对本课程的建议与看法： 得分： ","permalink":"http://yangchnet.github.io/Dessert/posts/python/2016%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B8%93%E4%B8%9A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E9%AA%8C%E8%AF%BE%E7%A8%8B%E8%80%83%E6%A0%B8/","summary":"2016统计学专业数据挖掘实验课程考核  此考核预计用时120m, 满分100分\n  姓名： 学号：  1. Wordcount（30分） 计算出下文中每个英文字母出现的次数（不区分大小写, 去除空格与标点符号），并进行输出。\nOur entire class is quaking in its boots. The reason, of course, is the upcoming meeting in which the teachers decide who\u0026rsquo;ll be promoted to the next grade and who\u0026rsquo;ll be kept back. Half the class is making bets. G.Z. and I laugh ourselves sick at the two boys behind us, C.N. and Jacques Kocernoot, who have staked their entire vacation savings on their bet.","title":"2016统计学专业数据挖掘实验课程考核"},{"content":"AutoReserve | 使用说明 1. 简介 AutoReserve是一个帮助同学们自动预约图书馆座位的系统，提供每晚代约座位服务，由一位热心同学开发和维护。\n2.访问站点 我们的网站是：http://59.110.140.133/reserve/ ，欢迎访问注册\n3.如何使用 访问网站-\u0026gt;注册账号-\u0026gt;填写表格-\u0026gt;审核通过-\u0026gt;成功加入预约名单\n3.1 访问并注册 访问http://59.110.140.133/reserve/主页\n点击免费注册或上方导航栏里的注册按钮进行注册，跳转到注册页面\n填写表单进行注册\n两次密码不一致会有检查提示\n注册完成后自动重定向到我的预约页面\n页面下方会显示系统数据库中已经被“预约”的位置\n按照提示填写表单后，页面上方会刷新出你的预约信息\n4. Q\u0026amp;A Q： 本系统的用户名必须和预约系统的账号一样吗？\nA： 不一定，你可以选择自己喜欢的用户名，比如：悲伤的雪，等等。在你填写自己的预约信息时要求的账号才是你图书馆预约系统的账号。\n Q： 可以100%保证我每天都可以约到想要的位置吗？\nA： 不一定，由于本系统也是模拟登录等行为来进行预约，因此也受到比如网络拥塞，系统卡顿等问题，因此不一定每天都可以约到想要的位置，当用户指定的位置未完成预约时，系统将自动预约与其相邻的下一个座位，保证用户有位置可坐。\n Q： 有时注册提交表单时会出现Internal Error 500,这代表什么意思，是注册失败了吗？\nA： Internal Error 500代表服务器内部错误，由于某些未知的原因，此类错误尚无法预知并排除，出现此类错误时，通常预约已经完成，用户可再次访问网页直接登录。推荐用户在浏览器内打开网址，不建议在QQ，微信环境下直接进入网页。\n Q： 我填写完表单后，当天就可以约到位置吗？\nA： 系统在每天晚上12点为用户预约“明天”的座位，也就是说，如果用户在1月1日中午填写表单，那么系统将在1月2日的凌晨00:00为其预约1月3日的座位，此后每天晚上都会再次预约。\n Q： 我今天不想去了，怎么才能取消今天的座位？\nA： 本系统不提供取消座位服务，用户可直接登录官方预约系统微信端（ http://libzwxt.ahnu.edu.cn/SeatWx/login.aspx ） 或网页端（ http://libzwxt.ahnu.edu.cn/SeatManage/ ） 进行取消。\n Q： 我想换个位置做，可以修改预约信息吗？\nA： 用户可直接登录系统，点击上方我的预约导航按钮，进入我的预约页面修改信息。\n Q： 一个账号可以提交多个预约信息吗？\nA： 不可以，每个账号只能为一个人预约。\n Q： 为什么我的座位没有预约成功？\nA： 系统可能存在未知的BUG， 反复出现此问题请联系管理员。\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/autoreserve%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/","summary":"AutoReserve | 使用说明 1. 简介 AutoReserve是一个帮助同学们自动预约图书馆座位的系统，提供每晚代约座位服务，由一位热心同学开发和维护。\n2.访问站点 我们的网站是：http://59.110.140.133/reserve/ ，欢迎访问注册\n3.如何使用 访问网站-\u0026gt;注册账号-\u0026gt;填写表格-\u0026gt;审核通过-\u0026gt;成功加入预约名单\n3.1 访问并注册 访问http://59.110.140.133/reserve/主页\n点击免费注册或上方导航栏里的注册按钮进行注册，跳转到注册页面\n填写表单进行注册\n两次密码不一致会有检查提示\n注册完成后自动重定向到我的预约页面\n页面下方会显示系统数据库中已经被“预约”的位置\n按照提示填写表单后，页面上方会刷新出你的预约信息\n4. Q\u0026amp;A Q： 本系统的用户名必须和预约系统的账号一样吗？\nA： 不一定，你可以选择自己喜欢的用户名，比如：悲伤的雪，等等。在你填写自己的预约信息时要求的账号才是你图书馆预约系统的账号。\n Q： 可以100%保证我每天都可以约到想要的位置吗？\nA： 不一定，由于本系统也是模拟登录等行为来进行预约，因此也受到比如网络拥塞，系统卡顿等问题，因此不一定每天都可以约到想要的位置，当用户指定的位置未完成预约时，系统将自动预约与其相邻的下一个座位，保证用户有位置可坐。\n Q： 有时注册提交表单时会出现Internal Error 500,这代表什么意思，是注册失败了吗？\nA： Internal Error 500代表服务器内部错误，由于某些未知的原因，此类错误尚无法预知并排除，出现此类错误时，通常预约已经完成，用户可再次访问网页直接登录。推荐用户在浏览器内打开网址，不建议在QQ，微信环境下直接进入网页。\n Q： 我填写完表单后，当天就可以约到位置吗？\nA： 系统在每天晚上12点为用户预约“明天”的座位，也就是说，如果用户在1月1日中午填写表单，那么系统将在1月2日的凌晨00:00为其预约1月3日的座位，此后每天晚上都会再次预约。\n Q： 我今天不想去了，怎么才能取消今天的座位？\nA： 本系统不提供取消座位服务，用户可直接登录官方预约系统微信端（ http://libzwxt.ahnu.edu.cn/SeatWx/login.aspx ） 或网页端（ http://libzwxt.ahnu.edu.cn/SeatManage/ ） 进行取消。\n Q： 我想换个位置做，可以修改预约信息吗？\nA： 用户可直接登录系统，点击上方我的预约导航按钮，进入我的预约页面修改信息。\n Q： 一个账号可以提交多个预约信息吗？\nA： 不可以，每个账号只能为一个人预约。\n Q： 为什么我的座位没有预约成功？\nA： 系统可能存在未知的BUG， 反复出现此问题请联系管理员。","title":"AutoReserve|使用说明"},{"content":"CentOS安装Python环境 吐槽：网上一堆从官网获取安装包然后自己编译的，慢不说，还容易出错\n可使用以下命令安装Python3环境：\n   yum install rh-python36 使用这条命令，安装Python3.6，但是安装后找不到，输入Python3后还是找不到命令\n scl enable rh-python36 bash\n上面的命令是调用/opt/rh/rh-python36/enable更改shell环境变量的脚本。\n如果再次检查Python版本，你会发现Python 3.6现在是当前shell中的默认版本。 需要指出的是，Python 3.6仅在此shell会话中设置为默认的Python版本。如果退出会话或从另一个终端打开一个新会话，Python 2.7将是默认的Python版本。\n  可使用当前shell窗口建立一个Python3虚拟环境，这样就可以使用Python3\n  #首先，创建项目目录并切换到它： mkdir ~/my_new_project cd ~/my_new_project #使用该scl工具激活Python 3.6 ： sl enable rh-python36 bash # 从项目根目录内部运行以下命令以创建名为的虚拟环境my_project_venv： python -m venv my_project_venv #要首先使用虚拟环境，我们需要输入以下命令来激活它： source my_project_venv/bin/activate #激活环境后，shell提示符将以环境名称作为前缀： (my_project_venv) user@host:~/my_new_project$ ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/centos%E5%AE%89%E8%A3%85python%E7%8E%AF%E5%A2%83/","summary":"CentOS安装Python环境 吐槽：网上一堆从官网获取安装包然后自己编译的，慢不说，还容易出错\n可使用以下命令安装Python3环境：\n   yum install rh-python36 使用这条命令，安装Python3.6，但是安装后找不到，输入Python3后还是找不到命令\n scl enable rh-python36 bash\n上面的命令是调用/opt/rh/rh-python36/enable更改shell环境变量的脚本。\n如果再次检查Python版本，你会发现Python 3.6现在是当前shell中的默认版本。 需要指出的是，Python 3.6仅在此shell会话中设置为默认的Python版本。如果退出会话或从另一个终端打开一个新会话，Python 2.7将是默认的Python版本。\n  可使用当前shell窗口建立一个Python3虚拟环境，这样就可以使用Python3\n  #首先，创建项目目录并切换到它： mkdir ~/my_new_project cd ~/my_new_project #使用该scl工具激活Python 3.6 ： sl enable rh-python36 bash # 从项目根目录内部运行以下命令以创建名为的虚拟环境my_project_venv： python -m venv my_project_venv #要首先使用虚拟环境，我们需要输入以下命令来激活它： source my_project_venv/bin/activate #激活环境后，shell提示符将以环境名称作为前缀： (my_project_venv) user@host:~/my_new_project$ ","title":"CentOS安装Python环境"},{"content":"Did you install mysqlclient? 在进行新的django项目时出现了这个错误\n解决方法  确保pymysql和mysqlcient都安装 在和setting.py同级的init.py中加入  import pymysql pymysql.install_as_MySQLdb() ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/did-you-install-mysqlclient/","summary":"Did you install mysqlclient? 在进行新的django项目时出现了这个错误\n解决方法  确保pymysql和mysqlcient都安装 在和setting.py同级的init.py中加入  import pymysql pymysql.install_as_MySQLdb() ","title":"Did you install mysqlclient?"},{"content":"Django, Forms 使用forms完成了用户登录 1、创建model class User(User): pass  这里使用了django提供的User类，直接继承\n2、创建UserForm类 class SigninFrom(forms.Form): user_name = forms.CharField() user_email = forms.EmailField() user_password = forms.CharField() 3、完成模板 \u0026lt;form action=\u0026#34;{% url \u0026#39;permission:signin\u0026#39; %}\u0026#34; accept-charset=\u0026#34;UTF-8\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;input name=\u0026#34;utf8\u0026#34; type=\u0026#34;hidden\u0026#34; value=\u0026#34;\u0026amp;#x2713;\u0026#34;/\u0026gt; {% csrf_token %} \u0026lt;dl class=\u0026#34;form-group mt-0\u0026#34;\u0026gt; \u0026lt;dt class=\u0026#34;input-label\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;form-label f5\u0026#34; for=\u0026#34;user[login]\u0026#34;\u0026gt;用户名\u0026lt;/label\u0026gt; \u0026lt;/dt\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;user_name\u0026#34; id=\u0026#34;user_name\u0026#34; class=\u0026#34;form-control form-control-lg input-block\u0026#34; placeholder=\u0026#34;{{ default_name }}\u0026#34; autofocus\u0026gt; \u0026lt;/dl\u0026gt; \u0026lt;dl class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;dt class=\u0026#34;input-label\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;form-label f5\u0026#34; for=\u0026#34;user[email]\u0026#34;\u0026gt;Email\u0026lt;/label\u0026gt; \u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;user_email\u0026#34; id=\u0026#34;user_email\u0026#34; class=\u0026#34;form-control form-control-lg input-block js-email-notice-trigger\u0026#34; placeholder=\u0026#34;you@example.com\u0026#34;\u0026gt; \u0026lt;/dd\u0026gt; \u0026lt;/dl\u0026gt; \u0026lt;dl class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;dt class=\u0026#34;input-label\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;form-label f5\u0026#34; for=\u0026#34;user[password]\u0026#34;\u0026gt;密码\u0026lt;/label\u0026gt; \u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt; \u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;user_password\u0026#34; id=\u0026#34;user_password\u0026#34; class=\u0026#34;form-control form-control-lg input-block\u0026#34; placeholder=\u0026#34;Create a password\u0026#34;\u0026gt; \u0026lt;/dd\u0026gt; \u0026lt;/dl\u0026gt; \u0026lt;button class=\u0026#34;btn-mktg btn-primary-mktg btn-large-mktg f4 btn-block\u0026#34; type=\u0026#34;submit\u0026#34; data-ga-click=\u0026#34;Signup, Attempt, location:teams;\u0026#34;\u0026gt;在Bodydetect上注册 \u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt;  注意，这里三者要统一用一样的name，否则会接收不到数据  views.py如何编写 def fun(request): if request.method == \u0026#39;GET\u0026#39;: ... return ... if request.method == \u0026#39;POST\u0026#39;: ... return  使用forms.cleaned_data访问数据，返回字典类型  你还可以  从view中返回form，django会自动为你渲染，当然，十分丑陋  例如：\n\u0026lt;form action = \u0026#34;{% url \u0026#39;permission:signin\u0026#39; %}\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; {% csrf_token %} \u0026lt;table\u0026gt; {{ form }} \u0026lt;/table\u0026gt; \u0026lt;button class=\u0026#34;btn-mktg btn-primary-mktg btn-large-mktg f4 btn-block\u0026#34; type=\u0026#34;submit\u0026#34; data-ga-click=\u0026#34;Signup, Attempt, location:teams;\u0026#34;\u0026gt;在Bodydetect上注册 \u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; 只需要为form保留位置\n渲染结果：\n不使用自动渲染\n使用ModelForm类  views.py\n from django.db import models class Article(models.Model): author = models.CharField(max_length = 150) title = models.CharField(max_lengeth = 100) content = models.CharField(max_lengeth = 10000) time = models.TimeField( )  model.py\n from django.forms import ModelForm class Article(ModelForm): class Meta: model = Article fields = [\u0026#39;author\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;content\u0026#39;, \u0026#39;time\u0026#39;] save方法  每个ModelForm还有一个save()方法，此方法根据绑定到表单的数据创建并保存数据库对象。   # Create a form instance with POST data. \u0026gt;\u0026gt;\u0026gt; f = AuthorForm(request.POST) # Create, but don't save the new author instance. \u0026gt;\u0026gt;\u0026gt; new_author = f.save(commit=False) # Modify the author in some way. \u0026gt;\u0026gt;\u0026gt; new_author.some_field = 'some_value' # Save the new instance. \u0026gt;\u0026gt;\u0026gt; new_author.save() # Now, save the many-to-many data for the form. \u0026gt;\u0026gt;\u0026gt; f.save_m2m() Reference Django 教程 9: 使用表单\n从模型创建表单 | Django documentation | Django\nThe Forms API | Django documentation | Django\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/django-form/","summary":"Django, Forms 使用forms完成了用户登录 1、创建model class User(User): pass  这里使用了django提供的User类，直接继承\n2、创建UserForm类 class SigninFrom(forms.Form): user_name = forms.CharField() user_email = forms.EmailField() user_password = forms.CharField() 3、完成模板 \u0026lt;form action=\u0026#34;{% url \u0026#39;permission:signin\u0026#39; %}\u0026#34; accept-charset=\u0026#34;UTF-8\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;input name=\u0026#34;utf8\u0026#34; type=\u0026#34;hidden\u0026#34; value=\u0026#34;\u0026amp;#x2713;\u0026#34;/\u0026gt; {% csrf_token %} \u0026lt;dl class=\u0026#34;form-group mt-0\u0026#34;\u0026gt; \u0026lt;dt class=\u0026#34;input-label\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;form-label f5\u0026#34; for=\u0026#34;user[login]\u0026#34;\u0026gt;用户名\u0026lt;/label\u0026gt; \u0026lt;/dt\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;user_name\u0026#34; id=\u0026#34;user_name\u0026#34; class=\u0026#34;form-control form-control-lg input-block\u0026#34; placeholder=\u0026#34;{{ default_name }}\u0026#34; autofocus\u0026gt; \u0026lt;/dl\u0026gt; \u0026lt;dl class=\u0026#34;form-group\u0026#34;\u0026gt; \u0026lt;dt class=\u0026#34;input-label\u0026#34;\u0026gt; \u0026lt;label class=\u0026#34;form-label f5\u0026#34; for=\u0026#34;user[email]\u0026#34;\u0026gt;Email\u0026lt;/label\u0026gt; \u0026lt;/dt\u0026gt; \u0026lt;dd\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;user_email\u0026#34; id=\u0026#34;user_email\u0026#34; class=\u0026#34;form-control form-control-lg input-block js-email-notice-trigger\u0026#34; placeholder=\u0026#34;you@example.","title":"Django,Forms"},{"content":"django中forms的定义 直接定义 class ContactForm(forms.Form): date = DateField(widget=CalendarWidget) name = CharField(max_length=40, widget=OtherWidget) widget参数定义了要使用的小部件，小部件选项可见这里\n通过模型定义 必须继承ModelForm类\nfrom django.forms import ModelForm class BlogForm(ModelForm): class Meta: model = Blog fields = [\u0026#39;author\u0026#39;, \u0026#39;essay\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;label\u0026#39;, \u0026#39;cover\u0026#39;] widgets = { \u0026#39;essay\u0026#39;: CKEditorWidget, \u0026#39;cover\u0026#39;: } 通过定义内部类来生命form的属性\n 常用内部类参数说明：\nmodel: 说明要继承的模型\nfield：说明要在表单中显示的字段，__all__表示所有\nexclude: 要从表单中排除的字段\nwidgets: 设置字段的小部件\n（详细文档）\n ","permalink":"http://yangchnet.github.io/Dessert/posts/django/django%E4%B8%ADform%E7%9A%84%E5%AE%9A%E4%B9%89/","summary":"django中forms的定义 直接定义 class ContactForm(forms.Form): date = DateField(widget=CalendarWidget) name = CharField(max_length=40, widget=OtherWidget) widget参数定义了要使用的小部件，小部件选项可见这里\n通过模型定义 必须继承ModelForm类\nfrom django.forms import ModelForm class BlogForm(ModelForm): class Meta: model = Blog fields = [\u0026#39;author\u0026#39;, \u0026#39;essay\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;label\u0026#39;, \u0026#39;cover\u0026#39;] widgets = { \u0026#39;essay\u0026#39;: CKEditorWidget, \u0026#39;cover\u0026#39;: } 通过定义内部类来生命form的属性\n 常用内部类参数说明：\nmodel: 说明要继承的模型\nfield：说明要在表单中显示的字段，__all__表示所有\nexclude: 要从表单中排除的字段\nwidgets: 设置字段的小部件\n（详细文档）\n ","title":"django中forms的定义"},{"content":"django中使用highchart 引入js文件  顺序不能错\n \u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.1.1.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://code.highcharts.com/highcharts.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 一点小插曲 在刚开始使用highchart时，由于使用的是继承模板，导致js文件引入的顺序没有把握好，导致不能显示图像，因此将上面两句提到base.html的前面，然后可以显示图像。\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/django%E4%B8%AD%E4%BD%BF%E7%94%A8highchart/","summary":"django中使用highchart 引入js文件  顺序不能错\n \u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.1.1.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://code.highcharts.com/highcharts.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 一点小插曲 在刚开始使用highchart时，由于使用的是继承模板，导致js文件引入的顺序没有把握好，导致不能显示图像，因此将上面两句提到base.html的前面，然后可以显示图像。","title":"django中使用highchart"},{"content":"Django 中图像的处理方法 图像的上传保存  前端图片的上传：  \u0026lt;form action=\u0026#34;/updateinfo\u0026#34; method=\u0026#34;POST\u0026#34; enctype=\u0026#34;multipart/form-data\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;updateImg\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ account.photo.url }}\u0026#34; alt=\u0026#34;\u0026#34;/\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;input name=\u0026#34;photo\u0026#34; type=\u0026#34;file\u0026#34; id=\u0026#34;exampleInputFile\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;photo\u0026#34; class=\u0026#34;btn btn-danger\u0026#34; type=\u0026#34;submit\u0026#34;\u0026gt;上传头像\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; 其中input标签的type为file， 2. 图片模型\nmodels.ImageField(upload_to=\u0026lsquo;path\u0026rsquo;) upload_to的储存路径是相对于MEDIA_ROOT而来的，若MEDIA_ROOT为/media/，upload_to路径为image，则图片上传后的储存路径为/media/image\n在前端显示上传的图片 {% load static %} \u0026lt;body data-media-url=\u0026#34;{% get_media_prefix %}\u0026#34;\u0026gt; 使用get_media_prefxi模板tag，代表MEDIA_URL变量\n\u0026lt;img src=\u0026#34;{% get_media_prefix %}/{{ page.cover }}\u0026#34; alt = \u0026#34;{{ page.cover }}\u0026#34;\u0026gt;  存在的问题  每个用户上传的图片集中在一个文件夹下，容易造成命名冲突，\n可参考这里\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%8A%E4%BC%A0%E4%BF%9D%E5%AD%98%E6%98%BE%E7%A4%BA/","summary":"Django 中图像的处理方法 图像的上传保存  前端图片的上传：  \u0026lt;form action=\u0026#34;/updateinfo\u0026#34; method=\u0026#34;POST\u0026#34; enctype=\u0026#34;multipart/form-data\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;updateImg\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ account.photo.url }}\u0026#34; alt=\u0026#34;\u0026#34;/\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;input name=\u0026#34;photo\u0026#34; type=\u0026#34;file\u0026#34; id=\u0026#34;exampleInputFile\u0026#34;\u0026gt; \u0026lt;button id=\u0026#34;photo\u0026#34; class=\u0026#34;btn btn-danger\u0026#34; type=\u0026#34;submit\u0026#34;\u0026gt;上传头像\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; 其中input标签的type为file， 2. 图片模型\nmodels.ImageField(upload_to=\u0026lsquo;path\u0026rsquo;) upload_to的储存路径是相对于MEDIA_ROOT而来的，若MEDIA_ROOT为/media/，upload_to路径为image，则图片上传后的储存路径为/media/image\n在前端显示上传的图片 {% load static %} \u0026lt;body data-media-url=\u0026#34;{% get_media_prefix %}\u0026#34;\u0026gt; 使用get_media_prefxi模板tag，代表MEDIA_URL变量\n\u0026lt;img src=\u0026#34;{% get_media_prefix %}/{{ page.cover }}\u0026#34; alt = \u0026#34;{{ page.cover }}\u0026#34;\u0026gt;  存在的问题  每个用户上传的图片集中在一个文件夹下，容易造成命名冲突，\n可参考这里","title":"Django中图像的处理方法"},{"content":"docker常用操作  启动docker服务 sudo systemctl start docker 查看本地镜像 sudo docker images 查看正在运行的镜像 sudo docker ps 查看所有镜像 sudo docker ps -a 停止正在运行的镜像 sudo docker stop container_name 开始运行某个镜像 sudo docker start container_name 删除某个镜像 sudo docker rmi container_name 进入某个正在运行的镜像 sudo docker attach container_name 导出容器 sudo docker export container_id \u0026gt; name.tar 导入容器 cat name.tar | sudo docker import -test/buntu:v1.0 从网络导入 sudo docker import http://example.com/exampleimage.tgz example/imagerepo  ","permalink":"http://yangchnet.github.io/Dessert/posts/cloud/docker/","summary":"docker常用操作  启动docker服务 sudo systemctl start docker 查看本地镜像 sudo docker images 查看正在运行的镜像 sudo docker ps 查看所有镜像 sudo docker ps -a 停止正在运行的镜像 sudo docker stop container_name 开始运行某个镜像 sudo docker start container_name 删除某个镜像 sudo docker rmi container_name 进入某个正在运行的镜像 sudo docker attach container_name 导出容器 sudo docker export container_id \u0026gt; name.tar 导入容器 cat name.tar | sudo docker import -test/buntu:v1.0 从网络导入 sudo docker import http://example.com/exampleimage.tgz example/imagerepo  ","title":"docker常用操作"},{"content":"1、docker的安装(Ubuntu) 1.1、 设置存储库  若是已安装旧版本的docker， 请卸载：sudo apt-get remove docker docker-engine docker.io containerd runc\n 1.1.1、更新apt索引 sudo apt-get update 1.1.2、安装依赖 sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common 1.1.3、添加docker官方的GPG秘钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -  在进行此步时，出现了sudo: unable to resolve host iZ2ze4512bfzoapfvch6btZ，这是因为机器不能反向解析 打开主机上的 /etc/hosts 添加： 127.0.0.1 【hostname】# 【hostname】用主机名替代 可在/etc/hostname中修改主机名，sudo shutdown -r now重启过后完成主机名修改\n 验证添加成功：\nsudo apt-key fingerprint 0EBFCD88 1.1.4、 设置存储库 sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) \\stable\u0026quot; 1.2、安装dockerCE 1.2.1 更新apt索引 sudo apt-get update 1.2.2、安装最新版本的dockerCE和containerd sudo apt-get install docker-ce docker-ce-cli containerd.io 1.2.3、通过运行hello-world验证是否正确安装了dockerCE sudo docker run hello-world 1.2.4、卸载dockerCE 1、卸载dockerCE软件包\nsudo apt-get purge docker-ce 2、主机上的图像，容器，卷或自定义配置文件不会自动删除。要删除所有图像，容器和卷：\nsudo rm -rf /var/lib/docker 1.3、在docker中运行应用 1.3.1、创建工作目录 mkdir dockerwork 进入：\ncd dockerwork 1.3.2、 创建DockerFile 内容如下：\nUse an official Python runtime as a parent image FROM python:2.7-slim # Set the working directory to /app  WORKDIR /app # Copy the current directory contents into the container at /app  COPY . /app # Install any needed packages specified in requirements.txt  RUN pip install --trusted-host pypi.python.org -r requirements.txt # Make port 80 available to the world outside this container  EXPOSE 80 # Define environment variable  ENV NAME World # Run app.py when the container launches  CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] 有关DockerFile的解释可见这里\n 其中 WORKDIR表示工作目录， COPY是把当前目录下（.）的内容复制到 /app\n 其中包含两个未建立的文件：requirements.txt \u0026amp; app.py 其内容如下：\n requirements.txt\n Flask Redis  app.py\n from flask import Flask from redis import Redis, RedisError import os import socket # Connect to Redis redis = Redis(host=\u0026#34;redis\u0026#34;, db=0, socket_connect_timeout=2, socket_timeout=2) app = Flask(__name__) @app.route(\u0026#34;/\u0026#34;) def hello(): try: visits = redis.incr(\u0026#34;counter\u0026#34;) except RedisError: visits = \u0026#34;\u0026lt;i\u0026gt;cannot connect to Redis, counter disabled\u0026lt;/i\u0026gt;\u0026#34; html = \u0026#34;\u0026lt;h3\u0026gt;Hello {name}!\u0026lt;/h3\u0026gt;\u0026#34; \\ \u0026#34;\u0026lt;b\u0026gt;Hostname:\u0026lt;/b\u0026gt; {hostname}\u0026lt;br/\u0026gt;\u0026#34; \\ \u0026#34;\u0026lt;b\u0026gt;Visits:\u0026lt;/b\u0026gt; {visits}\u0026#34; return html.format(name=os.getenv(\u0026#34;NAME\u0026#34;, \u0026#34;world\u0026#34;), hostname=socket.gethostname(), visits=visits) if __name__ == \u0026#34;__main__\u0026#34;: app.run(host=\u0026#39;0.0.0.0\u0026#39;, port=80) 1.3.3、构建应用程序 docker build --tag=friendlyhello . 1.3.4、运行应用程序 docker run -p 4000:80 friendlyhello  这是运行一般程序的步骤，要构建django项目，请到这里\n ","permalink":"http://yangchnet.github.io/Dessert/posts/cloud/docker-%E7%AC%AC%E4%B8%80%E7%AF%87/","summary":"1、docker的安装(Ubuntu) 1.1、 设置存储库  若是已安装旧版本的docker， 请卸载：sudo apt-get remove docker docker-engine docker.io containerd runc\n 1.1.1、更新apt索引 sudo apt-get update 1.1.2、安装依赖 sudo apt-get install \\  apt-transport-https \\  ca-certificates \\  curl \\  gnupg-agent \\  software-properties-common 1.1.3、添加docker官方的GPG秘钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -  在进行此步时，出现了sudo: unable to resolve host iZ2ze4512bfzoapfvch6btZ，这是因为机器不能反向解析 打开主机上的 /etc/hosts 添加： 127.0.0.1 【hostname】# 【hostname】用主机名替代 可在/etc/hostname中修改主机名，sudo shutdown -r now重启过后完成主机名修改\n 验证添加成功：\nsudo apt-key fingerprint 0EBFCD88 1.1.4、 设置存储库 sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.","title":"docker的安装（Ubuntu)"},{"content":"Flag包的基本用法  flag包用于处理golang命令行程序中的参数\n 1. 使用flag包的基本流程 使用flag包涉及三个步骤：\n 定义变量以捕获标志值 定义Go应用程序将使用的标志 在执行时解析提供给应用程序的标志。  flag软件包中的大多数功能都与定义标志并将其绑定到定义的变量有关。解析阶段由Parse()函数处理。\n一个例子 创建一个程序，该程序定义一个布尔标志，该标志会更改将打印到标准输出的消息。如果-color提供了一个标志，程序将以蓝色打印一条消息。如果未提供标志，则消息将被打印为没有任何颜色。\n// boolean.go import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; ) type Color string // 定义变量以捕获标志值  const ( ColorBlack Color = \u0026#34;\\u001b[30m\u0026#34; ColorRed = \u0026#34;\\u001b[31m\u0026#34; ColorGreen = \u0026#34;\\u001b[32m\u0026#34; ColorYellow = \u0026#34;\\u001b[33m\u0026#34; ColorBlue = \u0026#34;\\u001b[34m\u0026#34; ColorReset = \u0026#34;\\u001b[0m\u0026#34; ) func colorize(color Color, message string) { fmt.Println(string(color), message, string(ColorReset)) } func main() { useColor := flag.Bool(\u0026#34;color\u0026#34;, false, \u0026#34;display colorized output\u0026#34;) // 定义Go应用程序将使用的标志  flag.Parse() // 解析标志  if *useColor { colorize(ColorBlue, \u0026#34;Hello, DigitalOcean!\u0026#34;) return } fmt.Println(\u0026#34;Hello, DigitalOcean!\u0026#34;) } 在main函数中，使用flag.Bool定义了一个布尔型的标志color，其默认值为第二个参数false，在未提供color参数时将默认使用此值。最后一个参数是关于此标志用法的说明文档。\nflag.Bool返回值是一个bool指针类型，flag.Parse使用此bool指针根据用户传递的标志来设置变量。然后我们就可以通过引用这个指针来检查这个变量，通过变量的值控制程序的行为。\n2. 处理位置参数 通常，命令带有一定的参数，并且这些参数是命令工作的焦点所在，例如：python main.py,这里的main.py就是一个位置参数，是python这个命令的第一个参数。\n假如我们有个命令head，它打印一个文件的开始几行，用法为：head {inputfile}。\n通常，Parse()函数解析一个命令直到其检查到没有标志参数的存在。flag包通过Args和Arg函数来解决位置参数的问题。\n// head.go import ( \u0026#34;bufio\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; ) func main() { var count int flag.IntVar(\u0026amp;count, \u0026#34;n\u0026#34;, 5, \u0026#34;number of lines to read from the file\u0026#34;) flag.Parse() var in io.Reader if filename := flag.Arg(0); filename != \u0026#34;\u0026#34; { f, err := os.Open(filename) if err != nil { fmt.Println(\u0026#34;error opening file: err:\u0026#34;, err) os.Exit(1) } defer f.Close() in = f } else { in = os.Stdin } buf := bufio.NewScanner(in) for i := 0; i \u0026lt; count; i++ { if !buf.Scan() { break } fmt.Println(buf.Text()) } if err := buf.Err(); err != nil { fmt.Fprintln(os.Stderr, \u0026#34;error reading: err:\u0026#34;, err) } } 首先，我们定义了一个count变量来标记程序需要读取文件的前几行。通过flag.IntVar定义了-n参数。这个函数允许我们传递一个指针作为标记而不是像没有Var后缀的函数那样返回一个指针。flag.IntVar和flag.Int之间除了这一点不同外，其余参数的意义都相同。在使用flag.Parse解析后，接下来是根据参数来控制程序逻辑。\n在if部分，使用flag.Arg来访问在所有标志参数之后的第一个位置参数。\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/flag%E5%8C%85%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/","summary":"Flag包的基本用法  flag包用于处理golang命令行程序中的参数\n 1. 使用flag包的基本流程 使用flag包涉及三个步骤：\n 定义变量以捕获标志值 定义Go应用程序将使用的标志 在执行时解析提供给应用程序的标志。  flag软件包中的大多数功能都与定义标志并将其绑定到定义的变量有关。解析阶段由Parse()函数处理。\n一个例子 创建一个程序，该程序定义一个布尔标志，该标志会更改将打印到标准输出的消息。如果-color提供了一个标志，程序将以蓝色打印一条消息。如果未提供标志，则消息将被打印为没有任何颜色。\n// boolean.go import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; ) type Color string // 定义变量以捕获标志值  const ( ColorBlack Color = \u0026#34;\\u001b[30m\u0026#34; ColorRed = \u0026#34;\\u001b[31m\u0026#34; ColorGreen = \u0026#34;\\u001b[32m\u0026#34; ColorYellow = \u0026#34;\\u001b[33m\u0026#34; ColorBlue = \u0026#34;\\u001b[34m\u0026#34; ColorReset = \u0026#34;\\u001b[0m\u0026#34; ) func colorize(color Color, message string) { fmt.Println(string(color), message, string(ColorReset)) } func main() { useColor := flag.Bool(\u0026#34;color\u0026#34;, false, \u0026#34;display colorized output\u0026#34;) // 定义Go应用程序将使用的标志  flag.","title":"Flag包的基本用法"},{"content":"Github API问题 使用github的RESTful API 访问https://developer.github.com/v3/来查看帮助文档\nAPI访问限制 在云服务器上使用git的API时，发现出现message\u0026quot;:\u0026quot;API rate limit exceeded for 59.110.140.133. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\u0026quot;,\u0026quot;documentation_url\u0026quot;:\u0026quot;https://developer.github.com/v3/#rate-limiting提示信息，显然，这是存在着访问限制。\n查看访问限制 使用curl -i https://api.github.com/rate_limit查看自己的限制信息\nHTTP/1.1 200 OK Content-Type: application/json X-Ratelimit-Limit: 60 X-Ratelimit-Remaining: 59 X-Ratelimit-Reset: 1585470905 Date: Sun, 29 Mar 2020 07:49:35 GMT Content-Length: 482 Accept-Ranges: bytes X-GitHub-Request-Id: BB32:30AB:3EF690:50F336:5E80530E { \u0026#34;resources\u0026#34;: { \u0026#34;core\u0026#34;: { \u0026#34;limit\u0026#34;: 60, \u0026#34;remaining\u0026#34;: 59, \u0026#34;reset\u0026#34;: 1585470905 }, \u0026#34;graphql\u0026#34;: { \u0026#34;limit\u0026#34;: 0, \u0026#34;remaining\u0026#34;: 0, \u0026#34;reset\u0026#34;: 1585471775 }, \u0026#34;integration_manifest\u0026#34;: { \u0026#34;limit\u0026#34;: 5000, \u0026#34;remaining\u0026#34;: 5000, \u0026#34;reset\u0026#34;: 1585471775 }, \u0026#34;search\u0026#34;: { \u0026#34;limit\u0026#34;: 10, \u0026#34;remaining\u0026#34;: 10, \u0026#34;reset\u0026#34;: 1585468235 } }, \u0026#34;rate\u0026#34;: { \u0026#34;limit\u0026#34;: 60, \u0026#34;remaining\u0026#34;: 59, \u0026#34;reset\u0026#34;: 1585470905 } } rate.limit 为60次，访问60次后就被限制，很显然不太够用\n提高访问限制 通过在github注册应用程序，可提高到每小时5000次访问，一般应用程序足够了 在https://github.com/settings/applications/new注册应用程序\n就可以得到client_id和client_secret\n使用curl -u my_client_id:my_client_secret 'https://api.github.com/user/repos'进行身份验证，然后即可\n","permalink":"http://yangchnet.github.io/Dessert/posts/git/api%E9%99%90%E5%88%B6%E9%97%AE%E9%A2%98/","summary":"Github API问题 使用github的RESTful API 访问https://developer.github.com/v3/来查看帮助文档\nAPI访问限制 在云服务器上使用git的API时，发现出现message\u0026quot;:\u0026quot;API rate limit exceeded for 59.110.140.133. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\u0026quot;,\u0026quot;documentation_url\u0026quot;:\u0026quot;https://developer.github.com/v3/#rate-limiting提示信息，显然，这是存在着访问限制。\n查看访问限制 使用curl -i https://api.github.com/rate_limit查看自己的限制信息\nHTTP/1.1 200 OK Content-Type: application/json X-Ratelimit-Limit: 60 X-Ratelimit-Remaining: 59 X-Ratelimit-Reset: 1585470905 Date: Sun, 29 Mar 2020 07:49:35 GMT Content-Length: 482 Accept-Ranges: bytes X-GitHub-Request-Id: BB32:30AB:3EF690:50F336:5E80530E { \u0026#34;resources\u0026#34;: { \u0026#34;core\u0026#34;: { \u0026#34;limit\u0026#34;: 60, \u0026#34;remaining\u0026#34;: 59, \u0026#34;reset\u0026#34;: 1585470905 }, \u0026#34;graphql\u0026#34;: { \u0026#34;limit\u0026#34;: 0, \u0026#34;remaining\u0026#34;: 0, \u0026#34;reset\u0026#34;: 1585471775 }, \u0026#34;integration_manifest\u0026#34;: { \u0026#34;limit\u0026#34;: 5000, \u0026#34;remaining\u0026#34;: 5000, \u0026#34;reset\u0026#34;: 1585471775 }, \u0026#34;search\u0026#34;: { \u0026#34;limit\u0026#34;: 10, \u0026#34;remaining\u0026#34;: 10, \u0026#34;reset\u0026#34;: 1585468235 } }, \u0026#34;rate\u0026#34;: { \u0026#34;limit\u0026#34;: 60, \u0026#34;remaining\u0026#34;: 59, \u0026#34;reset\u0026#34;: 1585470905 } } rate.","title":"GithubAPI问题"},{"content":" 直接干终极方案: go env -w GOPROXY=https://goproxy.cn,direct\n 修改hosts，然后reboot\n添加\n192.30.253.112 github.com 151.101.185.194 github.global.ssl.fastly.net 到/etc/hosts 然后reboot\ngo get golang.org 在使用go get golang.org/...时，总是time out（就算fp也一样，fp之后可以访问golang.org），不知道为啥。\n幸好github上存在golang.org的镜像 例如\ngo get -u golang.org/x/net 那么这个包的位置在github上就是github.com/golang/net, 所以，我们可以手动建立golang.org/x/目录，并切换到该目录下，然后使用\ngit clone https://github.com/golang/net.git **注意：**要使用git clone命令，直接下载下来复制到目录下会提示找不到版本号。\n终极方案 go env -w GOPROXY=https://goproxy.cn,direct ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/go-get%E4%BB%A3%E7%90%86%E6%96%B9%E6%A1%88/","summary":" 直接干终极方案: go env -w GOPROXY=https://goproxy.cn,direct\n 修改hosts，然后reboot\n添加\n192.30.253.112 github.com 151.101.185.194 github.global.ssl.fastly.net 到/etc/hosts 然后reboot\ngo get golang.org 在使用go get golang.org/...时，总是time out（就算fp也一样，fp之后可以访问golang.org），不知道为啥。\n幸好github上存在golang.org的镜像 例如\ngo get -u golang.org/x/net 那么这个包的位置在github上就是github.com/golang/net, 所以，我们可以手动建立golang.org/x/目录，并切换到该目录下，然后使用\ngit clone https://github.com/golang/net.git **注意：**要使用git clone命令，直接下载下来复制到目录下会提示找不到版本号。\n终极方案 go env -w GOPROXY=https://goproxy.cn,direct ","title":"go get代理方案"},{"content":"golang中的print系函数详解 pirnt系函数来自fmt包，主要用于做各种格式的输出 这些函数主要有\n golang中的print系函数详解  fmt.Fprintf fmt.Printf fmt.Sprintf fmt.Fprint fmt.Print fmt.Sprint fmt.Fprintln fmt.Println fmt.Sprintln 总结    下面来逐个分析\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;io\u0026#34; ) fmt.Fprintf  函数原型：  Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error)   官方注释 Fprintf formats according to a format specifier and writes to w.It returns the number of bytes written and any write error encountered.\n  Arguement fmt.Fprintf() 依据指定的格式向第一个参数内写入字符串，第一参数必须实现了 io.Writer 接口。Fprintf() 能够写入任何类型，只要其实现了 Write 方法，包括 os.Stdout,文件（例如 os.File），管道，网络连接，通道等等，同样的也可以使用 bufio 包中缓冲写入。bufio 包中定义了 type Writer struct{...}。Fprintf：来格式化并输出到 io.Writers 而不是 os.Stdout。\n  // example func ExampleFprintf() { const name, age = \u0026#34;Kim\u0026#34;, 22 n, err := fmt.Fprintf(os.Stdout, \u0026#34;%s is %d years old.\\n\u0026#34;, name, age) // The n and err return values from Fprintf are \t// those returned by the underlying io.Writer. \tif err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Fprintf: %v\\n\u0026#34;, err) } fmt.Printf(\u0026#34;%d bytes written.\\n\u0026#34;, n) // Output: \t// Kim is 22 years old. \t// 21 bytes written. } ExampleFprintf() Kim is 22 years old. 21 bytes written.  fmt.Printf  函数原型  Printf(format string, a ...interface{}) (n int, err error)   官方注释 Printf formats according to a format specifier and writes to standard output.It returns the number of bytes written and any write error encountered.\n  Arguement 只可以打印出格式化的字符串。只可以直接输出字符串类型的变量（不可以输出整形变量和整形 等），其实这个函数是fmt.Fprintf的调用，其调用语句return Fprintf(os.Stdout, format, a...)，输出对象为标准输出。\n  optional\n     optional description     %d 十进制整数   %x, %o, %b 十六进制，八进制，二进制整数   %f, %g, %e 浮点数，3.141593 3.141592653589793 3.141593e+00   %t 布尔 ture或false   %c 字符（rune）（Uniode码点）   %s 字符串   %q 带双引号的字符串\u0026quot;abc\u0026quot;或带单引号的字符\u0026rsquo;c'   %v 变量的自然形式   %T 变量的类型   %% 字面上的百分号标志（无操作数）    fmt.Printf(\u0026#34;整数：%d\\t浮点数：%v\\t字符串：%s\\n\u0026#34;, 1, 3.14, \u0026#34;hello world\u0026#34;) func ExamplePrintf() { const name, age = \u0026#34;Kim\u0026#34;, 22 fmt.Printf(\u0026#34;%s is %d years old.\\n\u0026#34;, name, age) // It is conventional not to worry about any \t// error returned by Printf.  // Output: \t// Kim is 22 years old. } ExamplePrintf() 整数：1\t浮点数：3.14\t字符串：hello world Kim is 22 years old.  fmt.Sprintf  函数原型  func Sprintf(format string, a ...interface{}) string   官方注释 Sprintf formats according to a format specifier and returns the resulting string.\n  Argument 格式化并返回一个字符串而不带任何输出。可用于给其他函数传递格式化参数。\n  func ExampleSprintf() { const name, age = \u0026#34;Kim\u0026#34;, 22 s := fmt.Sprintf(\u0026#34;%s is %d years old.\\n\u0026#34;, name, age) io.WriteString(os.Stdout, s) // Ignoring error for simplicity.  // Output: \t// Kim is 22 years old. } ExampleSprintf() Kim is 22 years old.  fmt.Fprint  函数原型  Fprint(w io.Writer, a ...interface{}) (n int, err error)   官方注释 Fprint formats using the default formats for its operands and writes to w.Spaces are added between operands when neither is a string.It returns the number of bytes written and any write error encountered.\n  Argument 向io.Writer对象中输入字符，第一个参数为输入对象，后面的参数为字符串，直接输出\n  func ExampleFprint() { const name, age = \u0026#34;Kim\u0026#34;, 22 n, err := fmt.Fprint(os.Stdout, name, \u0026#34; is \u0026#34;, age, \u0026#34; years old.\\n\u0026#34;) // The n and err return values from Fprint are \t// those returned by the underlying io.Writer. \tif err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Fprint: %v\\n\u0026#34;, err) } fmt.Print(n, \u0026#34; bytes written.\\n\u0026#34;) // Output: \t// Kim is 22 years old. \t// 21 bytes written. } ExampleFprint() Kim is 22 years old. 21 bytes written.  fmt.Print  函数原型  func Print(a ...interface{}) (n int, err error) { return Fprint(os.Stdout, a...) }   官方注释 Print formats using the default formats for its operands and writes to standard output.Spaces are added between operands when neither is a string.It returns the number of bytes written and any write error encountered.\n  Arguement 显然，这个函数是对fmt.Fprint的调用，只是把输入对象默认为标准输出。\n  func ExamplePrint() { const name, age = \u0026#34;Kim\u0026#34;, 22 fmt.Print(name, \u0026#34; is \u0026#34;, age, \u0026#34; years old.\\n\u0026#34;) // It is conventional not to worry about any \t// error returned by Print.  // Output: \t// Kim is 22 years old. } ExamplePrint() Kim is 22 years old.  fmt.Sprint  函数原型  Sprint(a ...interface{}) string   官方注释 Sprint formats using the default formats for its operands and returns the resulting string.Spaces are added between operands when neither is a string.\n  Arguement 格式化并返回一个字符串而不带任何输出\n  func ExampleSprint() { const name, age = \u0026#34;Kim\u0026#34;, 22 s := fmt.Sprint(name, \u0026#34; is \u0026#34;, age, \u0026#34; years old.\\n\u0026#34;) io.WriteString(os.Stdout, s) // Ignoring error for simplicity.  // Output: \t// Kim is 22 years old. } ExampleSprint() Kim is 22 years old.  fmt.Fprintln  函数原型  Fprintln(w io.Writer, a ...interface{}) (n int, err error)   官方注释 Fprintln formats using the default formats for its operands and writes to w.Spaces are always added between operands and a newline is appended.It returns the number of bytes written and any write error encountered.\n  Arguement 输入字符串到io.Writer,并且带有换行\n  func ExampleFprintln() { const name, age = \u0026#34;Kim\u0026#34;, 22 n, err := fmt.Fprintln(os.Stdout, name, \u0026#34;is\u0026#34;, age, \u0026#34;years old.\u0026#34;) // The n and err return values from Fprintln are \t// those returned by the underlying io.Writer. \tif err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Fprintln: %v\\n\u0026#34;, err) } fmt.Println(n, \u0026#34;bytes written.\u0026#34;) // Output: \t// Kim is 22 years old. \t// 21 bytes written. } ExampleFprintln() Kim is 22 years old. 21 bytes written.  fmt.Println  函数原型  func Println(a ...interface{}) (n int, err error) { return Fprintln(os.Stdout, a...) }   官方注释 Println formats using the default formats for its operands and writes to standard output.Spaces are always added between operands and a newline is appended.It returns the number of bytes written and any write error encountered.\n  Arguement 显然这个函数是对fmt.Fprintln的调用，输入到os.Stdout\n  func ExamplePrintln() { const name, age = \u0026#34;Kim\u0026#34;, 22 fmt.Println(name, \u0026#34;is\u0026#34;, age, \u0026#34;years old.\u0026#34;) // It is conventional not to worry about any \t// error returned by Println.  // Output: \t// Kim is 22 years old. } ExamplePrintln() Kim is 22 years old.  fmt.Sprintln  函数原型  Sprintln(a ...interface{}) string   官方注释 Sprintln formats using the default formats for its operands and returns the resulting string.Spaces are always added between operands and a newline is appended.\n  Arguement 返回一个字符串，不做任何输出\n  func ExampleSprintln() { const name, age = \u0026#34;Kim\u0026#34;, 22 s := fmt.Sprintln(name, \u0026#34;is\u0026#34;, age, \u0026#34;years old.\u0026#34;) io.WriteString(os.Stdout, s) // Ignoring error for simplicity.  // Output: \t// Kim is 22 years old. } ExampleSprintln() Kim is 22 years old.  总结 所有的函数名都是_print_格式的，其中_代表某个标志，标志及其含义如下：\n   标志 含义     _f 格式化   _ln 换行   F_ 输入到某个对象   S_ 只返回而不做输出    且， fmt.Fprintf和fmt.Printf是一对\nfmt.Fprint和fmt.Print是一对\nfmt.Fprintln和fmt.Println是一对\n不带F前缀的函数均是对带F前缀函数的调用，其输入对象均设置为标准输出\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/golang%E4%B9%8Bprint/","summary":"golang中的print系函数详解 pirnt系函数来自fmt包，主要用于做各种格式的输出 这些函数主要有\n golang中的print系函数详解  fmt.Fprintf fmt.Printf fmt.Sprintf fmt.Fprint fmt.Print fmt.Sprint fmt.Fprintln fmt.Println fmt.Sprintln 总结    下面来逐个分析\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;io\u0026#34; ) fmt.Fprintf  函数原型：  Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error)   官方注释 Fprintf formats according to a format specifier and writes to w.It returns the number of bytes written and any write error encountered.\n  Arguement fmt.Fprintf() 依据指定的格式向第一个参数内写入字符串，第一参数必须实现了 io.","title":"golang中的print系函数详解"},{"content":"Go的http包详解 详细地解剖一下 http 包，看它到底是怎样实现整个过程的。\nGo 的 http 有两个核心功能：Conn、ServeMux\nConn的goroputine 为了实现高并发和高性能，go使用了goroutine来处理Conn的读写事件，这样每个请求都能保持独立，相互不会阻塞，可以高效的相应网络事件。\ngo在等待客户端请求中是这样的：\nc, err := srv.newConn(rw) if err != nil { continue } go c.serve() 可以看到，客户端的每次请求都会创建一个Conn，这个Conn里面保存了该次请求的信息，然后再传递到相应的handler，该handler中便可以读取到相应的header信息，这样保证了每个请求的独立性。\nServeMux的自定义 conn.server内部调用了http包默认的路由器，通过路由器把本次请求的信息传递到了后端的处理函数，那么这个路由器是怎么实现的呢？\n它的结构如下：\ntype ServeMux struct{ mu sync.RWMutext // 锁，请求涉及到并发处理，因此需要一个锁机制  m map[string]muxEntry // 路由规则，一个String对应一个mux实体，这里的String就是注册的一个路由表达式  hosts bool // 是否在任意的规则中带有host信息 } 下面看一下muxEntry\ntype muxEntry struct { explicit bool // 是否精确匹配  h Handler // 这个路由表达式对应哪个handler  pattern string // 匹配字符串 } 在看一下Handler的定义\ntype Handler interface { ServeHTTP(ResponseWriter, *Request) // 路由实现器 } Handler是一个接口，但是附中的sayhelloName函数中并没有实现ServeHTTP这个接口，为什么能添加呢？这是因为http包里面还定义了一个类型HandlerFunc，定义的函数sayhelloName就是这个HandlerFunc调用之后的结果，这个类型默认就实现了ServeHTTP这个方法，即我们调用了HandlerFunc(f)，强制类型转换f成为HandlerFunc类型，这样f就拥有了ServeHTTP方法。\ntype HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r) func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request){ f (w, r) }  路由器里面存储好了对应的路由规则之后，那么具体的请求又是怎么分发的呢？下面的代码，默认的路由器实现了ServeHTTP\nfunc (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURL == \u0026#34;*\u0026#34;{ w.Header().Set(\u0026#34;connection\u0026#34;, \u0026#34;clost\u0026#34;) w.WriteHeader(StatusBadReqest) return } h, _ := mux.Handler(r) h.ServeHTTP(w, r) } 路由器接收到请求后，如果是*那么关闭链接，否则调用mux.Handler(r)返回对应设置路由的处理Handler，然后执行h.ServeHTTP(w, r)，也就是调用对应路由的handler的ServeHTTP接口，那么mux.Handler(r)怎么处理的呢？\nfunc (mux *ServeMux) Handler(r *Request)(h Handler, pattern string){ if r.Method != \u0026#34;CONNECT\u0026#34; { if p := cleanPath(r.URL.Path); p != r.URL.Path{ _, pattern = mux.handler(r.Host, p) return RedirectHandler(p, StatusMovedPermanently), pattern } } return mux.handler(r.Host, r.URL.Path) } func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) { mux.mu.RLock() defer mux.mu.RUnlock() // Host-specific pattern takes precedence over generic ones  if mux.hosts { h, pattern = mux.match(host + path) } if h == nil { h, pattern = mux.match(path) } if h == nil { h, pattern = NotFoundHandler(), \u0026#34;\u0026#34; } return } 根据用户请求的URL和路由器里面存储的map来匹配，当匹配到之后返回存储的handler，调用这个handler的ServeHTTP接口就可以执行到相应的函数。\nGo 其实支持外部实现的路由器 ListenAndServe 的第二个参数就是用以配置外部路由器的，它是一个 Handler 接口，即外部路由器只要实现了 Handler 接口就可以，我们可以在自己实现的路由器的 ServeHTTP 里面实现自定义路由功能。 如下代码所示，我们自己实现了一个简易的路由器\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) type MyMux struct { } func (p *MyMux) ServeHTTP(w http.ResponseWriter, r *http.Request) { if r.URL.Path == \u0026#34;/\u0026#34; { sayhelloName(w, r) return } http.NotFound(w, r) return } func sayhelloName(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello myroute!\u0026#34;) } func main() { mux := \u0026amp;MyMux{} http.ListenAndServe(\u0026#34;:9090\u0026#34;, mux) } GO代码的执行流程 通过对http包的分析之后，现在让我们来梳理一下整个的代码执行过程。\n 首先调用Http.HandleFunc 按顺序做了几件事  调用了DefaultServeMux 的 HandleFunc 调用了 DefaultServeMux 的 Handle 往 DefaultServeMux 的 map [string] muxEntry 中增加对应的 handler 和路由规则   其次调用 http.ListenAndServe (\u0026quot;:9090\u0026quot;, nil)\n按顺序做了几件事  实例化 Server 调用 Server 的 ListenAndServe () 调用 net.Listen (\u0026ldquo;tcp\u0026rdquo;, addr) 监听端口 启动一个 for 循环，在循环体中 Accept 请求 对每个请求实例化一个 Conn，并且开启一个 goroutine 为这个请求进行服务 go c.serve () 读取每个请求的内容 w, err := c.readRequest () 判断 handler 是否为空，如果没有设置 handler（这个例子就没有设置 handler），handler 就设置为 DefaultServeMux 调用 handler 的 ServeHttp 在这个例子中，下面就进入到 DefaultServeMux.ServeHttp 根据 request 选择 handler，并且进入到这个 handler 的 ServeHTTP  mux.handler(r).ServeHTTP(w, r) 选择 handler：  判断是否有路由能满足这个 request（循环遍历 ServeMux 的 muxEntry） B 如果有路由满足，调用这个路由 handler 的 ServeHTTP 如果没有路由满足，调用 NotFoundHandler 的 ServeHTTP       附：\nfunc sayHelloName(w http.ResponseWriter, r *http.Request){ r.ParseForm() fmt.Println(r.Form) fmt.Println(\u0026#34;path\u0026#34;, r.URL.Path) fmt.Println(\u0026#34;scheme\u0026#34;, r.URL.Scheme) fmt.Println(r.Form[\u0026#34;url_long\u0026#34;]) for k, v := range r.Form{ fmt.Println(\u0026#34;key: \u0026#34;, k) fmt.Println(\u0026#34;val: \u0026#34;, strings.Join(v, \u0026#34; \u0026#34;)) } fmt.Println(w, \u0026#34;hello world\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, sayHelloName) err := http.ListenAndServe(\u0026#34;:9090\u0026#34;, nil) if err != nil { log.Fatal(\u0026#34;listenandserver: \u0026#34;, err) } } ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/go%E7%9A%84http%E5%8C%85%E8%AF%A6%E8%A7%A3/","summary":"Go的http包详解 详细地解剖一下 http 包，看它到底是怎样实现整个过程的。\nGo 的 http 有两个核心功能：Conn、ServeMux\nConn的goroputine 为了实现高并发和高性能，go使用了goroutine来处理Conn的读写事件，这样每个请求都能保持独立，相互不会阻塞，可以高效的相应网络事件。\ngo在等待客户端请求中是这样的：\nc, err := srv.newConn(rw) if err != nil { continue } go c.serve() 可以看到，客户端的每次请求都会创建一个Conn，这个Conn里面保存了该次请求的信息，然后再传递到相应的handler，该handler中便可以读取到相应的header信息，这样保证了每个请求的独立性。\nServeMux的自定义 conn.server内部调用了http包默认的路由器，通过路由器把本次请求的信息传递到了后端的处理函数，那么这个路由器是怎么实现的呢？\n它的结构如下：\ntype ServeMux struct{ mu sync.RWMutext // 锁，请求涉及到并发处理，因此需要一个锁机制  m map[string]muxEntry // 路由规则，一个String对应一个mux实体，这里的String就是注册的一个路由表达式  hosts bool // 是否在任意的规则中带有host信息 } 下面看一下muxEntry\ntype muxEntry struct { explicit bool // 是否精确匹配  h Handler // 这个路由表达式对应哪个handler  pattern string // 匹配字符串 } 在看一下Handler的定义\ntype Handler interface { ServeHTTP(ResponseWriter, *Request) // 路由实现器 } Handler是一个接口，但是附中的sayhelloName函数中并没有实现ServeHTTP这个接口，为什么能添加呢？这是因为http包里面还定义了一个类型HandlerFunc，定义的函数sayhelloName就是这个HandlerFunc调用之后的结果，这个类型默认就实现了ServeHTTP这个方法，即我们调用了HandlerFunc(f)，强制类型转换f成为HandlerFunc类型，这样f就拥有了ServeHTTP方法。","title":"Go的http包详解"},{"content":"1.值类型与引用类型 值类型：int、float、bool和string这些类型都属于值类型，使用这些类型的变量直接指向存在内存中的值，值类型的变量的值存储在栈中。当使用等号=将一个变量的值赋给另一个变量时，如 j = i ,实际上是在内存中将 i 的值进行了拷贝。可以通过 \u0026amp;i 获取变量 i 的内存地址\n引用类型：特指slice、map、channel这三种预定义类型。引用类型拥有更复杂的存储结构:(1)分配内存 (2)初始化一系列属性等。一个引用类型的变量r1存储的是r1的值所在的内存地址（数字），或内存地址中第一个字所在的位置，这个内存地址被称之为指针，这个指针实际上也被存在另外的某一个字中\n2.值类型与引用类型的区别 首先明确，golang中无论是什么类型，传参时都是传递的原值的复制，因此，值类型直接传入函数，任何变动都会反映到原值上，而对于引用类型，其传递的是原值类型的一个复制，因此在函数内的修改，可能会反映到原值中，也可能不会。具体取决引用类型的构造方式及其内部定义。可参考另一篇文章slice和数组的区别\n2.1.值类型 //先定义一个数组 var a = [5]int{1, 2, 3, 4, 5} //定义一个函数，将数组中的第一个值设为0 func change(a [5]int){ a[0] = 0 fmt.Println(a) } change(a) fmt.Println(a)  输出：\n [0 2 3 4 5] [1 2 3 4 5] 可以看到，数组在函数内部被变成{0,1,2,3,4}，但当函数结束，还是原来的值没有变。\n2.2 引用类型  map的构造函数返回的是一个指针，指向map对象，因此对于map的任何操作都会反映到原map中\n // 定义一个map var dit = make(map[string]int) dit[\u0026#34;one\u0026#34;] = 1 fmt.Println(dit) // 传参并做改变 func change(dit map[string]int){ dit[\u0026#34;two\u0026#34;] = 2 fmt.Println(dit) } change(dit) //输出原来的map fmt.Println(dit)  输出\n map[one:1] map[one:1 two:2] map[one:1 two:2] 将map作为参数传入函数，当在函数内部对参数进行修改时，原数据也随之变化。\n 引用类型包括slice,map,channel.\n内置函数new计算类型大小,为其分配零值内存,返回指针.而make会被编译器翻译成具体的创建函数,由其分配内存和初始化成员结构,返回对象而非指针.\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E5%80%BC%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B/","summary":"1.值类型与引用类型 值类型：int、float、bool和string这些类型都属于值类型，使用这些类型的变量直接指向存在内存中的值，值类型的变量的值存储在栈中。当使用等号=将一个变量的值赋给另一个变量时，如 j = i ,实际上是在内存中将 i 的值进行了拷贝。可以通过 \u0026amp;i 获取变量 i 的内存地址\n引用类型：特指slice、map、channel这三种预定义类型。引用类型拥有更复杂的存储结构:(1)分配内存 (2)初始化一系列属性等。一个引用类型的变量r1存储的是r1的值所在的内存地址（数字），或内存地址中第一个字所在的位置，这个内存地址被称之为指针，这个指针实际上也被存在另外的某一个字中\n2.值类型与引用类型的区别 首先明确，golang中无论是什么类型，传参时都是传递的原值的复制，因此，值类型直接传入函数，任何变动都会反映到原值上，而对于引用类型，其传递的是原值类型的一个复制，因此在函数内的修改，可能会反映到原值中，也可能不会。具体取决引用类型的构造方式及其内部定义。可参考另一篇文章slice和数组的区别\n2.1.值类型 //先定义一个数组 var a = [5]int{1, 2, 3, 4, 5} //定义一个函数，将数组中的第一个值设为0 func change(a [5]int){ a[0] = 0 fmt.Println(a) } change(a) fmt.Println(a)  输出：\n [0 2 3 4 5] [1 2 3 4 5] 可以看到，数组在函数内部被变成{0,1,2,3,4}，但当函数结束，还是原来的值没有变。\n2.2 引用类型  map的构造函数返回的是一个指针，指向map对象，因此对于map的任何操作都会反映到原map中\n // 定义一个map var dit = make(map[string]int) dit[\u0026#34;one\u0026#34;] = 1 fmt.Println(dit) // 传参并做改变 func change(dit map[string]int){ dit[\u0026#34;two\u0026#34;] = 2 fmt.","title":"Go语言中值类型与引用类型"},{"content":"Go语言中的字面量  Go语言中的字面量  什么是字面量 整型和浮点型的字面值 字符串的字面值 常量的字面值 数组的字面值 Slice的字面值 Map的字面值 结构体的字面值    什么是字面量 在计算机科学中，字面量（literal）是用于表达源代码中一个固定值的表示法（notation）。\n简单的说，字面量或者说字面值就是一个变量的值。\n整型和浮点型的字面值 var i int = 1 var f float64 = 3.14159 字符串的字面值 字符串值也可以用字符串面值方式编写，只要将一系列字节序列包含在双引号即可：\n\u0026#34;Hello 世界\u0026#34; `世界`  世界 一个原生的字符串面值形式是\n`...` 使用反引号代替双引号。在原生的字符串面值中，没有转义操作；全部的内容都是字面的意思，包含退格和换行，因此一个程序中的原生字符串面值可能跨越多行（译注：在原生字符串面值内部是无法直接写```````字符的，可以用八进制或十六进制转义或+\u0026quot;`\u0026quot;链接字符串常量完成）。唯一的特殊处理是会删除回车以保证在所有平台上的值都是一样的，包括那些把回车也放入文本文件的系统（译注：Windows系统会把回车和换行一起放入文本文件中）。\n原生字符串面值用于编写正则表达式会很方便，因为正则表达式往往会包含很多反斜杠。原生字符串面值同时被广泛应用于HTML模板、JSON面值、命令行提示信息以及那些需要扩展到多行的场景。\nconst GoUsage = `Go is a tool for managing Go source code. Usage: go command [arguments] ...` GoUsage  Go is a tool for managing Go source code. Usage: go command [arguments] ... 常量的字面值 对于常量面值，不同的写法可能会对应不同的类型。例如0、0.0、0i和\\u0000虽然有着相同的常量值，但是它们分别对应无类型的整数、无类型的浮点数、无类型的复数和无类型的字符等不同的常量类型。同样，true和false也是无类型的布尔类型，字符串面值常量是无类型的字符串类型。\n数组的字面值 使用数组字面值语法用一组值来初始化数组：\nvar q [3]int = [3]int{1, 2, 3} var r [3]int = [3]int{1, 2} r[2]  0 在数组字面值中，如果在数组的长度位置出现的是“\u0026hellip;”省略号，则表示数组的长度是根据初始化值的个数来计算。因此，上面q数组的定义可以简化为\nimport \u0026#34;fmt\u0026#34; q := [...]int{1, 2, 3} fmt.Printf(\u0026#34;%T\\n\u0026#34;, q) // \u0026#34;[3]int\u0026#34;  [3]int 7 \u0026lt;nil\u0026gt; 数组、slice、map和结构体字面值的写法都很相似。上面的形式是直接提供顺序初始化值序列，但是也可以指定一个索引和对应值列表的方式初始化，就像下面这样：\ntype Currency int const ( USD Currency = iota // 美元  EUR // 欧元  GBP // 英镑  RMB // 人民币 ) //这个看起来和字典很相似，但其实USD等标识是index下标 symbol := [...]string{USD: \u0026#34;$\u0026#34;, EUR: \u0026#34;€\u0026#34;, GBP: \u0026#34;￡\u0026#34;, RMB: \u0026#34;￥\u0026#34;} fmt.Println(RMB, symbol[RMB]) // \u0026#34;3 ￥\u0026#34; symbol[EUR] == symbol[1]  3 ￥ true Slice的字面值 slice和数组的字面值语法很类似，它们都是用花括弧包含一系列的初始化元素，但是对于slice并没有指明序列的长度。这会隐式地创建一个合适大小的数组，然后slice的指针指向底层的数组。就像数组字面值一样，slice的字面值也可以按顺序指定初始化值序列，或者是通过索引和元素值指定，或者两种风格的混合语法初始化。\ns := []int{0, 1, 2, 3, 4, 5} s  [0 1 2 3 4 5] a := []string{\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, 4:\u0026#34;three\u0026#34;} a[4]  three Map的字面值 用map字面值的语法创建map，同时还可以指定一些最初的key/value：\nages := map[string]int{ \u0026#34;alice\u0026#34;: 31, \u0026#34;charlie\u0026#34;: 34, } 这相当于\nages := make(map[string]int) ages[\u0026#34;alice\u0026#34;] = 31 ages[\u0026#34;charlie\u0026#34;] = 34 结构体的字面值 第一种写法\ntype Point struct{ X, Y int } p := Point{1, 2} 更常用的是第二种写法，以成员名字和相应的值来初始化，可以包含部分或全部的成员\nanim := gif.GIF{LoopCount: nframes}  repl.go:1:9: undefined \u0026quot;gif\u0026quot; in gif.GIF \u0026lt;*ast.SelectorExpr\u0026gt; 两种写法不能混用\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/go%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84%E5%AD%97%E9%9D%A2%E9%87%8F/","summary":"Go语言中的字面量  Go语言中的字面量  什么是字面量 整型和浮点型的字面值 字符串的字面值 常量的字面值 数组的字面值 Slice的字面值 Map的字面值 结构体的字面值    什么是字面量 在计算机科学中，字面量（literal）是用于表达源代码中一个固定值的表示法（notation）。\n简单的说，字面量或者说字面值就是一个变量的值。\n整型和浮点型的字面值 var i int = 1 var f float64 = 3.14159 字符串的字面值 字符串值也可以用字符串面值方式编写，只要将一系列字节序列包含在双引号即可：\n\u0026#34;Hello 世界\u0026#34; `世界`  世界 一个原生的字符串面值形式是\n`...` 使用反引号代替双引号。在原生的字符串面值中，没有转义操作；全部的内容都是字面的意思，包含退格和换行，因此一个程序中的原生字符串面值可能跨越多行（译注：在原生字符串面值内部是无法直接写```````字符的，可以用八进制或十六进制转义或+\u0026quot;`\u0026quot;链接字符串常量完成）。唯一的特殊处理是会删除回车以保证在所有平台上的值都是一样的，包括那些把回车也放入文本文件的系统（译注：Windows系统会把回车和换行一起放入文本文件中）。\n原生字符串面值用于编写正则表达式会很方便，因为正则表达式往往会包含很多反斜杠。原生字符串面值同时被广泛应用于HTML模板、JSON面值、命令行提示信息以及那些需要扩展到多行的场景。\nconst GoUsage = `Go is a tool for managing Go source code. Usage: go command [arguments] ...` GoUsage  Go is a tool for managing Go source code. Usage: go command [arguments] .","title":"Go语言中的字面量"},{"content":"0. 错误处理的编码风格 检查某个子函数是否失败后，我们通常将处理失败的逻辑代码放在处理成功的代码之前。如果某个错误会导致函数返回，那么成功的逻辑代码不应该放在else中，而应直接放在函数体中。\n1. 错误传播 函数某个子程序的失败，会变成该函数的失败\nresp, err := http.Get(url) if err != nil{ return nill, err } 或是构造新的错误信息返回给调用者\ndoc, err := html.Parse(resp.Body) resp.Body.Close() if err != nil { return nil, fmt.Errorf(\u0026#34;parsing %s as HTML: %v\u0026#34;, url,err) } 一般而言，被调函数f(x)会将调用信息和参数信息作为发生错误时的上下文放在错误信息中并返回给调用者，调用者需要添加一些错误信息中不包含的信息。\n2. 重试失败的操作 如果错误的发生是偶然的，或由不可预知的问题导致的。此时可重新尝试失败的操作，但是在重试时，要限制重试的时间间隔或重试的时间次数，防止无限制的重试。\nfunc WaitForServer(url string) error { const timeout = 1 * time.Minute deadline := time.Now().Add(timeout) for tries := 0; time.Now().Before(deadline); tries++ { _, err := http.Head(url) if err == nil { return nil // success  } log.Printf(\u0026#34;server not responding (%s);retrying…\u0026#34;, err) time.Sleep(time.Second \u0026lt;\u0026lt; uint(tries)) // exponential back-off  } return fmt.Errorf(\u0026#34;server %s failed to respond after %s\u0026#34;, url, timeout) } 3. 输出错误信息并结束程序 这种策略应只在main中使用，对于库函数而言，应仅向上传播错误，除非该错误意味着程序内部包含不一致性，即遇到了bug，才能在库函数中结束程序\n// (In function main.) if err := WaitForServer(url); err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Site is down: %v\\n\u0026#34;, err) os.Exit(1) } 4. 只输出错误信息 只输出错误信息，不需要中断函数的执行\nif err := Ping(); err != nil { log.Printf(\u0026#34;ping failed: %v; networking disabled\u0026#34;,err) } 5. 直接忽略错误 dir, err := ioutil.TempDir(\u0026#34;\u0026#34;, \u0026#34;scratch\u0026#34;) if err != nil { return fmt.Errorf(\u0026#34;failed to create temp dir: %v\u0026#34;,err) } // ...use temp dir… os.RemoveAll(dir) // ignore errors; $TMPDIR is cleaned periodically ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86%E7%AD%96%E7%95%A5/","summary":"0. 错误处理的编码风格 检查某个子函数是否失败后，我们通常将处理失败的逻辑代码放在处理成功的代码之前。如果某个错误会导致函数返回，那么成功的逻辑代码不应该放在else中，而应直接放在函数体中。\n1. 错误传播 函数某个子程序的失败，会变成该函数的失败\nresp, err := http.Get(url) if err != nil{ return nill, err } 或是构造新的错误信息返回给调用者\ndoc, err := html.Parse(resp.Body) resp.Body.Close() if err != nil { return nil, fmt.Errorf(\u0026#34;parsing %s as HTML: %v\u0026#34;, url,err) } 一般而言，被调函数f(x)会将调用信息和参数信息作为发生错误时的上下文放在错误信息中并返回给调用者，调用者需要添加一些错误信息中不包含的信息。\n2. 重试失败的操作 如果错误的发生是偶然的，或由不可预知的问题导致的。此时可重新尝试失败的操作，但是在重试时，要限制重试的时间间隔或重试的时间次数，防止无限制的重试。\nfunc WaitForServer(url string) error { const timeout = 1 * time.Minute deadline := time.Now().Add(timeout) for tries := 0; time.Now().Before(deadline); tries++ { _, err := http.Head(url) if err == nil { return nil // success  } log.","title":"Go语言中的错误处理策略"},{"content":"HINT: Add or change a related_name 解决方案：\n需要在setting中重载AUTH_USER_MODEL\nAUTH_USER_MODEL = \u0026lsquo;users.UserProfile\u0026rsquo;\nusers：你的app\nUserProfile：model\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/hint-add-or-change-a-relatedname/","summary":"HINT: Add or change a related_name 解决方案：\n需要在setting中重载AUTH_USER_MODEL\nAUTH_USER_MODEL = \u0026lsquo;users.UserProfile\u0026rsquo;\nusers：你的app\nUserProfile：model","title":"HINT : Add or change are lated_name"},{"content":"http/template 什么是模板 模板是一种常见的视图，通过它我们可以传递数据以使该视图有意义。可以以任何方式对其进行自定义以获取任何可能的输出。\n模板包 Go中的模板附带两个包text/template和html/template。文本包允许我们使用模板插入文本，而HTML模板通过提供安全的HTML代码来帮助我们。\nPart of template 1. 模板动作 模板动作是主要的控制流程，数据评估功能。这些动作控制最终输出将如何显示\n{{ /* a comment isside template */ }} 2. 控制结构 控制结构确定模板的控制流程，有助于产生结构化的输出，以下是模板中的一些控制结构 if语句\n{{ if .condition }} {{ else }} {{ end }} 循环块\n{{ range .Items }} {{ end }} 3. 功能 函数也可以在模板内部使用，可以使用管道符|来使用预定义的函数\n 如何预定义函数\n 下面的代码创建并分析上面定义的模板templ。注意方法调用链的顺序:template.New先创建并返回一个模板;Funcs方法将daysAgo等自定义函数注册到模板中,并返回模板;最后调用Parse函数分析模板。\nreport, err := template.New(\u0026#34;report\u0026#34;).Funcs(template.FuncMap{\u0026#34;daysAgo\u0026#34;: daysAgo}).Parse(templ) if err != nil { log.Fatal(err) } 在Go中解析模板 现在，我们来解析一些文本和HTML模板\n1. 访问数据 要访问传递的数据，使用点.，如下所示：\n{{ .data }} 2. 解析文本模板 现在，来解析一个文本模板\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;text/template\u0026#34; ) type User struct { Name string Bio string } func main() { u := User{\u0026#34;John\u0026#34;, \u0026#34;a regular user\u0026#34;} ut, err := template.New(\u0026#34;users\u0026#34;).Parse(\u0026#34;The user is {{ .Name }} and he is {{ .Bio }}.\u0026#34;) if err != nil { panic(err) } err = ut.Execute(os.Stdout, u) if err != nil { panic(err) } } 其输出如图所示：\n3. 解析HTML模板  hello.html\n \u0026lt;h1\u0026gt;Go templates\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;The user is {{ .Name }}\u0026lt;/p\u0026gt; \u0026lt;h2\u0026gt;Skills:\u0026lt;/h2\u0026gt; {{ range .Skills }} \u0026lt;p\u0026gt;{{ . }}\u0026lt;/p\u0026gt; {{ end }}  main.go\n package main import ( \u0026#34;os\u0026#34; \u0026#34;html/template\u0026#34; ) func main() { t, err := template.ParseFiles(\u0026#34;templates/hello.gohtml\u0026#34;) if err != nil { panic(err) } data := struct { Name string Skills []string }{ Name: \u0026#34;John Doe\u0026#34;, Skills: []string{ \u0026#34;C++\u0026#34;, \u0026#34;Java\u0026#34;, \u0026#34;Python\u0026#34;, }, } err = t.Execute(os.Stdout, data) if err != nil { panic(err) } } 则结果： Go中的模板验证 为了验证模板是否有效，我们使用template.Must()函数。它有助于在解析过程中验证模板。因为模板通常在编译时就测试好了,如果模板解析失败将是一个致命的错误template.Must 辅助函数可以简化这个致命错误的处理:它接受一个模板和一个error类型的参数,检测error是否为nil(如果不是nil则发出panic异常),然后返回传入的模板\nvar report = template.Must(template.New(\u0026#34;issuelist\u0026#34;). Funcs(template.FuncMap{\u0026#34;daysAgo\u0026#34;: daysAgo}). Parse(templ)) func main() { result, err := github.SearchIssues(os.Args[1:]) if err != nil { log.Fatal(err) } if err := report.Execute(os.Stdout, result); err != nil { log.Fatal(err) } } ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/template/","summary":"http/template 什么是模板 模板是一种常见的视图，通过它我们可以传递数据以使该视图有意义。可以以任何方式对其进行自定义以获取任何可能的输出。\n模板包 Go中的模板附带两个包text/template和html/template。文本包允许我们使用模板插入文本，而HTML模板通过提供安全的HTML代码来帮助我们。\nPart of template 1. 模板动作 模板动作是主要的控制流程，数据评估功能。这些动作控制最终输出将如何显示\n{{ /* a comment isside template */ }} 2. 控制结构 控制结构确定模板的控制流程，有助于产生结构化的输出，以下是模板中的一些控制结构 if语句\n{{ if .condition }} {{ else }} {{ end }} 循环块\n{{ range .Items }} {{ end }} 3. 功能 函数也可以在模板内部使用，可以使用管道符|来使用预定义的函数\n 如何预定义函数\n 下面的代码创建并分析上面定义的模板templ。注意方法调用链的顺序:template.New先创建并返回一个模板;Funcs方法将daysAgo等自定义函数注册到模板中,并返回模板;最后调用Parse函数分析模板。\nreport, err := template.New(\u0026#34;report\u0026#34;).Funcs(template.FuncMap{\u0026#34;daysAgo\u0026#34;: daysAgo}).Parse(templ) if err != nil { log.Fatal(err) } 在Go中解析模板 现在，我们来解析一些文本和HTML模板\n1. 访问数据 要访问传递的数据，使用点.，如下所示：\n{{ .data }} 2. 解析文本模板 现在，来解析一个文本模板","title":"http/template"},{"content":"%matplotlib inline ======================================= Receiver Operating Characteristic (ROC) Example of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality.\nROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the \u0026ldquo;ideal\u0026rdquo; point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\nThe \u0026ldquo;steepness\u0026rdquo; of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\nMulticlass settings ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output. One ROC curve can be drawn per label, but one can also draw a ROC curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging).\nAnother evaluation measure for multi-class classification is macro-averaging, which gives equal weight to the classification of each label.\nprint(__doc__) import numpy as np import matplotlib.pyplot as plt from itertools import cycle from sklearn import svm, datasets from sklearn.metrics import roc_curve, auc from sklearn.model_selection import train_test_split from sklearn.preprocessing import label_binarize from sklearn.multiclass import OneVsRestClassifier from scipy import interp # Import some data to play with iris = datasets.load_iris() X = iris.data y = iris.target # Binarize the output y = label_binarize(y, classes=[0, 1, 2]) n_classes = y.shape[1] # Add noisy features to make the problem harder random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)] # shuffle and split training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0) # Learn to predict each class against the other classifier = OneVsRestClassifier(svm.SVC(kernel=\u0026#39;linear\u0026#39;, probability=True, random_state=random_state)) y_score = classifier.fit(X_train, y_train).decision_function(X_test) # print(\u0026#39;y_score\u0026#39;) # print(y_score) # Compute ROC curve and ROC area for each class fpr = dict() tpr = dict() roc_auc = dict() for i in range(n_classes): fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i]) # print(fpr[i]) # print(tpr[i]) roc_auc[i] = auc(fpr[i], tpr[i]) # # Compute micro-average ROC curve and ROC area # fpr[\u0026#34;micro\u0026#34;], tpr[\u0026#34;micro\u0026#34;], _ = roc_curve(y_test.ravel(), y_score.ravel()) # roc_auc[\u0026#34;micro\u0026#34;] = auc(fpr[\u0026#34;micro\u0026#34;], tpr[\u0026#34;micro\u0026#34;]) # # print(roc_auc) Automatically created module for IPython interactive environment [0. 0. 0. 0.01851852 0.01851852 0.03703704 0.03703704 0.05555556 0.05555556 0.07407407 0.07407407 0.09259259 0.09259259 0.12962963 0.12962963 0.14814815 0.14814815 0.2037037 0.2037037 0.27777778 0.27777778 1. ] [0. 0.04761905 0.14285714 0.14285714 0.19047619 0.19047619 0.33333333 0.33333333 0.38095238 0.38095238 0.61904762 0.61904762 0.66666667 0.66666667 0.76190476 0.76190476 0.9047619 0.9047619 0.95238095 0.95238095 1. 1. ] [0. 0. 0. 0.02222222 0.02222222 0.11111111 0.11111111 0.17777778 0.17777778 0.2 0.2 0.24444444 0.24444444 0.26666667 0.26666667 0.37777778 0.37777778 0.42222222 0.42222222 0.48888889 0.48888889 0.55555556 0.55555556 0.62222222 0.62222222 0.64444444 0.64444444 0.66666667 0.66666667 0.73333333 0.73333333 0.75555556 0.75555556 0.88888889 0.88888889 1. ] [0. 0.03333333 0.13333333 0.13333333 0.16666667 0.16666667 0.2 0.2 0.26666667 0.26666667 0.33333333 0.33333333 0.4 0.4 0.43333333 0.43333333 0.5 0.5 0.56666667 0.56666667 0.6 0.6 0.63333333 0.63333333 0.7 0.7 0.73333333 0.73333333 0.9 0.9 0.93333333 0.93333333 0.96666667 0.96666667 1. 1. ] [0. 0. 0. 0.01960784 0.01960784 0.07843137 0.07843137 0.09803922 0.09803922 0.11764706 0.11764706 0.1372549 0.1372549 0.15686275 0.15686275 0.17647059 0.17647059 0.31372549 0.31372549 0.33333333 0.33333333 0.35294118 0.35294118 0.41176471 0.41176471 0.45098039 0.45098039 0.47058824 0.47058824 0.50980392 0.50980392 0.56862745 0.56862745 1. ] [0. 0.04166667 0.125 0.125 0.25 0.25 0.29166667 0.29166667 0.33333333 0.33333333 0.41666667 0.41666667 0.5 0.5 0.54166667 0.54166667 0.58333333 0.58333333 0.70833333 0.70833333 0.75 0.75 0.79166667 0.79166667 0.83333333 0.83333333 0.875 0.875 0.91666667 0.91666667 0.95833333 0.95833333 1. 1. ]  Plot of a ROC curve for a specific class\nplt.figure() lw = 2 plt.plot(fpr[2], tpr[2], color=\u0026#39;darkorange\u0026#39;, lw=lw, label=\u0026#39;ROC curve (area = %0.2f)\u0026#39; % roc_auc[2]) plt.plot([0, 1], [0, 1], color=\u0026#39;navy\u0026#39;, lw=lw, linestyle=\u0026#39;--\u0026#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Receiver operating characteristic example\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.show() Plot ROC curves for the multiclass problem\n# Compute macro-average ROC curve and ROC area # First aggregate all false positive rates all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) # Then interpolate all ROC curves at this points mean_tpr = np.zeros_like(all_fpr) for i in range(n_classes): mean_tpr += interp(all_fpr, fpr[i], tpr[i]) # Finally average it and compute AUC mean_tpr /= n_classes fpr[\u0026#34;macro\u0026#34;] = all_fpr tpr[\u0026#34;macro\u0026#34;] = mean_tpr roc_auc[\u0026#34;macro\u0026#34;] = auc(fpr[\u0026#34;macro\u0026#34;], tpr[\u0026#34;macro\u0026#34;]) # Plot all ROC curves plt.figure() plt.plot(fpr[\u0026#34;micro\u0026#34;], tpr[\u0026#34;micro\u0026#34;], label=\u0026#39;micro-average ROC curve (area = {0:0.2f})\u0026#39; \u0026#39;\u0026#39;.format(roc_auc[\u0026#34;micro\u0026#34;]), color=\u0026#39;deeppink\u0026#39;, linestyle=\u0026#39;:\u0026#39;, linewidth=4) plt.plot(fpr[\u0026#34;macro\u0026#34;], tpr[\u0026#34;macro\u0026#34;], label=\u0026#39;macro-average ROC curve (area = {0:0.2f})\u0026#39; \u0026#39;\u0026#39;.format(roc_auc[\u0026#34;macro\u0026#34;]), color=\u0026#39;navy\u0026#39;, linestyle=\u0026#39;:\u0026#39;, linewidth=4) colors = cycle([\u0026#39;aqua\u0026#39;, \u0026#39;darkorange\u0026#39;, \u0026#39;cornflowerblue\u0026#39;]) for i, color in zip(range(n_classes), colors): plt.plot(fpr[i], tpr[i], color=color, lw=lw, label=\u0026#39;ROC curve of class {0} (area = {1:0.2f})\u0026#39; \u0026#39;\u0026#39;.format(i, roc_auc[i])) plt.plot([0, 1], [0, 1], \u0026#39;k--\u0026#39;, lw=lw) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Some extension of Receiver operating characteristic to multi-class\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.show() ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/plot_roc/","summary":"%matplotlib inline ======================================= Receiver Operating Characteristic (ROC) Example of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality.\nROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the \u0026ldquo;ideal\u0026rdquo; point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.","title":"Importsomedatatoplaywith"},{"content":"Keras函数式API 使用函数式API，你可以直接操作张量，也可以把层当做函数来使用，接收张量并返回张量。\n# 简单的实例 import os # **** change the warning level **** os.environ[\u0026#39;TF_CPP_MIN_LOG_LEVEL\u0026#39;] = \u0026#39;3\u0026#39; from keras.models import Sequential, Model from keras import layers from keras import Input # 使用Sequential模型 seq_model = Sequential() seq_model.add(layers.Dense(32, activation=\u0026#39;relu\u0026#39;, input_shape=(64,))) seq_model.add(layers.Dense(32, activation=\u0026#39;relu\u0026#39;)) seq_model.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) # 对应的函数式API实现 input_tensor = Input(shape=(64,)) x = layers.Dense(32, activation=\u0026#39;relu\u0026#39;)(input_tensor) x = layers.Dense(32, activation=\u0026#39;softmax\u0026#39;)(x) output_tensor = layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)(x) model = Model(input_tensor, output_tensor) model.summary() Model: \u0026quot;model_2\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (None, 64) 0 _________________________________________________________________ dense_10 (Dense) (None, 32) 2080 _________________________________________________________________ dense_11 (Dense) (None, 32) 1056 _________________________________________________________________ dense_12 (Dense) (None, 10) 330 ================================================================= Total params: 3,466 Trainable params: 3,466 Non-trainable params: 0 _________________________________________________________________  Keras会在后台检索从input_tensor到output_tensor所包含的每一层，并将这些层组合成一个类图的数据结构，即一个Model。这种方法有效的原因在于，output_tensor是通过对input_tensor进行多次变换得到的。如果你试图利用不相关的输入和输出来构建一个模型，那么会得到RuntimeError\nunrelated_input = Input(shape=(32,)) bad_model = model = Model(unrelated_input, output_tensor) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) \u0026lt;ipython-input-3-54197a8d0ec3\u0026gt; in \u0026lt;module\u0026gt; 1 unrelated_input = Input(shape=(32,)) ----\u0026gt; 2 bad_model = model = Model(unrelated_input, output_tensor) /usr/lib/python3.7/site-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs) 89 warnings.warn('Update your `' + object_name + '` call to the ' + 90 'Keras 2 API: ' + signature, stacklevel=2) ---\u0026gt; 91 return func(*args, **kwargs) 92 wrapper._original_function = func 93 return wrapper /usr/lib/python3.7/site-packages/keras/engine/network.py in __init__(self, *args, **kwargs) 92 'inputs' in kwargs and 'outputs' in kwargs): 93 # Graph network ---\u0026gt; 94 self._init_graph_network(*args, **kwargs) 95 else: 96 # Subclassed network /usr/lib/python3.7/site-packages/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name, **kwargs) 239 # Keep track of the network's nodes and layers. 240 nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network( --\u0026gt; 241 self.inputs, self.outputs) 242 self._network_nodes = nodes 243 self._nodes_by_depth = nodes_by_depth /usr/lib/python3.7/site-packages/keras/engine/network.py in _map_graph_network(inputs, outputs) 1509 'The following previous layers ' 1510 'were accessed without issue: ' + -\u0026gt; 1511 str(layers_with_complete_input)) 1512 for x in node.output_tensors: 1513 computable_tensors.append(x) ValueError: Graph disconnected: cannot obtain value for tensor Tensor(\u0026quot;input_2:0\u0026quot;, shape=(None, 64), dtype=float32) at layer \u0026quot;input_2\u0026quot;. The following previous layers were accessed without issue: []  对这种Model实例进行编译、训练或评估时，其API与Sequential模型相同\nmodel.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;) import numpy as np x_train = np.random.random((1000, 64)) y_train = np.random.random((1000, 10)) model.fit(x_train, y_train, epochs=10, batch_size=128) score = model.evaluate(x_train, y_train) Epoch 1/10 1000/1000 [==============================] - 0s 83us/step - loss: 11.6211 Epoch 2/10 1000/1000 [==============================] - 0s 33us/step - loss: 11.6215 Epoch 3/10 1000/1000 [==============================] - 0s 32us/step - loss: 11.6216 Epoch 4/10 1000/1000 [==============================] - 0s 27us/step - loss: 11.6216 Epoch 5/10 1000/1000 [==============================] - 0s 36us/step - loss: 11.6216 Epoch 6/10 1000/1000 [==============================] - 0s 37us/step - loss: 11.6217 Epoch 7/10 1000/1000 [==============================] - 0s 37us/step - loss: 11.6217 Epoch 8/10 1000/1000 [==============================] - 0s 25us/step - loss: 11.6217 Epoch 9/10 1000/1000 [==============================] - 0s 40us/step - loss: 11.6217 Epoch 10/10 1000/1000 [==============================] - 0s 34us/step - loss: 11.6216 1000/1000 [==============================] - 0s 69us/step  ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/keras%E5%87%BD%E6%95%B0%E5%BC%8Fapi/","summary":"Keras函数式API 使用函数式API，你可以直接操作张量，也可以把层当做函数来使用，接收张量并返回张量。\n# 简单的实例 import os # **** change the warning level **** os.environ[\u0026#39;TF_CPP_MIN_LOG_LEVEL\u0026#39;] = \u0026#39;3\u0026#39; from keras.models import Sequential, Model from keras import layers from keras import Input # 使用Sequential模型 seq_model = Sequential() seq_model.add(layers.Dense(32, activation=\u0026#39;relu\u0026#39;, input_shape=(64,))) seq_model.add(layers.Dense(32, activation=\u0026#39;relu\u0026#39;)) seq_model.add(layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)) # 对应的函数式API实现 input_tensor = Input(shape=(64,)) x = layers.Dense(32, activation=\u0026#39;relu\u0026#39;)(input_tensor) x = layers.Dense(32, activation=\u0026#39;softmax\u0026#39;)(x) output_tensor = layers.Dense(10, activation=\u0026#39;softmax\u0026#39;)(x) model = Model(input_tensor, output_tensor) model.summary() Model: \u0026quot;model_2\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_2 (InputLayer) (None, 64) 0 _________________________________________________________________ dense_10 (Dense) (None, 32) 2080 _________________________________________________________________ dense_11 (Dense) (None, 32) 1056 _________________________________________________________________ dense_12 (Dense) (None, 10) 330 ================================================================= Total params: 3,466 Trainable params: 3,466 Non-trainable params: 0 _________________________________________________________________  Keras会在后台检索从input_tensor到output_tensor所包含的每一层，并将这些层组合成一个类图的数据结构，即一个Model。这种方法有效的原因在于，output_tensor是通过对input_tensor进行多次变换得到的。如果你试图利用不相关的输入和输出来构建一个模型，那么会得到RuntimeError","title":"Keras函数式API"},{"content":"Linux下的权限管理 Linux 系统中为什么需要设定不同的权限，所有用户都直接使用管理员（root）身份不好吗？\n 由于绝大多数用户使用的是个人计算机，使用者一般都是被信任的人（如家人、朋友等）。在这种情况下，大家都可以使用管理员身份直接登录。但在服务器上就不是这种情况了，往往运行的数据越重要（如游戏数据），价值越高（如电子商城数据、银行数据），则服务器中对权限的设定就要越详细，用户的分级也要越明确。\n和 Windows 系统不同，Linux 系统为每个文件都添加了很多的属性，最大的作用就是维护数据的安全。举个简单的例子，在你的 Linux 系统中，和系统服务相关的文件通常只有 root 用户才能读或写，就拿 /etc/shadow 这个文件来说，此文件记录了系统中所有用户的密码数据，非常重要，因此绝不能让任何人读取（否则密码数据会被窃取），只有 root 才可以有读取权限。 此外，如果你有一个软件开发团队，你希望团队中的每个人都可以使用某一些目录下的文件，而非团队的其他人则不予以开放。通过前面章节的学习我们知道，只需要将团队中的所有人加入新的群组，并赋予此群组读写目录的权限，即可实现要求。反之，如果你的目录权限没有做好，就很难防止其他人在你的系统中乱搞。 比如说，本来 root 用户才能做的开关机、ADSL 拨接程序，新增或删除用户等命令，一旦允许任何人拥有这些权限，系统很可能会经常莫名其妙的挂掉。而且，万一 root 用户的密码被其他人获取，他们就可以登录你的系统，从事一些只有 root 用户才能执行的操作，这是绝对不允许发生的。 因此，在服务器上，绝对不是所有的用户都使用 root 身份登录，而要根据不同的工作需要和职位需要，合理分配用户等级和权限等级。\n Linux 系统中，文件或目录的权限信息，可以使用 ls 命令查看，例如：\n[root@localhost ~]# ls -al total 156 drwxr-x---. 4 root root 4096 Sep 8 14:06 . drwxr-xr-x. 23 root root 4096 Sep 8 14:21 .. -rw-------. 1 root root 1474 Sep 4 18:27 anaconda-ks.cfg -rw-------. 1 root root 199 Sep 8 17:14 .bash_history -rw-r--r--. 1 root root 24 Jan 6 2007 .bash_logout 1. Linux chgrp命令：修改文件和目录的所属组 chgrp 命令用于修改文件（或目录）的所属组。 为了方便初学者记忆，可以将 chgrp 理解为是 \u0026ldquo;change group\u0026rdquo; 的缩写。\nchgrp 命令的用法很简单，其基本格式为：\n[root@localhost ~]# chgrp [-R] 所属组 文件名（目录名） -R（注意是大写）选项长作用于更改目录的所属组，表示更改连同子目录中所有文件的所属组信息。\n使用此命令需要注意的一点是，要被改变的群组名必须是真实存在的，否则命令无法正确执行，会提示 \u0026ldquo;invaild group name\u0026rdquo;。\n举个例子，当以 root 身份登录 Linux 系统时，主目录中会存在一个名为 install.log 的文件，我们可以使用如下方法修改此文件的所属组：\n[root@localhost ~]# groupadd group1 #新建用于测试的群组 group1 [root@localhost ~]# chgrp group1 install.log #修改install.log文件的所属组为group1 [root@localhost ~]# ll install.log -rw-r--r--. 1 root group1 78495 Nov 17 05:54 install.log #修改生效 [root@localhost ~]# chgrp testgroup install.log chgrp: invaild group name \u0026#39;testgroup\u0026#39; 可以看到，在具有 group1 群组的前提下，我们成功修改了 install.log 文件的所属组，但我们再次试图将所属组修改为 testgroup 时，命令执行失败，就是因为系统的 /etc/group 文件中，没有 testgroup 群组。\n2. Linux chown命令：修改文件和目录的所有者和所属组 chown 命令，可以认为是 \u0026ldquo;change owner\u0026rdquo; 的缩写，主要用于修改文件（或目录）的所有者，除此之外，这个命令也可以修改文件（或目录）的所属组。\n当只需要修改所有者时，可使用如下 chown 命令的基本格式：\n[root@localhost ~]# chown [-R] 所有者 文件或目录 -R（注意大写）选项表示连同子目录中的所有文件，都更改所有者。\n如果需要同时更改所有者和所属组，chown 命令的基本格式为：\n[root@localhost ~]# chown [-R] 所有者:所属组 文件或目录 注意，在 chown 命令中，所有者和所属组中间也可以使用点（.），但会产生一个问题，如果用户在设定账号时加入了小数点（例如 zhangsan.temp），就会造成系统误判。因此，建议大家使用冒号连接所有者和所属组。\n当然，chown 命令也支持单纯的修改文件或目录的所属组，例如 chown :group install.log 就表示修改 install.log 文件的所属组，但修改所属组通常使用 chgrp 命令，因此并不推荐大家使用 chown 命令。\n另外需要注意的一点是，使用 chown 命令修改文件或目录的所有者（或所属者）时，要保证使用者用户（或用户组）存在，否则该命令无法正确执行，会提示 \u0026ldquo;invalid user\u0026rdquo; 或者 \u0026ldquo;invaild group\u0026rdquo;。\n【例 1】\n其实，修改文件的所有者，更多时候是为了得到更高的权限，举一个实例：\n[root@localhost ~]# touch file #由root用户创建file文件 [root@localhost ~]# ll file -rw-r--r--. 1 root root 0 Apr 17 05:12 file #文件的所有者是root，普通用户user对这个文件拥有只读权限 [root@localhost ~]# chown user file #修改文件的所有者 [root@localhost ~]# ll file -rw-r--r--. 1 user root 0 Apr 17 05:12 file #所有者变成了user用户，这时user用户对这个文件就拥有了读、写权限 可以看到，通过修改 file 文件的所有者，user 用户从其他人身份（只对此文件有读取权限）转变成了所有者身份，对此文件拥有读和写权限。\n【例 2】\nLinux 系统中，用户等级权限的划分是非常清楚的，root 用户拥有最高权限，可以修改任何文件的权限，而普通用户只能修改自己文件的权限（所有者是自己的文件），例如：\n[root@localhost ~]# cd /home/user #进入user用户的主目录 [root@localhost user]# touch test #由root用户新建文件test [root@localhost user]# ll test -rw-r--r--. 1 root root 0 Apr 17 05:37 test #文件所有者和所属组都是root用户 [root@localhost user]# su - user #切换为user用户 [user@localhost ~]$ chmod 755 test chmod:更改\u0026#34;test\u0026#34;的权限：不允许的操作 #user用户不能修改test文件的权限 [user@localhost ~]$ exit #退回到root身份 [root@localhost user]# chown user test #由root用户把test文件的所有者改为user用户 [root@localhost user]# su - user #切换为user用户 [user@localhost ~]$ chmod 755 test #user用户由于是test文件的所有者，所以可以修改文件的权限 [user@localhost ~]$ ll test -rwxr-xr-x. 1 user root 0 Apr 17 05:37 test #查看权限 可以看到，user 用户无权更改所有者为 root 用户文件的权限，只有普通用户是这个文件的所有者，才可以修改文件的权限。\n【例 3】\n[root@localhost ~]# chown user:group file [root@localhost ~]# ll file -rw-r--r--. 1 user group 0 Apr 17 05:12 file 3. Linux权限位 Linux 系统，最常见的文件权限有 3 种，即对文件的读（用 r 表示）、写（用 w 表示）和执行（用 x 表示，针对可执行文件或目录）权限。在 Linux 系统中，每个文件都明确规定了不同身份用户的访问权限，通过 ls 命令即可看到。 除此之外，我们有时会看到 s（针对可执行文件或目录，使文件在执行阶段，临时拥有文件所有者的权限）和 t（针对目录，任何用户都可以在此目录中创建文件，但只能删除自己的文件），文件设置 s 和 t 权限，会占用 x 权限的位置。\n例如，我们以 root 的身份登陆 Linux，并执行如下指令：\n[root@localhost ~]# ls -al total 156 drwxr-x---. 4 root root 4096 Sep 8 14:06 . drwxr-xr-x. 23 root root 4096 Sep 8 14:21 .. -rw-------. 1 root root 1474 Sep 4 18:27 anaconda-ks.cfg -rw-------. 1 root root 199 Sep 8 17:14 .bash_history -rw-r--r--. 1 root root 24 Jan 6 2007 .bash_logout ... 可以看到，每行的第一列表示的就是各文件针对不同用户设定的权限，一共 11 位，但第 1 位用于表示文件的具体类型，最后一位此文件受 SELinux 的安全规则管理，不是本节关心的内容，放到后续章节做详细介绍。\n因此，为文件设定不同用户的读、写和执行权限，仅涉及到 9 位字符，以 ls 命令输出信息中的 .bash_logout 文件为例，设定不同用户的访问权限是 rw-r\u0026ndash;r\u0026ndash;，Linux 将访问文件的用户分为 3 类，分别是文件的所有者，所属组（也就是文件所属的群组）以及其他人。除了所有者，以及所属群组中的用户可以访问文件外，其他用户（其他群组中的用户）也可以访问文件，这部分用户都归为其他人范畴.\n很显然，Linux 系统为 3 种不同的用户身份，分别规定了是否对文件有读、写和执行权限。拿图 1 来说，文件所有者拥有对文件的读和写权限，但是没有执行权限；所属群组中的用户只拥有读权限，也就是说，这部分用户只能访问文件，无法修改文件；其他人也是只能访问文件。\nLinux 系统中，多数文件的文件所有者和所属群组都是 root（都是 root 账户创建的），这也就是为什么，root 用户是超级管理员，权限足够大的原因。\n4. Linux读写执行权限（-r、-w、-x）的真正含义 rwx 权限对文件的作用\n文件，是系统中用来存储数据的，包括普通的文本文件、数据库文件、二进制可执行文件，等等。不同的权限对文件的含义如表 1 所示。\n   rwx权限 对文件的作用     读权限（r） 表示可读取此文件中的实际内容，例如，可以对文件执行 cat、more、less、head、tail 等文件查看命令。   写权限（w） 表示可以编辑、新增或者修改文件中的内容，例如，可以对文件执行 vim、echo 等修改文件数据的命令。注意，无权限不赋予用户删除文件的权利，除非用户对文件的上级目录拥有写权限才可以。   执行权限（x） 表示该文件具有被系统执行的权限。Window系统中查看一个文件是否为可执行文件，是通过扩展名（.exe、.bat 等），但在 Linux 系统中，文件是否能被执行，是通过看此文件是否具有 x 权限来决定的。也就是说，只要文件拥有 x 权限，则此文件就是可执行文件。但是，文件到底能够正确运行，还要看文件中的代码是否正确。    rwx 权限对目录的作用 目录，主要用来记录文件名列表，不同的权限对目录的作用如表 2 所示。\n   rwx权限 对目录的作用     读权限（r） 表示具有读取目录结构列表的权限，也就是说，可以看到目录中有哪些文件和子目录。一旦对目录拥有 r 权限，就可以在此目录下执行 ls 命令，查看目录中的内容。   写权限（w） 对于目录来说，w 权限是最高权限。对目录拥有 w 权限，表示可以对目录做以下操作：     在此目录中建立新的文件或子目录； 删除已存在的文件和目录（无论子文件或子目录的权限是怎样的）； 对已存在的文件或目录做更名操作； 移动此目录下的文件和目录的位置. 一旦对目录拥有 w 权限，就可以在目录下执行 touch、rm、cp、mv 等命令。|     rwx权限 对目录的作用     执行权限（x） 目录是不能直接运行的，对目录赋予 x 权限，代表用户可以进入目录，也就是说，赋予 x 权限的用户或群组可以使用 cd 命令。    对目录来说，如果只赋予 r 权限，则此目录是无法使用的。很简单，只有 r 权限的目录，用户只能查看目录结构，根本无法进入目录（需要用 x 权限），更不用说使用了。\n因此，对于目录来说，常用来设定目录的权限其实只有 0（\u0026mdash;）、5（r-x）、7（rwx）这 3 种。\n【例 1】 某目录的权限如下所示：\ndrwxr--r--. 3 root root 4096 Jun 25 08:35 .ssh 系统有个账号名称为 vbird，此账户并不包含在 root 群组中，请问 vbird 对这个目录有何权限？是否可切换到此目录中？\n 答案：vbird 对此目录仅具有 r 的权限，因此 vbird 可以查询此目录下的文件名列表。因为 vbird 不具有 x 的权限，因此 vbird 并不能切换到此目录内。\n 【例 2】 假设有个账号名称为dmtsai，他的主目录在/home/dmtsai/，dmtsai对此目录具有[rwx]的权限。若在此目录下有个名为 the_root.data 的文件，该文件的权限如下：\n-rwx------. 1 root root 4365 Sep 19 23:20 the_root.data 请问 dmtsai 对此文件的权限为何？可否删除此文件？\n 答案：由于 dmtsai 对此文件来说是其他人的身份，因此这个文件他无法读、无法编辑也无法执行，也就是说，他无法变动这个文件的内容就是了。但是由于这个文件在他的主目录下，他在此目录下具有 rwx 的完整权限，因此对于 the_root.data 这个文件来说，是能够删除的。\n 5. Linux chmod命令：修改文件或目录的权限 既然我们已经知道文件权限对于一个系统的重要性，也知道每个文件都设定了针对不同用户的访问权限，那么，是否可以手动修改文件的访问权限呢？\n可以，通过 chmod 命令即可。chmod 命令设定文件权限的方式有 2 种，分别可以使用数字或者符号来进行权限的变更。 chmod命令使用数字修改文件权限 Linux 系统中，文件的基本权限由 9 个字符组成，以 rwxrw-r-x 为例，我们可以使用数字来代表各个权限，各个权限与数字的对应关系如下：\n r \u0026ndash;\u0026gt; 4 w \u0026ndash;\u0026gt; 2 x \u0026ndash;\u0026gt; 1  由于这 9 个字符分属 3 类用户，因此每种用户身份包含 3 个权限（r、w、x），通过将 3 个权限对应的数字累加，最终得到的值即可作为每种用户所具有的权限。\n拿 rwxrw-r-x 来说，所有者、所属组和其他人分别对应的权限值为：\n 所有者 = rwx = 4+2+1 = 7 所属组 = rw- = 4+2 = 6 其他人 = r-x = 4+1 = 5  所以，此权限对应的权限值就是 765。\n使用数字修改文件权限的 chmod 命令基本格式为：\n[root@localhost ~]# chmod [-R] 权限值 文件名 -R（注意是大写）选项表示连同子目录中的所有文件，也都修改设定的权限。\n例如，使用如下命令，即可完成对 .bashrc 目录文件的权限修改：\n[root@localhost ~]# ls -al .bashrc -rw-r--r--. 1 root root 176 Sep 22 2004 .bashrc [root@localhost ~]# chmod 777 .bashrc [root@localhost ~]# ls -al .bashrc -rwxrwxrwx. 1 root root 176 Sep 22 2004 .bashrc 再举个例子，通常我们以 Vim 编辑 Shell 文件批处理文件后，文件权限通常是 rw-rw-r\u0026ndash;（644），那么，如果要将该文件变成可执行文件，并且不让其他人修改此文件，则只需将此文件的权限该为 rwxr-xr-x（755）即可。\nchmod命令使用字母修改文件权限\n既然文件的基本权限就是 3 种用户身份（所有者、所属组和其他人）搭配 3 种权限（rwx），chmod 命令中用 u、g、o 分别代表 3 种身份，还用 a 表示全部的身份（all 的缩写）。另外，chmod 命令仍使用 r、w、x 分别表示读、写、执行权限。\n例如，如果我们要设定 .bashrc 文件的权限为 rwxr-xr-x，则可执行如下命令：\n[root@localhost ~]# chmod u=rwx,go=rx .bashrc [root@localhost ~]# ls -al .bashrc -rwxr-xr-x. 1 root root 176 Sep 22 2004 .bashrc 再举个例子，如果想要增加 .bashrc 文件的每种用户都可做写操作的权限，可以使用如下命令：\n[root@localhost ~]# ls -al .bashrc -rwxr-xr-x. 1 root root 176 Sep 22 2004 .bashrc [root@localhost ~]# chmod a+w .bashrc [root@localhost ~]# ls -al .bashrc -rwxrwxrwx. 1 root root 176 Sep 22 2004 .bashrc 6. Linux umask详解：令新建文件和目录拥有默认权限 Linux 是注重安全性的操作系统，而安全的基础在于对权限的设定，不仅所有已存在的文件和目录要设定必要的访问权限，创建新的文件和目录时，也要设定必要的初始权限。\nWindows 系统中，新建的文件和目录时通过继承上级目录的权限获得的初始权限，而 Linux 不同，它是通过使用 umask 默认权限来给所有新建的文件和目录赋予初始权限的。\n那么，我们如何得知 umask 默认权限的值呢？直接通过 umask 命令即可：\n[root@localhost ~]# umask 0022 #root用户默认是0022，普通用户默认是 0002 大家可能会问，不应该只有 3 个数字（分别对应 3 种用户身份）吗，为什么有 4 个？ umask 默认权限确实由 4 个八进制数组成，但第 1 个数代表的是文件所具有的特殊权限（SetUID、SetGID、Sticky BIT），此部分内容放到后续章节中讲解，现在先不讨论。也就是说，后 3 位数字 \u0026ldquo;022\u0026rdquo; 才是本节真正要用到的 umask 权限值，将其转变为字母形式为 \u0026mdash;-w\u0026ndash;w-。\n注意，虽然 umask 默认权限是用来设定文件或目录的初始权限，但并不是直接将 umask 默认权限作为文件或目录的初始权限，还要对其进行 \u0026ldquo;再加工\u0026rdquo;。\n文件和目录的真正初始权限，可通过以下的计算得到： 文件（或目录）的初始权限 = 文件（或目录）的最大默认权限 - umask权限\n如果按照官方的标准算法，需要将 umask 默认权限使用二进制并经过逻辑与和逻辑非运算后，才能得到最终文件或目录的初始权限，计算过程比较复杂，且容易出错，因此本节给大家介绍了更简单的计算方式。\n显然，如果想最终得到文件或目录的初始权限值，我们还需要了解文件和目录的最大默认权限值。在 Linux 系统中，文件和目录的最大默认权限是不一样的： 对文件来讲，其可拥有的最大默认权限是 666，即 rw-rw-rw-。也就是说，使用文件的任何用户都没有执行（x）权限。原因很简单，执行权限是文件的最高权限，赋予时绝对要慎重，因此绝不能在新建文件的时候就默认赋予，只能通过用户手工赋予。 对目录来讲，其可拥有的最大默认权限是 777，即 rwxrwxrwx。\n接下来，我们利用字母权限的方式计算文件或目录的初始权限。以 umask 值为 022 为例，分别计算新建文件和目录的初始权限： 文件的最大默认权限是 666，换算成字母就是 \u0026ldquo;-rw-rw-rw-\u0026quot;，umask 的值是 022，换算成字母为 \u0026ldquo;\u0026mdash;\u0026ndash;w\u0026ndash;w-\u0026quot;。把两个字母权限相减，得到 (-rw-rw-rw-) - (\u0026mdash;\u0026ndash;w\u0026ndash;w-) = (-rw-r\u0026ndash;r\u0026ndash;)，这就是新建文件的初始权限。我们测试一下：\n[root@localhost ~]# umask 0022 #默认umask的值是0022 [root@localhost ~]# touch file \u0026lt;--新建file空文件 [root@localhost ~]# ll -d file -rw-r--r--. 1 root root 0 Apr 18 02:36 file 目录的默认权限最大可以是 777，换算成字母就是 \u0026ldquo;drwxrwxrwx\u0026rdquo;，umask 的值是 022，也就是 \u0026ldquo;\u0026mdash;\u0026ndash;w\u0026ndash;w-\u0026quot;。把两个字母权限相减，得到的就是新建目录的默认权限，即 (drwxrwxrwx) - (\u0026mdash;\u0026ndash;w\u0026ndash;w-) = (drwxr-xr-x)。我们再来测试一下：\n[root@localhost ~]# umask 0022 [root@localhost ~]# mkdir catalog \u0026lt;--新建catalog目录 [root@localhost ~]# ll -d catalog drwxr-xr-x. 2 root root 4096 Apr 18 02:36 catalog 注意，在计算文件或目录的初始权限时，不能直接使用最大默认权限和 umask 权限的数字形式做减法，这是不对的。例如，若 umask 默认权限的值为 033，按照数字形式计算文件的初始权限，666-033=633，但我们按照字母的形式计算会得到 （rw-rw-rw-) - (\u0026mdash;-wx-wx) = (rw-r\u0026ndash;r\u0026ndash;)，换算成数字形式是 644。 这里的减法，其实是“遮盖”的意思，也就是说，最大默认权限中和 umask 权限公共的部分，通过减法运算会被遮盖掉，最终剩下的“最大默认权限”，才是最终赋予文件或目录的初始权限。\numask默认权限的修改方法 umask 权限值可以通过如下命令直接修改：\n[root@localhost ~]# umask 002 [root@localhost ~]# umask 0002 [root@localhost ~]# umask 033 [root@localhost ~]# umask 0033 不过，这种方式修改的 umask 只是临时有效，一旦重启或重新登陆系统，就会失效。如果想让修改永久生效，则需要修改对应的环境变量配置文件 /etc/profile。例如：\n[root@localhost ~]# vim /etc/profile ...省略部分内容... if [ $UID -gt 199]\u0026amp;\u0026amp;[ \u0026#34;\u0026#39;id -gn\u0026#39;\u0026#34; = \u0026#34;\u0026#39;id -un\u0026#39;\u0026#34; ]; then umask 002 #如果UID大于199（普通用户），则使用此umask值 else umask 022 #如果UID小于199（超级用户），则使用此umask值 fi ...省略部分内容... 这是一段 Shell 脚本程序，不懂也没关系，大家只需要知道，普通用户的 umask 由 if 语句的第一段定义，而超级用户 root 的 umask 值由 else 语句定义即可。 修改此文件，则 umask 值就会永久生效。\n7. ACL权限是什么，Linux ACL访问控制权限（包含开启方式） Linux 系统传统的权限控制方式，无非是利用 3 种身份（文件所有者，所属群组，其他用户），并分别搭配 3 种权限（读 r，写 w，访问 x）。比如，我们可以通过 ls -l 命令查看当前目录中所有文件的详细信息，其中就包含对各文件的权限设置：\n[root@localhost ~]# ls -l total 36 drwxr-xr-x. 2 root root 4096 Apr 15 16:33 Desktop drwxr-xr-x. 2 root root 4096 Apr 15 16:33 Documents ... -rwxr-xr-x. 2 root root 4096 Apr 15 16:33 post-install ... 以上输出信息中，“rwxr-xr-x”就指明了不同用户访问文件的权限，即文件所有者拥有对文件的读、写、访问权限（rwx），文件所属群组拥有对文件的读、访问权限（r-x），其他用户拥有对文件的读、访问权限（r-x）。 权限前的字符，表示文件的具体类型，比如 d 表示目录，- 表示普通文件，l 表示连接文件，b 表示设备文件，等等。\n但在实际应用中，以上这 3 种身份根本不够用，给大家举个例子。 上图的根目录中有一个 /project 目录，这是班级的项目目录。班级中的每个学员都可以访问和修改这个目录，老师需要拥有对该目录的最高权限，其他班级的学员当然不能访问这个目录。\n需要怎么规划这个目录的权限呢？应该这样，老师使用 root 用户，作为这个目录的属主，权限为 rwx；班级所有的学员都加入 tgroup 组，使 tgroup 组作为 /project 目录的属组，权限是 rwx；其他人的权限设定为 0（也就是 \u0026mdash;）。这样一来，访问此目录的权限就符合我们的要求了。\n有一天，班里来了一位试听的学员 st，她必须能够访问 /project 目录，所以必须对这个目录拥有 r 和 x 权限；但是她又没有学习过以前的课程，所以不能赋予她 w 权限，怕她改错了目录中的内容，所以学员 st 的权限就是 r-x。可是如何分配她的身份呢？变为属主？当然不行，要不 root 该放哪里？加入 tgroup 组？也不行，因为 tgroup 组的权限是 rwx，而我们要求学员 st 的权限是 r-x。如果把其他人的权限改为 r-x 呢？这样一来，其他班级的所有学员都可以访问 /project 目录了。\n显然，普通权限的三种身份不够用了，无法实现对某个单独的用户设定访问权限，这种情况下，就需要使用 ACL 访问控制权限。\nACL，是 Access Control List（访问控制列表）的缩写，在 Linux 系统中， ACL 可实现对单一用户设定访问文件的权限。也可以这么说，设定文件的访问权限，除了用传统方式（3 种身份搭配 3 种权限），还可以使用 ACL 进行设定。拿本例中的 st 学员来说，既然赋予它传统的 3 种身份，无法解决问题，就可以考虑使用 ACL 权限控制的方式，直接对 st 用户设定访问文件的 r-x 权限。\n开启 ACL 权限\nCentOS 6.x 系统中，ACL 权限默认处于开启状态，无需手工开启。但如果你的操作系统不是 CentOS 6.x，可以通过如下方式查看ACL权限是否开启：\n[root@localhost ~]# mount /dev/sda1 on /boot type ext4 (rw) /dev/sda3 on I type ext4 (rw) …省略部分输出… #使用mount命令可以看到系统中已经挂载的分区，但是并没有看到ACL权限的设置 [root@localhost ~]# dumpe2fs -h /dev/sda3 #dumpe2fs是查询指定分区文件系统详细信息的命令 …省略部分输出… Default mount options: user_xattr acl …省略部分输出… 其中，dumpe2fs 命令的 -h 选项表示仅显示超级块中的信息，而不显示磁盘块组的详细信息；\n使用 mount 命令可以查看到系统中已经挂载的分区，而使用 dumpe2fs 命令可以查看到这个分区文件系统的详细信息。大家可以看到，我们的 ACL 权限是 /dev/sda3 分区的默认挂载选项，所以不需要手工挂载。 如果 Linux 系统如果没有默认挂载，可以执行如下命令实现手动挂载：\n[root@localhost ~]# mount -o remount,acl /  #重新挂载根分区，并加入ACL权限  使用 mount 命令重新挂载，并加入 ACL 权限。但使用此命令只是临时生效，要想永久生效，需要修改 /etc/fstab 文件，修改方法如下：\n[root@localhost ~]#vi /etc/fstab UUID=c2ca6f57-b15c-43ea-bca0-f239083d8bd2 /ext4 defaults,acl 1 1 #加入ACL权限 [root@localhost ~]# mount -o remount / #重新挂载文件系统或重启系统，使修改生效 在你需要开启 ACL 权限的分区行上（也就是说 ACL 权限针对的是分区），手工在 defaults 后面加入 \u0026ldquo;，acl\u0026rdquo; 即可永久在此分区中开启 ACL 权限。\n8. Linux ACL权限设置（setfacl和getfacl） 通过上一节的学习，我们知道了什么是 ACL 权限，也了解了如何配置 Linux 系统使其开启 ACL 权限，本节来学习 ACL 设定文件访问权限的具体方法。\n设定 ACl 权限，常用命令有 2 个，分别是 setfacl 和 getfacl 命令，前者用于给指定文件或目录设定 ACL 权限，后者用于查看是否配置成功。\ngetfacl 命令用于查看文件或目录当前设定的 ACL 权限信息。该命令的基本格式为：\n[root@localhost ~]# getfacl 文件名 getfacl 命令的使用非常简单，且常和 setfacl 命令一起搭配使用。\nsetfacl 命令可直接设定用户或群组对指定文件的访问权限。此命令的基本格式为：\n[root@localhost ~]# setfacl 选项 文件名 表 1 罗列出了该命令可以使用的所用选项及功能。\n表 1 setfacl 命令选项及用法\n   选项 功能     -m 参数 设定 ACL 权限。如果是给予用户 ACL 权限，参数则使用 \u0026ldquo;u:用户名:权限\u0026rdquo; 的格式，例如 setfacl -m u:st:rx /project 表示设定 st 用户对 project 目录具有 rx 权限；如果是给予组 ACL 权限，参数则使用 \u0026ldquo;g:组名:权限\u0026rdquo; 格式，例如 setfacl -m g:tgroup:rx /project 表示设定群组 tgroup 对 project 目录具有 rx 权限。   -x 参数 删除指定用户（参数使用 u:用户名）或群组（参数使用 g:群组名）的 ACL 权限，例如 setfacl -x u:st /project 表示删除 st 用户对 project 目录的 ACL 权限。   -b 删除所有的 ACL 权限，例如 setfacl -b /project 表示删除有关 project 目录的所有 ACL 权限。   -d 设定默认 ACL 权限，命令格式为 \u0026ldquo;setfacl -m d:u:用户名:权限 文件名\u0026rdquo;（如果是群组，则使用 d:g:群组名:权限），只对目录生效，指目录中新建立的文件拥有此默认权限，例如 setfacl -m d:u:st:rx /project 表示 st 用户对 project 目录中新建立的文件拥有 rx 权限。   -R 递归设定 ACL 权限，指设定的 ACL 权限会对目录下的所有子文件生效，命令格式为 \u0026ldquo;setfacl -m u:用户名:权限 -R 文件名\u0026rdquo;（群组使用 g:群组名:权限），例如 setfacl -m u:st:rx -R /project 表示 st 用户对已存在于 project 目录中的子文件和子目录拥有 rx 权限。   -k 删除默认 ACL 权限。    setfacl -m：给用户或群组添加 ACL 权限 回归上一节案例，解决方案如下：\n 老师使用 root 用户，并作为 /project 的所有者，对 project 目录拥有 rwx 权限； 新建 tgroup 群组，并作为 project 目录的所属组，包含本班所有的班级学员（假定只有 zhangsan 和 lisi），拥有对 project 的 rwx 权限； 将其他用户访问 project 目录的权限设定为 0（也就是 \u0026mdash;）。 对于试听学员 st 来说，我们对其设定 ACL 权限，令该用户对 project 拥有 rx 权限。  具体的设置命令如下：\n[root@localhost ~]# useradd zhangsan [root@localhost ~]# useradd lisi [root@localhost ~]# useradd st [root@localhost ~]# groupadd tgroup \u0026lt;-- 添加需要试验的用户和用户组，省略设定密码的过程 [root@localhost ~]# mkdir /project \u0026lt;-- 建立需要分配权限的目录 [root@localhost ~]# chown root:tgroup /project \u0026lt;-- 改变/project目录的所有者和所属组 [root@localhost ~]# chmod 770 /project \u0026lt;-- 指定/project目录的权限 [root@localhost ~]# ll -d /project drwxrwx---. 2 root tgroup 4096 Apr 16 12:55 /project #这时st学员来试听了，如何给她分配权限 [root@localhost ~]# setfacl -m u:st:rx /project #给用户st赋予r-x权限，使用\u0026#34;u:用户名：权限\u0026#34; 格式 [root@localhost /]# cd / [root@localhost /]# ll -d /project drwxrwx---+ 2 root tgroup 4096 Apr 16 12:55 /project #如果查询时会发现，在权限位后面多了一个\u0026#34;+\u0026#34;，表示此目录拥有ACL权限 [root@localhost /]# getfacl project #查看/prpject目录的ACL权限 #file:project \u0026lt;--文件名 #owner:root \u0026lt;--文件的所有者 #group:tgroup \u0026lt;--文件的所属组 user::rwx \u0026lt;--用户名栏是空的，说明是所有者的权限 user:st:r-x \u0026lt;--用户st的权限 group::rwx \u0026lt;--组名栏是空的，说明是所属组的权限 mask::rwx \u0026lt;--mask权限 other::--- \u0026lt;--其他人的权限 可以看到，通过设定 ACL 权限，我们可以单独给 st 用户分配 r-x 权限，而无需给 st 用户设定任何身份。\n同样的道理，也可以给用户组设定 ACL 权限，例如：\n[root@localhost /]# groupadd tgroup2 #添加新群组 [root@localhost /]# setfacl -m g:tgroup2:rwx project #为组tgroup2纷配ACL权限 [root@localhost /]# ll -d project drwxrwx---+ 2 root tgroup 4096 1月19 04:21 project #属组并没有更改 [root@localhost /]# getfacl project #file: project #owner: root #group: tgroup user::rwx user:st:r-x group::rwx group:tgroup2:rwx \u0026lt;-用户组tgroup2拥有了rwx权限 mask::rwx other::--- setfacl -d：设定默认 ACL 权限 既然已经对 project 目录设定了 ACL 权限，那么，如果在这个目录中新建一些子文件和子目录，这些文件是否会继承父目录的 ACL 权限呢？执行以下命令进行验证：\n[root@localhost /]# cd project [root@localhost project]# touch abc [root@localhost project]# mkdir d1 #在/project目录中新建了abc文件和d1目录 [root@localhost project]#ll 总用量4 -rw-r--r-- 1 root root 01月19 05:20 abc drwxr-xr-x 2 root root 4096 1月19 05:20 d1 可以看到，这两个新建立的文件权限位后面并没有 \u0026ldquo;+\u0026quot;，表示它们没有继承 ACL 权限。这说明，后建立的子文件或子目录，并不会继承父目录的 ACL 权限。\n当然，我们可以手工给这两个文件分配 ACL 权限，但是如果在目录中再新建文件，都要手工指定，则显得过于麻烦。这时就需要用到默认 ACL 权限。\n默认 ACL 权限的作用是，如果给父目录设定了默认 ACL 权限，那么父目录中所有新建的子文件都会继承父目录的 ACL 权限。需要注意的是，默认 ACL 权限只对目录生效。\n例如，给 project 文件设定 st 用户访问 rx 的默认 ACL 权限，可执行如下指令：\n[root@localhost /]# setfacl -m d:u:st:rx project [root@localhost project]# getfacl project # file: project # owner: root # group: tgroup user:: rwx user:st:r-x group::rwx group:tgroup2:rwx mask::rwx other::--- default:user::rwx \u0026lt;--多出了default字段 default:user:st:r-x default:group::rwx default:mask::rwx default:other::--- [root@localhost /]# cd project [root@localhost project]# touch bcd [root@localhost project]# mkdir d2 #新建子文件和子目录 [root@localhost project]# ll 总用量8 -rw-r--r-- 1 root root 01月19 05:20 abc -rw-rw----+ 1 root root 01月19 05:33 bcd drwxr-xr-x 2 root root 4096 1月19 05:20 d1 drwxrwx---+ 2 root root 4096 1月19 05:33 d2 #新建的bcd和d2已经继承了父目录的ACL权限 大家发现了吗？原先的 abc 和 d1 还是没有 ACL 权限，因为默认 ACL 权限是针对新建立的文件生效的。\n对目录设定的默认 ACL 权限，可直接使用 setfacl -k 命令删除。例如：\n[root@localhost /]# setfacl -k project 通过此命令，即可删除 project 目录的默认 ACL 权限，读者可自行通过 getfacl 命令查看。 setfacl -R：设定递归 ACL 权限 递归 ACL 权限指的是父目录在设定 ACL 权限时，所有的子文件和子目录也会拥有相同的 ACL 权限。\n例如，给 project 目录设定 st 用户访问权限为 rx 的递归 ACL 权限，执行命令如下：\n[root@localhost project]# setfacl -m u:st:rx -R project [root@localhost project]# ll 总用量 8 -rw-r-xr--+ 1 root root 01月19 05:20 abc -rw-rwx--+ 1 root root 01月19 05:33 bcd drwxr-xr-x+ 2 root root 4096 1月19 05:20 d1 drwxrwx---+ 2 root root 4096 1月19 05:33 d2 #abc和d1也拥有了ACL权限 注意，默认 ACL 权限指的是针对父目录中后续建立的文件和目录会继承父目录的 ACL 权限；递归 ACL 权限指的是针对父目录中已经存在的所有子文件和子目录会继承父目录的 ACL 权限。 setfacl -x：删除指定的 ACL 权限 使用 setfacl -x 命令，可以删除指定的 ACL 权限，例如，删除前面建立的 st 用户对 project 目录的 ACL 权限，执行命令如下：\n[root@localhost /]# setfacl -x u:st project #删除指定用户和用户组的ACL权限 [root@localhost /]# getfacl project # file:project # owner: root # group: tgroup user::rwx group::rwx group:tgroup2:rwx mask::rwx other::--- #st用户的权限已被删除 setfacl -b：删除指定文件的所有 ACL 权限 此命令可删除所有与指定文件或目录相关的 ACL 权限。例如，现在我们删除一切与 project 目录相关的 ACL 权限，执行命令如下：\n[root@localhost /]# setfacl -b project #会删除文件的所有ACL权限 [root@localhost /]# getfacl project #file: project #owner: root # group: tgroup user::rwx group::rwx other::--- #所有ACL权限已被删除 9. Linux mask有效权限详解 前面，我们已经学习如何使用 setfacl 和 getfacl 为用户或群组添加针对某目录或文件的 ACL 权限。例如：\n[root@localhost /]# getfacl project #file: project \u0026lt;-文件名 #owner: root \u0026lt;-文件的属主 #group: tgroup \u0026lt;-文件的属组 user::rwx \u0026lt;-用户名栏是空的，说明是所有者的权限 group::rwx \u0026lt;-组名栏是空的，说明是所属组的权限 other::--- \u0026lt;-其他人的权限 [root@localhost ~]# setfacl -m u:st:rx /project #给用户st设定针对project目录的rx权限 [root@localhost /]# getfacl project #file: project  #owner: root #group: tgroup  user::rwx user:st:r-x \u0026lt;-用户 st 的权限 group::rwx mask::rwx \u0026lt;-mask 权限 other::--- 对比添加 ACL 权限前后 getfacl 命令的输出信息，后者多了 2 行信息，一行是我们对 st 用户设定的 r-x 权限，另一行就是 mask 权限。\nmask 权限，指的是用户或群组能拥有的最大 ACL 权限，也就是说，给用户或群组设定的 ACL 权限不能超过 mask 规定的权限范围，超出部分做无效处理。\n举个例子，如果像上面命令那样，给 st 用户赋予访问 project 目录的 r-x 权限，此时并不能说明 st 用户就拥有了对该目录的读和访问权限，还需要和 mask 权限对比，r-x 确实是在 rwx 范围内，这时才能说 st 用户拥有 r-x 权限。 需要注意的是，这里将权限进行对比的过程，实则是将两权限做“按位相与”运算，最终得出的值，即为 st 用户有效的 ACL 权限。这里以读（r）权限为例，做相与操作的结果如表 1 所示：\n表 1 读权限做相与操作\n   A B and     r r r   r - -   - r -   - - -    但是，如果把 mask 权限改为 r\u0026ndash;，再和 st 用户的权限 r-x 比对（r\u0026ndash; 和 r-w 做与运算），由于 r-w 超出 r\u0026ndash; 的权限范围，因此 st 用户最终只有 r 权限，手动赋予的 w 权限无效。这就是在设定 ACL 权限时 mask 权限的作用。\n大家可以这样理解 mask 权限的功能，它将用户或群组所设定的 ACL 权限限制在 mask 规定的范围内，超出部分直接失效。\nmask 权限可以使用 setfacl 命令手动更改，比如，更改 project 目录 mask 权限值为 r-x，可执行如下命令：\n[root@localhost ~]# setfacl -m m:rx /project #设定mask权限为r-x，使用\u0026#34;m:权限\u0026#34;格式 [root@localhost ~]# getfacl /project #file：project #owner：root #group：tgroup user::rwx group::rwx mask::r-x \u0026lt;--mask权限变为r-x other::--- 不过，我们一般不更改 mask 权限，只要赋予 mask 最大权限（也就是 rwx），则给用户或群组设定的 ACL 权限本身就是有效的。\n10. Linux SetUID（SUID）文件特殊权限用法详解 在讲解《权限位》一节时提到过，其实除了 rwx 权限，还会用到 s 权限，例如：\n[root@localhost ~]# ls -l /usr/bin/passwd -rwsr-xr-x. 1 root root 22984 Jan 7 2007 /usr/bin/passwd 可以看到，原本表示文件所有者权限中的 x 权限位，却出现了 s 权限，此种权限通常称为 SetUID，简称 SUID 特殊权限。\nSUID 特殊权限仅适用于可执行文件，所具有的功能是，只要用户对设有 SUID 的文件有执行权限，那么当用户执行此文件时，会以文件所有者的身份去执行此文件，一旦文件执行结束，身份的切换也随之消失。\n举一个例子，我们都知道，Linux 系统中所有用户的密码数据都记录在 /etc/shadow 这个文件中，通过 ll /etc/shadow 命令可以看到，此文件的权限是 0（\u0026mdash;\u0026mdash;\u0026mdash;），也就是说，普通用户对此文件没有任何操作权限。\n这就会产生一个问题，为什么普通用户可以使用 passwd 命令修改自己的密码呢？\n本节开头已经显示了 passwd 命令的权限配置，可以看到，此命令拥有 SUID 特殊权限，而且其他人对此文件也有执行权限，这就意味着，任何一个用户都可以用文件所有者，也就是 root 的身份去执行 passwd 命令。 Linux 系统中，绝对多数命令的文件所有者默认都是 root。\n换句话说，当普通用户使用 passwd 命令尝试更改自己的密码时，实际上是在以 root 的身份执行passwd命令，正因为 root 可以将密码写入 /etc/shadow 文件，所以普通用户也能做到。只不过，一旦命令执行完成，普通用户所具有的 root身份也随之消失。\n如果我们手动将 /usr/bin/passwd 文件的 SUID 权限取消，会发生什么呢？观察如下命令的执行过程：\n[root@localhost ~]# chmod u-s /usr/bin/passwd #属主取消SetUID权限 [root@localhost ~]# ll /usr/bin/passwd -rwxr-xr-x. 1 root root 30768 Feb 22 2012 /usr/bin/passwd [root@localhost ~]# su - lamp [lamp@localhost ~]$ passwd Changing password for user lamp. Changing password for user. (current) UNIX password: #看起来没有什么问题 New passwor: Retype new password: password:Authentication token manipulation error \u0026lt;--鉴定令牌操作错误 #最后密码没有生效 显然，虽然用户有执行 passwd 命令的权限，但无修改 /etc/shadow 文件的权限，因此最终密码修改失败。 注意，实验完成后，一定要再把 /usr/bin/passwd 文件的 SetUID 权限加上。\n那么，普通用户可以使用 cat 命令查看 /etc/shadow 文件吗？答案的否定的，因为 cat 不具有 SUID 权限，因此普通用户在执行 cat /etc/shadow 命令时，无法以 root 的身份，只能以普通用户的身份，因此无法成功读取。\n我们可以使用下面这张图来描述上述过程： 由此，我们可以总结出，SUID 特殊权限具有如下特点： 只有可执行文件才能设定 SetUID 权限，对目录设定 SUID，是无效的。 用户要对该文件拥有 x（执行）权限。 用户在执行该文件时，会以文件所有者的身份执行。 SetUID 权限只在文件执行过程中有效，一旦执行完毕，身份的切换也随之消失。\n11. SetUID（SUID）千万不要胡乱使用！ SetUID权限设置不当，会给 Linux 系统造成重大安全隐患。\n前面的例子中，我们试验了将 passwd 命令取消 SUID 权限，这会导致 passwd 命令的功能失效。那么，如果我们手动给默认无 SetUID 权限的系统命令赋予 SetUID 权限，会出现什么情况呢？\n比如说，我们尝试给 Vim 赋予 SetUID 权限：\n[root@localhost ~]# chmod u+s /usr/bin/vim [root@localhost ~]# ll /usr/bin/vim -rwsr-xr-x. 1 root root 1847752 Apr 5 2012 /usr/bin/vim 此时你会发现，即便是普通用户使用 vim 命令，都会暂时获得 root 的身份和权限，例如，很多原本普通用户不能查看和修改的文件，竟然可以查看了，以 /etc/passwd 和 /etc/shadow 文件为例，普通用户也可以将自己的 UID 手动修改为 0，这意味着，此用户升级成为了超级用户。除此之外，普通用户还可以修改例如 /etc/inittab 和 /etc/fstab 这样重要的系统文件，可以轻易地使系统瘫痪。\n其实，任何只有管理员可以执行的命令，如果被赋予了 SetUID 权限，那么后果都是灾难性的。普通用户可以随时重启服务器、随时关闭看得不顺眼的服务、随时添加其他普通用户的服务器，可以想象是什么样子。所以，SetUID 权限不能随便设置。\n有读者可能会问，如何防止他人（例如黑客）对 SetUID 权限的恶意篡改呢？这里，给大家提供以下几点建议： 关键目录要严格控制写权限，比如 \u0026ldquo;/\u0026quot;、\u0026quot;/usr\u0026rdquo; 等。 用户的密码设置要严格遵守密码规范。 对系统中默认应该有 SetUID 权限的文件制作一张列表，定时检査有没有列表之外的文件被设置了 SetUID 权限。\n前面 2 点不再做过多解释，这里就最后一点，给大家提供一个脚本，仅供参考。\n首先，在服务器第一次安装完成后，马上查找系统中所有拥有 SetUID 和 SetGID 权限的文件，把它们记录下来，作为扫描的参考模板。如果某次扫描的结果和本次保存下来的模板不一致，就说明有文件被修改了 SetUID 和 SetGID 权限。命令如下：\n[root@localhost ~]# find / -perm -4000 -o -perm -2000 \u0026gt; /root/suid.list #-perm安装权限査找。-4000对应的是SetUID权限，-2000对应的是SetGID权限 #-o是逻辑或\u0026#34;or\u0026#34;的意思。并把命令搜索的结果放在/root/suid.list文件中 接下来，只要定时扫描系统，然后和模板文件比对就可以了。脚本如下：\n[root@localhost ~]#vi suidcheck.sh #!/bin/bash find / -perm -4000 -o -perm -2000 \u0026gt; /tmp/setuid.check #搜索系统中所有拥有SetUID和SetGID权限的文件，并保存到临时目录中 for i in $(cat /tmp/setuid.check) #循环，每次循环都取出临时文件中的文件名 do grep $i /root/suid.list \u0026gt; /dev/null #比对这个文件名是否在模板文件中 if [\u0026#34;$?\u0026#34;!=\u0026#34;o\u0026#34;] #检测测上一条命令的返回值，如果不为0，则证明上一条命令报错 then echo \u0026#34;$iisn\u0026#39;t in listfile! \u0026#34; \u0026gt;\u0026gt;/root/suid_log_$(date +%F) #如果文件名不在模板文件中，则输出错误信息，并把报错写入日志中 fi done rm -rf/tmp/setuid.check #删除临时文件 [root@localhost ~]# chmod u+s /bin/vi #手工给vi加入SetUID权限 [root@localhost ~]# ./suidcheck.sh #执行检测脚本 [root@localhost ~]# cat suid_log_2013-01-20 /bin/vi isn\u0026#39;t in listfile! #报错了，vi不在模板文件中。代表vi被修改了SetUID权限 这个脚本成功的关键在于模板文件是否正常。所以一定要安装完系统就马上建立模板文件，并保证模板文件的安全。\n注意，除非特殊情况，否则不要手工修改 SetUID 和 SetGID 权限，这样做非常不安全。而且就算我们做实验修改了 SetUID 和 SetGID 权限，也要马上修改回来，以免造成安全隐患。 SetGID 特殊权限的相关内容，下节会做详细介绍。\n12. Linux SetGID（SGID）文件特殊权限用法详解 前面学习了 SetUID，那么，什么是 SetGID 呢？很简单，当 s 权限位于所属组的 x 权限位时，就被称为 SetGID，简称 SGID 特殊权限。例如：\n[root@localhost ~]# ll /usr/bin/locate -rwx--s--x. 1 root slocate 35612 8月24 2010 /usr/bin/locate 与 SUID 不同的是，SGID 既可以对文件进行配置，也可以对目录进行配置。 SetGID（SGID）对文件的作用 同 SUID 类似，对于文件来说，SGID 具有如下几个特点： SGID 只针对可执行文件有效，换句话说，只有可执行文件才可以被赋予 SGID 权限，普通文件赋予 SGID 没有意义。 用户需要对此可执行文件有 x 权限； 用户在执行具有 SGID 权限的可执行文件时，用户的群组身份会变为文件所属群组； SGID 权限赋予用户改变组身份的效果，只在可执行文件运行过程中有效； 其实，SGID 和 SUID 的不同之处就在于，SUID 赋予用户的是文件所有者的权限，而 SGID 赋予用户的是文件所属组的权限，就这么简单。\n就以本节开头的 locate 命令为例，可以看到，/usr/bin/locate 文件被赋予了 SGID 的特殊权限，这就意味着，当普通用户使用 locate 命令时，该用户的所属组会直接变为 locate 命令的所属组，也就是 slocate。\n我们知道，locate 命令是用于在系统中按照文件名查找符合条件的文件的，当执行搜索操作时，它会通过搜索 /var/lib/mlocate/mlocate.db 这个数据库中的数据找到答案，我们来看看此数据库的权限：\n[root@localhost ~]# ll /var/lib/mlocate/mlocate.db -rw-r-----. 1 root slocate 1838850 1月20 04:29 /var/lib/mlocate/mlocate.db 可以看到，mlocate.db 文件的所属组为 slocate，虽然对文件只拥有 r 权限，但对于普通用户执行 locate 命令来说，已经足够了。一方面，普通用户对 locate命令拥有执行权限，其次，locate 命令拥有 SGID 权限，这使得普通用户在执行 locate 命令时，所属组身份会变为 slocate，而 slocate 对 mlocate.db 数据库文件拥有 r 权限，所以即便是普通用户，也可以成功执行 locate 命令。 再次强调，无论是 SUID，还是 SGID，它们对用户身份的转换，只有在命令执行的过程中有效，一旦命令执行完毕，身份转换也随之失效。\nSetGID（SGID）对目录的作用 事实上，SGID 也能作用于目录，且这种用法很常见。\n当一个目录被赋予 SGID 权限后，进入此目录的普通用户，其有效群组会变为该目录的所属组，会就使得用户在创建文件（或目录）时，该文件（或目录）的所属组将不再是用户的所属组，而使用的是目录的所属组。\n也就是说，只有当普通用户对具有 SGID 权限的目录有 rwx 权限时，SGID 的功能才能完全发挥。比如说，如果用户对该目录仅有 rx 权限，则用户进入此目录后，虽然其有效群组变为此目录的所属组，但由于没有 x 权限，用户无法在目录中创建文件或目录，SGID 权限也就无法发挥它的作用。\n举个例子：\n[root@localhost ~]# cd /tmp #进入临时目录做此实验。因为只有临时目录才允许普通用户修改 [root@localhost tmp]# mkdir dtest #建立测试目录 [root@localhost tmp]# chmod g+s dtest #给测试目录赋予SetGID权限 [root@localhost tmp]# ll -d dtest drwxr-sr-x 2 root root 4096 Apr 19 06:04 dtest #SetGID权限已经生效 [root@localhost tmp]# chmod 777 dtest #给测试目录赋予777权限，让普通用户可以写 [root@localhost tmp]# su - lamp [lamp@localhost ~]# grep lamp /etc/passwd /etc/group /etc/passwd:lamp:x:501:501::/home/lamp:/bin/bash /etc/group:lamp:x:501: #切换成普通用户lamp，此用户的所属群组为 lamp [lamp@localhost ~]$ cd /tmp/dtest/ #普通用户进入测试目录 [lamp@localhost dtest]$ touch abc [lamp@localhost dtest]$ mkdir zimulu #在此目录中创建新的文件 abc 和子目录 zimulu [lamp@localhost dtest]$ ll total 0 -rw-rw-r--. 1 lamp root 0 Apr 19 06:07 abc drwxrwsr-x. 2 lamp root 40 Apr 19 06:07 zimulu 可以看到，虽然是 lamp 用户创建的 abc 文件和 zimulu 目录，但它们的所属组都不是 lamp（lamp 用户的所属组），而是 root（dtest 目录的所属组）。\n13. Linux Stick BIT（SBIT）文件特殊权限用法详解 Sticky BIT，简称 SBIT 特殊权限，可意为粘着位、粘滞位、防删除位等。\nSBIT 权限仅对目录有效，一旦目录设定了 SBIT 权限，则用户在此目录下创建的文件或目录，就只有自己和 root 才有权利修改或删除该文件。\n也就是说，当甲用户以目录所属组或其他人的身份进入 A 目录时，如果甲对该目录有 w 权限，则表示对于 A 目录中任何用户创建的文件或子目录，甲都可以进行修改甚至删除等操作。但是，如果 A 目录设定有 SBIT 权限，那就大不一样啦，甲用户只能操作自己创建的文件或目录，而无法修改甚至删除其他用户创建的文件或目录。\n举个例子，Linux 系统中，存储临时文件的 /tmp 目录就设定有 SBIT 权限：\n[root@localhost ~]# ll -d /tmp drwxrwxrwt. 4 root root 4096 Apr 19 06:17 /tmp 可以看到，在其他人身份的权限设定中，原来的 x 权限位被 t 权限占用了，这就表示此目录拥有 SBIT 权限。通过下面一系列的命令操作，我们来具体看看 SBIT 权限对 /tmp 目录的作用。 [root@localhost ~]# useradd lamp [root@localhost ~]# useradd lamp1 #建立测试用户lamp和lamp1，省略设置密码过程 [root@localhost ~]# su -lamp #切换为lamp用户 [lamp@localhost ~]$ cd /tmp [lamp@localhost tmp]$ touch ftest #建立测试文件 [lamp@localhost tmp]$ll ftest -rw-rw-r-- 1 lamp lamp Apr 19 06:36 ftest [lamp@localhost tmp]$ su - lamp1 #省略输入lamp1用户密码的过程，切换成lamp1用户 [lamp1 @localhost ~]$ cd /tmp/ [lamp1 @localhost tmp]$ rm -rf ftest rm:cannot remove \u0026#39;ftest\u0026#39;:Operation not permitted 可以看到，虽然 /tmp 目录的权限设定是 777，但由于其具有 SBIT 权限，因此 lamp 用户在此目录创建的文件 ftest，lamp1 用户删除失败。\n14. Linux文件特殊权限（SUID、SGID和SBIT）设置详解 前面已经学习 SUID、SGID、SBIT 特殊权限，以及各自的含义和功能，那么，如何给文件或目录手动设定这些特殊权限呢？\n还是要依赖 chmod 命令。我们知道，使用 chmod 命令给文件或目录设定权限，有 2 种方式，分别是使用数字形式和字母形式。例如：\n#数字形式 [root@localhost ~]# chmod 755 ftest #字母形式 [root@localhost ~]# chmod u=rwx,go=rx ftest 给文件或目录设定 SUID、SGID 和 SBIT 特殊权限，也可以使用这 2 种形式。\n我们知道，给 chmod 命令传递 3 个数字，即可实现给文件或目录设定普通权限。比如说，\u0026ldquo;755\u0026rdquo; 表示所有者拥有 rwx 权限，所属组拥有 rx 权限，其他人拥有 tx 权限。\n给文件或目录设定特殊权限，只需在这 3 个数字之前增加一个数字位，用来放置给文件或目录设定的特殊权限，就这么简单。\n因此，我们有必要知道 SUID、SGID、SBIT 分别对应的数字，如下所示：\n 4 \u0026ndash;\u0026gt; SUID 2 \u0026ndash;\u0026gt; SGID 1 \u0026ndash;\u0026gt; SBIT  举个例子，如果要将一个文件权限设置为 -rwsr-xr-x，怎么办呢？此文件的普通权限为 755，另外，此文件还有 SUID 权限，因此只需在 755 的前面，加上 SUID 对应的数字 4 即可。也就是说，只需执行chmod 4755 文件名命令，就完成了-rwsr-xr-x 权限的设定。 关于 -rwsr-xr-x 的普通权限是 755，你可以这样理解，标记有 s 和 t 的权限位，隐藏有 x 权限，对此，本节后续会给出更详细的解释。\n同样的道理，如果某文件拥有 SUID 和 SGID 权限，则只需要给 chmod 命令传递 6\u0026mdash;（- 表示数字）即可；如果某目录拥有 SGID 和 SBIT，只需要给 chmod 命令传递 3\u0026mdash; 即可。\n注意，不同的特殊权限，作用的对象是不同的，SUID 只对可执行文件有效；SGID 对可执行文件和目录都有效；SBIT 只对目录有效。当然，你也可以给文件设置 7\u0026mdash;，也就是将 SUID、SGID、SBIT赋予一个文件或目录，例如：\n[root@localhost ~]# chmod 7777 ftest #一次赋予SetUID、SetGID和SBIT权限 [root@localhost ~]# ll ftest -rwsrwsrwt. 1 root root Apr 19 23:54 ftest 执行过程虽然没有报错，但这样做，没有任何实际意义。\n除了赋予 chmod 命令 4 个数字设定特殊权限，还可以使用字母的形式。例如，可以通过 \u0026ldquo;u+s\u0026rdquo; 给文件赋予 SUID 权限；通过 \u0026ldquo;g+s\u0026rdquo; 给文件或目录赋予 SGID 权限；通过 \u0026ldquo;o+t\u0026rdquo; 给目录赋予 SBIT 权限。\n举一个例子：\n[root@localhost ~]#chmod u+s, g+s, o+t ftest #设置特殊权限 [root@localhost ~]# ll ftest -rwsr-sr-t. 1 root root Apr 19 23:54 ftest [root@localhost ~]# chmod u-s, g-s, o-t ftest #取消特殊权限 [root@localhost ~]# ll ftest -rwxr-xr-x. 1 root root Apr 19 23:54 ftest 例子中，通过字母的形式成功给 ftest 文件赋予了 3 种特殊权限，此做法仅为验证字母形式的可行性，对 ftest 文件来说，并无实际意义。\n细心的读者可能发现这样一个问题，使用 chmod 命令给文件或目录赋予特殊权限时，原文件或目录中存在的 x 权限会被替换成 s 或 t，而当我们使用 chmod 命令消除文件或目录的特殊权限时，原本消失的 x 权限又会显现出来。\n这是因为，无论是 SUID、SGID 还是 SBIT，它们只针对具有 x 权限的文件或目录有效。没有 x 权限的文件或目录，即便赋予特殊权限，也无法发挥它们的功能，没有任何意义。\n例如，我们就是要给不具有 x 权限的文件或目录赋予特殊权限，看看有什么效果：\n[root@localhost ~]# chmod 7666 ftest [root@localhost ~]# ll ftest -rwSrwSrwT. 1 root root Apr 23:54 ftest 可以看到，相应的权限位会被标记为 S（大写）和 T（大写），指的就是设置的 SUID、SGID 和 SBIT 权限没有意义。\n15. Linux chattr命令详解：修改文件系统的权限属性 管理 Linux 系统中的文件和目录，除了可以设定普通权限和特殊权限外，还可以利用文件和目录具有的一些隐藏属性。\nchattr 命令，专门用来修改文件或目录的隐藏属性，只有 root 用户可以使用。该命令的基本格式为：\n[root@localhost ~]# chattr [+-=] [属性] 文件或目录名  表示给文件或目录添加属性，- 表示移除文件或目录拥有的某些属性，= 表示给文件或目录设定一些属性。  表 1 列出了常用的一些属性及功能。 表 1 chattr 命令常用的属性选项及功能\n   属性选项 功能     i 如果对文件设置 i 属性，那么不允许对文件进行删除、改名，也不能添加和修改数据；如果对目录设置 i 属性，那么只能修改目录下文件中的数据，但不允许建立和删除文件；   a 如果对文件设置 a 属性，那么只能在文件中増加数据，但是不能删除和修改数据； 如果对目录设置 a 属性，那么只允许在目录中建立和修改文件，但是不允许删除文件；   u 设置此属性的文件或目录，在删除时，其内容会被保存，以保证后期能够恢复，常用来防止意外删除文件或目录。   s 和 u 相反，删除文件或目录时，会被彻底删除（直接从硬盘上删除，然后用 0 填充所占用的区域），不可恢复。    【例 1】 给文件赋予 i 属性。\n[root@localhost ~]# touch ftest #建立测试文件 [root@localhost ~]# chattr +i ftest [root@localhost ~]# rm -rf ftest rm:cannot remove \u0026#39;ftest\u0026#39;:Operation not permitted #无法删除\u0026#34;ftesr\u0026#34;，操作不允许 #被赋予i属性后，root不能删除 [root@localhost ~]# echo 111\u0026gt;\u0026gt;ftest bash:ftest:Permission denied #权限不够，不能修改文件中的数据 可以看到，设置有 i 属性的文件，即便是 root 用户，也无法删除和修改数据。\n【例 2】为目录赋予 i 属性。\n[root@localhost ~]# mkdir dtest #建立测试目录 [root@localhost dtest]# touch dtest/abc #再建立一个测试文件abc [root@localhost ~]# chattr +i dtest #给目录赋予i属性 [root@localhost ~]# cd dtest [root@localhost dtest]# touch bed touch: cannot touch \u0026#39;bed\u0026#39;:Permission denied #无法创建\u0026#34;bcd\u0026#34;，权限不够，dtest目录不能新建文件 [root@localhost dtest]# echo 11\u0026gt;\u0026gt;abc [root@localhost dtest]# cat abc 11 #可以修改文件内容 [root@localhost dtest]# rm -rf abc rm: cannot remove \u0026#39;abc\u0026#39;: Permission denied #无法删除\u0026#34;abc\u0026#34;，权限不够 一旦给目录设置 i 属性，即使是 root 用户，也无法在目录内部新建或删除文件，但可以修改文件内容。 给设置有 i 属性的文件删除此属性也很简单，只需将 chattr 命令中 + 改为 - 即可。\n【例 3】演示 a 属性的作用。 假设有这样一种应用，我们每天自动实现把服务器的日志备份到指定目录，备份目录可设置 a 属性，变为只可创建文件而不可删除。命令如下：\n[root@localhost ~]# mkdir -p /back/log #建立备份目录 [root@localhost ~]# chattr +a /back/log #赋予a属性 [root@localhost ~]# cp /var/log/messages /back/log #可以复制文件和新建文件到指定目录中 [root@localhost ~]# rm -rf /back/log/messages rm: cannot remove \u0026#39;/back/log/messages\u0026#39;: Permission denied #无法删除 /back/log/messages，操作不允许 注意，通常情况下，不要使用 chattr 命令修改 /、/dev/、/tmp/、/var/ 等目录的隐藏属性，很容易导致系统无法启动。另外，chatrr 命令常与 lsattr 命令合用，前者修改文件或目录的隐藏属性，后者用于查看是否修改成功。有关 lsattr 命令，放到下节讲解。\n16. Linux lsattr命令：查看文件系统属性 使用 chattr 命令配置文件或目录的隐藏属性后，可以使用 lsattr 命令查看。\nlsattr 命令，用于显示文件或目录的隐藏属性，其基本格式如下：\n[root@localhost ~]# lsattr [选项] 文件或目录名 常用选项有以下 3 种：\n -a：后面不带文件或目录名，表示显示所有文件和目录（包括隐藏文件和目录） -d：如果目标是目录，只会列出目录本身的隐藏属性，而不会列出所含文件或子目录的隐藏属性信息； -R：和 -d 恰好相反，作用于目录时，会连同子目录的隐藏信息数据也一并显示出来。  【例 1】\n[root@localhost ~]# touch attrtest -----------e- attrtest [root@localhost ~]# chattr +aij attrtest [root@localhost ~]# lsattr attrtest ----ia---j-e- attrtest 注意，不使用任何选项，仅用于显示文件的隐藏信息，不适用于目录。\n【例 2】\n[root@localhost ~]#lsattr -a -----------e- ./. ------------- ./.. -----------e- ./.gconfd -----------e- ./.bashrc 【例 3】\n[root@localhost ~]#lsattr -d /back/log -----a------e- /back/log #查看/back/log目录，其拥有a和e属性 17. Linux sudo命令用法详解：系统权限管理 我们知道，使用 su 命令可以让普通用户切换到 root 身份去执行某些特权命令，但存在一些问题，比如说： 仅仅为了一个特权操作就直接赋予普通用户控制系统的完整权限； 当多人使用同一台主机时，如果大家都要使用 su 命令切换到 root 身份，那势必就需要 root 的密码，这就导致很多人都知道 root 的密码；\n考虑到使用 su 命令可能对系统安装造成的隐患，最常见的解决方法是使用 sudo 命令，此命令也可以让你切换至其他用户的身份去执行命令。\n相对于使用 su 命令还需要新切换用户的密码，sudo 命令的运行只需要知道自己的密码即可，甚至于，我们可以通过手动修改 sudo 的配置文件，使其无需任何密码即可运行。\nsudo 命令默认只有 root 用户可以运行，该命令的基本格式为：\n[root@localhost ~]# sudo [-b] [-u 新使用者账号] 要执行的命令 常用的选项与参数：\n -b ：将后续的命令放到背景中让系统自行运行，不对当前的 shell 环境产生影响。 -u ：后面可以接欲切换的用户名，若无此项则代表切换身份为 root 。 -l：此选项的用法为 sudo -l，用于显示当前用户可以用 sudo 执行那些命令。  【例 1】\n[root@localhost ~]# grep sshd /etc/passwd sshd:x:74:74:privilege-separated SSH:/var/empty/sshd:/sbin.nologin [root@localhost ~]# sudo -u sshd touch /tmp/mysshd [root@localhost ~]# ll /tmp/mysshd -rw-r--r-- 1 sshd sshd 0 Feb 28 17:42 /tmp/mysshd 本例中，无法使用 su - sshd 的方式成功切换到 sshd 账户中，因为此用户的默认 Shell 是 /sbin/nologin。这时就显现出 sudo 的优势，我们可以使用 sudo 以 sshd 的身份在 /tmp 目录下创建 mysshd 文件，可以看到，新创建的 mysshd 文件的所有者确实是 sshd。\n【例 2】\n[root@localhost ~]# sudo -u vbird1 sh -c \u0026#34;mkdir ~vbird1/www; cd ~vbird1/www; \\ \u0026gt; echo \u0026#39;This is index.html file\u0026#39; \u0026gt; index.html\u0026#34; [root@localhost ~]# ll -a ~vbird1/www drwxr-xr-x 2 vbird1 vbird1 4096 Feb 28 17:51 . drwx------ 5 vbird1 vbird1 4096 Feb 28 17:51 .. -rw-r--r-- 1 vbird1 vbird1 24 Feb 28 17:51 index.html 这个例子中，使用 sudo 命令切换至 vbird1 身份，并运行 sh -c 的方式来运行一连串的命令。\n前面说过，默认情况下 sudo 命令，只有 root 身份可以使用，那么，如何让普通用户也能使用它呢？\n解决这个问题之前，先给大家分析一下 sudo 命令的执行过程。sudo命令的运行，需经历如下几步： 当用户运行 sudo 命令时，系统会先通过 /etc/sudoers 文件，验证该用户是否有运行 sudo 的权限； 确定用户具有使用 sudo 命令的权限后，还要让用户输入自己的密码进行确认。出于对系统安全性的考虑，如果用户在默认时间内（默认是 5 分钟）不使用 sudo 命令，此后使用时需要再次输入密码； 密码输入成功后，才会执行 sudo 命令后接的命令。\n显然，能否使用 sudo 命令，取决于对 /etc/sudoers 文件的配置（默认情况下，此文件中只配置有 root 用户）。所以接下来，我们学习对 /etc/sudoers 文件进行合理的修改。 sudo命令的配置文件/etc/sudoers 修改 /etc/sudoers，不建议直接使用 vim，而是使用 visudo。因为修改 /etc/sudoers 文件需遵循一定的语法规则，使用 visudo 的好处就在于，当修改完毕 /etc/sudoers 文件，离开修改页面时，系统会自行检验 /etc/sudoers 文件的语法。\n因此，修改 /etc/sudoers 文件的命令如下：\n[root@localhost ~]# visudo …省略部分输出… root ALL=(ALL) ALL \u0026lt;--大约 76 行的位置 # %wheel ALL=(ALL) ALL \u0026lt;--大约84行的位置 #这两行是系统为我们提供的模板，我们参照它写自己的就可以了 …省略部分输出… 通过 visudo 命令，我们就打开了 /etc/sudoers 文件，可以看到如上显示的 2 行信息，这是系统给我们提供的 2 个模板，分别用于添加用户和群组，使其能够使用 sudo 命令。\n这两行模板的含义分为是：\nroot ALL=(ALL) ALL #用户名 被管理主机的地址=(可使用的身份) 授权命令(绝对路径) #%wheel ALL=(ALL) ALL #%组名 被管理主机的地址=(可使用的身份) 授权命令(绝对路径) 表 1 对以上 2 个模板的各部分进行详细的说明。 表 1 /etc/sudoers 用户和群组模板的含义\n   模块 含义     用户名或群组名 表示系统中的那个用户或群组，可以使用 sudo 这个命令。   被管理主机的地址 用户可以管理指定 IP 地址的服务器。这里如果写 ALL，则代表用户可以管理任何主机；如果写固定 IP，则代表用户可以管理指定的服务器。如果我们在这里写本机的 IP 地址，不代表只允许本机的用户使用指定命令，而是代表指定的用户可以从任何 IP 地址来管理当前服务器。   可使用的身份 就是把来源用户切换成什么身份使用，（ALL）代表可以切换成任意身份。这个字段可以省略。   授权命令 表示 root 把什么命令命令授权给用户，换句话说，可以用切换的身份执行什么命令。需要注意的是，此命令必须使用绝对路径写。默认值是 ALL，表示可以执行任何命令。    【例 3】 授权用户 lamp 可以重启服务器，由 root 用户添加，可以在 /etc/sudoers 模板下添加如下语句：\n[root@localhost ~]# visudo lamp ALL=/sbin/shutdown -r now 注意，这里也可以写多个授权命令，之间用逗号分隔。用户 lamp 可以使用 sudo -l 查看授权的命令列表：\n[root@localhost ~]# su - lamp #切换成lamp用户 [lamp@localhost ~]$ sudo -l [sudo] password for lamp: #需要输入lamp用户的密码 User lamp may run the following commands on this host: (root) /sbin/shutdown -r now 可以看到，lamp 用户拥有了 shutdown -r now 的权限。这时，lamp 用户就可以使用 sudo 执行如下命令重启服务器：\n[lamp@localhost ~]$ sudo /sbin/shutdown -r now 再次强调，授权命令要使用绝对路径（或者把 /sbin 路径导入普通用户 PATH 路径中，不推荐使用此方式），否则无法执行。\n【例 4】 假设现在有 pro1，pro2，pro3 这 3 个用户，还有一个 group 群组，我们可以通过在 /etc/sudoers 文件配置 wheel 群组信息，令这 3 个用户同时拥有管理系统的权限。\n首先，向 /etc/sudoers 文件中添加群组配置信息：\n[root@localhost ~]# visudo ....(前面省略).... %group ALL=(ALL) ALL #在 84 行#wheel这一行后面写入 此配置信息表示，group 这个群组中的所有用户都能够使用 sudo 切换任何身份，执行任何命令。接下来，我们使用 usermod 命令将 pro1 加入 group 群组，看看有什么效果：\n[root@localhost ~]# usermod -a -G group pro1 [pro1@localhost ~]# sudo tail -n 1 /etc/shadow \u0026lt;==注意身份是 pro1 ....(前面省略).... Password: \u0026lt;==输入 pro1 的口令喔！ pro3:$1$GfinyJgZ$9J8IdrBXXMwZIauANg7tW0:14302:0:99999:7::: [pro2@localhost ~]# sudo tail -n 1 /etc/shadow \u0026lt;==注意身份是 pro2 Password: pro2 is not in the sudoers file. This incident will be reported. #此错误信息表示 pro2 不在 /etc/sudoers 的配置中。 可以看到，由于 pro1 加入到了 group 群组，因此 pro1 就可以使用 sudo 命令，而 pro2 不行。同样的道理，如果我们想让 pro3 也可以使用 sudo 命令，不用再修改 /etc/sudoers 文件，只需要将 pro3 加入 group 群组即可。\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/linux%E4%B8%8B%E7%9A%84%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/","summary":"Linux下的权限管理 Linux 系统中为什么需要设定不同的权限，所有用户都直接使用管理员（root）身份不好吗？\n 由于绝大多数用户使用的是个人计算机，使用者一般都是被信任的人（如家人、朋友等）。在这种情况下，大家都可以使用管理员身份直接登录。但在服务器上就不是这种情况了，往往运行的数据越重要（如游戏数据），价值越高（如电子商城数据、银行数据），则服务器中对权限的设定就要越详细，用户的分级也要越明确。\n和 Windows 系统不同，Linux 系统为每个文件都添加了很多的属性，最大的作用就是维护数据的安全。举个简单的例子，在你的 Linux 系统中，和系统服务相关的文件通常只有 root 用户才能读或写，就拿 /etc/shadow 这个文件来说，此文件记录了系统中所有用户的密码数据，非常重要，因此绝不能让任何人读取（否则密码数据会被窃取），只有 root 才可以有读取权限。 此外，如果你有一个软件开发团队，你希望团队中的每个人都可以使用某一些目录下的文件，而非团队的其他人则不予以开放。通过前面章节的学习我们知道，只需要将团队中的所有人加入新的群组，并赋予此群组读写目录的权限，即可实现要求。反之，如果你的目录权限没有做好，就很难防止其他人在你的系统中乱搞。 比如说，本来 root 用户才能做的开关机、ADSL 拨接程序，新增或删除用户等命令，一旦允许任何人拥有这些权限，系统很可能会经常莫名其妙的挂掉。而且，万一 root 用户的密码被其他人获取，他们就可以登录你的系统，从事一些只有 root 用户才能执行的操作，这是绝对不允许发生的。 因此，在服务器上，绝对不是所有的用户都使用 root 身份登录，而要根据不同的工作需要和职位需要，合理分配用户等级和权限等级。\n Linux 系统中，文件或目录的权限信息，可以使用 ls 命令查看，例如：\n[root@localhost ~]# ls -al total 156 drwxr-x---. 4 root root 4096 Sep 8 14:06 . drwxr-xr-x. 23 root root 4096 Sep 8 14:21 .. -rw-------. 1 root root 1474 Sep 4 18:27 anaconda-ks.cfg -rw-------. 1 root root 199 Sep 8 17:14 .","title":"Linux下的权限管理"},{"content":"Linux后台运行程序 使用screen\nscreen介绍 Screen是一个控制台应用程序，允许您在一个窗口中使用多个终端会话。该程序在shell会话中运行，并充当其他终端会话的容器和管理器，类似于窗口管理器管理窗口的方式。\n在许多情况下，创建多个终端窗口是不可能或不理想的。您可能需要在没有运行X服务器的情况下管理多个控制台会话，您可能需要轻松访问许多远程云服务器，或者您可能需要在处理其他任务时监视正在运行的程序的输出。所有需求都可以通过屏幕的强大功能轻松解决。\n安装srceen ubuntu下\nsudo apt-get install screen manjaro下\nsudo pacman -S screen 基本使用方法 使用screen命令打开一个新的窗口，在其中运行你想运行的脚本。\n开始运行后， 按ctrl+ad退出窗口\n使用screen -r重新进入窗口\n附：重定向    命令 说明     command \u0026gt; file 将输出重定向到 file。   command \u0026lt; file 将输入重定向到 file。   command \u0026raquo; file 将输出以追加的方式重定向到 file。   n \u0026gt; file 将文件描述符为 n 的文件重定向到 file。   n \u0026raquo; file 将文件描述符为 n 的文件以追加的方式重定向到 file。   n \u0026gt;\u0026amp; m 将输出文件 m 和 n 合并。   n \u0026lt;\u0026amp; m 将输入文件 m 和 n 合并。   \u0026laquo; tag 将开始标记 tag 和结束标记 tag 之间的内容作为输入。    Edited by Li Chang\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/linux%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F/","summary":"Linux后台运行程序 使用screen\nscreen介绍 Screen是一个控制台应用程序，允许您在一个窗口中使用多个终端会话。该程序在shell会话中运行，并充当其他终端会话的容器和管理器，类似于窗口管理器管理窗口的方式。\n在许多情况下，创建多个终端窗口是不可能或不理想的。您可能需要在没有运行X服务器的情况下管理多个控制台会话，您可能需要轻松访问许多远程云服务器，或者您可能需要在处理其他任务时监视正在运行的程序的输出。所有需求都可以通过屏幕的强大功能轻松解决。\n安装srceen ubuntu下\nsudo apt-get install screen manjaro下\nsudo pacman -S screen 基本使用方法 使用screen命令打开一个新的窗口，在其中运行你想运行的脚本。\n开始运行后， 按ctrl+ad退出窗口\n使用screen -r重新进入窗口\n附：重定向    命令 说明     command \u0026gt; file 将输出重定向到 file。   command \u0026lt; file 将输入重定向到 file。   command \u0026raquo; file 将输出以追加的方式重定向到 file。   n \u0026gt; file 将文件描述符为 n 的文件重定向到 file。   n \u0026raquo; file 将文件描述符为 n 的文件以追加的方式重定向到 file。   n \u0026gt;\u0026amp; m 将输出文件 m 和 n 合并。   n \u0026lt;\u0026amp; m 将输入文件 m 和 n 合并。   \u0026laquo; tag 将开始标记 tag 和结束标记 tag 之间的内容作为输入。    Edited by Li Chang","title":"Linux后台运行程序"},{"content":"manjaro换源 有关manjaro换源的文件：\n/etc/pacman.d/mirrorlist\n网上教程：\nsudo pacman-mirrors -gb testing -c China //选择中国源并更新 sudo pacman -Syyu //更新系统 manjaro更新\npacman -Sc //清空并且下载新数据 pacman-mirrors -gb testing -c China //更新源 or pacman-mirrors -c China -g //更新源 pacman -Syu //更新 pacman -Syy //更新源数据库 pacman -Syyu //安装更新 ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/manjaro%E6%8D%A2%E6%BA%90/","summary":"manjaro换源 有关manjaro换源的文件：\n/etc/pacman.d/mirrorlist\n网上教程：\nsudo pacman-mirrors -gb testing -c China //选择中国源并更新 sudo pacman -Syyu //更新系统 manjaro更新\npacman -Sc //清空并且下载新数据 pacman-mirrors -gb testing -c China //更新源 or pacman-mirrors -c China -g //更新源 pacman -Syu //更新 pacman -Syy //更新源数据库 pacman -Syyu //安装更新 ","title":"manjaro换源"},{"content":"1. 安装mysql sudo apt update sudo apt install mysql-server 2. 配置mysql 运行 security script\nsudo mysql_secure_installation 根据提示进行一些必要的配置\n进入mysql\nsudo mysql 接下来，通过以下命令检查每个 MySQL 用户帐户使用的认证方法：\nSELECT user,authentication_string,plugin,host FROM mysql.user; 在输出中\n+------------------+-------------------------------------------+-----------------------+-----------+ | user | authentication_string | plugin | host | +------------------+-------------------------------------------+-----------------------+-----------+ | root | | auth_socket | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *E6CD266C880D217453293A0247D0142C9CF52730 | mysql_native_password | localhost | +------------------+-------------------------------------------+-----------------------+-----------+ 可以看出，root用户使用插件进行身份验证（进入时不需要输入密码）。如果想要root用户使用密码登陆，可使用如下命令进行配置：\nALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;password\u0026#39;;  将上面的password更改为你想要设置的密码\n 这条命令完成后，使用如下命令重新刷新配置：\nFLUSH PRIVILEGES; 再次检查用户认证方法\nSELECT user,authentication_string,plugin,host FROM mysql.user; 输出为：\n+------------------+-------------------------------------------+-----------------------+-----------+ | user | authentication_string | plugin | host | +------------------+-------------------------------------------+-----------------------+-----------+ | root | *35235E6B287036E68CD22314AFE8D80A0BB26D79 | mysql_native_password | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *E6CD266C880D217453293A0247D0142C9CF52730 | mysql_native_password | localhost | +------------------+-------------------------------------------+-----------------------+-----------+ 3. 新建一个用户 新建一个用户\nCREATE USER \u0026#39;sammy\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;;  localhost表示只允许本地访问，如果想要允许远程访问，可使用\u0026rsquo;user'@'%'， %表示所有网络。\n 更改用户权限\nGRANT ALL PRIVILEGES ON *.* TO \u0026#39;sammy\u0026#39;@\u0026#39;localhost\u0026#39; WITH GRANT OPTION;  这里需要注意的一点是，我们不需要再使用FLUSH PRIVILEGES;来刷新配置，因为只有在grant表上进行过INSERT, UPDATE, 或者DELETE之后才需要用到FLUSH PRIVILEGES;。Because you created a new user, instead of modifying an existing one, FLUSH PRIVILEGES is unnecessary here.\n 退出\nexit 4. Test Mysql 检查mysql服务状态\nsystemctl status mysql.service 输出为：\n● mysql.service - MySQL Community Server Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2021-02-20 16:09:11 CST; 27min ago Process: 31328 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/run/mysqld/mysqld.pid (code=exited, status=0/SUCCESS) Process: 31319 ExecStartPre=/usr/share/mysql/mysql-systemd-start pre (code=exited, status=0/SUCCESS) Main PID: 31330 (mysqld) Tasks: 28 (limit: 2341) CGroup: /system.slice/mysql.service └─31330 /usr/sbin/mysqld --daemonize --pid-file=/run/mysqld/mysqld.pid ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql%E5%AE%89%E8%A3%85linux/","summary":"1. 安装mysql sudo apt update sudo apt install mysql-server 2. 配置mysql 运行 security script\nsudo mysql_secure_installation 根据提示进行一些必要的配置\n进入mysql\nsudo mysql 接下来，通过以下命令检查每个 MySQL 用户帐户使用的认证方法：\nSELECT user,authentication_string,plugin,host FROM mysql.user; 在输出中\n+------------------+-------------------------------------------+-----------------------+-----------+ | user | authentication_string | plugin | host | +------------------+-------------------------------------------+-----------------------+-----------+ | root | | auth_socket | localhost | | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | localhost | | debian-sys-maint | *E6CD266C880D217453293A0247D0142C9CF52730 | mysql_native_password | localhost | +------------------+-------------------------------------------+-----------------------+-----------+ 可以看出，root用户使用插件进行身份验证（进入时不需要输入密码）。如果想要root用户使用密码登陆，可使用如下命令进行配置：","title":"MySql安装和配置"},{"content":"Mysql无法远程访问 在使用navicat远程连接阿里云的时候，出现“2003 can t connect to mysql server on 10061”错误\n经过艰难的谷歌百度stackflow后，发现是3306端口没有监听外部连接，只接收内部ip访问。\n解决方案  首先保证阿里云服务器3306端口开放 使用netstat -ntpl |grep 3306命令查看3306端口状态 tcp 0 0 127.0.0.1:22 0.0.0.0:* LISTEN - 可看出只接收内部访问 打开/etc/mysql/mysql.conf.d/mysqld.cnf(网上大部分说是:/etc/mysql/my.cnf) 将bind-address = 127.0.0.1改成bind-address = 0.0.0.0 再次使用netstat -ntpl |grep 3306命令查看 tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - 此时3306端口开始监听所有网络访问 **如果是ipv6主机，则改为 bind-address = :: ,表示监听所有网络**  主机\u0026rsquo;xxx.xx.xxx.xxx\u0026rsquo;不允许连接到此MySQL服务器 在进行连接ipv6主机的时候出现了如下问题：django.db.utils.InternalError: (1130, \u0026ldquo;Host \u0026lsquo;2409:8930:1450:316:6179:c54:5901:2f2b\u0026rsquo; is not allowed to connect to this MySQL server\u0026rdquo;) 解决方法如下：\nmysql\u0026gt; CREATE USER \u0026#39;monty\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;some_pass\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;monty\u0026#39;@\u0026#39;localhost\u0026#39; -\u0026gt; WITH GRANT OPTION; mysql\u0026gt; CREATE USER \u0026#39;monty\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;some_pass\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;monty\u0026#39;@\u0026#39;%\u0026#39; -\u0026gt; WITH GRANT OPTION; \u0026lsquo;user\u0026rsquo;@'%\u0026lsquo;后面的百分号代表所有网络\n有时候用root用户还不行，要重新建一个用户 ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql%E6%97%A0%E6%B3%95%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE/","summary":"Mysql无法远程访问 在使用navicat远程连接阿里云的时候，出现“2003 can t connect to mysql server on 10061”错误\n经过艰难的谷歌百度stackflow后，发现是3306端口没有监听外部连接，只接收内部ip访问。\n解决方案  首先保证阿里云服务器3306端口开放 使用netstat -ntpl |grep 3306命令查看3306端口状态 tcp 0 0 127.0.0.1:22 0.0.0.0:* LISTEN - 可看出只接收内部访问 打开/etc/mysql/mysql.conf.d/mysqld.cnf(网上大部分说是:/etc/mysql/my.cnf) 将bind-address = 127.0.0.1改成bind-address = 0.0.0.0 再次使用netstat -ntpl |grep 3306命令查看 tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - 此时3306端口开始监听所有网络访问 **如果是ipv6主机，则改为 bind-address = :: ,表示监听所有网络**  主机\u0026rsquo;xxx.xx.xxx.xxx\u0026rsquo;不允许连接到此MySQL服务器 在进行连接ipv6主机的时候出现了如下问题：django.db.utils.InternalError: (1130, \u0026ldquo;Host \u0026lsquo;2409:8930:1450:316:6179:c54:5901:2f2b\u0026rsquo; is not allowed to connect to this MySQL server\u0026rdquo;) 解决方法如下：\nmysql\u0026gt; CREATE USER \u0026#39;monty\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;some_pass\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON *.","title":"Mysql无法远程访问"},{"content":"Nginx负载均衡配置\u0026ndash;简介  在使用tomcat部署静态网站的时候，由于服务器比较垃圾，所以如果多人同时访问的话，可能会造成卡顿，影响用户体验。所以想到了使用负载均衡。\n 1. 什么是负载均衡 负载平衡是高可用性基础架构的关键组件，通常用于通过在多个服务器之间分配工作负载来提高网站，应用程序，数据库和其他服务的性能和可靠性。\n没有负载平衡的Web基础结构可能如下所示：\n在此示例中，用户直接连接到web服务器yourdomain.com。如果此单个Web服务器出现故障，用户将无法再访问该网站。此外，如果许多用户尝试同时访问服务器并且无法处理负载，则可能会遇到加载时间缓慢或根本无法连接的情况。\n通过在后端引入负载均衡器和至少一个额外的Web服务器，可以减轻此单点故障。通常，所有后端服务器都将提供相同的内容，以便用户无论哪个服务器响应都会收到一致的内容。 在上面说明的示例中，用户访问负载均衡器，负载均衡器将用户的请求转发到后端服务器，后端服务器然后直接响应用户的请求。在这种情况下，单点故障现在是负载平衡器本身。这可以通过引入第二个负载均衡器来缓解.\n2. 负载均衡器可以处理什么样的流量   HTTP - 标准HTTP平衡基于标准HTTP机制定向请求。负载均衡器设置X-Forwarded-For，X-Forwarded-Proto以及X-Forwarded-Port头，提供有关原始请求的后端信息。\n  HTTPS - HTTPS平衡功能与HTTP平衡功能相同，但增加了加密功能。加密以两种方式之一处理：使用SSL直通，一直保持加密到后端，或者使用SSL终止，将解密负担放在负载均衡器上，但将未加密的流量发送到后端。\n  TCP - 对于不使用HTTP或HTTPS的应用程序，也可以平衡TCP流量。例如，数据库集群的流量可以分布在所有服务器上。\n  UDP\u0026ndash;最近，一些负载均衡器增加了对使用UDP的核心互联网协议（如DNS和syslogd）的负载平衡的支持。\n  这些转发规则将定义负载均衡器本身的协议和端口，并将它们映射到负载均衡器将用于将流量路由到后端的协议和端口。\n3. 负载均衡器如何选择后端服务器 负载均衡器根据两个因素的组合选择将请求转发到哪个服务器。他们将首先确保他们可以选择的任何服务器实际上对请求做出适当的响应，然后使用预先配置的规则从该健康池中进行选择。\n3.1 健康检查 负载均衡器应仅将流量转发到“健康”的后端服务器。要监视后端服务器的运行状况，运行状况检查会定期尝试使用转发规则定义的协议和端口连接到后端服务器，以确保服务器正在侦听。如果服务器未通过运行状况检查，因此无法提供请求，则会自动将其从池中删除，并且在再次响应运行状况检查之前，流量将不会转发给它。\n3.2 负载平衡算法 使用的负载平衡算法确定将选择后端中的哪些正常服务器。一些常用的算法是：\n  Round Robin - Round Robin意味着将按顺序选择服务器。负载均衡器将在其列表中为第一个请求选择第一个服务器，然后按顺序向下移动列表，当它到达结尾时从顶部开始。\n  least_conn - least_conn意味着负载均衡器将选择连接最少的服务器，并且当流量导致更长的会话时建议使用。\n  ip_hash：此平衡算法根据客户端的IP地址将请求分发到不同的服务器。前三个八位字节用作决定服务器处理请求的密钥。结果是客户端每次都倾向于由同一服务器提供服务，这有助于会话一致性。\n  hash：此平衡算法主要用于memcached代理。基于任意提供的散列密钥的值来划分服务器。这可以是文本，变量或组合。这是唯一需要用户提供数据的平衡方法，这是应该用于哈希的密钥。\n  管理员可用的算法取决于所使用的特定负载平衡技术。\n3.3 负载平衡器如何处理状态 某些应用程序要求用户继续连接到同一后端服务器。Source算法根据客户端IP信息创建关联。在Web应用程序级别实现此目的的另一种方法是通过粘性会话，其中负载平衡器设置cookie，并且来自该会话的所有请求都定向到同一物理服务器。\n4. 冗余负载均衡器 要将负载均衡器作为单点故障移除，可以将第二个负载均衡器连接到第一个负载均衡器以形成一个集群，其中每个负载均衡器监控其他负载平衡器的运行状况。每个人都具有同样的故障检测和恢复能力。如果主负载均衡器发生故障，DNS必须将用户带到第二个负载均衡器。由于DNS更改可能需要花费大量时间在Internet上传播并自动进行此故障转移，因此许多管理员将使用允许灵活IP地址重新映射的系统，例如浮动IP。按需IP地址重新映射通过提供可在需要时轻松重新映射的静态IP地址，消除了DNS更改中固有的传播和缓存问题。域名可以保持与相同的IP地址关联，而IP地址本身在服务器之间移动。\n这就是使用浮动IP的高可用性基础架构的外观： 使用 1. 一般代理信息 如果您过去仅使用Web服务器进行简单的单服务器配置，您可能想知道为什么需要代理请求。\n从Nginx代理其他服务器的一个原因是能够扩展您的基础架构。构建Nginx是为了同时处理许多并发连接。这使其成为客户联系点的理想选择。服务器可以将请求传递给任意数量的后端服务器，以处理大部分工作，从而在整个基础架构中分散负载。此设计还为您提供了轻松添加后端服务器或根据维护需要将其下载的灵活性。\nhttp代理可能有用的另一个实例是使用可能无法构建的应用程序服务器来直接处理来自生产环境中的客户端的请求。许多框架都包含Web服务器，但大多数框架都不如Nginx等高性能服务器那么强大。将Nginx放在这些服务器之前可以为用户带来更好的体验并提高安全性。\n在Nginx中代理是通过操纵针对Nginx服务器的请求并将其传递给其他服务器以进行实际处理来完成的。请求的结果将传递回Nginx，然后Nginx将信息中继到客户端。此实例中的其他服务器可以是远程计算机，本地服务器，甚至是Nginx中定义的其他虚拟服务器。Nginx代理请求的服务器称为上游服务器。\nNginx可以将请求代理到使用http（s），FastCGI，SCGI和uwsgi或memcached协议通过每种代理类型的单独指令集进行通信的服务器。在本指南中，我们将重点关注http协议。Nginx实例负责传递请求并将任何消息组件按摩成上游服务器可以理解的格式。\n2. 解构基本HTTP代理通行证 最直接的代理类型涉及将请求切换到可以使用http进行通信的单个服务器。这种类型的代理称为通用“代理传递”，由适当命名的proxy_pass指令处理。\n该proxy_pass指令主要在位置上下文中找到。它if在位置上下文和limit_except上下文中的块中也是有效的。当请求与具有proxy_pass指令的位置匹配时，请求将转发到指令给出的URL。\n我们来看一个例子：\n# server context location /match/here { proxy_pass http://example.com; } . . . 在上面的配置代码段中，proxy_pass定义中服务器末尾没有给出URI 。对于符合此模式的定义，客户端请求的URI将按原样传递到上游服务器。\n例如，当/match/here/please此块处理请求时，请求URI将作为发送到example.com服务器http://example.com/match/here/please。\n我们来看看替代方案：\n# server context location /match/here { proxy_pass http://example.com/new/prefix; } . . . 在上面的示例中，代理服务器在end（/new/prefix）上定义了一个URI段。当在proxy_pass定义中给出URI时，在传递期间，请求的与位置定义匹配的部分将被此URI替换。\n例如，/match/here/pleaseNginx服务器上的请求将作为传递给上游服务器http://example.com/new/prefix/please 该/match/here所取代/new/prefix。这是一个要记住的重点。\n有时，这种替换是不可能的。在这些情况下，将proxy_pass忽略定义末尾的URI，并且客户端的原始URI或其他指令修改的URI将传递给上游服务器。\n例如，当使用正则表达式匹配位置时，Nginx无法确定URI的哪个部分与表达式匹配，因此它发送原始客户端请求URI。另一个例子是在同一位置使用重写指令时，导致客户端URI被重写，但仍然在同一个块中处理。在这种情况下，将传递重写的URI。\n3. 了解Nginx如何处理Headers 有一点可能不会立即明确的是，如果您希望上游服务器正确处理请求，则传递的不仅仅是URI。代表客户端来自Nginx的请求与直接来自客户端的请求看起来不同。其中很大一部分是与请求一起出现的标头。\n当Nginx代理请求时，它会自动对从客户端收到的请求标头进行一些调整：\nNginx摆脱任何空头。没有必要将空值传递给另一台服务器; 它只会使请求膨胀。 默认情况下，Nginx会将包含下划线的任何标头视为无效。它将从代理请求中删除它们。如果你希望Nginx将这些解释为有效，你可以将underscores_in_headers指令设置为“on”，否则你的标题永远不会进入后端服务器。 “Host”头被重写为$proxy_host 变量定义的值。这将是上游的IP地址或名称和端口号，直接由proxy_pass指令定义。 “连接”标题更改为“关闭”。该标头用于表示关于双方之间建立的特定连接的信息。在这种情况下，Nginx将其设置为“关闭”以向上游服务器指示一旦原始请求被响应，该连接将被关闭。上游不应期望这种连接是持久的。 我们可以从上面推断的第一点是，您不希望传递的任何头应该设置为空字符串。具有空值的标头将从传递的请求中完全删除。\n从上述信息中收集的下一个要点是，如果您的后端应用程序将处理非标准标头，则必须确保它们没有下划线。如果需要使用下划线的标头，可以underscores_in_headers在配置中将指令设置为“on”（在http上下文或IP地址/端口组合的默认服务器声明的上下文中有效）。如果不这样做，Nginx会将这些标头标记为无效，并在传递给您的上游之前静默删除它们。\n“主机”标头在大多数代理方案中特别重要。如上所述，默认情况下，这将设置为$proxy_host 一个变量的值，该变量将包含直接从proxy_pass定义中获取的域名或IP地址和端口。这是默认情况下选择的，因为它是Nginx可以确定上游服务器响应的唯一地址（因为它是直接从连接信息中提取的）。\n“Host”标头的最常见值如下：\n  $proxy_host：这将“主机”标头设置为从proxy_pass定义中获取的域名或IP地址和端口组合。从Nginx的角度来看，这是默认的“安全”，但通常不是代理服务器正确处理请求所需要的。\n  $http_host： 将“Host”标头设置为客户端请求的“Host”标头。客户端发送的标头始终在Nginx中作为变量提供。变量将以$http_前缀开头，后跟小写的标题名称，任何短划线都用下划线替换。尽管$http_host变量在大多数情况下都有效，但是当客户端请求没有有效的“主机”标头时，这可能会导致传递失败。\n  $host：此变量按优先顺序设置：请求行本身的主机名，客户端请求的“主机”标头或与请求匹配的服务器名称。\n  在大多数情况下，您需要将“Host”标头设置为$host变量。它是最灵活的，通常会为代理服务器提供尽可能准确填写的“主机”标头\n4. 设置或重置标题 要调整或设置代理连接的标头，我们可以使用该proxy_set_header指令。例如，要像我们讨论的那样更改“Host”标头，并添加一些与代理请求相同的其他标头，我们可以使用以下内容：\n# server context location /match/here { proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://example.com/new/prefix; } . . . 上述请求将“Host”标头设置为$host变量，该变量应包含有关所请求的原始主机的信息的X-Forwarded-Proto报头给出了关于原始客户机请求的架构中的代理的服务器信息（它是否是一个HTTP或HTTPS请求）。\n将X-Real-IP其设置为客户端的IP地址，以便代理可以根据此信息正确地做出决策或记录。该X-Forwarded-For标题是包含每一个客户端已通过代理到这一点的服务器的IP地址的列表。在上面的例子中，我们将其设置为$proxy_add_x_forwarded_for变量。此变量X-Forwarded-For获取从客户端检索的原始标头的值，并将Nginx服务器的IP地址添加到结尾。\n当然，我们可以将proxy_set_header指令移到服务器或http上下文中，允许它在多个位置引用：\n# server context proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location /match/here { proxy_pass http://example.com/new/prefix; } location /different/match { proxy_pass http://example.com; } 5. 为负载平衡代理连接定义上游上下文 在前面的示例中，我们演示了如何对单个后端服务器执行简单的http代理。Nginx允许我们通过指定可以传递请求的整个后端服务器池来轻松扩展此配置。\n我们可以通过使用该upstream指令来定义服务器池来实现此目的。此配置假定所列出的任何一个服务器都能够处理客户端的请求。这使我们可以毫不费力地扩展我们的基础设施。upstream必须在Nginx配置的http上下文中设置该指令。\n我们来看一个简单的例子：\n# http context upstream backend_hosts { server host1.example.com; server host2.example.com; server host3.example.com; } server { listen 80; server_name example.com; location /proxy-me { proxy_pass http://backend_hosts; } } 在上面的例子中，我们设置了一个名为的上游上下文backend_hosts。一旦定义，此名称将可用于代理传递，就像它是常规域名一样。如您所见，在我们的服务器块中，我们将任何请求传递example.com/proxy-me/…给我们上面定义的池。在该池中，通过应用可配置算法来选择主机。默认情况下，这只是一个简单的循环选择过程（每个请求将依次路由到不同的主机）。\n5.1 改变上游平衡算法 您可以通过在上游上下文中包含指令或标志来修改上游池使用的平衡算法：\n（循环法）：如果不存在其他平衡指令，则使用默认的负载平衡算法。在上游上下文中定义的每个服务器依次顺序传递请求。 least_conn：指定应始终为具有最少活动连接数的后端提供新连接。在与后端的连接可能持续一段时间的情况下，这尤其有用。 ip_hash：此平衡算法根据客户端的IP地址将请求分发到不同的服务器。前三个八位字节用作决定服务器处理请求的密钥。结果是客户端每次都倾向于由同一服务器提供服务，这有助于会话一致性。 hash：此平衡算法主要用于memcached代理。基于任意提供的散列密钥的值来划分服务器。这可以是文本，变量或组合。这是唯一需要用户提供数据的平衡方法，这是应该用于哈希的密钥。 更改平衡算法时，块可能如下所示：\n# http context upstream backend_hosts { least_conn; server host1.example.com; server host2.example.com; server host3.example.com; } . . . 在上面的示例中，将根据哪个连接最少来选择服务器。该ip_hash指令可以以相同的方式设置以获得一定量的会话“粘性”。\n至于hash方法，您必须提供哈希的密钥。这可以是你想要的任何东西：\n# http context upstream backend_hosts { hash $remote_addr$remote_port consistent; server host1.example.com; server host2.example.com; server host3.example.com; } . . . 上面的示例将根据客户端IP地址和端口的值分发请求。我们还添加了可选参数consistent，该参数实现了ketama一致性哈希算法。基本上，这意味着如果您的上游服务器发生更改，则对缓存的影响最小。\n5.2 设置服务器权重以进行平衡 在后端服务器的声明中，默认情况下，每个服务器都是“加权”的。这假设每个服务器可以并且应该处理相同的负载量（考虑到平衡算法的影响）。但是，您还可以在声明期间为服务器设置替代权重：\n# http context upstream backend_hosts { server host1.example.com weight=3; server host2.example.com; server host3.example.com; } . . . 在上面的例子中，host1.example.com将接收三倍于其他两个服务器的流量。默认情况下，为每个服务器分配权重1。\n6. 使用缓冲区释放后端服务器 涉及许多用户的代理问题是向流程添加其他服务器对性能的影响。在大多数情况下，利用Nginx的缓冲和缓存功能可以大大减轻这种影响。\n代理到另一台服务器时，两个不同连接的速度将影响客户端的体验：\n从客户端到Nginx代理的连接。 从Nginx代理到后端服务器的连接。 Nginx能够根据您希望优化的这些连接中的任何一个来调整其行为。\n没有缓冲区，数据从代理服务器发送并立即开始传输到客户端。如果假设客户端速度很快，则可以关闭缓冲，以便尽快将数据传送到客户端。使用缓冲区，Nginx代理将临时存储后端的响应，然后将此数据提供给客户端。如果客户端很慢，这允许Nginx服务器更快地关闭到后端的连接。然后，它可以处理以任何可能的速度将数据分发给客户端。\nNginx默认采用缓冲设计，因为客户端的连接速度往往差异很大。我们可以使用以下指令调整缓冲行为。这些可以在http，服务器或位置上下文中设置。重要的是要记住，每个请求都配置了大小调整指令，因此当有许多客户端请求时，增加它们超出您的需要会影响您的性能：\nproxy_buffering：此指令控制是否启用此上下文和子上下文的缓冲。默认情况下，这是“打开”。 proxy_buffers：此指令控制代理响应的缓冲区的数量（第一个参数）和大小（第二个参数）。默认设置是配置8个大小等于一个内存页面的缓冲区（4k或者8k）。增加缓冲区数量可以让您缓冲更多信息。 proxy_buffer_size：来自后端服务器的响应的初始部分（包含标头）与响应的其余部分分开缓冲。该指令设置响应的这一部分的缓冲区大小。默认情况下，这与大小相同proxy_buffers，但由于这用于标题信息，因此通常可以将其设置为较低的值。 proxy_busy_buffers_size：此指令设置可以标记为“客户端就绪”并因此忙碌的缓冲区的最大大小。虽然客户端一次只能从一个缓冲区读取数据，但缓冲区放在队列中以便以串联形式发送到客户端。该指令控制允许处于此状态的缓冲区空间的大小。 proxy_max_temp_file_size：这是磁盘上临时文件的每个请求的最大大小。这些是在上游响应太大而无法放入缓冲区时创建的。 proxy_temp_file_write_size：这是当代理服务器的响应对于配置的缓冲区而言太大时，Nginx将一次写入临时文件的数据量。 proxy_temp_path：这是磁盘上区域的路径，当上游服务器的响应无法容纳到配置的缓冲区时，Nginx应该存储任何临时文件。 如您所见，Nginx提供了许多不同的指令来调整缓冲行为。大多数情况下，您不必担心大多数这些，但调整其中一些值可能很有用。调整最有用的可能是proxy_buffers和proxy_buffer_size指令。\n增加每个上游请求的可用代理缓冲区数量的示例，同时减少可能存储标头的缓冲区将如下所示：\n# server context proxy_buffering on; proxy_buffer_size 1k; proxy_buffers 24 4k; proxy_busy_buffers_size 8k; proxy_max_temp_file_size 2048m; proxy_temp_file_write_size 32k; location / { proxy_pass http://example.com; } 相反，如果您想要立即向其提供数据的快速客户端，则可以完全关闭缓冲。如果上游比客户端更快，Nginx实际上仍将使用缓冲区，但它会立即尝试将数据刷新到客户端，而不是等待缓冲区池。如果客户端速度很慢，这可能导致上游连接保持打开状态，直到客户端赶上。当缓冲“关闭”时，仅proxy_buffer_size使用指令定义的缓冲区：\n# server context proxy_buffering off; proxy_buffer_size 4k; location / { proxy_pass http://example.com; } 6.1 高可用性（可选） 通过添加一组冗余的负载均衡器，可以使Nginx代理更加健壮，从而创建高可用性基础架构。\n甲高可用性（HA）的设置是无故障的单个点处的基础设施，和你的负载平衡器是该构造的一部分。通过使用多个负载均衡器，可以防止在负载均衡器不可用或需要将其关闭进行维护时可能导致的停机。\n以下是基本高可用性设置的图表： 在此示例中，您有一个静态IP地址后面的多个负载平衡器（一个活动和一个或多个被动），可以从一个服务器重新映射到另一个服务器。客户端请求从静态IP路由到活动负载平衡器，然后路由到后端服务器。要了解更多信息，请阅读如何使用浮动IP的这一节。\n7. 配置代理缓存以减少响应时间 虽然缓冲可以帮助释放后端服务器以处理更多请求，但Nginx还提供了一种从后端服务器缓存内容的方法，从而无需为许多请求连接到上游。\n7.1 配置代理缓存 要设置用于代理内容的缓存，我们可以使用该proxy_cache_path指令。这将创建一个区域，可以保留从代理服务器返回的数据。该proxy_cache_path指令必须在http上下文中设置。\n在下面的示例中，我们将配置此命令和一些相关指令来设置我们的缓存系统。\n# http context proxy_cache_path /var/lib/nginx/cache levels=1:2 keys_zone=backcache:8m max_size=50m; proxy_cache_key \u0026quot;$scheme$request_method$host$request_uri$is_args$args\u0026quot;; proxy_cache_valid 200 302 10m; proxy_cache_valid 404 1m; 使用该proxy_cache_path指令，我们已经在文件系统上定义了一个目录，我们希望存储缓存。在这个例子中，我们选择了/var/lib/nginx/cache目录。如果此目录不存在，您可以通过键入以下内容以正确的权限和所有权创建它：\nsudo mkdir -p /var/lib/nginx/cache sudo chown www-data /var/lib/nginx/cache sudo chmod 700 /var/lib/nginx/cache 该levels=参数指定缓存的组织方式。Nginx将通过散列键的值（下面配置）来创建缓存键。我们在上面选择的级别规定将创建一个单个字符目录（这将是散列值的最后一个字符），其中包含一个两个字符的子目录（取自散列值末尾的下两个字符）。您通常不必关心此细节，但它有助于Nginx快速找到相关值。\n该keys_zone=参数定义了我们调用的此缓存区的名称backcache。这也是我们定义要存储多少元数据的地方。在这种情况下，我们存储8 MB的密钥。对于每兆字节，Nginx可以存储大约8000个条目。该max_size参数设置实际缓存数据的最大大小。\n我们上面使用的另一个指令是proxy_cache_key。这用于设置将用于存储缓存值的键。该相同密钥用于检查是否可以从缓存提供请求。我们将此设置为方案（http或https），HTTP请求方法以及请求的主机和URI的组合。\n该proxy_cache_valid指令可以多次指定。它允许我们根据状态代码配置存储值的时间长度。在我们的示例中，我们存储成功和重定向10分钟，并且每分钟使缓存过期404响应。\n现在，我们已经配置了缓存区域，但我们仍然需要告诉Nginx何时使用缓存。\n在我们代理后端的位置，我们可以配置此缓存的使用：\n# server context location /proxy-me { proxy_cache backcache; proxy_cache_bypass $http_cache_control; add_header X-Proxy-Cache $upstream_cache_status; proxy_pass http://backend; } . . . 使用该proxy_cache指令，我们可以指定backcache缓存区域应该用于此上下文。在传递到后端之前，Nginx将在此处检查有效条目。\n该proxy_cache_bypass指令设置为$http_cache_control 变量。这将包含一个指示器，指示客户端是否明确请求资源的新的非缓存版本。设置此指令允许Nginx正确处理这些类型的客户端请求。无需进一步配置。\n我们还添加了一个名为的额外标题X-Proxy-Cache。我们将此标头设置为$upstream_cache_status变量的值。基本上，这会设置一个标头，允许我们查看请求是否导致缓存命中，缓存未命中，或者是否明确绕过了缓存。这对于调试尤其有用，但对于客户端也是有用的信息。\n7.2 有关缓存结果的说明 缓存可以极大地提高代理的性能。但是，配置缓存时必须牢记一些注意事项。\n首先，任何用户相关的数据应该不会被缓存。这可能导致一个用户的数据被呈现给另一个用户。如果您的网站完全是静态的，这可能不是问题。\n如果您的站点有一些动态元素，则必须在后端服务器中对此进行说明。如何处理这取决于处理后端处理的应用程序或服务器。对于私有内容，您应将Cache-Control标头设置为no-cache，no-store或private，具体取决于数据的性质： no-cache：表示如果没有先检查后端的数据是否未更改，则不应再次提供响应。如果数据是动态且重要的，则可以使用此方法。在每个请求上检查ETag散列元数据头，并且如果后端返回相同的散列值，则可以提供先前的值\nno-store ：表示在任何时候都不应该缓存接收到的数据。这是私有数据最安全的选项，因为这意味着每次都必须从服务器检索数据。\nprivate：这表示没有共享缓存空间应该缓存此数据。这可用于指示用户的浏览器可以缓存数据，但代理服务器不应认为此数据对后续请求有效。\npublic：这表示响应是可以在连接中的任何位置缓存的公共数据。\n可以控制此行为的相关标头是max-age标头，它指示应缓存任何资源的秒数。\n正确设置这些标题（取决于内容的敏感性）将有助于您利用缓存，同时保护您的私人数据安全并使您的动态数据保持新鲜。\n如果你的后端也使用Nginx，你可以使用expires指令设置一些，这将设置max-age为Cache-Control：\nlocation / { expires 60m; } location /check-me { expires -1; } 在上面的示例中，第一个块允许将内容缓存一小时。第二个块将Cache-Control标头设置为“no-cache”。要设置其他值，可以使用该add_header指令，如下所示：\nlocation /private { expires -1; add_header Cache-Control \u0026quot;no-store\u0026quot;; } 8. 结论 Nginx首先是一个反向代理，它也恰好具有作为Web服务器工作的能力。由于此设计决策，向其他服务器代理请求非常简单。Nginx非常灵活，如果需要，可以对代理配置进行更复杂的控制\n原文地址\n参考\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%85%A5%E9%97%A8/","summary":"Nginx负载均衡配置\u0026ndash;简介  在使用tomcat部署静态网站的时候，由于服务器比较垃圾，所以如果多人同时访问的话，可能会造成卡顿，影响用户体验。所以想到了使用负载均衡。\n 1. 什么是负载均衡 负载平衡是高可用性基础架构的关键组件，通常用于通过在多个服务器之间分配工作负载来提高网站，应用程序，数据库和其他服务的性能和可靠性。\n没有负载平衡的Web基础结构可能如下所示：\n在此示例中，用户直接连接到web服务器yourdomain.com。如果此单个Web服务器出现故障，用户将无法再访问该网站。此外，如果许多用户尝试同时访问服务器并且无法处理负载，则可能会遇到加载时间缓慢或根本无法连接的情况。\n通过在后端引入负载均衡器和至少一个额外的Web服务器，可以减轻此单点故障。通常，所有后端服务器都将提供相同的内容，以便用户无论哪个服务器响应都会收到一致的内容。 在上面说明的示例中，用户访问负载均衡器，负载均衡器将用户的请求转发到后端服务器，后端服务器然后直接响应用户的请求。在这种情况下，单点故障现在是负载平衡器本身。这可以通过引入第二个负载均衡器来缓解.\n2. 负载均衡器可以处理什么样的流量   HTTP - 标准HTTP平衡基于标准HTTP机制定向请求。负载均衡器设置X-Forwarded-For，X-Forwarded-Proto以及X-Forwarded-Port头，提供有关原始请求的后端信息。\n  HTTPS - HTTPS平衡功能与HTTP平衡功能相同，但增加了加密功能。加密以两种方式之一处理：使用SSL直通，一直保持加密到后端，或者使用SSL终止，将解密负担放在负载均衡器上，但将未加密的流量发送到后端。\n  TCP - 对于不使用HTTP或HTTPS的应用程序，也可以平衡TCP流量。例如，数据库集群的流量可以分布在所有服务器上。\n  UDP\u0026ndash;最近，一些负载均衡器增加了对使用UDP的核心互联网协议（如DNS和syslogd）的负载平衡的支持。\n  这些转发规则将定义负载均衡器本身的协议和端口，并将它们映射到负载均衡器将用于将流量路由到后端的协议和端口。\n3. 负载均衡器如何选择后端服务器 负载均衡器根据两个因素的组合选择将请求转发到哪个服务器。他们将首先确保他们可以选择的任何服务器实际上对请求做出适当的响应，然后使用预先配置的规则从该健康池中进行选择。\n3.1 健康检查 负载均衡器应仅将流量转发到“健康”的后端服务器。要监视后端服务器的运行状况，运行状况检查会定期尝试使用转发规则定义的协议和端口连接到后端服务器，以确保服务器正在侦听。如果服务器未通过运行状况检查，因此无法提供请求，则会自动将其从池中删除，并且在再次响应运行状况检查之前，流量将不会转发给它。\n3.2 负载平衡算法 使用的负载平衡算法确定将选择后端中的哪些正常服务器。一些常用的算法是：\n  Round Robin - Round Robin意味着将按顺序选择服务器。负载均衡器将在其列表中为第一个请求选择第一个服务器，然后按顺序向下移动列表，当它到达结尾时从顶部开始。\n  least_conn - least_conn意味着负载均衡器将选择连接最少的服务器，并且当流量导致更长的会话时建议使用。\n  ip_hash：此平衡算法根据客户端的IP地址将请求分发到不同的服务器。前三个八位字节用作决定服务器处理请求的密钥。结果是客户端每次都倾向于由同一服务器提供服务，这有助于会话一致性。\n  hash：此平衡算法主要用于memcached代理。基于任意提供的散列密钥的值来划分服务器。这可以是文本，变量或组合。这是唯一需要用户提供数据的平衡方法，这是应该用于哈希的密钥。\n  管理员可用的算法取决于所使用的特定负载平衡技术。\n3.3 负载平衡器如何处理状态 某些应用程序要求用户继续连接到同一后端服务器。Source算法根据客户端IP信息创建关联。在Web应用程序级别实现此目的的另一种方法是通过粘性会话，其中负载平衡器设置cookie，并且来自该会话的所有请求都定向到同一物理服务器。\n4. 冗余负载均衡器 要将负载均衡器作为单点故障移除，可以将第二个负载均衡器连接到第一个负载均衡器以形成一个集群，其中每个负载均衡器监控其他负载平衡器的运行状况。每个人都具有同样的故障检测和恢复能力。如果主负载均衡器发生故障，DNS必须将用户带到第二个负载均衡器。由于DNS更改可能需要花费大量时间在Internet上传播并自动进行此故障转移，因此许多管理员将使用允许灵活IP地址重新映射的系统，例如浮动IP。按需IP地址重新映射通过提供可在需要时轻松重新映射的静态IP地址，消除了DNS更改中固有的传播和缓存问题。域名可以保持与相同的IP地址关联，而IP地址本身在服务器之间移动。","title":"Nginx负载均衡配置--简介"},{"content":"python与其他语言的对比（hello world）  C语言\n include\u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello world\u0026#34;); return 0; }  Java语言\n public class HelloWorld{ public static void main(String[] args) { System.out.println(\u0026#34;Hello World!\u0026#34;); } }  Python\n print(\u0026#39;hello world\u0026#39;) python中的常用数据类型  Number String List Tuple Dictionary  # Number a = 1 b = True c = 3.15 d = 1.1+2.2j # 字符串 str1 = \u0026#39;hello\u0026#39; str1_1 = \u0026#34;hello\u0026#34; str2 = \u0026#34;world\u0026#34; print(str1==str1_1) # 字符串连接 str3 = str1 + str2 print(str3) # 转义字符 str4 = \u0026#39;hello \\nworld\u0026#39; print(str4) str5 = \u0026#39;hello \\\\n world\u0026#39; print(str5) # 格式化输出 print(\u0026#39;str1:%s.\u0026#39;%str1) # 切片 print(str1[1:4]) True helloworld hello world hello \\n world str1:hello. ell  # 列表 list1 = [\u0026#39;google\u0026#39;, \u0026#39;alibaba\u0026#39;, 2001, 3.14] # 通过下标访问 print(list1[0]) # 更新列表 list1[2] = \u0026#39;baidu\u0026#39; print(list1) # 删除元素 del list1[3] print(list1) # 拼接列表 list2 = [\u0026#39;microsoft\u0026#39;, \u0026#39;amazon\u0026#39;] list3 = list1 + list2 print(list3) # 增添列表项 list1.append(\u0026#39;jingdong\u0026#39;) print(list1) google ['google', 'alibaba', 'baidu', 3.14] ['google', 'alibaba', 'baidu'] ['google', 'alibaba', 'baidu', 'microsoft', 'amazon'] ['google', 'alibaba', 'baidu', 'jingdong']  # 元组：类似列表,是一系列元素的有序集合,但元组中的元素无法修改 tuple1 = (\u0026#39;google\u0026#39;, \u0026#39;alibaba\u0026#39;, \u0026#39;baidu\u0026#39;) tuple1[0] = \u0026#39;amazon\u0026#39; # 不能被改变 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-23-4ed3e334c834\u0026gt; in \u0026lt;module\u0026gt; 1 # 元组：类似列表,是一系列元素的有序集合,但元组中的元素无法修改 2 tuple1 = ('google', 'alibaba', 'baidu') ----\u0026gt; 3 tuple1[0] = 'amazon' TypeError: 'tuple' object does not support item assignment  # 字典 dict1 = { \u0026#39;color\u0026#39;: \u0026#39;green\u0026#39;, \u0026#39;points\u0026#39;: 5 } # 访问列表中的值 print(dict1[\u0026#39;color\u0026#39;]) # 增加字典中键值对 dict1[\u0026#39;x_pos\u0026#39;] = 0 dict1[\u0026#39;y_pos\u0026#39;] =4 print(dict1) green {'color': 'green', 'points': 5, 'x_pos': 0, 'y_pos': 4}  python中的结构语句 if条件语句 car = \u0026#39;bmw\u0026#39; if car == \u0026#39;bmw\u0026#39;: print(car.upper()) # 输出car的大写版本 else: print(car.title()) # 输出car的标题版本 Bmw  现实世界中,很多情况下需要考虑的情形都超过两个。例如,来看一个根据年龄段收费的 游乐场:\n 4岁以下免费; 4~18岁收费5美元; 18岁(含)以上收费10美元。   如果只使用一条 if 语句,如何确定门票价格呢?下面的代码确定一个人所属的年龄段,并打印一条包含门票价格的消息:\n age = 12 if age \u0026lt; 4: print(\u0026#34;Your admission cost is $0.\u0026#34;) elif age \u0026lt; 18: print(\u0026#34;Your admission cost is $5.\u0026#34;) else: print(\u0026#34;Your admission cost is $10.\u0026#34;) Your admission cost is $5.  for循环语句 fruits = [\u0026#39;banana\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;mango\u0026#39;] for fruit in fruits: print(\u0026#39;当前水果：%s\u0026#39;%fruit) 当前水果：banana 当前水果：apple 当前水果：mango  # range 步长为1 for i in range(0, 6): print(i) 1 2 3 4 5  # range 步长为2 for i in range(0, 6, 2): print(i) 0 2 4  # break 和 continue for i in range(0, 6): if i == 3: break print(i) for i in range(0, 6): if i == 3: continue print(i, end=\u0026#39;\u0026#39;) 0 1 2 01245  # while循环 current_number = 1 while current_number \u0026lt;= 5: print(current_number) current_number += 1 1 2 3 4 5  函数 def greet_user(username): \u0026#34;\u0026#34;\u0026#34; 显示简单的问候语 \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Hello, \u0026#34; + username.title() + \u0026#34;!\u0026#34;) greet_user(\u0026#39;jesse\u0026#39;) greet_user(\u0026#39;jack\u0026#39;) Hello, Jesse! Hello, Jack!  # 有返回值的函数 def add(a, b): return a+b print(\u0026#39;第一个函数：%d\u0026#39;%add(2, 3)) # 列表作为参数的函数 def add_l(mylist): result = 0 for l in mylist: result += l return result print(\u0026#39;第二个函数：%d\u0026#39;%add_l([1, 2, 3, 4])) # 有多个返回值的函数 def muti_re(mylist): a = max(mylist) b = min(mylist) return a, b a, b = muti_re([1, 2, 3, 4]) print(\u0026#39;第三个函数, 最大值：%d, 最小值：%d\u0026#39;%(a,b)) 第一个函数：5 第二个函数：10 第三个函数, 最大值：4, 最小值：1  ","permalink":"http://yangchnet.github.io/Dessert/posts/django/python%E5%9F%BA%E7%A1%80/","summary":"python与其他语言的对比（hello world）  C语言\n include\u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello world\u0026#34;); return 0; }  Java语言\n public class HelloWorld{ public static void main(String[] args) { System.out.println(\u0026#34;Hello World!\u0026#34;); } }  Python\n print(\u0026#39;hello world\u0026#39;) python中的常用数据类型  Number String List Tuple Dictionary  # Number a = 1 b = True c = 3.15 d = 1.1+2.2j # 字符串 str1 = \u0026#39;hello\u0026#39; str1_1 = \u0026#34;hello\u0026#34; str2 = \u0026#34;world\u0026#34; print(str1==str1_1) # 字符串连接 str3 = str1 + str2 print(str3) # 转义字符 str4 = \u0026#39;hello \\nworld\u0026#39; print(str4) str5 = \u0026#39;hello \\\\n world\u0026#39; print(str5) # 格式化输出 print(\u0026#39;str1:%s.","title":"python与其他语言的对比（helloworld）"},{"content":"Python中的拷贝   直接赋值：其实就是对象的引用（别名）.两个对象是引用的同一块内存区域\n  浅拷贝(copy)：拷贝父对象，不会拷贝对象的内部的子对象。\n  深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象。\n  引用示例\na = [1,2,3,4] b = a a.append(5) print(a, b) [1, 2, 3, 4, 5] [1, 2, 3, 4, 5]  浅拷贝示例\nimport copy a = [1, 2, 3, 4, [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]] b = copy.copy(a) a.append(5) a[4].append(\u0026#39;c\u0026#39;) print(a) print(b) [1, 2, 3, 4, ['a', 'b', 'c'], 5] [1, 2, 3, 4, ['a', 'b', 'c']]  可以看到,父对象被拷贝了,当直接对父对象做修改时,拷贝值也相应的得到了变化,但是对子对象修改时,拷贝值不变\n深拷贝示例\nimport copy c = [1, 2, 3, 4, [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]] d = copy.deepcopy(a) c.append(5) c[4].append(\u0026#39;c\u0026#39;) print(c) print(d) [1, 2, 3, 4, ['a', 'b', 'c'], 5] [1, 2, 3, 4, ['a', 'b', 'c'], 5]  ","permalink":"http://yangchnet.github.io/Dessert/posts/python/%E6%B7%B1%E6%B5%85%E6%8B%B7%E8%B4%9D/","summary":"Python中的拷贝   直接赋值：其实就是对象的引用（别名）.两个对象是引用的同一块内存区域\n  浅拷贝(copy)：拷贝父对象，不会拷贝对象的内部的子对象。\n  深拷贝(deepcopy)： copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象。\n  引用示例\na = [1,2,3,4] b = a a.append(5) print(a, b) [1, 2, 3, 4, 5] [1, 2, 3, 4, 5]  浅拷贝示例\nimport copy a = [1, 2, 3, 4, [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;]] b = copy.copy(a) a.append(5) a[4].append(\u0026#39;c\u0026#39;) print(a) print(b) [1, 2, 3, 4, ['a', 'b', 'c'], 5] [1, 2, 3, 4, ['a', 'b', 'c']]  可以看到,父对象被拷贝了,当直接对父对象做修改时,拷贝值也相应的得到了变化,但是对子对象修改时,拷贝值不变","title":"Python中的拷贝"},{"content":"Python换源 1. 临时换源 可以在使用pip的时候在后面加上-i参数，指定pip源\npip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple 2. 永久换源 永久修改： linux: 修改 ~/.pip/pip.conf (没有就创建一个)， 内容如下：\n[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple ","permalink":"http://yangchnet.github.io/Dessert/posts/python/python%E6%8D%A2%E6%BA%90/","summary":"Python换源 1. 临时换源 可以在使用pip的时候在后面加上-i参数，指定pip源\npip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple 2. 永久换源 永久修改： linux: 修改 ~/.pip/pip.conf (没有就创建一个)， 内容如下：\n[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple ","title":"Python换源"},{"content":"python 网络编程 使用socket模块，即套接字 使用socket来创建套接字的语法如下： socket.socket(family[, type[, proto]])\n 参数解释：\n  family: 套接字家族可以使AF_UNIX或者AF_INET type: 套接字类型可以根据是面向连接的还是非连接分为SOCK_STREAM或SOCK_DGRAM protocol：一般不填默认为0  socket对象的方法  s.bind() 绑定地址（host,port）到套接字， 在AF_INET下,以元组（host,port）的形式表示地址。 s.listen() 开始TCP监听。backlog指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为1，大部分应用程序设为5就可以了。 s.accept() 被动接受TCP客户端连接,(阻塞式)等待连接的到来 客户端套接字 s.connect() 主动初始化TCP服务器连接，。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。 s.connect_ex() connect()函数的扩展版本,出错时返回出错码,而不是抛出异常 公共用途的套接字函数 s.recv() 接收TCP数据，数据以字符串形式返回，bufsize指定要接收的最大数据量。flag提供有关消息的其他信息，通常可以忽略。 s.send() 发送TCP数据，将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。 s.sendall() 完整发送TCP数据，完整发送TCP数据。将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成功返回None，失败则抛出异常。 s.recvfrom() 接收UDP数据，与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。 s.sendto() 发送UDP数据，将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。 s.close() 关闭套接字 s.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。 s.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port) s.setsockopt(level,optname,value) 设置给定套接字选项的值。 s.getsockopt(level,optname[.buflen]) 返回套接字选项的值。 s.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒。值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()） s.gettimeout() 返回当前超时期的值，单位是秒，如果没有设置超时期，则返回None。 s.fileno() 返回套接字的文件描述符。 s.setblocking(flag) 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。 s.makefile() 创建一个与该套接字相关连的文件  网络编程的基本设置步骤 服务端配置  导入socket模块 使用bind方法创建套接字 使用listen方法等待连接， 使用accept方法被动接收tcp连接 使用send或recv方法进行收发  客户端配置  导入socket模块 使用bind方法创建套接字 使用connect方法进行主动tcp连接 使用send或recv方法进行收发  ","permalink":"http://yangchnet.github.io/Dessert/posts/python/python%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","summary":"python 网络编程 使用socket模块，即套接字 使用socket来创建套接字的语法如下： socket.socket(family[, type[, proto]])\n 参数解释：\n  family: 套接字家族可以使AF_UNIX或者AF_INET type: 套接字类型可以根据是面向连接的还是非连接分为SOCK_STREAM或SOCK_DGRAM protocol：一般不填默认为0  socket对象的方法  s.bind() 绑定地址（host,port）到套接字， 在AF_INET下,以元组（host,port）的形式表示地址。 s.listen() 开始TCP监听。backlog指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为1，大部分应用程序设为5就可以了。 s.accept() 被动接受TCP客户端连接,(阻塞式)等待连接的到来 客户端套接字 s.connect() 主动初始化TCP服务器连接，。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。 s.connect_ex() connect()函数的扩展版本,出错时返回出错码,而不是抛出异常 公共用途的套接字函数 s.recv() 接收TCP数据，数据以字符串形式返回，bufsize指定要接收的最大数据量。flag提供有关消息的其他信息，通常可以忽略。 s.send() 发送TCP数据，将string中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于string的字节大小。 s.sendall() 完整发送TCP数据，完整发送TCP数据。将string中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成功返回None，失败则抛出异常。 s.recvfrom() 接收UDP数据，与recv()类似，但返回值是（data,address）。其中data是包含接收数据的字符串，address是发送数据的套接字地址。 s.sendto() 发送UDP数据，将数据发送到套接字，address是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。 s.close() 关闭套接字 s.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。 s.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port) s.setsockopt(level,optname,value) 设置给定套接字选项的值。 s.getsockopt(level,optname[.buflen]) 返回套接字选项的值。 s.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒。值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()） s.gettimeout() 返回当前超时期的值，单位是秒，如果没有设置超时期，则返回None。 s.fileno() 返回套接字的文件描述符。 s.setblocking(flag) 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。 s.makefile() 创建一个与该套接字相关连的文件  网络编程的基本设置步骤 服务端配置  导入socket模块 使用bind方法创建套接字 使用listen方法等待连接， 使用accept方法被动接收tcp连接 使用send或recv方法进行收发  客户端配置  导入socket模块 使用bind方法创建套接字 使用connect方法进行主动tcp连接 使用send或recv方法进行收发  ","title":"python网络编程"},{"content":"RESTful API 入门 1. 简介 表现层状态转换（英语：Representational State Transfer，缩写：REST）是Roy Thomas Fielding博士于2000年在他的博士论文中提出来的一种万维网软件架构风格，目的是便于不同软件/程序在网络（例如互联网）中互相传递信息。表现层状态转换是根基于超文本传输协议（HTTP）之上而确定的一组约束和属性，是一种设计提供万维网络服务的软件构建风格。符合或兼容于这种架构风格（简称为 REST 或 RESTful）的网络服务，允许客户端发出以统一资源标识符访问和操作网络资源的请求，而与预先定义好的无状态操作集一致化。因此表现层状态转换提供了在互联网络的计算系统之间，彼此资源可交互使用的协作性质（interoperability）。相对于其它种类的网络服务，例如SOAP服务，则是以本身所定义的操作集，来访问网络上的资源。\n2. REST 架构约束   客户端－服务器 从本质上讲，这意味着客户端应用程序和服务器应用程序必须能够独立发展而彼此之间没有任何依赖关系。客户端应该只知道资源URI，仅此而已。今天，这是Web开发中的常规做法，因此您不需要任何花哨。把事情简单化。\n 服务器和客户端也可以独立替换和开发，只要它们之间的接口没有更改即可。\n   无状态\nRoy fielding的灵感来自HTTP，因此它反映了这一约束。使所有客户端-服务器交互都变为无状态。服务器将不存储有关客户端发出的最新HTTP请求的任何内容。它将每个请求视为新请求。没有会议，没有历史。\n如果客户端应用程序需要是最终用户的有状态应用程序，则用户必须登录一次并在此之后执行其他授权操作，则来自客户端的每个请求都应包含服务于该请求的所有必要信息，包括身份验证和授权细节。\n 请求之间不得在服务器上存储任何客户端上下文。客户端负责管理应用程序的状态。\n   统一的接口 在约束名称本身适用的情况下，您必须为系统内部暴露给API使用者并认真遵循的资源确定API接口。系统中的资源应仅具有一个逻辑URI，并且应提供一种获取相关或附加数据的方式。最好将资源与网页同义。\n任何单个资源都不应太大，并在其表示中包含所有内容。只要相关，资源应包含指向相对URI的链接（HATEOAS），以获取相关信息。\n此外，整个系统上的资源表示应遵循特定的准则，例如命名约定，链接格式或数据格式（XML或/和JSON）。\n所有资源都应通过通用方法（例如HTTP GET）进行访问，并使用一致的方法进行类似的修改。\n 一旦开发人员熟悉您的一个API，他就应该能够对其他API遵循类似的方法。\n   分层系统\nREST允许您使用分层的系统架构，在该架构中，您可以在服务器A上部署API，并在服务器B上存储数据并在服务器C中对请求进行身份验证。客户端通常无法确定它是直接连接到最终服务器还是中​​间连接。\n  可缓存的\n在当今世界中，缓存数据和响应在任何适用/可能的地方都至关重要。我们阅读的网页也是HTML页面的缓存版本。缓存可以提高客户端的性能，并为服务器提供更好的可伸缩性。\n在REST中，缓存应在适用时应用于资源，然后这些资源必须声明自己可缓存。可以在服务器或客户端上实现缓存。\n 管理良好的缓存部分或完全消除了某些客户端-服务器交互，从而进一步提高了可伸缩性和性能。\n   按需代码（可选）\n好吧，这个约束是可选的。大多数时候，您将以XML或JSON的形式发送资源的静态表示。但是，如果需要，您可以自由地return executable code支持应用程序的一部分，例如，客户端可以调用您的API来获取UI小部件呈现代码。这是允许的。\n 以上所有约束条件都可以帮助您构建真正的RESTful API，并且应该遵循它们。不过，有时您可能会发现自己违反了一两个约束。别担心; 您仍在制作RESTful API，但不是“真正的RESTful”。\n   3. REST资源命名指南 在REST中，主要数据表示称为Resource。从长远来看，拥有一个强大且一致的REST资源命名策略–无疑将证明是最佳的设计决策之一。\n REST中信息的关键抽象是一种资源。可以命名的任何信息都可以是资源：文档或图像，临时服务（例如“洛杉矶今天的天气”），其他资源的集合，非虚拟对象（例如人）等上。换句话说，任何可能成为作者超文本引用目标的概念都必须符合资源的定义。资源是到一组实体的概念映射，而不是在任何特定时间点对应于该映射的实体。\n 一个资源可以是单个的或一个集合。例如，在一个银行应用中，customers代表一个资源集合，customer代表单个资源。我们可以使用URL\u0026quot;/customers\u0026ldquo;来标记customers，可以使用URL\u0026rdquo;/customers/{customerId}\u0026ldquo;来标记customer\n一个资源可能包括多个子资源。例如，一个customer的子资源account可以使用URL\u0026rdquo;/customers/{customerId}/accounts\u0026ldquo;来标记；类似的，accounts的子资源account可以标记如下：URL\u0026rdquo;```/customers/{customerId}/accounts/{accountId}\n4. REST资源命名实践 4.1 使用名词来表示资源 RESTful URI应该引用作为事物（名词）的资源，而不是引用动作（动词），因为名词具有动词所没有的属性–类似于资源具有属性。资源的一些示例是：\n Users of the system User Accounts Network Devices etc.  他们的资源URL应该如下：\n* http://api.example.com/device-management/managed-devices * http://api.example.com/device-management/managed-devices/{device-id} * http://api.example.com/user-management/users/ * http://api.example.com/user-management/users/{id} 为了更加清晰，让我们将资源原型分为四类**（document, collection, store and controller）**，然后您始终应该以将资源放入一个原型为目标，然后一致地使用其命名约定。为了统一起见，请抵制设计资源的诱惑，这些资源是多个原型的混合体。\n  Document\n文档资源是单个概念，类似于对象实例或数据库记录。在REST中，您可以将其视为资源集合中的单个资源。文档的状态表示形式通常包括带有值的字段以及指向其他相关资源的链接。\n使用“单数”名称表示文档资源原型。\nhttp://api.example.com/device-management/managed-devices/{device-id} http://api.example.com/user-management/users/{id} http://api.example.com/user-management/users/admin   Collection\nCollection资源是服务器管理的资源目录。客户可以建议将新资源添加到Collection中。但是，由Collection决定是否创建新资源。Collection资源选择要包含的内容，并确定每个包含的资源的URI。\n使用复数名称表示Collection资源原型。\nhttp://api.example.com/device-management/managed-devices http://api.example.com/user-management/users http://api.example.com/user-management/users/{id}/admin   Store\nStore是客户端管理的资源存储库。Store资源可让API客户端放入资源，将其撤回并决定何时删除它们。Store永远不会生成新的URI。取而代之的是，每个存储的资源都有一个URI，该URI是客户端在最初将其放入Store时选择的。\n使用复数名称表示Store资源原型。\nhttp://api.example.com/cart-management/users/{id}/carts http://api.example.com/song-management/users/{id}/accounts   Controller\nController 资源为过程概念建模。Controller 资源就像可执行函数一样，带有参数和返回值。输入和输出。\n使用动词表示控制器原型。\nhttp://api.example.com/cart-management/users/{id}/cart/checkout http://api.example.com/song-management/users/{id}/playlist/play   4.2. 一致性是关键 使用一致的资源命名约定和URI格式，以最大程度地减少歧义并最大程度地提高可读性和可维护性。您可以实现以下设计提示以实现一致性：\n  使用正斜杠（/）表示层次关系\nURI的路径部分使用正斜杠（/）字符表示资源之间的层次关系。例如\nhttp://api.example.com/device-management http://api.example.com/device-management/managed-devices http://api.example.com/device-management/managed-devices/{id} http://api.example.com/device-management/managed-devices/{id}/scripts http://api.example.com/device-management/managed-devices/{id}/scripts/{id}   不要在URI中使用结尾的正斜杠(/)\n作为URI路径中的最后一个字符，正斜杠（/）不添加语义值，并且可能引起混淆。最好将它们完全删除。\nhttp://api.example.com/device-management/managed-devices/ http://api.example.com/device-management/managed-devices # 这是更好的版本 ```   使用连字符（-）来提高URI的可读性\n为了使你的URI易于人们扫描和解释，请使用连字符（-）来提高长路径段中名称的可读性。\nhttp://api.example.com/inventory-management/managed-entities/{id}/install-script-location # 可读性强 http://api.example.com/inventory-management/managedEntities/{id}/installScriptLocation # 可读性较低   请勿使用下划线（_）\n可以使用下划线代替连字符作为分隔符–但是，根据应用程序的字体，下划线（_）字符可能会在某些浏览器或屏幕中被部分遮盖或完全隐藏。\n为避免这种混淆，请使用连字符（-）代替下划线（_）。\nhttp://api.example.com/inventory-management/managed-entities/{id}/install-script-location # More readable http://api.example.com/inventory_management/managed_entities/{id}/install_script_location # More error prone   在URI中使用小写字母\n方便时，在URI路径中应始终首选小写字母。\nRFC 3986将URI定义为区分大小写，但方案和主机组件除外。例如\nhttp://api.example.org/my-folder/my-doc // 1 HTTP://API.EXAMPLE.ORG/my-folder/my-doc // 2 http://api.example.org/My-Folder/my-doc // 3 在上面的示例中，1和2相同，但3不是，因为它以大写字母使用My-Folder。\n  不要使用文件扩展名\n文件扩展名看起来很糟糕，并且没有任何优势。删除它们也会减少URI的长度。没有理由保留它们。\n除上述原因外，如果要使用文件扩展名突出显示API的媒体类型，则应依靠通过Content-Type标头传达的媒体类型来确定如何处理正文内容。\nhttp://api.example.com/device-management/managed-devices.xml # 请勿使用 http://api.example.com/device-management/managed-devices # 这是正确的URI   5. 不要在URL中使用CRUD函数名称 URI不应用于指示执行CRUD功能。URI应该用于唯一标识资源，而不是对资源进行任何操作。应该使用HTTP请求方法来指示执行了哪个CRUD功能。\nHTTP GET http://api.example.com/device-management/managed-devices //获取所有设备 HTTP POST http://api.example.com/device-management/managed-devices //创建新设备 HTTP GET http://api.example.com/device-management/managed-devices/{id} //获取给定ID的设备 HTTP PUT http://api.example.com/device-management/managed-devices/{id} //更新给定ID的设备 HTTP DELETE http://api.example.com/device-management/managed-devices/{id} //删除给定ID的设备 6. 使用查询组件过滤URL集合 很多时候，您会遇到需求，在这些需求中，您将需要根据某些资源属性对资源进行排序，过滤或限制的集合。为此，请勿创建新的API，而应在资源收集API中启用排序，过滤和分页功能，并将输入参数作为查询参数传递。例如\nhttp://api.example.com/device-management/managed-devices http://api.example.com/device-management/managed-devices?region=USA http://api.example.com/device-management/managed-devices?region=USA\u0026amp;brand=XYZ http://api.example.com/device-management/managed-devices?region=USA\u0026amp;brand=XYZ\u0026amp;sort=installation-date 7. 使用正确的HTTP Status Code表示访问状态 8. 错误返回使用明确易懂的文本 ","permalink":"http://yangchnet.github.io/Dessert/posts/net/restful-api/","summary":"RESTful API 入门 1. 简介 表现层状态转换（英语：Representational State Transfer，缩写：REST）是Roy Thomas Fielding博士于2000年在他的博士论文中提出来的一种万维网软件架构风格，目的是便于不同软件/程序在网络（例如互联网）中互相传递信息。表现层状态转换是根基于超文本传输协议（HTTP）之上而确定的一组约束和属性，是一种设计提供万维网络服务的软件构建风格。符合或兼容于这种架构风格（简称为 REST 或 RESTful）的网络服务，允许客户端发出以统一资源标识符访问和操作网络资源的请求，而与预先定义好的无状态操作集一致化。因此表现层状态转换提供了在互联网络的计算系统之间，彼此资源可交互使用的协作性质（interoperability）。相对于其它种类的网络服务，例如SOAP服务，则是以本身所定义的操作集，来访问网络上的资源。\n2. REST 架构约束   客户端－服务器 从本质上讲，这意味着客户端应用程序和服务器应用程序必须能够独立发展而彼此之间没有任何依赖关系。客户端应该只知道资源URI，仅此而已。今天，这是Web开发中的常规做法，因此您不需要任何花哨。把事情简单化。\n 服务器和客户端也可以独立替换和开发，只要它们之间的接口没有更改即可。\n   无状态\nRoy fielding的灵感来自HTTP，因此它反映了这一约束。使所有客户端-服务器交互都变为无状态。服务器将不存储有关客户端发出的最新HTTP请求的任何内容。它将每个请求视为新请求。没有会议，没有历史。\n如果客户端应用程序需要是最终用户的有状态应用程序，则用户必须登录一次并在此之后执行其他授权操作，则来自客户端的每个请求都应包含服务于该请求的所有必要信息，包括身份验证和授权细节。\n 请求之间不得在服务器上存储任何客户端上下文。客户端负责管理应用程序的状态。\n   统一的接口 在约束名称本身适用的情况下，您必须为系统内部暴露给API使用者并认真遵循的资源确定API接口。系统中的资源应仅具有一个逻辑URI，并且应提供一种获取相关或附加数据的方式。最好将资源与网页同义。\n任何单个资源都不应太大，并在其表示中包含所有内容。只要相关，资源应包含指向相对URI的链接（HATEOAS），以获取相关信息。\n此外，整个系统上的资源表示应遵循特定的准则，例如命名约定，链接格式或数据格式（XML或/和JSON）。\n所有资源都应通过通用方法（例如HTTP GET）进行访问，并使用一致的方法进行类似的修改。\n 一旦开发人员熟悉您的一个API，他就应该能够对其他API遵循类似的方法。\n   分层系统\nREST允许您使用分层的系统架构，在该架构中，您可以在服务器A上部署API，并在服务器B上存储数据并在服务器C中对请求进行身份验证。客户端通常无法确定它是直接连接到最终服务器还是中​​间连接。\n  可缓存的\n在当今世界中，缓存数据和响应在任何适用/可能的地方都至关重要。我们阅读的网页也是HTML页面的缓存版本。缓存可以提高客户端的性能，并为服务器提供更好的可伸缩性。\n在REST中，缓存应在适用时应用于资源，然后这些资源必须声明自己可缓存。可以在服务器或客户端上实现缓存。\n 管理良好的缓存部分或完全消除了某些客户端-服务器交互，从而进一步提高了可伸缩性和性能。\n   按需代码（可选）\n好吧，这个约束是可选的。大多数时候，您将以XML或JSON的形式发送资源的静态表示。但是，如果需要，您可以自由地return executable code支持应用程序的一部分，例如，客户端可以调用您的API来获取UI小部件呈现代码。这是允许的。\n 以上所有约束条件都可以帮助您构建真正的RESTful API，并且应该遵循它们。不过，有时您可能会发现自己违反了一两个约束。别担心; 您仍在制作RESTful API，但不是“真正的RESTful”。\n   3. REST资源命名指南 在REST中，主要数据表示称为Resource。从长远来看，拥有一个强大且一致的REST资源命名策略–无疑将证明是最佳的设计决策之一。","title":"RESTfulAPI入门"},{"content":"Simple Support Vector Machine First we will import numpy to easily manage linear algebra and calculus operations in python. To plot the learning progress later on, we will use matplotlib.\nimport numpy as np from matplotlib import pyplot as plt %matplotlib inline Stochastic Gradient Descent The svm will learn using the stochastic gradient descent algorithm (SGD). Gradient Descent minimizes a function by following the gradients of the cost function.\nCalculating the Error To calculate the error of a prediction we first need to define the objective function of the svm.\nHinge Loss Function To do this, we need to define the loss function, to calculate the prediction error. We will use hinge loss for our perceptron:\n$$c(x, y, f(x)) = (1 - y * f(x))_+$$\n$c$ is the loss function, $x$ the sample, $y$ is the true label, $f(x)$ the predicted label.\nThis means the following: $$ c(x, y, f(x))= \\begin{cases} 0,\u0026amp; \\text{if } yf(x)\\geq 1\\\n1-yf(x), \u0026amp; \\text{else} \\end{cases} $$\nSo consider, if y and f(x) are signed values $(+1,-1)$:\nObjective Function As we defined the loss function, we can now define the objective function for the svm:\n$$\\underset{w}{min}\\ \\lambda\\parallel w\\parallel^2 + \\ \\sum_{i=1}^n\\big(1-y_i \\langle x_i,w \\rangle\\big)_+$$\nAs you can see, our objective of a svm consists of two terms. The first term is a regularizer, the second term the loss. The regularizer balances between margin maximization and loss. To get more informations I advice you the tutorial introduction of the above adviced Schölkopf \u0026amp; Smola book.\nDerive the Objective Function To minimize this function, we need the gradients of this function.\nAs we have two terms, we will derive them seperately using the sum rule in differentiation.\n$$ \\frac{\\delta}{\\delta w_k} \\lambda\\parallel w\\parallel^2 \\ = 2 \\lambda w_k $$\n$$ \\frac{\\delta}{\\delta w_k} \\big(1-y_i \\langle x_i,w \\rangle\\big)+ \\ = \\begin{cases} 0,\u0026amp; \\text{if } y_i \\langle x_i,w \\rangle\\geq 1\\\n-y_ix{ik}, \u0026amp; \\text{else} \\end{cases} $$\nThis means, if we have a misclassified sample $x_i$, respectively $y_i \\langle x_i,w \\rangle \\ \u0026lt; \\ 1$, we update the weight vector w using the gradients of both terms, if $y_i \\langle x_i,w \\rangle \\geq 1$ we just update w by the gradient of the regularizer. To sum it up, our stochastic gradient descent for the svm looks like this:\nif $y_i⟨x_i,w⟩ \u0026lt; 1$: $$ w = w + \\eta (y_ix_i - 2\\lambda w) $$ else: $$ w = w + \\eta (-2\\lambda w) $$\nOur Data Set First we need to define a labeled data set. If you read the perceptron tutorial you will already know it.\nX = np.array([ [-2, 4], [4, 1], [1, 6], [2, 4], [6, 2] ]) y = np.array([-1,-1,1,1,1]) For simplicity\u0026rsquo;s sake we again fold the bias term into the data set:\nX = np.array([ [-2,4,-1], [4,1,-1], [1, 6, -1], [2, 4, -1], [6, 2, -1], ]) y = np.array([-1,-1,1,1,1]) This small toy data set contains two samples labeled with $-1$ and three samples labeled with $+1$. This means we have a binary classification problem, as the data set contains two sample classes. Lets plot the dataset to see, that is is linearly seperable:\nfor d, sample in enumerate(X): # Plot the negative samples if d \u0026lt; 2: plt.scatter(sample[0], sample[1], s=120, marker=\u0026#39;_\u0026#39;, linewidths=2) # Plot the positive samples else: plt.scatter(sample[0], sample[1], s=120, marker=\u0026#39;+\u0026#39;, linewidths=2) # Print a possible hyperplane, that is seperating the two classes. plt.plot([-2,6],[6,0.5]) [\u0026lt;matplotlib.lines.Line2D at 0x7f4b846e42e8\u0026gt;]  Lets Start implementing Stochastic Gradient Descent Finally we can code our SGD algorithm using our update rules. In opposite to the perceptrons objective function, we use a regularizer in our algorithm. As we have a small data set, which is easily lineary seperable, this is actually not needed and our stochastic gradient descent algorithm would probably converge faster without it. To give you a more powerfull code at hand, I will keep it in the following algorithm.\nTo keep it simple, we will linearly loop over the sample set. For larger data sets it makes sence, to randomly pick a sample during each iteration in the for-loop.\ndef svm_sgd(X, Y): w = np.zeros(len(X[0])) eta = 1 epochs = 100000 for epoch in range(1,n): for i, x in enumerate(X): if (Y[i]*np.dot(X[i], w)) \u0026lt; 1: w = w + eta * ( (X[i] * Y[i]) + (-2 *(1/epoch)* w) ) else: w = w + eta * (-2 *(1/epoch)* w) return w We will run the sgd $100000$ times. Our learning parameter eta is set to $1$. As a regulizing parameter we choose $1/t$, so this parameter will decrease, as the number of epochs increases.\nCode Description Line by Line line 2: Initialize the weight vector for the perceptron with zerosline 3: Set the learning rate to 1line 4: Set the number of epochsline 6: Iterate n times over the whole data set. The Iterator is begins with $1$ to avoid division by zero during regularization parameter calculationline 7: Iterate over each sample in the data set. line 8: Misclassification condition $y_i \\langle x_i,w \\rangle \u0026lt; 1$line 9: Update rule for the weights $w = w + \\eta (y_ix_i - 2\\lambda w)$ including the learning rate $\\eta$ and the regularizer $\\lambda$line 11: If classified correctly just update the weight vector by the derived regularizer term $w = w + \\eta (-2\\lambda w)$.Let the SVM learn! Next we can execute our code, to calculate the proper weight vector, which fits out training data. If there are misclassified samples we will print the number of misclassified and correctly classified samples.\ndef svm_sgd_plot(X, Y): w = np.zeros(len(X[0])) eta = 1 epochs = 100000 errors = [] for epoch in range(1,epochs): error = 0 for i, x in enumerate(X): if (Y[i]*np.dot(X[i], w)) \u0026lt; 1: w = w + eta * ( (X[i] * Y[i]) + (-2 *(1/epoch)* w) ) error = 1 else: w = w + eta * (-2 *(1/epoch)* w) errors.append(error) print(w) plt.plot(errors, \u0026#39;|\u0026#39;) plt.ylim(0.5,1.5) plt.axes().set_yticklabels([]) plt.xlabel(\u0026#39;Epoch\u0026#39;) plt.ylabel(\u0026#39;Misclassified\u0026#39;) plt.show() svm_sgd_plot(X,y) [ 1.58876117 3.17458055 11.11863105] /usr/lib/python3.7/site-packages/matplotlib/figure.py:98: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance. In a future version, a new instance will always be created and returned. Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance. \u0026quot;Adding an axes using the same arguments as a previous axes \u0026quot;  Homework Complete your own Support Vector Machine!\n","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/svm-primal/","summary":"Simple Support Vector Machine First we will import numpy to easily manage linear algebra and calculus operations in python. To plot the learning progress later on, we will use matplotlib.\nimport numpy as np from matplotlib import pyplot as plt %matplotlib inline Stochastic Gradient Descent The svm will learn using the stochastic gradient descent algorithm (SGD). Gradient Descent minimizes a function by following the gradients of the cost function.\nCalculating the Error To calculate the error of a prediction we first need to define the objective function of the svm.","title":"SimpleSupportVectorMachine"},{"content":"import tensorflow as tf import numpy as np from IPython.display import Image tf.ones_like() 创建一个所有元素设置为1的tensor\ntf.subtract() 两个矩阵相减\n decision_p_comp = tf.subtract(tf.ones_like(decision_p), decision_p)\n  这一句计算出1-d\n tf.stack 矩阵拼接，例如\na = tf.constant([1,2,3]) b = tf.constant([4,5,6]) c = tf.stack([a, b], axis = 0) d = tf.stack([a, b], axis = 1) sess = tf.Session() print(sess.run(c)) print(sess.run(d)) [[1 2 3] [4 5 6]] [[1 4] [2 5] [3 6]]  tf.expand_dims 在axis位置增加一个维度\ntf.tile 在同一维度上进行复制\nwith tf.Graph().as_default(): a = tf.constant([1,2],name=\u0026#39;a\u0026#39;) b = tf.tile(a,[3]) sess = tf.Session() print(sess.run(b)) [1 2 1 2 1 2]  axis 0 代表行 1 代表列\ntf.reduce_mean 计算张量的各个维度上的元素的平均值。\nx = tf.constant([[1., 1.], [2., 2.]]) tf.reduce_mean(x) # 1.5 tf.reduce_mean(x, 0) # [1.5, 1.5] tf.reduce_mean(x, 1) # [1., 2.] \u0026lt;tf.Tensor 'Mean_2:0' shape=(2,) dtype=float32\u0026gt;  tf.nn.dropout dropout的作用是为了防止或减轻过拟合，它一般用在全连接层 通过在不同的训练过程中随机扔掉一部分神经元，也就是让某个神经元的激活值以一定的概率p, 让其停止工作，这次训练过程中不更新权值，也不参加神经网络的计算，但是他的权重得保存下来，待下次样本输入时可能会重新工作。 其函数原型： tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None,name=None) 第一个参数为输入，第二个参数设置神经元被选中的概率，\ntf.gather 根据索引，从输入张量中依次取元素，构成一个新的张量\n","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/tensorflowapi/","summary":"import tensorflow as tf import numpy as np from IPython.display import Image tf.ones_like() 创建一个所有元素设置为1的tensor\ntf.subtract() 两个矩阵相减\n decision_p_comp = tf.subtract(tf.ones_like(decision_p), decision_p)\n  这一句计算出1-d\n tf.stack 矩阵拼接，例如\na = tf.constant([1,2,3]) b = tf.constant([4,5,6]) c = tf.stack([a, b], axis = 0) d = tf.stack([a, b], axis = 1) sess = tf.Session() print(sess.run(c)) print(sess.run(d)) [[1 2 3] [4 5 6]] [[1 4] [2 5] [3 6]]  tf.expand_dims 在axis位置增加一个维度\ntf.tile 在同一维度上进行复制\nwith tf.","title":"tf.ones_like()"},{"content":"ubuntu中增加用户 sudo useradd -m [username] -s /bin/bash #创建账户，使用/bin/bash作为shell sudo passwd [username] #设置密码 sudo adduser [username] sudo #添加管理员权限 su [username]#切换用户 ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/ubuntu%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7/","summary":"ubuntu中增加用户 sudo useradd -m [username] -s /bin/bash #创建账户，使用/bin/bash作为shell sudo passwd [username] #设置密码 sudo adduser [username] sudo #添加管理员权限 su [username]#切换用户 ","title":"ubuntu中增加用户"},{"content":"Ubuntu完全删除nginx 1. 卸载nginx及相关软件  卸载nginx  sudo apt-get --purge remove nginx  移除全部无用包  sudo apt-get autoremove  列出与nginx相关的软件  dpkg --get-selections | grep nginx  删除之  sudo apt-get --purge remove nginx-common sudo apt-get --purge remove nginx-core 2. 停止所有与nginx有关的进程  查看相关进程  ps -ef | grep nginx  停止这些进程  sudo kill -9 {process_id}  00:00:00 grep \u0026ndash;color=auto nginx 这个不是\n 3. 查找主机中与nginx相关的文件 使用命令：\nsudo find / -name nginx* 删除之\nsudo rm -rf {dir} 4. 现在可以重新安装nginx sudo apt-get update sudo apt-get install nginx ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/%E5%AE%8C%E5%85%A8%E5%88%A0%E9%99%A4nginx/","summary":"Ubuntu完全删除nginx 1. 卸载nginx及相关软件  卸载nginx  sudo apt-get --purge remove nginx  移除全部无用包  sudo apt-get autoremove  列出与nginx相关的软件  dpkg --get-selections | grep nginx  删除之  sudo apt-get --purge remove nginx-common sudo apt-get --purge remove nginx-core 2. 停止所有与nginx有关的进程  查看相关进程  ps -ef | grep nginx  停止这些进程  sudo kill -9 {process_id}  00:00:00 grep \u0026ndash;color=auto nginx 这个不是\n 3. 查找主机中与nginx相关的文件 使用命令：\nsudo find / -name nginx* 删除之","title":"Ubuntu完全删除nginx"},{"content":"Vue中的指令介绍 指令  解释：指令 (Directives) 是带有 v- 前缀的特殊属性 作用：当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM  v-text 解释：更新元素的 textContent\n\u0026lt;h1 v-text=\u0026#34;msg\u0026#34;\u0026gt;\u0026lt;/h1\u0026gt; v-html 解释：更新元素的 innerHTML\n\u0026lt;h1 v-html=\u0026#34;msg\u0026#34;\u0026gt;\u0026lt;/h1\u0026gt; v-bind 作用：当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM.响应式地更新 HTML attribute： 语法：v-bind:title=\u0026quot;msg\u0026quot; 简写：:title=\u0026quot;msg\u0026quot;\n\u0026lt;!-- 完整语法 --\u0026gt; \u0026lt;a v-bind:href=\u0026#34;url\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- 缩写 --\u0026gt; \u0026lt;a :href=\u0026#34;url\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;script\u0026gt; // 2 创建 Vue 的实例对象  var vm = new Vue({ // el 用来指定vue挂载到页面中的元素，值是：选择器  // 理解：用来指定vue管理的HTML区域  el: \u0026#39;#app\u0026#39;, // 数据对象，用来给视图中提供数据的  data: { url: \u0026#39;http://www.baidu.com\u0026#39; } }) \u0026lt;/script\u0026gt; v-on 作用：绑定事件 语法：v-on:click=\u0026quot;say\u0026quot; or v-on:click=\u0026quot;say('参数', $event)\u0026quot; 简写：@click=\u0026quot;say\u0026quot; 说明：绑定的事件从methods中获取\n\u0026lt;!-- 完整语法 --\u0026gt; \u0026lt;a v-on:click=\u0026#34;doSomething\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- 缩写 --\u0026gt; \u0026lt;a @click=\u0026#34;doSomething\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- 方法传参 --\u0026gt; \u0026lt;a @click=\u0026#34;doSomething（“123”）\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;script\u0026gt; // 2 创建 Vue 的实例对象  var vm = new Vue({ el: \u0026#39;#app\u0026#39;, // methods属性用来给vue实例提供方法（事件）  methods: { doSomething: function(str) { //接受参数，并输出  console.log(str); } } }) \u0026lt;/script\u0026gt; 事件修饰符  .stop 阻止冒泡，调用 event.stopPropagation() .prevent 阻止默认事件，调用 event.preventDefault() .capture 添加事件侦听器时使用事件捕获模式 .self 只当事件在该元素本身（比如不是子元素）触发时触发回调 .once 事件只触发一次  v-model 作用：在表单元素上创建双向数据绑定 说明：监听用户的输入事件以更新数据\n\u0026lt;input v-model=\u0026#34;message\u0026#34; placeholder=\u0026#34;edit me\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Message is: {{ message }}\u0026lt;/p\u0026gt; v-for 作用：基于源数据多次渲染元素或模板块\n\u0026lt;!-- 1 基础用法 --\u0026gt; \u0026lt;div v-for=\u0026#34;item in items\u0026#34;\u0026gt; {{ item.text }} \u0026lt;/div\u0026gt; \u0026lt;!-- item 为当前项，index 为索引 --\u0026gt; \u0026lt;p v-for=\u0026#34;(item, index) in list\u0026#34;\u0026gt;{{item}} -- {{index}}\u0026lt;/p\u0026gt; \u0026lt;!-- item 为值，key 为键，index 为索引 --\u0026gt; \u0026lt;p v-for=\u0026#34;(item, key, index) in obj\u0026#34;\u0026gt;{{item}} -- {{key}}\u0026lt;/p\u0026gt; \u0026lt;p v-for=\u0026#34;item in 10\u0026#34;\u0026gt;{{item}}\u0026lt;/p\u0026gt; key属性 推荐：使用 v-for 的时候提供 key 属性，以获得性能提升。 说明：使用 key，VUE会基于 key 的变化重新排列元素顺序，并且会移除 key 不存在的元素。\n\u0026lt;div v-for=\u0026#34;item in items\u0026#34; :key=\u0026#34;item.id\u0026#34;\u0026gt; \u0026lt;!-- 内容 --\u0026gt; \u0026lt;/div\u0026gt; 样式处理 class和style 说明：这两个都是HTML元素的属性，使用v-bind，只需要通过表达式计算出字符串结果即可 表达式的类型：字符串、数组、对象 语法：\n\u0026lt;!-- 1 --\u0026gt; \u0026lt;div v-bind:class=\u0026#34;{ active: true }\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; ===\u0026gt; \u0026lt;div class=\u0026#34;active\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- 2 --\u0026gt; \u0026lt;div :class=\u0026#34;[\u0026#39;active\u0026#39;, \u0026#39;text-danger\u0026#39;]\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; ===\u0026gt; \u0026lt;div class=\u0026#34;active text-danger\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- 3 --\u0026gt; \u0026lt;div v-bind:class=\u0026#34;[{ active: true }, errorClass]\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; ===\u0026gt; \u0026lt;div class=\u0026#34;active text-danger\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; --- style --- \u0026lt;!-- 1 --\u0026gt; \u0026lt;div v-bind:style=\u0026#34;{ color: activeColor, fontSize: fontSize + \u0026#39;px\u0026#39; }\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- 2 将多个 样式对象 应用到一个元素上--\u0026gt; \u0026lt;div v-bind:style=\u0026#34;[baseStyles, overridingStyles]\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; v-if 和 v-show*  条件渲染 v-if：根据表达式的值的真假条件，销毁或重建元素 v-show：根据表达式之真假值，切换元素的 display CSS 属性  提升用户体验：v-cloak 这个指令保持在元素上直到关联实例结束编译。和 CSS 规则如 [v-cloak] { display: none } 一起用时，这个指令可以隐藏未编译的 Mustache 标签直到实例准备完毕。 防止刷新页面，网速慢的情况下出现{{ message }}等数据格式\n\u0026lt;div v-cloak\u0026gt; {{ message }} \u0026lt;/div\u0026gt; 提升性能：v-pre 说明：跳过这个元素和它的子元素的编译过程。可以用来显示原始 Mustache 标签。跳过大量没有指令的节点会加快编译。\n\u0026lt;span vpre\u0026gt;{{ this will not be compiled }}\u0026lt;/span\u0026gt; 提升性能：v-once 说明：只渲染元素和组件一次。随后的重新渲染，元素/组件及其所有的子节点将被视为静态内容并跳过。这可以用于优化更新性能。\n\u0026lt;span v-once\u0026gt;This will never change: {{msg}}\u0026lt;/span\u0026gt; ","permalink":"http://yangchnet.github.io/Dessert/posts/%E5%89%8D%E7%AB%AF/vue%E6%8C%87%E4%BB%A4/","summary":"Vue中的指令介绍 指令  解释：指令 (Directives) 是带有 v- 前缀的特殊属性 作用：当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM  v-text 解释：更新元素的 textContent\n\u0026lt;h1 v-text=\u0026#34;msg\u0026#34;\u0026gt;\u0026lt;/h1\u0026gt; v-html 解释：更新元素的 innerHTML\n\u0026lt;h1 v-html=\u0026#34;msg\u0026#34;\u0026gt;\u0026lt;/h1\u0026gt; v-bind 作用：当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM.响应式地更新 HTML attribute： 语法：v-bind:title=\u0026quot;msg\u0026quot; 简写：:title=\u0026quot;msg\u0026quot;\n\u0026lt;!-- 完整语法 --\u0026gt; \u0026lt;a v-bind:href=\u0026#34;url\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- 缩写 --\u0026gt; \u0026lt;a :href=\u0026#34;url\u0026#34;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;script\u0026gt; // 2 创建 Vue 的实例对象  var vm = new Vue({ // el 用来指定vue挂载到页面中的元素，值是：选择器  // 理解：用来指定vue管理的HTML区域  el: \u0026#39;#app\u0026#39;, // 数据对象，用来给视图中提供数据的  data: { url: \u0026#39;http://www.baidu.com\u0026#39; } }) \u0026lt;/script\u0026gt; v-on 作用：绑定事件 语法：v-on:click=\u0026quot;say\u0026quot; or v-on:click=\u0026quot;say('参数', $event)\u0026quot; 简写：@click=\u0026quot;say\u0026quot; 说明：绑定的事件从methods中获取","title":"Vue中的指令介绍"},{"content":"Windows装系统遇到的问题 1. 问题描述 windows无法安装到这个磁盘，选中的磁盘具有MBR分区表。在EFI系统上，Windows只能安装到GPT磁盘\n2. 解决办法   首先选择U盘安装，进入安装界面\n  按shift+F10打开命令行\n  输入diskpart并回车\n  输入list disk查看磁盘，一般会出现两个磁盘，一个是机器本身的磁盘，编号为0，另一个为U盘，编号为1\n  输入select disk x（x为要选择的磁盘编号） cmd会提示当前选择的磁盘为x\n  执行clean命令清除该磁盘上所有分区信息，并且会清空所有硬盘数据\n  执行convert gpt，将该硬盘转化为GPT格式\n  完成，继续安装系统\n  ","permalink":"http://yangchnet.github.io/Dessert/posts/windows/mbr-gpt/","summary":"Windows装系统遇到的问题 1. 问题描述 windows无法安装到这个磁盘，选中的磁盘具有MBR分区表。在EFI系统上，Windows只能安装到GPT磁盘\n2. 解决办法   首先选择U盘安装，进入安装界面\n  按shift+F10打开命令行\n  输入diskpart并回车\n  输入list disk查看磁盘，一般会出现两个磁盘，一个是机器本身的磁盘，编号为0，另一个为U盘，编号为1\n  输入select disk x（x为要选择的磁盘编号） cmd会提示当前选择的磁盘为x\n  执行clean命令清除该磁盘上所有分区信息，并且会清空所有硬盘数据\n  执行convert gpt，将该硬盘转化为GPT格式\n  完成，继续安装系统\n  ","title":"Windows装系统遇到的问题"},{"content":"WordPress安装踩坑 1. 第一个坑，忘了安装PHP。。。 2. 第二个坑，访问页面not found 发现是因为同时开了apache2和nginx,导致冲突了，把nginx关掉就好了\n3. 第三个坑，打开页面全是源代码  打开/etc/apache2/apache2.conf，将以下内容添加到文件的底部：  \u0026lt;FilesMatch \\ .php $\u0026gt; SetHandler application / x-httpd-php \u0026lt;/ FilesMatch\u0026gt; 为了使PHP正常运行，您必须禁用mpm_event模块并启用mpm_prefork和php7模块。为此，请返回您的终端窗口并发出命令：  sudo a2dismod mpm_event \u0026amp;\u0026amp; sudo a2enmod mpm_prefork \u0026amp;\u0026amp; sudo a2enmod php7.0 4. 在执行上面的命令时，遇到了第四个坑 ERROR: Module php7.0 does not exist! 解决办法\nsudo apt-get install libapache2-mod-php7.0 5. 您的PHP似乎没有安装运行WordPress所必需的MySQL扩展。 sudo apt-get install php-mysql 爬出来了。。\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/wordpress%E5%AE%89%E8%A3%85%E8%B8%A9%E5%9D%91/","summary":"WordPress安装踩坑 1. 第一个坑，忘了安装PHP。。。 2. 第二个坑，访问页面not found 发现是因为同时开了apache2和nginx,导致冲突了，把nginx关掉就好了\n3. 第三个坑，打开页面全是源代码  打开/etc/apache2/apache2.conf，将以下内容添加到文件的底部：  \u0026lt;FilesMatch \\ .php $\u0026gt; SetHandler application / x-httpd-php \u0026lt;/ FilesMatch\u0026gt; 为了使PHP正常运行，您必须禁用mpm_event模块并启用mpm_prefork和php7模块。为此，请返回您的终端窗口并发出命令：  sudo a2dismod mpm_event \u0026amp;\u0026amp; sudo a2enmod mpm_prefork \u0026amp;\u0026amp; sudo a2enmod php7.0 4. 在执行上面的命令时，遇到了第四个坑 ERROR: Module php7.0 does not exist! 解决办法\nsudo apt-get install libapache2-mod-php7.0 5. 您的PHP似乎没有安装运行WordPress所必需的MySQL扩展。 sudo apt-get install php-mysql 爬出来了。。","title":"WordPress安装踩坑"},{"content":"使用sklearn的贝叶斯分类器进行文本分类 1、sklearn简介 sklearn是一个Python第三方提供的非常强力的机器学习库，它包含了从数据预处理到训练模型的各个方面。在实战使用scikit-learn中可以极大的节省我们编写代码的时间以及减少我们的代码量，使我们有更多的精力去分析数据分布，调整模型和修改超参。\n2、朴素贝叶斯在文本分类中的常用模型：多项式、伯努利 朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模(Bernoulli model)即文档型。二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。这里暂不虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。\n2.1、多项式模型 2.2、伯努利模型 2.3、两个模型的区别 3、实战演练 使用在康奈尔大学下载的2M影评作为训练数据和测试数据，里面共同、共有1400条，好评和差评各自700条，我选择总数的70%作为训练数据，30%作为测试数据，来检测sklearn自带的贝叶斯分类器的分类效果。\n  读取全部数据，并随机打乱\n import os import random def get_dataset(): data = [] for root, dirs, files in os.walk(\u0026#39;../dataset/aclImdb/neg\u0026#39;): for file in files: realpath = os.path.join(root, file) with open(realpath, errors=\u0026#39;ignore\u0026#39;) as f: data.append((f.read(), 0)) for root, dirs, files in os.walk(r\u0026#39;../dataset/aclImdb/pos\u0026#39;): for file in files: realpath = os.path.join(root, file) with open(realpath, errors=\u0026#39;ignore\u0026#39;) as f: data.append((f.read(), 1)) random.shuffle(data) return data data = get_dataset() data[:2] [(\u0026quot;Being a fan of Andy Goldsworthy's art for a while now, and owning some of his books, I had some expectations of what I would see. What I got was something completely satisfying, and quite a bit more than I expected. Being an artist myself (I work in clay), finding inspiration within our surroundings to make good art is imperative, and it is something Andy Goldsworthy has mastered. Following him over the course of a year, the director captures the spontaneous energy, skill, and devotion to the artists connection with nature with dratic inspiring flair. The music set to the film is embracing and intoxicating. If you are an artist in need of inspiration, or anyone else in need of an uplifting experience, then SEE THIS MOVIE. I for one am glad to know that Andy is somewhere out there. Creating, dancing, wrestling with the forces of nature to make our world more beautiful.\u0026quot;, 1), (\u0026quot;A film I expected very little from, and only watched to pass a quiet hour - but what an hour it turned out to be. Roll is an excellent if none-too-serious little story of 'country-boy-lost-in-the-big-city-makes-good', it is funny throughout, the characters are endearing and the pace is just right.\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;Toby Malone is the true star of the film with his endearing portrayal of Matt, said country boy and local Aussie Rules football hero come to the big city to try out for one of the big teams. He is supported superbly by John Batchelor as local gangster Tiny. Watch out for these two.\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;Highly recommended.\u0026quot;, 1)]    按照7:3的比例划分训练集和测试集\n def train_and_test_data(data_): filesize = int(0.7 * len(data_)) # 训练集和测试集的比例为7:3 train_data_ = [each[0] for each in data_[:filesize]] train_target_ = [each[1] for each in data_[:filesize]] test_data_ = [each[0] for each in data_[filesize:]] test_target_ = [each[1] for each in data_[filesize:]] return train_data_, train_target_, test_data_, test_target_ train_data, train_target, test_data, test_target = train_and_test_data(data)   使用多项式贝叶斯分类器\n from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer from sklearn import metrics from sklearn.naive_bayes import BernoulliNB nbc = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer()), (\u0026#39;clf\u0026#39;, MultinomialNB(alpha=1.0)), ]) nbc.fit(train_data, train_target) #训练我们的多项式模型贝叶斯分类器 predict = nbc.predict(test_data) #在测试集上预测结果 y_score = nbc.fit(train_data, train_target).predict_proba(test_data) print(y_score) count = 0 #统计预测正确的结果个数 for left , right in zip(predict, test_target): if left == right: count += 1 print(count/len(test_target)) [[0.21379806 0.78620194] [0.61108605 0.38891395] [0.25629837 0.74370163] ... [0.33889503 0.66110497] [0.73665026 0.26334974] [0.1870178 0.8129822 ]] 0.8596    使用伯努利模型分类器\n nbc_1= Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer()), (\u0026#39;clf\u0026#39;, BernoulliNB(alpha=0.1)), ]) nbc_1.fit(train_data, train_target) predict = nbc_1.predict(test_data) #在测试集上预测结果 count = 0 #统计预测正确的结果个数 for left , right in zip(predict, test_target): if left == right: count += 1 print(count/len(test_target)) 0.8818635607321131   从分类结果可以看出，和多项式模型相比，使用伯努利模型的贝叶斯分类器，在文本分类方面的精度相比，差别不大，我们可以针对我们面对的具体问题，进行实验，选择最为合适的分类器。\n作业 sklearn中一共提供了四种贝叶斯分类器：\n 高斯朴素贝叶斯 多项式朴素贝叶斯 补充朴素贝叶斯 伯努利朴素贝叶斯  从四种贝叶斯分类器模型中找出具有最佳分类效果的分类器，并用直方图直观表示其分类准确率。\n参考资料 sklearn官方网站\nhttps://scikit-learn.org/stable/index.html\nsklearn:朴素贝叶斯 https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes\n","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/sklearn_%E8%B4%9D%E5%8F%B6%E6%96%AF/","summary":"使用sklearn的贝叶斯分类器进行文本分类 1、sklearn简介 sklearn是一个Python第三方提供的非常强力的机器学习库，它包含了从数据预处理到训练模型的各个方面。在实战使用scikit-learn中可以极大的节省我们编写代码的时间以及减少我们的代码量，使我们有更多的精力去分析数据分布，调整模型和修改超参。\n2、朴素贝叶斯在文本分类中的常用模型：多项式、伯努利 朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模(Bernoulli model)即文档型。二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。这里暂不虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。\n2.1、多项式模型 2.2、伯努利模型 2.3、两个模型的区别 3、实战演练 使用在康奈尔大学下载的2M影评作为训练数据和测试数据，里面共同、共有1400条，好评和差评各自700条，我选择总数的70%作为训练数据，30%作为测试数据，来检测sklearn自带的贝叶斯分类器的分类效果。\n  读取全部数据，并随机打乱\n import os import random def get_dataset(): data = [] for root, dirs, files in os.walk(\u0026#39;../dataset/aclImdb/neg\u0026#39;): for file in files: realpath = os.path.join(root, file) with open(realpath, errors=\u0026#39;ignore\u0026#39;) as f: data.append((f.read(), 0)) for root, dirs, files in os.walk(r\u0026#39;../dataset/aclImdb/pos\u0026#39;): for file in files: realpath = os.path.join(root, file) with open(realpath, errors=\u0026#39;ignore\u0026#39;) as f: data.append((f.read(), 1)) random.shuffle(data) return data data = get_dataset() data[:2] [(\u0026quot;Being a fan of Andy Goldsworthy's art for a while now, and owning some of his books, I had some expectations of what I would see.","title":"使用sklearn的贝叶斯分类器进行文本分类"},{"content":"分类指标作业（第二题） 题目 给定完整数据集，分别计算在使用完整数据集的10%,30%,50%,80%,100%数据时的查准率、查全率，f1度量和ROC，使用折线图表现出这些指标的变化情况，并画出在不同数据量下的ROC曲线\n加载数据集 import os import random def get_dataset(): data = [] for root, dirs, files in os.walk(\u0026#39;../dataset/aclImdb/neg\u0026#39;): for file in files: realpath = os.path.join(root, file) with open(realpath, errors=\u0026#39;ignore\u0026#39;) as f: data.append((f.read(), 0)) for root, dirs, files in os.walk(r\u0026#39;../dataset/aclImdb/pos\u0026#39;): for file in files: realpath = os.path.join(root, file) with open(realpath, errors=\u0026#39;ignore\u0026#39;) as f: data.append((f.read(), 1)) random.shuffle(data) return data data = get_dataset() data[:2] [('Unless you are between the ages of 10 and 14 (except for the R rating), there are very few things to like here. One or two lines from Kenan Thompson, David Koechner (we really should see him more) and Sam Jackson are humorous and Julianna Margulies is as good as she can be considering her surroundings, but sadly, that\\'s it. Poor plot. Poor acting. Worse writing and delivery. The special effects are dismal. As much as the entire situation is an odd and awful joke, the significant individual embedded situations are all equally terrible. If we consider the action portions, well there are unbelievable action sequences in some films that make you giddy and there are some that make you groan. This movie only contains the latter kind. This leaves little left. I\\'m so glad I did not pay for this.\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;Despite any hype, I can read and think, so as I sat down to watch, I did not expect anything good. I had no expectations, but was somewhat worried going in. Yet, like a train wreck, one cannot merely look away. And even with no expectations, I was let down. Bad. Not even \\'so bad, it\\'s good\\' material. I\\'m _very_ tolerant of bad movies, but this makes \u0026quot;Six String Samurai\u0026quot; (which I liked) Oscar worthy.\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;No, this piece of over CGI\\'d rubbish is in the same company as Battlefield Earth, Little Man and Gigli. How this is currently rated a 7.2 completely mystifies me. Brainwashing or somehow stacking the voting system is all that I can think of as answers.\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;I could go on and on but suffice to say that tonight, I witnessed a train wreck. I need to go wash my eyes. 1 of 10', 0), (\u0026quot;BEFORE THE DEVIL KNOWS YOU'RE DEAD starts off promisingly, setting up a simple heist that goes awry, told from varying perspectives (in RASHOMON style). At around the hour mark, Sidney Lumet transforms this film into something that is so much more than the sum of its parts; it eventually morphs into a multi-faceted family drama, exploring the full realm of human emotions/relations, as the story comes to its chilling climax.\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;As is the case with Lumet, he manages to coax exceptional performances out of his star-studded cast, without any notion of over-acting or hyperbole. Philip Seymour Hoffman, in one of his best roles, is a complex, mysterious, and interesting character, and oftentimes dwarfs Ethan Hawke, who plays his brother, Hank. That's not to say that Hawke is not bad; in fact he is quite above adequate, in a troubled role that suits his style. Marisa Tomei is excellent for her relatively short appearance (the fact that she bares her flesh adds to this). Albert Finney's character (Andy and Hank's father) is the most intriguing, and in my opinion, he deserved a bit more screen-time. Amy Ryan also performs her job adequately.\u0026lt;br /\u0026gt;\u0026lt;br /\u0026gt;BEFORE THE DEVIL KNOWS YOU'RE DEAD is not an exceptional movie, but it proves that Lumet is still near the top of his game at the (apparent) twilight of an illustrious career. Many of his characteristics and trademarks appear here, not least of which involves the use of his characters. Infused with a killer script (no pun intended), smart dialogue and pacing, and a decent score, BEFORE THE DEVIL KNOWS YOU'RE DEAD is a must-see. A truly underrated gem. 8/10. 3 stars (out of 4). Should just enter my Top 250 at 248. Highly recommended.\u0026quot;, 1)]  按7：3划分数据集 def train_and_test_data(data_): filesize = int(0.7 * len(data_)) # 训练集和测试集的比例为7:3 train_data_ = [each[0] for each in data_[:filesize]] train_target_ = [each[1] for each in data_[:filesize]] test_data_ = [each[0] for each in data_[filesize:]] test_target_ = [each[1] for each in data_[filesize:]] return train_data_, train_target_, test_data_, test_target_ train_data, train_target, test_data, test_target = train_and_test_data(data) 定义分类器 from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer from sklearn import metrics from sklearn.naive_bayes import BernoulliNB nbc = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer()), (\u0026#39;clf\u0026#39;, MultinomialNB(alpha=1.0)), ]) nbc.fit(train_data, train_target) #训练我们的多项式模型贝叶斯分类器 predict = nbc.predict(test_data) #在测试集上预测结果 y_score = nbc.fit(train_data, train_target).predict_proba(test_data) print(y_score) count = 0 #统计预测正确的结果个数 for left , right in zip(predict, test_target): if left == right: count += 1 print(count/len(test_target)) [[0.51083071 0.48916929] [0.73102053 0.26897947] [0.30725664 0.69274336] ... [0.7051641 0.2948359 ] [0.95670055 0.04329945] [0.96212725 0.03787275]] 0.8618666666666667  from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer from sklearn import metrics from sklearn.naive_bayes import BernoulliNB from sklearn.metrics import roc_curve, auc import matplotlib.pyplot as plt from itertools import cycle nbc = Pipeline([ (\u0026#39;vect\u0026#39;, TfidfVectorizer()), (\u0026#39;clf\u0026#39;, MultinomialNB(alpha=1.0)), ]) fpr = [] tpr = [] roc_auc = [] for k, v in enumerate([0.1, 0.3, 0.5, 0.8, 1]): train_data_ = train_data[0: int(len(train_data)*v)] train_target_ = train_target[0: int(len(train_data)*v)] test_data_ = test_data[0: int(len(train_data)*v)] test_target_ = test_target[0: int(len(train_data)*v)] nbc.fit(train_data_, train_target_) predict = nbc.predict(test_data_) y_score = nbc.fit(train_data_, train_target_).predict_proba(test_data_) print(y_score) f, t , _ = roc_curve(test_target_, y_score[:, 1]) fpr.append(f) tpr.append(t) roc_auc.append(auc(fpr[k], tpr[k])) plt.figure(figsize=(8,6), dpi=200) colors = cycle([\u0026#39;aqua\u0026#39;, \u0026#39;darkorange\u0026#39;, \u0026#39;cornflowerblue\u0026#39;, \u0026#39;navy\u0026#39;, \u0026#39;green\u0026#39;]) for i, color in zip(range(5), colors): plt.plot(fpr[i], tpr[i], color=color, lw=lw, label=\u0026#39;ROC curve of {0} (area = {1:0.2f})\u0026#39; \u0026#39;\u0026#39;.format([0.1, 0.3, 0.5, 0.8, 1][i], roc_auc[i])) plt.plot([0, 1], [0, 1], color=\u0026#39;black\u0026#39;, lw=lw, linestyle=\u0026#39;--\u0026#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Receiver operating characteristic example\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.show() [[0.52592994 0.47407006] [0.50514363 0.49485637] [0.38757784 0.61242216] ... [0.56621401 0.43378599] [0.35758914 0.64241086] [0.31767289 0.68232711]] [[0.49465951 0.50534049] [0.70456524 0.29543476] [0.42657598 0.57342402] ... [0.49094946 0.50905054] [0.92783796 0.07216204] [0.8024358 0.1975642 ]] [[0.49445208 0.50554792] [0.72624226 0.27375774] [0.35817261 0.64182739] ... [0.72512198 0.27487802] [0.95407643 0.04592357] [0.95014618 0.04985382]] [[0.47846521 0.52153479] [0.74802248 0.25197752] [0.30833458 0.69166542] ... [0.7419486 0.2580514 ] [0.95732057 0.04267943] [0.95765587 0.04234413]] [[0.51083071 0.48916929] [0.73102053 0.26897947] [0.30725664 0.69274336] ... [0.7051641 0.2948359 ] [0.95670055 0.04329945] [0.96212725 0.03787275]]  ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/%E5%88%86%E7%B1%BB%E6%8C%87%E6%A0%87%E4%BD%9C%E4%B8%9A%E7%AC%AC%E4%BA%8C%E9%A2%98/","summary":"分类指标作业（第二题） 题目 给定完整数据集，分别计算在使用完整数据集的10%,30%,50%,80%,100%数据时的查准率、查全率，f1度量和ROC，使用折线图表现出这些指标的变化情况，并画出在不同数据量下的ROC曲线\n加载数据集 import os import random def get_dataset(): data = [] for root, dirs, files in os.walk(\u0026#39;../dataset/aclImdb/neg\u0026#39;): for file in files: realpath = os.path.join(root, file) with open(realpath, errors=\u0026#39;ignore\u0026#39;) as f: data.append((f.read(), 0)) for root, dirs, files in os.walk(r\u0026#39;../dataset/aclImdb/pos\u0026#39;): for file in files: realpath = os.path.join(root, file) with open(realpath, errors=\u0026#39;ignore\u0026#39;) as f: data.append((f.read(), 1)) random.shuffle(data) return data data = get_dataset() data[:2] [('Unless you are between the ages of 10 and 14 (except for the R rating), there are very few things to like here.","title":"分类指标作业（第二题）"},{"content":"删除WordPress  删除网络文件：  rm -Rf /var/www/html/* 删除数据库。首先获取mysql的root密码（通过ssh登录时显示在MOTD中）。  mysql -uroot -p 输入密码后使用语句：\nDROP DATABASE wordpress; 删除WordPress数据库\nexit; ","permalink":"http://yangchnet.github.io/Dessert/posts/linux/%E5%88%A0%E9%99%A4wordpress/","summary":"删除WordPress  删除网络文件：  rm -Rf /var/www/html/* 删除数据库。首先获取mysql的root密码（通过ssh登录时显示在MOTD中）。  mysql -uroot -p 输入密码后使用语句：\nDROP DATABASE wordpress; 删除WordPress数据库\nexit; ","title":"删除WordPress"},{"content":"四种常见的POST类型 1. application/x-www-form-urlencoded 这应该是最常见的 POST 提交数据的方式了。浏览器的原生 表单，如果不设置 enctype 属性，那么最终就会以 application/x-www-form-urlencoded 方式提交数据。请求类似于下面这样（无关的请求头在本文中都省略掉了）：\nPOST http://www.example.com HTTP/1.1 Content-Type: application/x-www-form-urlencoded;charset=utf-8 title=test\u0026amp;sub%5B%5D=1\u0026amp;sub%5B%5D=2\u0026amp;sub%5B%5D=3 首先，Content-Type 被指定为 application/x-www-form-urlencoded；其次，提交的数据按照 key1=val1\u0026amp;key2=val2 的方式进行编码，key 和 val 都进行了 URL 转码。大部分服务端语言都对这种方式有很好的支持。例如 PHP 中，$_POST[\u0026lsquo;title\u0026rsquo;] 可以获取到 title 的值，$_POST[\u0026lsquo;sub\u0026rsquo;] 可以得到 sub 数组。\n很多时候，我们用 Ajax 提交数据时，也是使用这种方式。例如 JQuery 和 QWrap 的 Ajax，Content-Type 默认值都是「application/x-www-form-urlencoded;charset=utf-8」。\n2. multipart/form-data 这又是一个常见的 POST 数据提交的方式。我们使用表单上传文件时，必须让 \u0026lt;form\u0026gt; 表单的 enctype 等于 multipart/form-data。直接来看一个请求示例：\nPOST http://www.example.com HTTP/1.1 Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA ------WebKitFormBoundaryrGKCBY7qhFd3TrwA Content-Disposition: form-data; name=\u0026#34;text\u0026#34; title ------WebKitFormBoundaryrGKCBY7qhFd3TrwA Content-Disposition: form-data; name=\u0026#34;file\u0026#34;; filename=\u0026#34;chrome.png\u0026#34; Content-Type: image/png PNG ... content of chrome.png ... ------WebKitFormBoundaryrGKCBY7qhFd3TrwA-- 这个例子稍微复杂点。首先生成了一个 boundary 用于分割不同的字段，为了避免与正文内容重复，boundary 很长很复杂。然后 Content-Type 里指明了数据是以 multipart/form-data 来编码，本次请求的 boundary 是什么内容。消息主体里按照字段个数又分为多个结构类似的部分，每部分都是以 \u0026ndash;boundary 开始，紧接着是内容描述信息，然后是回车，最后是字段具体内容（文本或二进制）。如果传输的是文件，还要包含文件名和文件类型信息。消息主体最后以 \u0026ndash;boundary\u0026ndash; 标示结束。关于 multipart/form-data 的详细定义，请前往 rfc1867 查看。\n这种方式一般用来上传文件，各大服务端语言对它也有着良好的支持。\n上面提到的这两种 POST 数据的方式，都是浏览器原生支持的，而且现阶段标准中原生 \u0026lt;form\u0026gt; 表单也只支持这两种方式（通过 \u0026lt;form\u0026gt; 元素的 enctype 属性指定，默认为 application/x-www-form-urlencoded。其实 enctype 还支持 text/plain，不过用得非常少）。\n随着越来越多的 Web 站点，尤其是 WebApp，全部使用 Ajax 进行数据交互之后，我们完全可以定义新的数据提交方式，给开发带来更多便利。\n3. application/json application/json 这个 Content-Type 作为响应头大家肯定不陌生。实际上，现在越来越多的人把它作为请求头，用来告诉服务端消息主体是序列化后的 JSON 字符串。由于 JSON 规范的流行，除了低版本 IE 之外的各大浏览器都原生支持 JSON.stringify，服务端语言也都有处理 JSON 的函数，使用 JSON 不会遇上什么麻烦。\nJSON 格式支持比键值对复杂得多的结构化数据，这一点也很有用。记得我几年前做一个项目时，需要提交的数据层次非常深，我就是把数据 JSON 序列化之后来提交的。不过当时我是把 JSON 字符串作为 val，仍然放在键值对里，以 x-www-form-urlencoded 方式提交。\nGoogle 的 AngularJS 中的 Ajax 功能，默认就是提交 JSON 字符串。例如下面这段代码：\nvar data = {\u0026#39;title\u0026#39;:\u0026#39;test\u0026#39;, \u0026#39;sub\u0026#39; : [1,2,3]}; $http.post(url, data).success(function(result) { ... }); 最终发送的请求是：\nPOST http://www.example.com HTTP/1.1 Content-Type: application/json;charset=utf-8 {\u0026#34;title\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;sub\u0026#34;:[1,2,3]}\\ 这种方案，可以方便的提交复杂的结构化数据，特别适合 RESTful 的接口。各大抓包工具如 Chrome 自带的开发者工具、Firebug、Fiddler，都会以树形结构展示 JSON 数据，非常友好。但也有些服务端语言还没有支持这种方式，例如 php 就无法通过 $_POST 对象从上面的请求中获得内容。这时候，需要自己动手处理下：在请求头中 Content-Type 为 application/json 时，从 php://input 里获得原始输入流，再 json_decode 成对象。一些 php 框架已经开始这么做了。\n4. text/xml XML-RPC（XML Remote Procedure Call）是一种使用 HTTP 作为传输协议，XML 作为编码方式的远程调用规范。典型的 XML-RPC 请求是这样的：\nPOST http://www.example.com HTTP/1.1 Content-Type: text/xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;methodCall\u0026gt; \u0026lt;methodName\u0026gt;examples.getStateName\u0026lt;/methodName\u0026gt; \u0026lt;params\u0026gt; \u0026lt;param\u0026gt; \u0026lt;value\u0026gt;\u0026lt;i4\u0026gt;41\u0026lt;/i4\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;/param\u0026gt; \u0026lt;/params\u0026gt; \u0026lt;/methodCall\u0026gt; XML-RPC 协议简单、功能够用，各种语言的实现都有。它的使用也很广泛，如 WordPress 的 XML-RPC Api，搜索引擎的 ping 服务等等。JavaScript 中，也有现成的库支持以这种方式进行数据交互，能很好的支持已有的 XML-RPC 服务。\n","permalink":"http://yangchnet.github.io/Dessert/posts/net/%E5%9B%9B%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84post%E7%B1%BB%E5%9E%8B/","summary":"四种常见的POST类型 1. application/x-www-form-urlencoded 这应该是最常见的 POST 提交数据的方式了。浏览器的原生 表单，如果不设置 enctype 属性，那么最终就会以 application/x-www-form-urlencoded 方式提交数据。请求类似于下面这样（无关的请求头在本文中都省略掉了）：\nPOST http://www.example.com HTTP/1.1 Content-Type: application/x-www-form-urlencoded;charset=utf-8 title=test\u0026amp;sub%5B%5D=1\u0026amp;sub%5B%5D=2\u0026amp;sub%5B%5D=3 首先，Content-Type 被指定为 application/x-www-form-urlencoded；其次，提交的数据按照 key1=val1\u0026amp;key2=val2 的方式进行编码，key 和 val 都进行了 URL 转码。大部分服务端语言都对这种方式有很好的支持。例如 PHP 中，$_POST[\u0026lsquo;title\u0026rsquo;] 可以获取到 title 的值，$_POST[\u0026lsquo;sub\u0026rsquo;] 可以得到 sub 数组。\n很多时候，我们用 Ajax 提交数据时，也是使用这种方式。例如 JQuery 和 QWrap 的 Ajax，Content-Type 默认值都是「application/x-www-form-urlencoded;charset=utf-8」。\n2. multipart/form-data 这又是一个常见的 POST 数据提交的方式。我们使用表单上传文件时，必须让 \u0026lt;form\u0026gt; 表单的 enctype 等于 multipart/form-data。直接来看一个请求示例：\nPOST http://www.example.com HTTP/1.1 Content-Type:multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwA ------WebKitFormBoundaryrGKCBY7qhFd3TrwA Content-Disposition: form-data; name=\u0026#34;text\u0026#34; title ------WebKitFormBoundaryrGKCBY7qhFd3TrwA Content-Disposition: form-data; name=\u0026#34;file\u0026#34;; filename=\u0026#34;chrome.png\u0026#34; Content-Type: image/png PNG .","title":"四种常见的POST类型"},{"content":"在docker中构建django项目 （需安装docker-compose, 安装教程）\n1. 定义项目组件 对于此项目，您需要创建Dockerfile，Python依赖项文件和docker-compose.yml文件。（您可以使用此文件的扩展名.yml或.yaml扩展名。）\n1.1. 创建一个空目录 该目录应仅包含构建该映像的资源。\n1.2 创建Dockerfile 内容如下：\nFROM python:3 ENV PYTHONUNBUFFERED 1 RUN mkdir /code WORKDIR /code COPY requirements.txt /code/ RUN pip install -r requirements.txt COPY . /code/ 对于DockerFile的解释\n1.3 创建requirements.txt 内容如下：\ndjango django-ckeditor pillow numpy 1.4 创建docker-compose.yml 该docker-compose.yml文件描述了构成应用程序的服务。在此示例中，这些服务是Web服务器和数据库。撰写文件还描述了这些服务使用哪些Docker映像，它们如何链接在一起，以及它们可能需要安装在容器内的任何卷。最后，该docker-compose.yml文件描述了这些服务公开的端口。有关此文件如何工作的更多信息，请参阅docker-compose.yml参考。 内容如下：\nversion: \u0026#39;3\u0026#39; services: db: image: postgres web: build: . command: python manage.py runserver 0.0.0.0:8000 volumes: - .:/code ports: - \u0026#34;8000:8000\u0026#34; depends_on: - db 2 创建django项目  切换到项目跟目录】 通过运行docker-compose run 命令创建django项目  sudo docker-compose run web django-admin startproject mysite . 查看项目内容\nls -l 更改文件所有权  sudo chown -R $USER:$USER . 3 连接数据库  打开mysite/setting.py\n改为：  DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.postgresql\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;db\u0026#39;, \u0026#39;PORT\u0026#39;: 5432, } } 从项目的顶级目录运行 docker-compose up命令  ","permalink":"http://yangchnet.github.io/Dessert/posts/cloud/%E5%9C%A8docker%E4%B8%AD%E6%9E%84%E5%BB%BAdjango%E9%A1%B9%E7%9B%AE/","summary":"在docker中构建django项目 （需安装docker-compose, 安装教程）\n1. 定义项目组件 对于此项目，您需要创建Dockerfile，Python依赖项文件和docker-compose.yml文件。（您可以使用此文件的扩展名.yml或.yaml扩展名。）\n1.1. 创建一个空目录 该目录应仅包含构建该映像的资源。\n1.2 创建Dockerfile 内容如下：\nFROM python:3 ENV PYTHONUNBUFFERED 1 RUN mkdir /code WORKDIR /code COPY requirements.txt /code/ RUN pip install -r requirements.txt COPY . /code/ 对于DockerFile的解释\n1.3 创建requirements.txt 内容如下：\ndjango django-ckeditor pillow numpy 1.4 创建docker-compose.yml 该docker-compose.yml文件描述了构成应用程序的服务。在此示例中，这些服务是Web服务器和数据库。撰写文件还描述了这些服务使用哪些Docker映像，它们如何链接在一起，以及它们可能需要安装在容器内的任何卷。最后，该docker-compose.yml文件描述了这些服务公开的端口。有关此文件如何工作的更多信息，请参阅docker-compose.yml参考。 内容如下：\nversion: \u0026#39;3\u0026#39; services: db: image: postgres web: build: . command: python manage.py runserver 0.0.0.0:8000 volumes: - .:/code ports: - \u0026#34;8000:8000\u0026#34; depends_on: - db 2 创建django项目  切换到项目跟目录】 通过运行docker-compose run 命令创建django项目  sudo docker-compose run web django-admin startproject mysite .","title":"在docker中构建django项目"},{"content":"在通过日期获取数据库条目时，出现 django It must be in YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ] format.\u0026quot;]错误 由于日期是从前端获取的，因此将前端日期引用标签改为：{{ comment.time|date:\u0026quot;Y-m-d H:i:s.u\u0026quot;}}\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/yyyy-mm-dd%E9%94%99%E8%AF%AF/","summary":"在通过日期获取数据库条目时，出现 django It must be in YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ] format.\u0026quot;]错误 由于日期是从前端获取的，因此将前端日期引用标签改为：{{ comment.time|date:\u0026quot;Y-m-d H:i:s.u\u0026quot;}}","title":"在通过日期获取数据库条目时，出现djangoItmustbeinYYYY-MM-DDHH:MM[:ss[.uuuuuu]][TZ]format.\"]错误"},{"content":"基于卷积神经网络和决策树的体域网数据融合方法 现阶段想法:在softmax层后接随机森林，通过种树增加分类准确率\nimport tensorflow as tf import numpy as np import tensorflow.examples.tutorials.mnist.input_data as input_data import scipy as sp %matplotlib inline sess = tf.Session() DEPTH = 3 # Depth of a tree N_LEAF = 2 ** (DEPTH + 1) # Number of leaf node N_LABEL = 10 # Number of classes N_TREE = 5 # Number of trees (ensemble) N_BATCH = 128 # Number of data points per mini-batch 分批训练，每一批128个  初始化矩阵 def init_weights(shape): return tf.Variable(tf.random_normal(shape, stddev=0.01)) def init_prob_weights(shape, minval=-5, maxval=5): return tf.Variable(tf.random_uniform(shape, minval, maxval))  定义模型  a表示alive,激活之意，eg:l1a,表示layer_1_alive，第一个激活层 w表示weight,权重  def model(X, w, w2, w3, w4_e, w_d_e, w_l_e, p_keep_conv, p_keep_hidden): # 激活层1 \u0026amp; 池化层1 \u0026amp; dropout l1a = tf.nn.relu(tf.nn.conv2d(X, w, [1, 1, 1, 1], \u0026#39;SAME\u0026#39;)) l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\u0026#39;SAME\u0026#39;) l1 = tf.nn.dropout(l1, p_keep_conv) # 激活层2 \u0026amp; 池化层 \u0026amp; dropout l2a = tf.nn.relu(tf.nn.conv2d(l1, w2, [1, 1, 1, 1], \u0026#39;SAME\u0026#39;)) l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\u0026#39;SAME\u0026#39;) l2 = tf.nn.dropout(l2, p_keep_conv) # 激活层3 \u0026amp; 池化层 \u0026amp; full connected layer \u0026amp; dropout l3a = tf.nn.relu(tf.nn.conv2d(l2, w3, [1, 1, 1, 1], \u0026#39;SAME\u0026#39;)) l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\u0026#39;SAME\u0026#39;) l3 = tf.reshape(l3, [-1, w4_e[0].get_shape().as_list()[0]]) l3 = tf.nn.dropout(l3, p_keep_conv) # decision node \u0026amp; prediction node (leaf node) decision_p_e = [] leaf_p_e = [] for w4, w_d, w_l in zip(w4_e, w_d_e, w_l_e): l4 = tf.nn.relu(tf.matmul(l3, w4)) l4 = tf.nn.dropout(l4, p_keep_conv) decision_p = tf.nn.sigmoid(tf.matmul(l4, w_d)) # 从这一句看，好像叶子节点不与决策节点相关 leaf_p = tf.nn.softmax(w_l) decision_p_e.append(decision_p) leaf_p_e.append(leaf_p) return decision_p_e, leaf_p_e 创建占位符作为输入 X = tf.placeholder(\u0026#34;float\u0026#34;, [N_BATCH, 28, 28, 1]) Y = tf.placeholder(\u0026#34;float\u0026#34;, [N_BATCH, N_LABEL]) 初始化参数 w = init_weights([3, 3, 1, 32]) w2 = init_weights([3, 3, 32, 64]) w3 = init_weights([3, 3, 64, 128]) w4_ensemble = [] w_d_ensemble = [] w_l_ensemble = [] for i in range(N_TREE): w4_ensemble.append(init_weights([128*4*4, 625])) w_d_ensemble.append(init_prob_weights([625, N_LEAF], -1, 1)) w_l_ensemble.append(init_prob_weights([N_LEAF, N_LABEL], -2, 2)) p_keep_conv = tf.placeholder(\u0026#34;float\u0026#34;) p_keep_hidden = tf.placeholder(\u0026#34;float\u0026#34;) 定义一个完全可微deep-ndf decision_p_e, leaf_p_e = model(X, w, w2, w3, w4_ensemble, w_d_ensemble, w_l_ensemble, p_keep_conv, p_keep_hidden) flat_decision_p_e = [] for decision_p in decision_p_e: # decision_p是d, decision_p_comp是1-d decision_p_comp = tf.subtract(tf.ones_like(decision_p), decision_p) decision_p_pack = tf.stack([decision_p, decision_p_comp]) flat_decision_p = tf.reshape(decision_p_pack, [-1]) flat_decision_p_e.append(flat_decision_p) batch_0_indices = \\ tf.tile(tf.expand_dims(tf.range(0, N_BATCH * N_LEAF, N_LEAF), 1), [1, N_LEAF])  sess = tf.Session() sess.run(batch_0_indices) array([[ 0, 0, 0, ..., 0, 0, 0], [ 16, 16, 16, ..., 16, 16, 16], [ 32, 32, 32, ..., 32, 32, 32], ..., [2000, 2000, 2000, ..., 2000, 2000, 2000], [2016, 2016, 2016, ..., 2016, 2016, 2016], [2032, 2032, 2032, ..., 2032, 2032, 2032]], dtype=int32)  batch_0_indices.shape = 128 * 16\nin_repeat = N_LEAF / 2 out_repeat = N_BATCH batch_complement_indices = \\ np.array([[0] * int(in_repeat), [N_BATCH * N_LEAF] \\ * int(in_repeat)] * out_repeat).reshape(N_BATCH, N_LEAF) print(batch_complement_indices) [[ 0 0 0 ... 2048 2048 2048] [ 0 0 0 ... 2048 2048 2048] [ 0 0 0 ... 2048 2048 2048] ... [ 0 0 0 ... 2048 2048 2048] [ 0 0 0 ... 2048 2048 2048] [ 0 0 0 ... 2048 2048 2048]]  sess.run(tf.add(batch_0_indices, batch_complement_indices)) array([[ 0, 0, 0, ..., 2048, 2048, 2048], [ 16, 16, 16, ..., 2064, 2064, 2064], [ 32, 32, 32, ..., 2080, 2080, 2080], ..., [2000, 2000, 2000, ..., 4048, 4048, 4048], [2016, 2016, 2016, ..., 4064, 4064, 4064], [2032, 2032, 2032, ..., 4080, 4080, 4080]], dtype=int32)  ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/dnf/","summary":"基于卷积神经网络和决策树的体域网数据融合方法 现阶段想法:在softmax层后接随机森林，通过种树增加分类准确率\nimport tensorflow as tf import numpy as np import tensorflow.examples.tutorials.mnist.input_data as input_data import scipy as sp %matplotlib inline sess = tf.Session() DEPTH = 3 # Depth of a tree N_LEAF = 2 ** (DEPTH + 1) # Number of leaf node N_LABEL = 10 # Number of classes N_TREE = 5 # Number of trees (ensemble) N_BATCH = 128 # Number of data points per mini-batch 分批训练，每一批128个  初始化矩阵 def init_weights(shape): return tf.","title":"基于卷积神经网络和决策树的体域网数据融合方法"},{"content":"基本计算指令  这里的汇编指令均基于x86-64架构\n 0. 先验知识 0.1 寄存器设置 一个x86-64的中央处理单元包含一组16个64位通用目的寄存器。这些寄存器用来存储整数数据和指针。指令可以对这16个寄存器的低位字节中存放的不同大小的数据进行操作。字节级操作可以访问最低的字节，16位操作可以访问最低的2个字节，32位操作可以访问最低的4个字节，而64位操作可以访问整个寄存器。\n3. 寻址方式 1. 数据传送指令 最简单形式的数据传送指令\u0026ndash;mov类。这些指令把数据从源位置复制到目的位置，不做任何变化。mov类指令由四条指令组成：movb, movw, movl,movq.这些指令都执行相同的操作，区别在于它们操作的数据大小不同：分别是1，2，4，8字节。\n 由于历史原因，Intel处理器将16位作为一个字（w），8位为一个字节(b)，32位为双字(l),64位为4字（q）\n    指令 效果 描述     MOV S D D \u0026lt;- S 传送   movb  传送字节   movw  传送字   movl  传送双字   movq  传送四字   movabsq I, R  传送绝对的四字    传送指令的两个操作数不能都指向内存位置。将一个值从一个内存位置复制到另一个内存位置需要两个步骤，第一个指令将源值加载到寄存器，第二条指令将该寄存器写入目的位置。\nexample\n C code\n long exchange(lone *xp, long y){ long x = *xp; *xp = y; return x; }  汇编代码\n exchange: movq (%rdi), %rax get x at xp, Set as return value movq %rsi, (%rdi) Store y at xp. ret 2. 压栈和出栈 在x86-64中，程序栈存放在内存中某个区域，栈向下增长，这样，栈顶元素的地址是所有栈中元素地址最低的。栈指针%rsp保存着栈顶元素的地址。\npushq指令的功能是把数据压入到栈上，而popq指令是弹出数据。这些指令都只有一个操作数\u0026ndash;压入的数据源和弹出的数据目的。\n将一个四字值压入栈中，首先要将栈指针减8，然后再将值写入到新的栈顶地址。因此，pushq %rbp的行为等价于下面两条命令：\nsubq $8, %rsp Decrement stack pointer movq %rbp, (%rsp) Store %rbp on stack 弹出一个四字的操作包括从栈顶位置读出数据，然后将栈指针加8，因此，指令popq %rax等价于下面两条指令：\nmovq (%rsp) , %rax Read %rax from stack addq $8, %rsp Increment stack pointer example 3. 算术和逻辑操作  每个指令类都有b, w, l, q这四种不同大小的指令\n 算术和逻辑操作指令\n3.1 加载有效地址 加载有效地址（load effective address）指令leaq实际上是movq指令的变形。它的指令形式是从内存读数据到寄存器，但实际上它根本就没有引用内存。它的第一个操作数看上去是一个内存引用，但该指令并不是从指定的位置读入数据，而是将有效地址写入到目的操作数。\nlea还可以简洁的描述普通的算术操作。例如，如果寄存器%rdx的值为x， 那么指令leaq 7(%rdx, %rdx, 4),%rax将设置寄存器%rax的值为5x+7\nexample\n c code\n long scale(long x, long y, long z){ long t = x + 4 * y + 12 * z; return t; }  汇编指令\n lone scale(long x, long y, long z) x in %rdi, y in %rsi, z in %rdx scale: leaq (%rdi, %rsi, 4), %rax x + 4 * y leaq (%rdx, %rdx, 2), %rax z + 2 * z = 3 * z leaq (%rax, %rdx, 4), %rax (x+4×y) + 4*(3*z) = x + 4*y + 12*z ret 3.2 一元和二元操作 上表中的第二组中的操作是一元操作，只有一个操作数，既是源又是目的。这个操作数可以是寄存器，也可以是内存位置。比如说，指令incq (%rsp)会使栈顶的8字节元素+1.\n第三组是二元操作，其中第二个操作数既是源又是目的。不过，要注意，源操作数是第一个，目的操作数是第二个，对于不可交换来说，这看上去很奇特。例如，指令subq %rax, %rdx使寄存器%rdx的值减去%rax中的值。\n3.3 移位操作 最后一组是移位操作，先给出移位量，然后第二项给出的是要移位的数。可以进行算术和逻辑右移。移位量可以是一个立即数，或者放在单字节寄存器%cl中。\n左移指令有两个名字：sal和shl。二者的效果是一样的，都是将右边填上0.右移指令不同，sar执行算术移位（填上符号位），而shr执行逻辑移位（填上0）.\n 当寄存器%cl的十六进制值为0xFF时，指令salb会移7位，salw会移15为，sall会移31位，salq会移63位`\n 3.4 特殊的算术操作 两个64位有符号或无符号整数相乘得到的乘积需要128位来表示。intel把16字节的数成为八字（oct word）.\n 特殊的算术操作\n example: 如何从两个无符号64位数字x和y生成128位的乘积\n c code\n #include\u0026lt;inttypes.h\u0026gt; typedef unsigned __int128 uint128_t void store_uprod(uint128_t *dest, uint64_t x, uint64_t y){ *dest = x * (uint128_t) y; }  汇编代码\n dest in %rdi, x in %rsi, y in %rdx store_uprod: movq %rsi, %rax copy x to multiplicand mulq %rdx multiply by y movq %rax, (%rdi) store lower 8 bytes at dest movq %rdx, 8(%rdi) store upper 8 bytes at dest+8 ret example： 如何实现除法\n c code\n void remdiv(long x, long y, long *qp, long *rp){ long q = x/y; long r = x%y; *qp = q; *rp = r; }  汇编代码\n x in %rdi, y in %rsi, qp in %rdx, rp in %rcx remdiv: movq %rdx, %r8 copy qp movq %rdi, %rax mov x to lower 8 bytes of divident cqto idivq %rsi divide by y movq %rax, (%r8) store quotient at qp movq %rdx, (%rcx) store remainder at rp  cqto指令不需要操作数，它隐含读出%rax的符号位，并将它复制到%rdx的所有位\n ","permalink":"http://yangchnet.github.io/Dessert/posts/%E6%B1%87%E7%BC%96/%E5%9F%BA%E6%9C%AC%E8%AE%A1%E7%AE%97%E6%8C%87%E4%BB%A4/","summary":"基本计算指令  这里的汇编指令均基于x86-64架构\n 0. 先验知识 0.1 寄存器设置 一个x86-64的中央处理单元包含一组16个64位通用目的寄存器。这些寄存器用来存储整数数据和指针。指令可以对这16个寄存器的低位字节中存放的不同大小的数据进行操作。字节级操作可以访问最低的字节，16位操作可以访问最低的2个字节，32位操作可以访问最低的4个字节，而64位操作可以访问整个寄存器。\n3. 寻址方式 1. 数据传送指令 最简单形式的数据传送指令\u0026ndash;mov类。这些指令把数据从源位置复制到目的位置，不做任何变化。mov类指令由四条指令组成：movb, movw, movl,movq.这些指令都执行相同的操作，区别在于它们操作的数据大小不同：分别是1，2，4，8字节。\n 由于历史原因，Intel处理器将16位作为一个字（w），8位为一个字节(b)，32位为双字(l),64位为4字（q）\n    指令 效果 描述     MOV S D D \u0026lt;- S 传送   movb  传送字节   movw  传送字   movl  传送双字   movq  传送四字   movabsq I, R  传送绝对的四字    传送指令的两个操作数不能都指向内存位置。将一个值从一个内存位置复制到另一个内存位置需要两个步骤，第一个指令将源值加载到寄存器，第二条指令将该寄存器写入目的位置。\nexample\n C code","title":"基本计算指令"},{"content":"处理文本数据 1. 单词和字符的one-hot编码 one-hot编码是将标记转换为向量的最常用，最基本的方法。它将每个单词与一个唯一的整数索引相关联，然后将这个整数索引i转换为长度为N的二进制向量（N是词表大小），这个向量只有第i个元素是1，其余元素都是0.\n当然，也可以进行字符级的one-hot编码。\n1.1. 单词级的one-hot编码 import numpy as np samples = [\u0026#39;The cat sat on the mat.\u0026#39;, \u0026#39;The dog ate my homework.\u0026#39;] token_index = {} for sample in samples: for word in sample.split(): if word not in token_index: token_index[word] = len(token_index) + 1 # 为每个唯一单词指定一个唯一索引，没有为0索引指定单词 max_length = 10 results = np.zeros(shape=(len(samples), max_length, max(token_index.values())+1)) for i, sample in enumerate(samples): for j, word in list(enumerate(sample.split()))[:max_length]: index = token_index.get(word) results[i, j, index] = 1 results array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])  1.2. 字符级的one-hot编码 import string samples = [\u0026#39;The cat sat on the mat.\u0026#39;, \u0026#39;The dog ate my homework.\u0026#39;] characters = string.printable # 可打印的所有字符，共101个 token_index = dict(zip(range(1, len(characters) + 1), characters)) max_length = 50 results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1)) for i, sample in enumerate(samples): for j, character in enumerate(sample): index = token_index.get(character) results[i, j, index] = 1 results.shape (2, 50, 101)  1.3. 使用keras的内置函数实现one-hot编码 Keras的内置函数可以对原始文本数据进行单词级或字符级的one-hot编码。我们应该使用这些函数，它们实现了许多重要的特性，比如从字符串中去除特殊字符、只考虑数据集中前N个常见的单词等。\nfrom keras.preprocessing.text import Tokenizer samples = [\u0026#39;The cat sat on the mat.\u0026#39;, \u0026#39;The dog ate my homework.\u0026#39;] tokenizer = Tokenizer(num_words=1000) # 创建一个分词器，只考虑前1000个单词 tokenizer.fit_on_texts(samples) sequences = tokenizer.texts_to_sequences(samples) # 将字符串转化为整数索引构建的列表 one_hot_results = tokenizer.texts_to_matrix(samples, mode=\u0026#39;binary\u0026#39;) word_index = tokenizer.word_index # 找回单词索引 print(\u0026#39;Found %sunique tokens.\u0026#39; % len(word_index)) one_hot_results.shape Found 9 unique tokens. (2, 1000)  1.4. one-hot散列技巧 所谓的one-hot散列技巧是one-hot编码的一种变体，如果词表中唯一标记的数量太大而无法直接处理，就可以使用这种技巧。这种方法没有为每个单词显式分配一个索引并将这些索引保存在一个字典中，而是将单词散列编码为固定长度的向量，通常用一个非常简单的散列函数来实现。这种方法的主要优点在于，它避免了维护一个显式的单词索引，从而节省内存并允许数据的在线编码（在读取完所有数据之前，你就可以立刻生成标记向量）。\n这种方法有个缺点，就是可能会出现散列冲突，即两个不同的单词可能具有相同的散列值，随后任何机器学习模型观察这些散列值，都无法区分它们对应的单词。如果散列空间的维度远大于需要散列的唯一标记的个数，散列冲突的可能性会减小。\n# 使用散列技巧的单词级的one-hot编码 samples = [\u0026#39;The cat sat on the mat.\u0026#39;, \u0026#39;The dog ate my homework.\u0026#39;] dimensionality = 1000 max_length = 10 results = np.zeros((len(samples), max_length, dimensionality)) for i, sample in enumerate(samples): for j, word in list(enumerate(sample.split()))[:max_length]: index = abs(hash(word)) % dimensionality # 将单词散列为0-1000范围内的一个随机整数索引。 results[i, j, index] = 1 results.shape (2, 10, 1000)  2. 使用词嵌入 密集的词向量，也叫词嵌入。one-hot编码得到的向量是二进制的、稀疏的（绝大部分元素都是0）维度很高的（维度大小等于词表中的单词个数），而词嵌入是低维的浮点数向量（即密集向量、与稀疏向量相对）。与one-hot编码得到的词向量不同，词嵌入是从数据中学习得到的。常见的词向量维度是256， 512或1024（处理非常大的词表时）。于此相对，one-hot编码的词向量维度通常为20000或更高（对应包含20000个标记的词表），因此词向量可以将更多的信息塞入更低的维度中\n获取词嵌入有两种方法:\n 在完成主任务（比如文档分类或情感预测）的同时学习词嵌入。在这种情况下，一开始是随机的词向量，然后对这些词向量进行学习，其学习方式与学习神经网络的权重相同。 在不同于待解决问题的机器学习任务上预计算好词嵌入，然后将其加载到模型中。这些词嵌入叫做预训练词嵌入  2.1. 利用Embedding层学习词嵌入 要将一个词与一个密集向量相关联，最简单的方法就是随机选择向量。这种方法的问题在于，得到的嵌入空间没有任何结构。说的更抽象一点，词向量之间的几何关系应该表示这些词之间的语义关系。词嵌入的作用应该是将人类的语言映射到几何空间中。例如，在一个合理的嵌入空间中，同义词应该被嵌入到相似的词向量中，一般来说，任意两个词向量之间的几何距离（比如L2距离）应该和这两个词的语义距离有关。.\n一个好的词嵌入空间在很大程度上取决于你的任务，某些语义关系的重要性因任务而异。因此，合理的做法是对每个新任务都学习一个新的嵌入空间。\n# 将一个Embedding 层实例化 from keras.layers import Embedding embedding_layer = Embedding(1000, 64) #(标记的个数， 嵌入的维度) 可以将Embedding层理解为一个字典，将整数索引（表示特定单词）映射为密集向量。它接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。Embedding实际上是一种字典查找。\n# 加载IMDB数据，准备用于Embedding层 from keras.datasets import imdb from keras import preprocessing max_features = 10000 #作为特征的单词个数 maxlen = 20 # 这么多单词后截断文本 # 将数据加载为整数列表 (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) x_train[0:2] array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]), list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])], dtype=object)  #将整数列表转换成形状为（samples, maxlen）的二维整数张量 x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen) x_train[0:2] array([[ 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32], [ 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]], dtype=int32)  y_train.shape (25000,)  # 在IMDB数据集上使用Embedding层和分类器 from keras.models import Sequential from keras.layers import Flatten, Dense, Embedding model = Sequential() model.add(Embedding(10000, 8, input_length=maxlen)) model.add(Flatten()) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) model.summary() history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2) Model: \u0026quot;sequential_2\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_3 (Embedding) (None, 20, 8) 80000 _________________________________________________________________ flatten_2 (Flatten) (None, 160) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 161 ================================================================= Total params: 80,161 Trainable params: 80,161 Non-trainable params: 0 _________________________________________________________________ /usr/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. \u0026quot;Converting sparse IndexedSlices to a dense Tensor of unknown shape. \u0026quot; Train on 20000 samples, validate on 5000 samples Epoch 1/10 20000/20000 [==============================] - 3s 146us/step - loss: 0.6799 - acc: 0.5913 - val_loss: 0.6468 - val_acc: 0.6846 Epoch 2/10 20000/20000 [==============================] - 2s 103us/step - loss: 0.5643 - acc: 0.7456 - val_loss: 0.5399 - val_acc: 0.7220 Epoch 3/10 20000/20000 [==============================] - 2s 97us/step - loss: 0.4697 - acc: 0.7858 - val_loss: 0.5068 - val_acc: 0.7436 Epoch 4/10 20000/20000 [==============================] - 2s 100us/step - loss: 0.4239 - acc: 0.8084 - val_loss: 0.4975 - val_acc: 0.7500 Epoch 5/10 20000/20000 [==============================] - 2s 98us/step - loss: 0.3931 - acc: 0.8238 - val_loss: 0.4984 - val_acc: 0.7560 Epoch 6/10 20000/20000 [==============================] - 2s 97us/step - loss: 0.3684 - acc: 0.8400 - val_loss: 0.4990 - val_acc: 0.7558 Epoch 7/10 20000/20000 [==============================] - 2s 115us/step - loss: 0.3462 - acc: 0.8519 - val_loss: 0.5049 - val_acc: 0.7554 Epoch 8/10 20000/20000 [==============================] - 2s 97us/step - loss: 0.3268 - acc: 0.8640 - val_loss: 0.5121 - val_acc: 0.7556 Epoch 9/10 20000/20000 [==============================] - 2s 98us/step - loss: 0.3082 - acc: 0.8734 - val_loss: 0.5203 - val_acc: 0.7528 Epoch 10/10 20000/20000 [==============================] - 2s 102us/step - loss: 0.2905 - acc: 0.8841 - val_loss: 0.5298 - val_acc: 0.7506  2.2. 使用预训练的词嵌入 ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/%E5%A4%84%E7%90%86%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE/","summary":"处理文本数据 1. 单词和字符的one-hot编码 one-hot编码是将标记转换为向量的最常用，最基本的方法。它将每个单词与一个唯一的整数索引相关联，然后将这个整数索引i转换为长度为N的二进制向量（N是词表大小），这个向量只有第i个元素是1，其余元素都是0.\n当然，也可以进行字符级的one-hot编码。\n1.1. 单词级的one-hot编码 import numpy as np samples = [\u0026#39;The cat sat on the mat.\u0026#39;, \u0026#39;The dog ate my homework.\u0026#39;] token_index = {} for sample in samples: for word in sample.split(): if word not in token_index: token_index[word] = len(token_index) + 1 # 为每个唯一单词指定一个唯一索引，没有为0索引指定单词 max_length = 10 results = np.zeros(shape=(len(samples), max_length, max(token_index.values())+1)) for i, sample in enumerate(samples): for j, word in list(enumerate(sample.split()))[:max_length]: index = token_index.get(word) results[i, j, index] = 1 results array([[[0.","title":"处理文本数据"},{"content":"多输入模型 函数式API可以用于构建具有多个输入的模型，通常情况下，这种模型会在某一时刻用一个可以组合多个张量的层将不同的输入分支合并，张量组合方式可能是相加，连接等。这通常利用Keras的合并运算来实现，比如keras.layers.add, keras.layers.concatenate等。\n下面来看一个非常简单的多输入模型示例：一个问答模型\n典型的问答模型有两个输入，一个自然语言描述的问题和一个文本片段（比如新闻文章），后者提供用于回答问题的信息。然后模型要生成一个回答，在最简单的情况下，这个回答只包含一个词，可以通过对某个预定义的词表做softmax得到。\n# 具有两个输入的模型 from keras.models import Model from keras import layers from keras import Input text_vocabulary_size = 10000 question_vocabulary_size = 10000 answer_vocabulary_size = 500 text_input = Input(shape=(None, ), dtype=\u0026#39;int32\u0026#39;, name=\u0026#39;text\u0026#39;) embedded_text = layers.Embedding( text_vocabulary_size, 64) (text_input) # 将输入嵌入到长度为64的向量 encoded_text = layers.LSTM(32)(embedded_text) # 对问题进行相同的处理，使用不同的层实例 question_input = Input(shape=(None, ), dtype=\u0026#39;int32\u0026#39;, name=\u0026#39;question\u0026#39;) embedded_question = layers.Embedding( question_vocabulary_size, 32)(question_input) encoded_question = layers.LSTM(16)(embedded_question) # 将编码后的问题和文本连接起来 concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1) # 在上面添加一个softmax分类器 answer = layers.Dense(answer_vocabulary_size, activation=\u0026#39;softmax\u0026#39;)(concatenated) model = Model([text_input, question_input], answer) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) Using TensorFlow backend. /usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint8 = np.dtype([(\u0026quot;qint8\u0026quot;, np.int8, 1)]) /usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint8 = np.dtype([(\u0026quot;quint8\u0026quot;, np.uint8, 1)]) /usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint16 = np.dtype([(\u0026quot;qint16\u0026quot;, np.int16, 1)]) /usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_quint16 = np.dtype([(\u0026quot;quint16\u0026quot;, np.uint16, 1)]) /usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. _np_qint32 = np.dtype([(\u0026quot;qint32\u0026quot;, np.int32, 1)]) /usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'. np_resource = np.dtype([(\u0026quot;resource\u0026quot;, np.ubyte, 1)])  ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/%E5%A4%9A%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B/","summary":"多输入模型 函数式API可以用于构建具有多个输入的模型，通常情况下，这种模型会在某一时刻用一个可以组合多个张量的层将不同的输入分支合并，张量组合方式可能是相加，连接等。这通常利用Keras的合并运算来实现，比如keras.layers.add, keras.layers.concatenate等。\n下面来看一个非常简单的多输入模型示例：一个问答模型\n典型的问答模型有两个输入，一个自然语言描述的问题和一个文本片段（比如新闻文章），后者提供用于回答问题的信息。然后模型要生成一个回答，在最简单的情况下，这个回答只包含一个词，可以通过对某个预定义的词表做softmax得到。\n# 具有两个输入的模型 from keras.models import Model from keras import layers from keras import Input text_vocabulary_size = 10000 question_vocabulary_size = 10000 answer_vocabulary_size = 500 text_input = Input(shape=(None, ), dtype=\u0026#39;int32\u0026#39;, name=\u0026#39;text\u0026#39;) embedded_text = layers.Embedding( text_vocabulary_size, 64) (text_input) # 将输入嵌入到长度为64的向量 encoded_text = layers.LSTM(32)(embedded_text) # 对问题进行相同的处理，使用不同的层实例 question_input = Input(shape=(None, ), dtype=\u0026#39;int32\u0026#39;, name=\u0026#39;question\u0026#39;) embedded_question = layers.Embedding( question_vocabulary_size, 32)(question_input) encoded_question = layers.LSTM(16)(embedded_question) # 将编码后的问题和文本连接起来 concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1) # 在上面添加一个softmax分类器 answer = layers.","title":"多输入模型"},{"content":"安装完virtualbox后，提示内核问题 解决方法 安装内核匹配版本 sudo pacman -S linux419-virtualbox-host-modules\n重新加载内核模块 sudo /sbin/rcvboxdrv 然后重启即可\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/virtualbox%E7%9A%84%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98/","summary":"安装完virtualbox后，提示内核问题 解决方法 安装内核匹配版本 sudo pacman -S linux419-virtualbox-host-modules\n重新加载内核模块 sudo /sbin/rcvboxdrv 然后重启即可","title":"安装完virtualbox后，提示内核问题"},{"content":"快速入门Matplotlib教程 介绍 Matplotlib 可能是 Python 2D-绘图领域使用最广泛的套件。它能让使用者很轻松地将数据图形化，并且提供多样化的输出格式。\npylab pylab 是 matplotlib 面向对象绘图库的一个接口。它的语法和 Matlab 十分相近。也就是说，它主要的绘图命令和 Matlab 对应的命令有相似的参数。\n初级绘制 这一节中，我们将从简到繁：先尝试用默认配置在同一张图上绘制正弦和余弦函数图像，然后逐步美化它。\n第一步，是取得正弦函数和余弦函数的值：\nimport numpy as np X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) X 是一个 numpy 数组，包含了从 −π−π 到 +π+π 等间隔的 256 个值。C和 S 则分别是这 256 个值对应的余弦和正弦函数值组成的 numpy 数组。\nnp.linspace\n使用默认配置 Matplotlib 的默认配置都允许用户自定义。你可以调整大多数的默认配置：图片大小和分辨率（dpi）、线宽、颜色、风格、坐标轴、坐标轴以及网格的属性、文字与字体属性等。不过，matplotlib 的默认配置在大多数情况下已经做得足够好，你可能只在很少的情况下才会想更改这些默认配置。\nplot函数详解\nfrom pylab import * plot(X,C) plot(X,S) show() \u0026lt;Figure size 640x480 with 1 Axes\u0026gt;  默认配置的具体内容 下面的代码中，我们展现了 matplotlib 的默认配置并辅以注释说明，这部分配置包含了有关绘图样式的所有配置。代码中的配置与默认配置完全相同，你可以在交互模式中修改其中的值来观察效果。\n# 导入 matplotlib 的所有内容（nympy 可以用 np 这个名字来使用） from pylab import * # 创建一个 8 * 6 点（point）的图，并设置分辨率为 80 figure(figsize=(8,6), dpi=80) # 创建一个新的 1 * 1 的子图，接下来的图样绘制在其中的第 1 块（也是唯一的一块） subplot(1,1,1) X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) # 绘制余弦曲线，使用蓝色的、连续的、宽度为 1 （像素）的线条 plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) # 绘制正弦曲线，使用绿色的、连续的、宽度为 1 （像素）的线条 plot(X, S, color=\u0026#34;green\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) # 设置横轴的上下限 xlim(-4.0,4.0) # 设置横轴记号 xticks(np.linspace(-4,4,9,endpoint=True)) # 设置纵轴的上下限 ylim(-1.0,1.0) # 设置纵轴记号 yticks(np.linspace(-1,1,5,endpoint=True)) # 以分辨率 72 来保存图片 # savefig(\u0026#34;exercice_2.png\u0026#34;,dpi=72) # 在屏幕上显示 show() \u0026lt;Figure size 640x480 with 1 Axes\u0026gt;  改变线条的颜色和粗细 figure(figsize=(10,6), dpi=80) plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=10, linestyle=\u0026#34;-\u0026#34;) plot(X, S, color=\u0026#34;red\u0026#34;, linewidth=2.5, linestyle=\u0026#34;-\u0026#34;) [\u0026lt;matplotlib.lines.Line2D at 0x7f6c66774470\u0026gt;]  设置图片边界 当前的图片边界设置得不好，所以有些地方看得不是很清楚。\nfrom pylab import * figure(figsize=(8,6), dpi=80) subplot(1,1,1) X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) plot(X, S, color=\u0026#34;green\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) xticks(np.linspace(-4,4,9,endpoint=True)) xlim(X.min()*1.1, X.max()*1.1) # 此处发生改变 ylim(C.min()*1.1, C.max()*1.1) # 此处发生改变 yticks(np.linspace(-1,1,5,endpoint=True)) show() 设置记号 我们讨论正弦和余弦函数的时候，通常希望知道函数在 $±π±π$ 和 $±π/2±π/2$ 的值。这样看来，当前的设置就不那么理想了。\nfrom pylab import * figure(figsize=(8,6), dpi=80) subplot(1,1,1) X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) plot(X, S, color=\u0026#34;green\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) xlim(X.min()*1.1, X.max()*1.1) ylim(C.min()*1.1, C.max()*1.1) xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi]) # 此处发生改变 yticks([-1, 0, +1]) # 此处发生改变 show() 设置记号的标签 记号现在没问题了，不过标签却不大符合期望。我们可以把 3.142 当做是 $π$，但毕竟不够精确。当我们设置记号的时候，我们可以同时设置记号的标签。注意这里使用了 LaTeX。(以后会讲LaTeX)\nfrom pylab import * figure(figsize=(8,6), dpi=80) subplot(1,1,1) X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) plot(X, S, color=\u0026#34;green\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) xlim(X.min()*1.1, X.max()*1.1) ylim(C.min()*1.1, C.max()*1.1) plt.xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi],[r\u0026#39;$-\\pi$\u0026#39;, r\u0026#39;$-\\pi/2$\u0026#39;, r\u0026#39;$0$\u0026#39;, r\u0026#39;$+\\pi/2$\u0026#39;, r\u0026#39;$+\\pi$\u0026#39;]) # 此处发生改变 plt.yticks([-1, 0, +1],[r\u0026#39;$-1$\u0026#39;, r\u0026#39;$0$\u0026#39;, r\u0026#39;$+1$\u0026#39;]) # 此处发生改变 show() 移动脊柱 坐标轴线和上面的记号连在一起就形成了脊柱（Spines，一条线段上有一系列的凸起，是不是很像脊柱骨啊~），它记录了数据区域的范围。它们可以放在任意位置，不过至今为止，我们都把它放在图的四边。\n实际上每幅图有四条脊柱（上下左右），为了将脊柱放在图的中间，我们必须将其中的两条（上和右）设置为无色，然后调整剩下的两条到合适的位置——数据空间的 0 点。\nfrom pylab import * figure(figsize=(8,6), dpi=80) subplot(1,1,1) X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) plot(X, S, color=\u0026#34;green\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) xlim(X.min()*1.1, X.max()*1.1) plt.xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi],[r\u0026#39;$-\\pi$\u0026#39;, r\u0026#39;$-\\pi/2$\u0026#39;, r\u0026#39;$0$\u0026#39;, r\u0026#39;$+\\pi/2$\u0026#39;, r\u0026#39;$+\\pi$\u0026#39;]) ylim(C.min()*1.1, C.max()*1.1) plt.yticks([-1, 0, +1],[r\u0026#39;$-1$\u0026#39;, r\u0026#39;$0$\u0026#39;, r\u0026#39;$+1$\u0026#39;]) # 移动脊柱 ax = plt.gca() ax.spines[\u0026#39;right\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) ax.spines[\u0026#39;bottom\u0026#39;].set_position((\u0026#39;data\u0026#39;,0)) ax.yaxis.set_ticks_position(\u0026#39;left\u0026#39;) ax.spines[\u0026#39;left\u0026#39;].set_position((\u0026#39;data\u0026#39;,0)) show() 添加图例 我们在图的左上角添加一个图例。为此，我们只需要在 plot 函数里以「键 - 值」的形式增加一个参数。\nfrom pylab import * figure(figsize=(8,6), dpi=80) subplot(1,1,1) X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) plot(X, S, color=\u0026#34;green\u0026#34;, linewidth=1.0, linestyle=\u0026#34;-\u0026#34;) xlim(X.min()*1.1, X.max()*1.1) plt.xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi],[r\u0026#39;$-\\pi$\u0026#39;, r\u0026#39;$-\\pi/2$\u0026#39;, r\u0026#39;$0$\u0026#39;, r\u0026#39;$+\\pi/2$\u0026#39;, r\u0026#39;$+\\pi$\u0026#39;]) ylim(C.min()*1.1, C.max()*1.1) plt.yticks([-1, 0, +1],[r\u0026#39;$-1$\u0026#39;, r\u0026#39;$0$\u0026#39;, r\u0026#39;$+1$\u0026#39;]) # 移动脊柱 ax = plt.gca() ax.spines[\u0026#39;right\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.spines[\u0026#39;top\u0026#39;].set_color(\u0026#39;none\u0026#39;) ax.xaxis.set_ticks_position(\u0026#39;bottom\u0026#39;) ax.spines[\u0026#39;bottom\u0026#39;].set_position((\u0026#39;data\u0026#39;,0)) ax.yaxis.set_ticks_position(\u0026#39;left\u0026#39;) ax.spines[\u0026#39;left\u0026#39;].set_position((\u0026#39;data\u0026#39;,0)) # 增加图例 plt.plot(X, C, color=\u0026#34;blue\u0026#34;, linewidth=2.5, linestyle=\u0026#34;-\u0026#34;, label=\u0026#34;cosine\u0026#34;) plt.plot(X, S, color=\u0026#34;red\u0026#34;, linewidth=2.5, linestyle=\u0026#34;-\u0026#34;, label=\u0026#34;sine\u0026#34;) plt.legend(loc=\u0026#39;upper left\u0026#39;, frameon=False) show() 其他类型的图 普通图 import numpy as np import matplotlib.pyplot as plt n = 256 X = np.linspace(-np.pi,np.pi,n,endpoint=True) Y = np.sin(2*X) plt.axes([0.025,0.025,0.95,0.95]) plt.plot (X, Y+1, color=\u0026#39;blue\u0026#39;, alpha=1.00) plt.fill_between(X, 1, Y+1, color=\u0026#39;blue\u0026#39;, alpha=.25) plt.plot (X, Y-1, color=\u0026#39;blue\u0026#39;, alpha=1.00) plt.fill_between(X, -1, Y-1, (Y-1) \u0026gt; -1, color=\u0026#39;blue\u0026#39;, alpha=.25) plt.fill_between(X, -1, Y-1, (Y-1) \u0026lt; -1, color=\u0026#39;red\u0026#39;, alpha=.25) plt.xlim(-np.pi,np.pi), plt.xticks([]) plt.ylim(-2.5,2.5), plt.yticks([]) # savefig(\u0026#39;../figures/plot_ex.png\u0026#39;,dpi=48) plt.show() 散点图 import numpy as np import matplotlib.pyplot as plt n = 1024 X = np.random.normal(0,1,n) Y = np.random.normal(0,1,n) T = np.arctan2(Y,X) plt.axes([0.025,0.025,0.95,0.95]) plt.scatter(X,Y, s=75, c=T, alpha=.5) plt.xlim(-1.5,1.5), plt.xticks([]) plt.ylim(-1.5,1.5), plt.yticks([]) # savefig(\u0026#39;../figures/scatter_ex.png\u0026#39;,dpi=48) plt.show() 条形图 import numpy as np import matplotlib.pyplot as plt n = 12 X = np.arange(n) Y1 = (1-X/float(n)) * np.random.uniform(0.5,1.0,n) Y2 = (1-X/float(n)) * np.random.uniform(0.5,1.0,n) plt.axes([0.025,0.025,0.95,0.95]) plt.bar(X, +Y1, facecolor=\u0026#39;#9999ff\u0026#39;, edgecolor=\u0026#39;white\u0026#39;) plt.bar(X, -Y2, facecolor=\u0026#39;#ff9999\u0026#39;, edgecolor=\u0026#39;white\u0026#39;) for x,y in zip(X,Y1): plt.text(x+0.4, y+0.05, \u0026#39;%.2f\u0026#39; % y, ha=\u0026#39;center\u0026#39;, va= \u0026#39;bottom\u0026#39;) for x,y in zip(X,Y2): plt.text(x+0.4, -y-0.05, \u0026#39;%.2f\u0026#39; % y, ha=\u0026#39;center\u0026#39;, va= \u0026#39;top\u0026#39;) plt.xlim(-.5,n), plt.xticks([]) plt.ylim(-1.25,+1.25), plt.yticks([]) # savefig(\u0026#39;../figures/bar_ex.png\u0026#39;, dpi=48) plt.show() 等高线图 import numpy as np import matplotlib.pyplot as plt def f(x,y): return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2) n = 256 x = np.linspace(-3,3,n) y = np.linspace(-3,3,n) X,Y = np.meshgrid(x,y) plt.axes([0.025,0.025,0.95,0.95]) plt.contourf(X, Y, f(X,Y), 8, alpha=.75, cmap=plt.cm.hot) C = plt.contour(X, Y, f(X,Y), 8, colors=\u0026#39;black\u0026#39;, linewidth=.5) plt.clabel(C, inline=1, fontsize=10) plt.xticks([]), plt.yticks([]) # savefig(\u0026#39;../figures/contour_ex.png\u0026#39;,dpi=48) plt.show() /usr/lib/python3.7/site-packages/matplotlib/contour.py:1000: UserWarning: The following kwargs were not used by contour: 'linewidth' s)  灰度图 import numpy as np import matplotlib.pyplot as plt def f(x,y): return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2) n = 10 x = np.linspace(-3,3,3.5*n) y = np.linspace(-3,3,3.0*n) X,Y = np.meshgrid(x,y) Z = f(X,Y) plt.axes([0.025,0.025,0.95,0.95]) plt.imshow(Z,interpolation=\u0026#39;bicubic\u0026#39;, cmap=\u0026#39;bone\u0026#39;, origin=\u0026#39;lower\u0026#39;) plt.colorbar(shrink=.92) plt.xticks([]), plt.yticks([]) # savefig(\u0026#39;../figures/imshow_ex.png\u0026#39;, dpi=48) plt.show() /usr/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: object of type \u0026lt;class 'float'\u0026gt; cannot be safely interpreted as an integer. /usr/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: object of type \u0026lt;class 'float'\u0026gt; cannot be safely interpreted as an integer. if __name__ == '__main__':  饼状图 import numpy as np import matplotlib.pyplot as plt n = 20 Z = np.ones(n) Z[-1] *= 2 plt.axes([0.025, 0.025, 0.95, 0.95]) plt.pie(Z, explode=Z*.05, colors=[\u0026#39;%f\u0026#39; % (i/float(n)) for i in range(n)], wedgeprops={\u0026#34;linewidth\u0026#34;: 1, \u0026#34;edgecolor\u0026#34;: \u0026#34;black\u0026#34;}) plt.gca().set_aspect(\u0026#39;equal\u0026#39;) plt.xticks([]), plt.yticks([]) # savefig(\u0026#39;../figures/pie_ex.png\u0026#39;,dpi=48) plt.show() 极轴图 import numpy as np import matplotlib.pyplot as plt ax = plt.axes([0.025,0.025,0.95,0.95], polar=True) N = 20 theta = np.arange(0.0, 2*np.pi, 2*np.pi/N) radii = 10*np.random.rand(N) width = np.pi/4*np.random.rand(N) bars = plt.bar(theta, radii, width=width, bottom=0.0) for r,bar in zip(radii, bars): bar.set_facecolor( plt.cm.jet(r/10.)) bar.set_alpha(0.5) ax.set_xticklabels([]) ax.set_yticklabels([]) # savefig(\u0026#39;../figures/polar_ex.png\u0026#39;,dpi=48) plt.show() 3D图 import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax = Axes3D(fig) X = np.arange(-4, 4, 0.25) Y = np.arange(-4, 4, 0.25) X, Y = np.meshgrid(X, Y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R) ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.hot) ax.contourf(X, Y, Z, zdir=\u0026#39;z\u0026#39;, offset=-2, cmap=plt.cm.hot) ax.set_zlim(-2,2) # savefig(\u0026#39;../figures/plot3d_ex.png\u0026#39;,dpi=48) plt.show() 作业 熟悉以上画图方法，从其官方文档中了解“子图”的概念，将[散点图， 条形图， 等高线图， 灰度图， 饼状图， 极轴图， 3D图]放在一张$3*2$的子图中\n附：参考资料  matplotlib官方文档 matplotlib中文文档 matplotlib官方教程 详解图像各个部分 matplotlib  np.linspace numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)\n在指定的间隔内返回均匀间隔的数字。\n返回num均匀分布的样本，在[start, stop]。\n这个区间的端点可以任意的被排除在外。\n 参数：   start ： array_like\n序列的起始值。\n  stop ： array_like 序列的结束值，除非端点设置为False。在这种情况下，序列由除 均匀间隔的样本之外的所有样本组成，因此不包括停止。请注意，当端点为False 时，步长会发生变化。num + 1\n  num ： int，可选 要生成的样本数。默认值为50.必须为非负数。\n  endpoint ： bool，可选 如果为True，则stop是最后一个样本。否则，它不包括在内。默认为True。\n  retstep ： bool，可选 如果为True，则返回（samples，step），其中step是样本之间的间距。\n  dtype ： dtype，可选 输出数组的类型。如果dtype未给出，则从其他输入参数推断数据类型。\n   返回   samples : ndarray 有NUM同样在闭区间隔开的样品 或半开间隔 （取决于是否端点是真或假）。[start, stop][start, stop)\n  step : float, optional 仅在retstep为True 时才返回样本之间的间距大小。\n    返回初级配置\n","permalink":"http://yangchnet.github.io/Dessert/posts/python/matplotlib%E5%88%9D%E7%BA%A7%E6%95%99%E7%A8%8B/","summary":"快速入门Matplotlib教程 介绍 Matplotlib 可能是 Python 2D-绘图领域使用最广泛的套件。它能让使用者很轻松地将数据图形化，并且提供多样化的输出格式。\npylab pylab 是 matplotlib 面向对象绘图库的一个接口。它的语法和 Matlab 十分相近。也就是说，它主要的绘图命令和 Matlab 对应的命令有相似的参数。\n初级绘制 这一节中，我们将从简到繁：先尝试用默认配置在同一张图上绘制正弦和余弦函数图像，然后逐步美化它。\n第一步，是取得正弦函数和余弦函数的值：\nimport numpy as np X = np.linspace(-np.pi, np.pi, 256,endpoint=True) C,S = np.cos(X), np.sin(X) X 是一个 numpy 数组，包含了从 −π−π 到 +π+π 等间隔的 256 个值。C和 S 则分别是这 256 个值对应的余弦和正弦函数值组成的 numpy 数组。\nnp.linspace\n使用默认配置 Matplotlib 的默认配置都允许用户自定义。你可以调整大多数的默认配置：图片大小和分辨率（dpi）、线宽、颜色、风格、坐标轴、坐标轴以及网格的属性、文字与字体属性等。不过，matplotlib 的默认配置在大多数情况下已经做得足够好，你可能只在很少的情况下才会想更改这些默认配置。\nplot函数详解\nfrom pylab import * plot(X,C) plot(X,S) show() \u0026lt;Figure size 640x480 with 1 Axes\u0026gt;  默认配置的具体内容 下面的代码中，我们展现了 matplotlib 的默认配置并辅以注释说明，这部分配置包含了有关绘图样式的所有配置。代码中的配置与默认配置完全相同，你可以在交互模式中修改其中的值来观察效果。","title":"快速入门Matplotlib教程"},{"content":"数据库表结构说明 \u0026amp;\u0026amp; 远程访问说明  code by lichang\n 数据库表结构说明 数据库名为django_mysql\n1.用户  所有和用户有关的数据\n 1.1 mhuse_mhuser表  用户总表，包含基本用户信息\n  id(key) password（密文密码） last_login is_superuser username first_name last_name email is_staff is_active date_joined usertype(normal, doctor) deviceid （设备id） mypassword(明文密码)  1.2 mhuser_normal表  普通用户表,包含普通用户的个人信息\n  user (foreign key, mhuser_mhuser.id) age [IntegerField, blank=True] gender [CharField,default=\u0026lsquo;man\u0026rsquo;,choice=(\u0026lsquo;man\u0026rsquo;,\u0026lsquo;woman\u0026rsquo;), max_length=10, blank=True] (性别) weight [FloatField, blank=True] （体重） marry [BooleanField, blank=True]（婚否） career [CharField, blank=True]（职业） signature [CharField, blank=True]（个性签名） medicalhistory [TextField, max_length=1000, blank=True] （用药史） avatar [ImageField, blank=True] (头像)  1.3 mhuser_doctoruser表  医生用户表, 包含医生用户的基本信息\n  user (foreign key, mhuser_mhuser.id) age *[IntegerField, blank=True] gender [CharField,default=\u0026lsquo;man\u0026rsquo;,choice=(\u0026lsquo;man\u0026rsquo;,\u0026lsquo;woman\u0026rsquo;), max_length=10, blank=True] (性别) signature [CharField, blank=True]（个性签名） expert [CharField, blank=True] (擅长) avatar [ImageField, blank=True] (头像)  1.4 mhuser_match表  普通用户\u0026amp;医生匹配,包含负责项\n  normaluser [foreign key, mhusr_normaluser.user] doctor [foreign key, mhuser_doctoruser.user] charged [CharField, choice=((\u0026lsquo;pressure\u0026rsquo;, \u0026lsquo;血压数据\u0026rsquo;),(\u0026lsquo;oxygen\u0026rsquo;,\u0026lsquo;血氧数据\u0026rsquo;),(\u0026lsquo;heartbeat\u0026rsquo;, \u0026lsquo;心跳数据\u0026rsquo;),(\u0026lsquo;tem\u0026rsquo;, \u0026lsquo;体温数据\u0026rsquo;)] (负责的部分)  1.5 mhuser_temdata表  普通用户个人体温信息表\n  own [foreign key, mhuser_normaluser.user] deviceid [CharField, default='', max_length=50] time [DateTimeField] tem_value [IntegerField] ××.×× 整数两位，小数两位  1.6 mhuser_heartdata表  普通用户个人心率信息表\n  own [foreign key, mhuser_normaluser.user] time [DateTimeField] deviceid [CharField, default='', max_length=50] b_value [IntegerField] 心率 3位整数 q_value [IntegerField] 心率间隔 3位整数 s_value [IntegerField] 信号强度 3位整数  1.7 mhuser_oxygendata表  普通用户个人血氧信息表\n  own [foreign key, mhuser_normaluser.user] time [DateTimeField] deviceid [CharField, default='', max_length=50] hr_value [IntegerField] 心率 3位整数 spo2_value [IntegerField] 血氧 3位整数  1.8 mhuser_pressuredata表  普通用户个人血压信息表\n  own [foreign key, mhuser_normaluser.user] time [DateTimeField] deviceid [CharField, default='', max_length=50] bpss_value [IntegerField] 舒张压 3位整数 bpsz_value [IntegerField] 收缩压 3位整数  2. 留言 2.1 explain_explain表  对普通用户健康数据的评论,称留言\n  match [foreign key, mhuser_match] author [foreign key, mhuser_mhuser] touserid [foreign key, mhuser_normaluser] time [DateTimeField] context [RichTextField, max_length=10000] read [CharField]   author对应fromuserid, touserid为接受意见的用户，是普通用户。\n 3. 设备 3.1 device_device表  有关设备,包含5张封面图,5张详情图\n  name [CharField, max_length=30] cover1 [ImageField] cover2 [ImageField, blank=True] cover3 [ImageField, blank=True] cover4 [ImageField, blank=True] cover5 [ImageField, blank=True] label [TextField, max_length=50] (标签) sales [IntegerField, default=0] (销量) price [FloatField, default=0.0] (价格) detailimage1 [ImageField, blank=True] (详情图片) detailimage2 [ImageField, blank=True] (详情图片) detailimage3 [ImageField, blank=True] (详情图片) detailimage4 [ImageField, blank=True] (详情图片) detailimage5 [ImageField, blank=True] (详情图片)  4. 评论  对博客的评论系统\n 4.1 comment_blogcomment表  一级评论， 对博客进行评论\n  author [foreign key, mhuser_mhuser] time [DateTimeField] comment [RichTextField] (评论内容) followed_blog [foreign key, blog_blog] (评论对象)  4.2 comment_bottomcomment表  二级评论，对一级评论进行回复或自回复）\n  author [foreign key, mhuser_mhuser] time [DateTimeField] comment [RichTextField] (评论内容) followed_comment [foreign key, blog_blog] (评论的评论对象) followed_self [foreign key, self] (评论的二级评论对象)  5. 博客 5.1 blog_blog表  博客的基本数据\n  author [foreign key, mhuser_mhuser] date [DateTimeField] essay [RichTexTextFieldtField] (博客正文) label [CharField, max_length=20] (标签) views [IntegerField, default=0] (观看次数)  远程访问说明（通过navicat）  也可使用代码段从服务器的ip地址访问MySql默认的3306端口，或者通过服务器的ssh代理访问。具体每种语言不尽相同，不再详说。\n 1、直接访问服务器的3306端口  navicat配置如下：\n 其中，用户名和密码为MySQL数据库的用户名和密码，分别是：\n用户名：root\n密码：Lichang1-\n  2、通过ssh代理访问  navicat配置：\n 常规选项卡中主机地址配置为：0.0.0.0，其他不变，另外还要开启ssh代理，选中ssh选项卡，勾选使用ssh通道，ssh具体配置如下：\n主机：59.110.140.133\n端口：22\n用户名：ahnu\n密码：ahnu2019\n  配置截图如下：\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%B4%E6%98%8E/","summary":"数据库表结构说明 \u0026amp;\u0026amp; 远程访问说明  code by lichang\n 数据库表结构说明 数据库名为django_mysql\n1.用户  所有和用户有关的数据\n 1.1 mhuse_mhuser表  用户总表，包含基本用户信息\n  id(key) password（密文密码） last_login is_superuser username first_name last_name email is_staff is_active date_joined usertype(normal, doctor) deviceid （设备id） mypassword(明文密码)  1.2 mhuser_normal表  普通用户表,包含普通用户的个人信息\n  user (foreign key, mhuser_mhuser.id) age [IntegerField, blank=True] gender [CharField,default=\u0026lsquo;man\u0026rsquo;,choice=(\u0026lsquo;man\u0026rsquo;,\u0026lsquo;woman\u0026rsquo;), max_length=10, blank=True] (性别) weight [FloatField, blank=True] （体重） marry [BooleanField, blank=True]（婚否） career [CharField, blank=True]（职业） signature [CharField, blank=True]（个性签名） medicalhistory [TextField, max_length=1000, blank=True] （用药史） avatar [ImageField, blank=True] (头像)  1.","title":"数据库表结构说明\u0026\u0026远程访问说明"},{"content":"数据集的清洗 一、一般数据集的处理 1、读取 首先创建文件对象，然后进行读取，两种写法\n# 第一种 f = open(\u0026#39;./Chinese.txt\u0026#39;, \u0026#39;r\u0026#39;) # 节选自：《父亲》（朱自清） article = [] # 创建一个列表 for l in f.readlines(): article.append(l) article ['我说道，“爸爸，你走吧。”他望车外看了看，说，“我买几个橘子去。你就在此地，不要走动。”我看那边月台的栅栏外有几个卖东西的等着顾客。走到那边月台，须穿过铁道，须跳下去又爬上去。父亲是一个胖子，走过去自然要费事些。我本来要去的，他不肯，只好让他去。我看见他戴着黑布小帽，穿着黑布大马褂，深青布棉袍，蹒跚地走到铁道边，慢慢探身下去，尚不大难。可是他穿过铁道，要爬上那边月台，就不容易了。他用两手攀着上面，两脚再向上缩；他肥胖的身子向左微倾，显出努力的样子。这时我看见他的背影，我的泪很快地流下来了。我赶紧拭干了泪，怕他看见，也怕别人看见。我再向外看时，他已抱了朱红的橘子望回走了。过铁道时，他先将橘子散放在地上，自己慢慢爬下，再抱起橘子走。到这边时，我赶紧去搀他。他和我走到车上，将橘子一股脑儿放在我的皮大衣上。于是扑扑衣上的泥土，心里很轻松似的，过一会说，“我走了；到那边来信！”我望着他走出去。他走了几步，回过头看见我，说，“进去吧，里边没人。”等他的背影混入来来往往的人里，再找不着了，我便进来坐下，我的眼泪又来了。']  # 第二种 article = [] # 创建一个列表 with open(\u0026#39;./Chinese.txt\u0026#39;, \u0026#39;r\u0026#39;) as file_project: for l in file_project.readlines(): article.append(l) article ['我说道，“爸爸，你走吧。”他望车外看了看，说，“我买几个橘子去。你就在此地，不要走动。”我看那边月台的栅栏外有几个卖东西的等着顾客。走到那边月台，须穿过铁道，须跳下去又爬上去。父亲是一个胖子，走过去自然要费事些。我本来要去的，他不肯，只好让他去。我看见他戴着黑布小帽，穿着黑布大马褂，深青布棉袍，蹒跚地走到铁道边，慢慢探身下去，尚不大难。可是他穿过铁道，要爬上那边月台，就不容易了。他用两手攀着上面，两脚再向上缩；他肥胖的身子向左微倾，显出努力的样子。这时我看见他的背影，我的泪很快地流下来了。我赶紧拭干了泪，怕他看见，也怕别人看见。我再向外看时，他已抱了朱红的橘子望回走了。过铁道时，他先将橘子散放在地上，自己慢慢爬下，再抱起橘子走。到这边时，我赶紧去搀他。他和我走到车上，将橘子一股脑儿放在我的皮大衣上。于是扑扑衣上的泥土，心里很轻松似的，过一会说，“我走了；到那边来信！”我望着他走出去。他走了几步，回过头看见我，说，“进去吧，里边没人。”等他的背影混入来来往往的人里，再找不着了，我便进来坐下，我的眼泪又来了。']  2、把每一句单独放在一行，并去除标点 # 首先读取到字符串 article = \u0026#39;\u0026#39; # 创建一个字符串 with open(\u0026#39;./Chinese.txt\u0026#39;, \u0026#39;r\u0026#39;) as file_project: for l in file_project.readlines(): article += l article '我说道，“爸爸，你走吧。”他望车外看了看，说，“我买几个橘子去。你就在此地，不要走动。”我看那边月台的栅栏外有几个卖东西的等着顾客。走到那边月台，须穿过铁道，须跳下去又爬上去。父亲是一个胖子，走过去自然要费事些。我本来要去的，他不肯，只好让他去。我看见他戴着黑布小帽，穿着黑布大马褂，深青布棉袍，蹒跚地走到铁道边，慢慢探身下去，尚不大难。可是他穿过铁道，要爬上那边月台，就不容易了。他用两手攀着上面，两脚再向上缩；他肥胖的身子向左微倾，显出努力的样子。这时我看见他的背影，我的泪很快地流下来了。我赶紧拭干了泪，怕他看见，也怕别人看见。我再向外看时，他已抱了朱红的橘子望回走了。过铁道时，他先将橘子散放在地上，自己慢慢爬下，再抱起橘子走。到这边时，我赶紧去搀他。他和我走到车上，将橘子一股脑儿放在我的皮大衣上。于是扑扑衣上的泥土，心里很轻松似的，过一会说，“我走了；到那边来信！”我望着他走出去。他走了几步，回过头看见我，说，“进去吧，里边没人。”等他的背影混入来来往往的人里，再找不着了，我便进来坐下，我的眼泪又来了。'  # 使用replace函数，将标点替换为换行符 result_1 = article.replace(\u0026#39;，\u0026#39;,\u0026#39;\\n\u0026#39;).\\ replace(\u0026#39;“\u0026#39;, \u0026#39;\u0026#39;).\\ replace(\u0026#39;”\u0026#39;, \u0026#39;\u0026#39;).\\ replace(\u0026#39;！\u0026#39;, \u0026#39;\\n\u0026#39;).\\ replace(\u0026#39;。\u0026#39;, \u0026#39;\\n\u0026#39;).\\ replace(\u0026#39;；\u0026#39;, \u0026#39;\u0026#39;) with open(\u0026#39;./result.txt\u0026#39;, \u0026#39;w\u0026#39;) as file_project: file_project.write(result_1) file_project.close() result_1 '我说道\\n爸爸\\n你走吧\\n他望车外看了看\\n说\\n我买几个橘子去\\n你就在此地\\n不要走动\\n我看那边月台的栅栏外有几个卖东西的等着顾客\\n走到那边月台\\n须穿过铁道\\n须跳下去又爬上去\\n父亲是一个胖子\\n走过去自然要费事些\\n我本来要去的\\n他不肯\\n只好让他去\\n我看见他戴着黑布小帽\\n穿着黑布大马褂\\n深青布棉袍\\n蹒跚地走到铁道边\\n慢慢探身下去\\n尚不大难\\n可是他穿过铁道\\n要爬上那边月台\\n就不容易了\\n他用两手攀着上面\\n两脚再向上缩他肥胖的身子向左微倾\\n显出努力的样子\\n这时我看见他的背影\\n我的泪很快地流下来了\\n我赶紧拭干了泪\\n怕他看见\\n也怕别人看见\\n我再向外看时\\n他已抱了朱红的橘子望回走了\\n过铁道时\\n他先将橘子散放在地上\\n自己慢慢爬下\\n再抱起橘子走\\n到这边时\\n我赶紧去搀他\\n他和我走到车上\\n将橘子一股脑儿放在我的皮大衣上\\n于是扑扑衣上的泥土\\n心里很轻松似的\\n过一会说\\n我走了到那边来信\\n我望着他走出去\\n他走了几步\\n回过头看见我\\n说\\n进去吧\\n里边没人\\n等他的背影混入来来往往的人里\\n再找不着了\\n我便进来坐下\\n我的眼泪又来了\\n'  3、去除常见词 \u0026amp;\u0026amp; 分词 （此处把出现次数最多的5个字作为停用词去除）\n# 首先做字符出现次数统计 word_count = {} # 初始化一个字典，用于储存字符出现次数 # 首先去除标点符号 result_2 = article.replace(\u0026#39;，\u0026#39;,\u0026#39;\u0026#39;).\\ replace(\u0026#39;“\u0026#39;, \u0026#39;\u0026#39;).\\ replace(\u0026#39;”\u0026#39;, \u0026#39;\u0026#39;).\\ replace(\u0026#39;！\u0026#39;, \u0026#39;\u0026#39;).\\ replace(\u0026#39;。\u0026#39;, \u0026#39;\u0026#39;).\\ replace(\u0026#39;；\u0026#39;, \u0026#39;\u0026#39;) for w in result_2: word_count.setdefault(w, 0) word_count[w] += 1 sorted(word_count.items(), key=lambda k:k[1], reverse=True) #排序后原字典不变 # word_count [('我', 17), ('他', 16), ('的', 14), ('走', 11), ('了', 10), ('去', 10), ('看', 9), ('子', 8), ('上', 8), ('边', 7), ('来', 7), ('着', 6), ('过', 6), ('道', 5), ('橘', 5), ('不', 5), ('到', 5), ('下', 5), ('见', 5), ('说', 4), ('地', 4), ('要', 4), ('那', 4), ('铁', 4), ('慢', 4), ('再', 4), ('时', 4), ('望', 3), ('外', 3), ('几', 3), ('个', 3), ('在', 3), ('月', 3), ('台', 3), ('穿', 3), ('爬', 3), ('是', 3), ('一', 3), ('布', 3), ('大', 3), ('向', 3), ('泪', 3), ('人', 3), ('里', 3), ('爸', 2), ('你', 2), ('吧', 2), ('车', 2), ('就', 2), ('等', 2), ('须', 2), ('又', 2), ('胖', 2), ('自', 2), ('黑', 2), ('身', 2), ('两', 2), ('；', 2), ('出', 2), ('这', 2), ('背', 2), ('影', 2), ('很', 2), ('赶', 2), ('紧', 2), ('怕', 2), ('抱', 2), ('回', 2), ('将', 2), ('放', 2), ('衣', 2), ('扑', 2), ('进', 2), ('往', 2), ('买', 1), ('此', 1), ('动', 1), ('栅', 1), ('栏', 1), ('有', 1), ('卖', 1), ('东', 1), ('西', 1), ('顾', 1), ('客', 1), ('跳', 1), ('父', 1), ('亲', 1), ('然', 1), ('费', 1), ('事', 1), ('些', 1), ('本', 1), ('肯', 1), ('只', 1), ('好', 1), ('让', 1), ('戴', 1), ('小', 1), ('帽', 1), ('马', 1), ('褂', 1), ('深', 1), ('青', 1), ('棉', 1), ('袍', 1), ('蹒', 1), ('跚', 1), ('探', 1), ('尚', 1), ('难', 1), ('可', 1), ('容', 1), ('易', 1), ('用', 1), ('手', 1), ('攀', 1), ('面', 1), ('脚', 1), ('缩', 1), ('肥', 1), ('左', 1), ('微', 1), ('倾', 1), ('显', 1), ('努', 1), ('力', 1), ('样', 1), ('快', 1), ('流', 1), ('拭', 1), ('干', 1), ('也', 1), ('别', 1), ('已', 1), ('朱', 1), ('红', 1), ('先', 1), ('散', 1), ('己', 1), ('起', 1), ('搀', 1), ('和', 1), ('股', 1), ('脑', 1), ('儿', 1), ('皮', 1), ('于', 1), ('泥', 1), ('土', 1), ('心', 1), ('轻', 1), ('松', 1), ('似', 1), ('会', 1), ('信', 1), ('步', 1), ('头', 1), ('没', 1), ('混', 1), ('入', 1), ('找', 1), ('便', 1), ('坐', 1), ('眼', 1)]  # 提取出出现次数最多的字符作为停用词 stop_words = [] count = 5 # print(word_count) sorted_dict = sorted(word_count.items(), key=lambda k:k[1], reverse=True) stop_words = [] for i in range(5): stop_words.append(sorted_dict[i][0]) # 分词 import jieba r_f = open(\u0026#39;./result.txt\u0026#39;, \u0026#39;r\u0026#39;) w_f = open(\u0026#39;./result_1.txt\u0026#39;, \u0026#39;w\u0026#39;) for row in r_f.readlines(): line = \u0026#39;\u0026#39; for r in row: if r in stop_words: continue else: line += r words = jieba.cut(line) w_f.write(\u0026#39; \u0026#39;.join(words)) 作业： 处理以下课文片段  要求：\n  从文件中读取 选取出现次数最多的5个字作为停用词 利用jieba分词进行分词处理  真的猛士，敢于直面惨淡的人生，敢于正视淋漓的鲜血。这是怎样的哀痛者和幸福者？然而造化又常常为庸人设计，以时间的流驶，来洗涤旧迹，仅使留下淡红的血色和微漠的悲哀。在这淡红的血色和微漠的悲哀中，又给人暂得偷生，维持着这似人非人的世界。我不知道这样的世界何时是一个尽头！ 我们还在这样的世上活着；我也早觉得有写一点东西的必要了。离三月十八日也已有两星期，忘却的救主快要降临了罢，我正有写一点东西的必要了。 （节选自《纪念刘和珍君》（鲁迅））\n","permalink":"http://yangchnet.github.io/Dessert/posts/python/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/","summary":"数据集的清洗 一、一般数据集的处理 1、读取 首先创建文件对象，然后进行读取，两种写法\n# 第一种 f = open(\u0026#39;./Chinese.txt\u0026#39;, \u0026#39;r\u0026#39;) # 节选自：《父亲》（朱自清） article = [] # 创建一个列表 for l in f.readlines(): article.append(l) article ['我说道，“爸爸，你走吧。”他望车外看了看，说，“我买几个橘子去。你就在此地，不要走动。”我看那边月台的栅栏外有几个卖东西的等着顾客。走到那边月台，须穿过铁道，须跳下去又爬上去。父亲是一个胖子，走过去自然要费事些。我本来要去的，他不肯，只好让他去。我看见他戴着黑布小帽，穿着黑布大马褂，深青布棉袍，蹒跚地走到铁道边，慢慢探身下去，尚不大难。可是他穿过铁道，要爬上那边月台，就不容易了。他用两手攀着上面，两脚再向上缩；他肥胖的身子向左微倾，显出努力的样子。这时我看见他的背影，我的泪很快地流下来了。我赶紧拭干了泪，怕他看见，也怕别人看见。我再向外看时，他已抱了朱红的橘子望回走了。过铁道时，他先将橘子散放在地上，自己慢慢爬下，再抱起橘子走。到这边时，我赶紧去搀他。他和我走到车上，将橘子一股脑儿放在我的皮大衣上。于是扑扑衣上的泥土，心里很轻松似的，过一会说，“我走了；到那边来信！”我望着他走出去。他走了几步，回过头看见我，说，“进去吧，里边没人。”等他的背影混入来来往往的人里，再找不着了，我便进来坐下，我的眼泪又来了。']  # 第二种 article = [] # 创建一个列表 with open(\u0026#39;./Chinese.txt\u0026#39;, \u0026#39;r\u0026#39;) as file_project: for l in file_project.readlines(): article.append(l) article ['我说道，“爸爸，你走吧。”他望车外看了看，说，“我买几个橘子去。你就在此地，不要走动。”我看那边月台的栅栏外有几个卖东西的等着顾客。走到那边月台，须穿过铁道，须跳下去又爬上去。父亲是一个胖子，走过去自然要费事些。我本来要去的，他不肯，只好让他去。我看见他戴着黑布小帽，穿着黑布大马褂，深青布棉袍，蹒跚地走到铁道边，慢慢探身下去，尚不大难。可是他穿过铁道，要爬上那边月台，就不容易了。他用两手攀着上面，两脚再向上缩；他肥胖的身子向左微倾，显出努力的样子。这时我看见他的背影，我的泪很快地流下来了。我赶紧拭干了泪，怕他看见，也怕别人看见。我再向外看时，他已抱了朱红的橘子望回走了。过铁道时，他先将橘子散放在地上，自己慢慢爬下，再抱起橘子走。到这边时，我赶紧去搀他。他和我走到车上，将橘子一股脑儿放在我的皮大衣上。于是扑扑衣上的泥土，心里很轻松似的，过一会说，“我走了；到那边来信！”我望着他走出去。他走了几步，回过头看见我，说，“进去吧，里边没人。”等他的背影混入来来往往的人里，再找不着了，我便进来坐下，我的眼泪又来了。']  2、把每一句单独放在一行，并去除标点 # 首先读取到字符串 article = \u0026#39;\u0026#39; # 创建一个字符串 with open(\u0026#39;./Chinese.txt\u0026#39;, \u0026#39;r\u0026#39;) as file_project: for l in file_project.readlines(): article += l article '我说道，“爸爸，你走吧。”他望车外看了看，说，“我买几个橘子去。你就在此地，不要走动。”我看那边月台的栅栏外有几个卖东西的等着顾客。走到那边月台，须穿过铁道，须跳下去又爬上去。父亲是一个胖子，走过去自然要费事些。我本来要去的，他不肯，只好让他去。我看见他戴着黑布小帽，穿着黑布大马褂，深青布棉袍，蹒跚地走到铁道边，慢慢探身下去，尚不大难。可是他穿过铁道，要爬上那边月台，就不容易了。他用两手攀着上面，两脚再向上缩；他肥胖的身子向左微倾，显出努力的样子。这时我看见他的背影，我的泪很快地流下来了。我赶紧拭干了泪，怕他看见，也怕别人看见。我再向外看时，他已抱了朱红的橘子望回走了。过铁道时，他先将橘子散放在地上，自己慢慢爬下，再抱起橘子走。到这边时，我赶紧去搀他。他和我走到车上，将橘子一股脑儿放在我的皮大衣上。于是扑扑衣上的泥土，心里很轻松似的，过一会说，“我走了；到那边来信！”我望着他走出去。他走了几步，回过头看见我，说，“进去吧，里边没人。”等他的背影混入来来往往的人里，再找不着了，我便进来坐下，我的眼泪又来了。'  # 使用replace函数，将标点替换为换行符 result_1 = article.","title":"数据集的清洗"},{"content":"朴素贝叶斯 1、理论部分 1.1、贝叶斯公式 $$P(c|x)=\\frac{P(c)P(x|c)}{P(x)}\\qquad\\dots(1)$$\n其中，$P(c)$是类“先验概率”；$P(x|c)$是样本$x$相对于类标记$c$的类条件概率，或称为“似然”；$P(x)$是用于归一化的“证据因子”。对给定样本$x$，证据因子$P(x)$与类标记无关，因此估计$P(c|x)$的问题就转化为如何基于训练数据$D$来估计先验$P(c)$和似然$P(x|c)$\n类先验概率$P(c)$表达了样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足的独立同分布样本时，$P(c)$可通过各类样本出现的频率来进行估计。\n对类条件概率$(P(x|c))$来说，由于它涉及关于$x$所有属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难。为避开这个障碍，朴素贝叶斯分类器采用了“属性条件独立性假设”；对已知类别，假设所有属性相互独立。换言之，假设每个属性独立的对分类结果产生影响。\n基于属性条件独立性假设，贝叶斯公式可重写为： $$P(c|x)=\\frac{P(c)P(x|c)}{P(x)}\\qquad=\\frac{P(c)}{P(x)}\\prod_{i=1}^d{P(x_i|c)}\\dots(2)$$ 其中$d$为属性数目，$x_i$为$x$在第i个属性上的取值\n由于对于所有类别来说$P(x)$相同，因此贝叶斯判定准则：$$h_{nb}(x)=arg max_{c\\in y}P(c)\\prod_{i=1}^d{P(x_i|c)}\\dots(3)$$\n显然，朴素贝叶斯分类器的训练过程就是基于训练集$D$来估计类先验概率$P(c)$，并为每个属性估计条件概率$P(x_i|c)$\n令$D_c$表示训练集$D$中第$c$类样本组成的集合，若有充足的独立同分布样本，则可容易的估计出先验概率：$$P(c)=\\frac{|D_c|}{|D|}\\dots(4)$$\n对离散属性而言，令$D_{c,x_i}$表示$D_c$中在第$i$个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i|c)$可估计为$$P(x_i|c)=\\frac{|D_{c,x_i}|}{|D_c|}\\qquad\\dots(5)$$ 为了避免其他属性携带的信息被训练集中未出现的属性值抹去，在估计概率值时通常要进行“平滑”，常用“拉普拉斯修正”。具体来说，令$N$表示训练集$D$中可能的类别数，$N_i$表示第$i$个属性可能的取值数，则(4)(5)两式分别修正为：$$\\hat{P}(c)=\\frac{D_c+1}{|D|+N}\\qquad\\dots(6)$$ $$\\hat{P}(x_i|c)=\\frac{D_{c,x_i}+1}{|D|+N}\\qquad\\dots(7)$$\n2、实战演练 2.1、加载数据集 import numpy as np def loadDataSet(): \u0026#34;\u0026#34;\u0026#34; 导入数据， 1代表脏话 @ return postingList: 数据集 @ return classVec: 分类向量 \u0026#34;\u0026#34;\u0026#34; postingList = [[\u0026#39;my\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;has\u0026#39;, \u0026#39;flea\u0026#39;, \u0026#39;problems\u0026#39;, \u0026#39;help\u0026#39;, \u0026#39;please\u0026#39;], [\u0026#39;maybe\u0026#39;, \u0026#39;not\u0026#39;, \u0026#39;take\u0026#39;, \u0026#39;him\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;park\u0026#39;, \u0026#39;stupid\u0026#39;], [\u0026#39;my\u0026#39;, \u0026#39;dalmation\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;so\u0026#39;, \u0026#39;cute\u0026#39;, \u0026#39;I\u0026#39;, \u0026#39;love\u0026#39;, \u0026#39;him\u0026#39;], [\u0026#39;stop\u0026#39;, \u0026#39;posting\u0026#39;, \u0026#39;stupid\u0026#39;, \u0026#39;worthless\u0026#39;, \u0026#39;garbage\u0026#39;], [\u0026#39;mr\u0026#39;, \u0026#39;licks\u0026#39;, \u0026#39;ate\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;steak\u0026#39;, \u0026#39;how\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;stop\u0026#39;, \u0026#39;him\u0026#39;], [\u0026#39;quit\u0026#39;, \u0026#39;buying\u0026#39;, \u0026#39;worthless\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;food\u0026#39;, \u0026#39;stupid\u0026#39;]] classVec = [0, 1, 0, 1, 0, 1] return postingList, classVec 导入训练集及其分类，1代表是脏话，0代表不是\nloadDataSet() ([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']], [0, 1, 0, 1, 0, 1])  2.2、创建单词表 将训练数据中的每一个词都存储到词库中。\ndef createVocabList(dataSet): \u0026#34;\u0026#34;\u0026#34; 创建词库 @ param dataSet: 数据集 @ return vocabSet: 词库 \u0026#34;\u0026#34;\u0026#34; vocabSet = set([]) for document in dataSet: # 求并集 vocabSet = vocabSet | set(document) return list(vocabSet) listOPosts, listClasses = loadDataSet() myVocabList = createVocabList(listOPosts) myVocabList ['quit', 'how', 'problems', 'mr', 'dalmation', 'garbage', 'love', 'stop', 'maybe', 'him', 'take', 'to', 'stupid', 'food', 'not', 'cute', 'buying', 'flea', 'park', 'help', 'ate', 'dog', 'licks', 'please', 'so', 'has', 'my', 'is', 'posting', 'I', 'steak', 'worthless']  2.3、生成词向量 def setOfWords2Vec(vocabList, inputSet): \u0026#34;\u0026#34;\u0026#34; 文本词向量.词库中每个词当作一个特征，文本中有该词，该词特征就是1，没有就是0 @ param vocabList: 词表 @ param inputSet: 输入的数据集 @ return returnVec: 返回的向量 \u0026#34;\u0026#34;\u0026#34; returnVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print(\u0026#34;单词: %s不在词库中!\u0026#34; % word) return returnVec testEntry = [\u0026#39;love\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;dalmation\u0026#39;] thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) thisDoc array([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])  2.4、训练分类器 def trainNB0(trainMatrix, trainCategory): \u0026#34;\u0026#34;\u0026#34; 训练 @ param trainMatrix: 训练集 @ param trainCategory: 分类 \u0026#34;\u0026#34;\u0026#34; numTrainDocs = len(trainMatrix) # 训练数据的长度 numWords = len(trainMatrix[0]) # 训练数据的词汇量 pAbusive = sum(trainCategory) / float(numTrainDocs) # 防止某个类别计算出的概率为0，导致最后相乘都为0，所以初始词都赋值1，分母赋值为2. 拉普拉斯修正 p0Num = np.ones(numWords) # 分子 p1Num = np.ones(numWords) p0Denom = 2 # 分母 p1Denom = 2 for i in range(numTrainDocs): if trainCategory[i] == 1: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) # 这里使用log函数，方便计算，因为最后是比较大小，所有对结果没有影响。 p1Vect = np.log(p1Num / p1Denom) # P^(x_1|c) p0Vect = np.log(p0Num / p0Denom) # P^(x_2|c) return p0Vect, p1Vect, pAbusive 2.5、 进行分类 def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): \u0026#34;\u0026#34;\u0026#34; 判断大小 \u0026#34;\u0026#34;\u0026#34; p1 = sum(vec2Classify * p1Vec) # P(c)*P(x_1|c) p0 = sum(vec2Classify * p0Vec) # P(c)*P(x_2|c) if p1 \u0026gt; p0: return 1 else: return 0 2.6、测试 def testingNB(): listOPosts, listClasses = loadDataSet() myVocabList = createVocabList(listOPosts) trainMat = [] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) print(trainMat) # 查看训练集矩阵 p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses)) testEntry = [\u0026#39;love\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;dalmation\u0026#39;] thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry, \u0026#39;classified as: \u0026#39;, classifyNB(thisDoc, p0V, p1V, pAb)) testEntry = [\u0026#39;stupid\u0026#39;, \u0026#39;garbage\u0026#39;] thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry, \u0026#39;classified as: \u0026#39;, classifyNB(thisDoc, p0V, p1V, pAb)) testingNB() [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]] ['love', 'my', 'dalmation'] classified as: 0 ['stupid', 'garbage'] classified as: 1  ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/","summary":"朴素贝叶斯 1、理论部分 1.1、贝叶斯公式 $$P(c|x)=\\frac{P(c)P(x|c)}{P(x)}\\qquad\\dots(1)$$\n其中，$P(c)$是类“先验概率”；$P(x|c)$是样本$x$相对于类标记$c$的类条件概率，或称为“似然”；$P(x)$是用于归一化的“证据因子”。对给定样本$x$，证据因子$P(x)$与类标记无关，因此估计$P(c|x)$的问题就转化为如何基于训练数据$D$来估计先验$P(c)$和似然$P(x|c)$\n类先验概率$P(c)$表达了样本空间中各类样本所占的比例，根据大数定律，当训练集包含充足的独立同分布样本时，$P(c)$可通过各类样本出现的频率来进行估计。\n对类条件概率$(P(x|c))$来说，由于它涉及关于$x$所有属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难。为避开这个障碍，朴素贝叶斯分类器采用了“属性条件独立性假设”；对已知类别，假设所有属性相互独立。换言之，假设每个属性独立的对分类结果产生影响。\n基于属性条件独立性假设，贝叶斯公式可重写为： $$P(c|x)=\\frac{P(c)P(x|c)}{P(x)}\\qquad=\\frac{P(c)}{P(x)}\\prod_{i=1}^d{P(x_i|c)}\\dots(2)$$ 其中$d$为属性数目，$x_i$为$x$在第i个属性上的取值\n由于对于所有类别来说$P(x)$相同，因此贝叶斯判定准则：$$h_{nb}(x)=arg max_{c\\in y}P(c)\\prod_{i=1}^d{P(x_i|c)}\\dots(3)$$\n显然，朴素贝叶斯分类器的训练过程就是基于训练集$D$来估计类先验概率$P(c)$，并为每个属性估计条件概率$P(x_i|c)$\n令$D_c$表示训练集$D$中第$c$类样本组成的集合，若有充足的独立同分布样本，则可容易的估计出先验概率：$$P(c)=\\frac{|D_c|}{|D|}\\dots(4)$$\n对离散属性而言，令$D_{c,x_i}$表示$D_c$中在第$i$个属性上取值为$x_i$的样本组成的集合，则条件概率$P(x_i|c)$可估计为$$P(x_i|c)=\\frac{|D_{c,x_i}|}{|D_c|}\\qquad\\dots(5)$$ 为了避免其他属性携带的信息被训练集中未出现的属性值抹去，在估计概率值时通常要进行“平滑”，常用“拉普拉斯修正”。具体来说，令$N$表示训练集$D$中可能的类别数，$N_i$表示第$i$个属性可能的取值数，则(4)(5)两式分别修正为：$$\\hat{P}(c)=\\frac{D_c+1}{|D|+N}\\qquad\\dots(6)$$ $$\\hat{P}(x_i|c)=\\frac{D_{c,x_i}+1}{|D|+N}\\qquad\\dots(7)$$\n2、实战演练 2.1、加载数据集 import numpy as np def loadDataSet(): \u0026#34;\u0026#34;\u0026#34; 导入数据， 1代表脏话 @ return postingList: 数据集 @ return classVec: 分类向量 \u0026#34;\u0026#34;\u0026#34; postingList = [[\u0026#39;my\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;has\u0026#39;, \u0026#39;flea\u0026#39;, \u0026#39;problems\u0026#39;, \u0026#39;help\u0026#39;, \u0026#39;please\u0026#39;], [\u0026#39;maybe\u0026#39;, \u0026#39;not\u0026#39;, \u0026#39;take\u0026#39;, \u0026#39;him\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;park\u0026#39;, \u0026#39;stupid\u0026#39;], [\u0026#39;my\u0026#39;, \u0026#39;dalmation\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;so\u0026#39;, \u0026#39;cute\u0026#39;, \u0026#39;I\u0026#39;, \u0026#39;love\u0026#39;, \u0026#39;him\u0026#39;], [\u0026#39;stop\u0026#39;, \u0026#39;posting\u0026#39;, \u0026#39;stupid\u0026#39;, \u0026#39;worthless\u0026#39;, \u0026#39;garbage\u0026#39;], [\u0026#39;mr\u0026#39;, \u0026#39;licks\u0026#39;, \u0026#39;ate\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;steak\u0026#39;, \u0026#39;how\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;stop\u0026#39;, \u0026#39;him\u0026#39;], [\u0026#39;quit\u0026#39;, \u0026#39;buying\u0026#39;, \u0026#39;worthless\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;food\u0026#39;, \u0026#39;stupid\u0026#39;]] classVec = [0, 1, 0, 1, 0, 1] return postingList, classVec 导入训练集及其分类，1代表是脏话，0代表不是","title":"朴素贝叶斯"},{"content":"检查Apache配置文件语法错误 在/etc/apache2目录下输入apache2ctl configtest即可检查错误\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/%E6%A3%80%E6%9F%A5apache%E7%9A%84%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE%E8%AF%AD%E6%B3%95%E9%94%99%E8%AF%AF/","summary":"检查Apache配置文件语法错误 在/etc/apache2目录下输入apache2ctl configtest即可检查错误","title":"检查Apache配置文件语法错误"},{"content":"检查nginx的语法错误 使用nginx -t\n","permalink":"http://yangchnet.github.io/Dessert/posts/linux/%E6%A3%80%E6%9F%A5nginx%E7%9A%84%E8%AF%AD%E6%B3%95%E9%94%99%E8%AF%AF/","summary":"检查nginx的语法错误 使用nginx -t","title":"检查nginx的语法错误"},{"content":"模型评估 此节内容只针对分类模型，使用sklearn库\n1、准确率 accuracy_score函数计算精度，在多标签分类中，该函数返回子集精度。如果样本的整个预测标签集与真实的标签集严格匹配，则子集精度为1.0; 否则它是0.0。如果$\\hat{y}i$是第$i$类样本预测值，$y_i$是相应的真值，那么正确预测的分数$n\\text{samples}$被定义为$$\\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i = y_i)$$\nimport numpy as np from sklearn.metrics import accuracy_score y_pred = [0, 2, 1, 3] y_true = [0, 1, 2, 3] accuracy_score(y_true, y_pred) 0.5  accuracy_score(y_true, y_pred, normalize=False) # 若normalize为False,则返回正确分类的样本数 2  2、混淆矩阵 该confusion_matrix函数通过计算混淆矩阵来评估分类准确性，行对应于真正的类，列表示预测值。\nfrom sklearn.metrics import confusion_matrix y_true = [2, 0, 2, 2, 0, 1] y_pred = [0, 0, 2, 2, 0, 2] confusion_matrix(y_true, y_pred) array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])  3、汉明损失 如果$\\hat{y}j$是预测为第$j$类的样本，$y_j$是真值，$n\\text{labels}$是类别的数目，则两个样本之间的汉明损失定义为：$$L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_\\text{labels}} \\sum_{j=0}^{n_\\text{labels} - 1} 1(\\hat{y}_j \\not= y_j)$$ $1(x)$是指标函数\nfrom sklearn.metrics import hamming_loss y_pred = [1, 2, 3, 4] y_true = [2, 2, 3, 4] hamming_loss(y_true, y_pred) 0.25  4、查准率、查全率和f1度量 在二元分类任务中，术语“正”和“负”指的是分类器的预测，术语“真”和“假”指的是该预测是否与外部判断相对应。\n   。 实际类别 。     预测类别 tp(真 正) fp(假 正)   - fn(假 负) tn(真 负)    在这种情况下，我们可以定义查准率，查全率和f1度量的概念： $$\\text{precision} = \\frac{tp}{tp + fp},$$ $$\\text{recall} = \\frac{tp}{tp + fn},$$ $$F_\\beta = (1 + \\beta^2) \\frac{\\text{precision} \\times \\text{recall}}{\\beta^2 \\text{precision} + \\text{recall}}.$$\n查准率与查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。例如：若希望将正样本尽可能多的选出来，则可通过增加选择样本的数量来实现，如果将所有的样本都选中，那么所有的正样本也必然都被选上了，但这样查准率就会较低；若希望选出的样本中正样本比例尽可能高，则可只挑选最有把握的样本，但这样难免就会漏掉不少正样本，使得查全率较低。通常只有在一些简单任务中，才可能使查全率和查准率都很高。f1度量的一般形式\u0026mdash;$F_\\beta$，能让我们表达出对查准率/查全率的不同偏好。\n 详情可参考周志华《机器学习》P30-P32\n from sklearn import metrics y_pred = [0, 1, 0, 0] y_true = [0, 1, 0, 1] metrics.precision_score(y_true, y_pred) 1.0   metrics.recall_score(y_true, y_pred) 0.5   metrics.f1_score(y_true, y_pred) 0.6666666666666666   metrics.fbeta_score(y_true, y_pred, beta=0.5) 0.8333333333333334   metrics.fbeta_score(y_true, y_pred, beta=1) 0.6666666666666666   metrics.fbeta_score(y_true, y_pred, beta=2) 0.5555555555555556   metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5) (array([0.66666667, 1. ]), array([1. , 0.5]), array([0.71428571, 0.83333333]), array([2, 2]))  precision_recall_fscore_support计算每个类的查准率，查全率、f1度量和support，返回值：presion, recall, fbeta_score, support(每个标签出现的次数)\n 5、分类报告 该classification_report函数构建一个显示主要分类指标的文本报告\nfrom sklearn.metrics import classification_report y_true = [0, 1, 2, 2, 0] y_pred = [0, 0, 2, 1, 0] target_names = [\u0026#39;class 0\u0026#39;, \u0026#39;class 1\u0026#39;, \u0026#39;class 2\u0026#39;] print(classification_report(y_true, y_pred, target_names=target_names))  precision recall f1-score support class 0 0.67 1.00 0.80 2 class 1 0.00 0.00 0.00 1 class 2 1.00 0.50 0.67 2 micro avg 0.60 0.60 0.60 5 macro avg 0.56 0.50 0.49 5 weighted avg 0.67 0.60 0.59 5  6、 ROC曲线 ROC全称是“受试者工作特征”，根据学习器的预测结果对样例进行排序，按此顺序把样本作为正例进行预测，每次计算出两个重要的值，分别以它们为横纵坐标作图，就得到了ROC曲线。ROC曲线的纵轴是“真正例率（TPR）”，横轴是“假正例率（FPR）”，二者定义为：$$\\text{TPR} = \\frac{tp}{tp + fn}$$, $$\\text{FPR} = \\frac{fp}{tn + fp}$$\nROC曲线通常在Y轴上具有真正例率，在X轴上具有假正例率，这意味着图的左上角是“理想”点-假阳性率为零，真阳性率为1。因此，曲线下面积越大越好，The “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\nimport numpy as np # from pylab import * from sklearn.metrics import roc_curve, roc_auc_score y = np.array([1, 1, 2, 2]) scores = np.array([0.1, 0.4, 0.35, 0.8]) fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2) roc_auc = roc_auc_score(y, scores) plt.figure() lw = 2 plt.plot(fpr, tpr, color=\u0026#39;darkorange\u0026#39;, lw=lw, label=\u0026#39;ROC curve (area = %0.2f)\u0026#39; % roc_auc) plt.plot([0, 1], [0, 1], color=\u0026#39;navy\u0026#39;, lw=lw, linestyle=\u0026#39;--\u0026#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.title(\u0026#39;Receiver operating characteristic example\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.show()  由于数据过于简单，因此画出的图形也比较简单：(\n 作业  在上节课实验的基础上，计算出朴素贝叶斯针对所给数据集的准确率，汉明损失，混淆矩阵，查准率、查全率，f1度量和ROC，并画出ROC曲线 给定完整数据集，分别计算在使用完整数据集的10%,30%,50%,80%,100%数据时的查准率、查全率，f1度量和ROC，使用折线图表现出这些指标的变化情况，并画出在不同数据量下的ROC曲线  ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/%E5%88%86%E7%B1%BB%E6%8C%87%E6%A0%87/","summary":"模型评估 此节内容只针对分类模型，使用sklearn库\n1、准确率 accuracy_score函数计算精度，在多标签分类中，该函数返回子集精度。如果样本的整个预测标签集与真实的标签集严格匹配，则子集精度为1.0; 否则它是0.0。如果$\\hat{y}i$是第$i$类样本预测值，$y_i$是相应的真值，那么正确预测的分数$n\\text{samples}$被定义为$$\\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i = y_i)$$\nimport numpy as np from sklearn.metrics import accuracy_score y_pred = [0, 2, 1, 3] y_true = [0, 1, 2, 3] accuracy_score(y_true, y_pred) 0.5  accuracy_score(y_true, y_pred, normalize=False) # 若normalize为False,则返回正确分类的样本数 2  2、混淆矩阵 该confusion_matrix函数通过计算混淆矩阵来评估分类准确性，行对应于真正的类，列表示预测值。\nfrom sklearn.metrics import confusion_matrix y_true = [2, 0, 2, 2, 0, 1] y_pred = [0, 0, 2, 2, 0, 2] confusion_matrix(y_true, y_pred) array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])  3、汉明损失 如果$\\hat{y}j$是预测为第$j$类的样本，$y_j$是真值，$n\\text{labels}$是类别的数目，则两个样本之间的汉明损失定义为：$$L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_\\text{labels}} \\sum_{j=0}^{n_\\text{labels} - 1} 1(\\hat{y}_j \\not= y_j)$$ $1(x)$是指标函数","title":"模型评估"},{"content":"正则表达式 1、什么是正则表达式 正则表达式，又称规则表达式。（英语：Regular Expression，在代码中常简写为regex、regexp或RE），计算机科学的一个概念。正则表达式通常被用来检索、替换那些符合某个模式(规则)的文本。（摘自百度百科）\n你可能熟悉文本查找,即按下 Ctrl-F,输入你要查找的词。“正则表达式”更进一步,它们让你指定要查找的“模式”。你也许不知道一家公司的准确电话号码,但如果你住在美国或加拿大,你就知道有3 位数字,然后是一个短横线,然后是 4 位数字(有时候以 3 位区号开始)。因此作为一个人,你看到一个电话号码就知道:415-555-1234 是电话号码,但 4,155,551,234 不是。 正则表达式很有用,但如果不是程序员,很少会有人了解,它,尽管大多数现代文本编辑器和文字处理器(诸如微软的 Word 或 OpenOffice)都有查找和查找替换功能,可以根据正则表达式查找。正则表达式可以节约大量时间,不仅适用于软件用户,也适用于程序员。实际上,技术作家 Cory Doctorow 声称,甚至应该在教授编程之前,先教授正则表达式: “知道[正则表达式]可能意味着用 3 步解决一个问题,而不是用 3000 步。如果你是一个技术怪侠,别忘了你用几次击键就能解决的问题,其他人需要数天的烦琐工作才能解决,而且他们容易犯错。” 1 (摘自《Python编程快速上手—让繁琐工作自动化》)\n2、不用正则表达式来查找文本模式 假设你希望在字符串中查找电话号码。你知道模式:3 个数字,一个短横线,3 个数字,一个短横线,再是 4 个数字。例如:415-555-4242。 假定我们用一个名为 isPhoneNumber()的函数,来检查字符串是否匹配模式,它 返回 True 或 False。\ndef isPhoneNumber(text): if len(text) != 12: return False for i in range(0, 3): if not text[i].isdecimal(): return False if text[3] != \u0026#39;-\u0026#39;: return False for i in range(4, 7): if not text[i].isdecimal(): return False if text[7] != \u0026#39;-\u0026#39;: return False for i in range(8, 12): if not text[i].isdecimal(): return False return True print(\u0026#39;415-555-4242 is a phone number:\u0026#39;) print(isPhoneNumber(\u0026#39;415-555-4242\u0026#39;)) print(\u0026#39;Moshi moshi is a phone number:\u0026#39;) print(isPhoneNumber(\u0026#39;Moshi moshi\u0026#39;)) 415-555-4242 is a phone number: True Moshi moshi is a phone number: False  添加更多代码,才能在更长的字符串中寻找这种文本模式\nmessage = \u0026#39;Call me at 415-555-1011 tomorrow. 415-555-9999 is my office.\u0026#39; for i in range(len(message)): chunk = message[i:i+12] if isPhoneNumber(chunk): print(\u0026#39;Phone number found: \u0026#39; + chunk) print(\u0026#39;Done\u0026#39;) Phone number found: 415-555-1011 Phone number found: 415-555-9999 Done  3、使用正则表达式进行匹配 3.1、 创建正则表达式对象 # 导入re模块来使用正则 import re # 创建正则表达式 phoneNumRegex = re.compile(r\u0026#39;\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d\u0026#39;) 3.2、使用正则进行查询 phoneNumRegex = re.compile(r\u0026#39;\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d\u0026#39;) mo = phoneNumRegex.search(\u0026#39;My number is 415-555-4242.\u0026#39;) print(\u0026#39;Phone number found: \u0026#39; + mo.group()) Phone number found: 415-555-4242  3.3、利用括号分组 假定想要将区号从电话号码中分离。添加括号将在正则表达式中创建“分组”(\\d\\d\\d)-(\\d\\d\\d-\\d\\d\\d\\d)。然后可以使用 group()匹配对象方法,从个分组中获取匹配的文本。正则表达式字符串中的第一对括号是第 1 组。第二对括号是第 2 组。向 group()匹配对象方法传入整数 1 或 2,就可以取得匹配文本的不同部分。向 group()方法传入 0 或不传入参数,将返回整个匹配的文本\nphoneNumRegex = re.compile(r\u0026#39;(\\d\\d\\d)-(\\d\\d\\d-\\d\\d\\d\\d)\u0026#39;) mo = phoneNumRegex.search(\u0026#39;My number is 415-555-4242.\u0026#39;) # 第一个分组 mo.group(1) # 第二个分组 # mo.group(2) # 全部 # mo.group(0) # mo.group() '415'  3.4、用管道匹配多个分组 字符|称为“管道”正则表达式 r\u0026rsquo;Batman|Tina Fey\u0026rsquo;将匹配\u0026rsquo;Batman\u0026rsquo;或\u0026rsquo;Tina Fey'。如果 Batman 和 Tina Fey 都出现在被查找的字符串中,第一次出现的匹配文本,将作为 Match 对象返回\nheroRegex = re.compile (r\u0026#39;Batman|Tina Fey\u0026#39;) mo1 = heroRegex.search(\u0026#39;Batman and Tina Fey.\u0026#39;) mo1.group() 'Tina Fey'  mo2 = heroRegex.search(\u0026#39;Tina Fey and Batman.\u0026#39;) mo2.group() 'Tina Fey'  3.5、 用问号实现可选匹配 有时候,想匹配的模式是可选的。就是说,不论这段文本在不在,正则表达式都会认为匹配。字符?表明它前面的分组在这个模式中是可选的。\nbatRegex = re.compile(r\u0026#39;Bat(wo)?man\u0026#39;) mo1 = batRegex.search(\u0026#39;The Adventures of Batman\u0026#39;) mo1.group() 'Batwoman'  mo2 = batRegex.search(\u0026#39;The Adventures of Batwoman\u0026#39;) mo2.group() 'Batwoman'  3.6、 用星号匹配零次或多次 *(称为星号)意味着“匹配零次或多次”,即星号之前的分组,可以在文本中出现任意次。它可以完全不存在,或一次又一次地重复。\nbatRegex = re.compile(r\u0026#39;Bat(wo)*man\u0026#39;) mo1 = batRegex.search(\u0026#39;The Adventures of Batman\u0026#39;) mo1.group() 'Batman'  mo2 = batRegex.search(\u0026#39;The Adventures of Batwoman\u0026#39;) mo2.group() 'Batwoman'  mo3 = batRegex.search(\u0026#39;The Adventures of Batwowowowoman\u0026#39;) mo3.group() 'Batwowowowoman'  3.7、 用加号匹配一次或多次 *意味着“匹配零次或多次”,+(加号)则意味着“匹配一次或多次”。星号不要求分组出现在匹配的字符串中,但加号不同,加号前面的分组必须“至少出现一次”。这不是可选的。\nbatRegex = re.compile(r\u0026#39;Bat(wo)+man\u0026#39;) mo1 = batRegex.search(\u0026#39;The Adventures of Batwoman\u0026#39;) mo1.group() 'Batwoman'  mo2 = batRegex.search(\u0026#39;The Adventures of Batwowowowoman\u0026#39;) mo2.group() 'Batwowowowoman'  mo3 = batRegex.search(\u0026#39;The Adventures of Batman\u0026#39;) mo3 == None True   未完待续\n ","permalink":"http://yangchnet.github.io/Dessert/posts/python/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","summary":"正则表达式 1、什么是正则表达式 正则表达式，又称规则表达式。（英语：Regular Expression，在代码中常简写为regex、regexp或RE），计算机科学的一个概念。正则表达式通常被用来检索、替换那些符合某个模式(规则)的文本。（摘自百度百科）\n你可能熟悉文本查找,即按下 Ctrl-F,输入你要查找的词。“正则表达式”更进一步,它们让你指定要查找的“模式”。你也许不知道一家公司的准确电话号码,但如果你住在美国或加拿大,你就知道有3 位数字,然后是一个短横线,然后是 4 位数字(有时候以 3 位区号开始)。因此作为一个人,你看到一个电话号码就知道:415-555-1234 是电话号码,但 4,155,551,234 不是。 正则表达式很有用,但如果不是程序员,很少会有人了解,它,尽管大多数现代文本编辑器和文字处理器(诸如微软的 Word 或 OpenOffice)都有查找和查找替换功能,可以根据正则表达式查找。正则表达式可以节约大量时间,不仅适用于软件用户,也适用于程序员。实际上,技术作家 Cory Doctorow 声称,甚至应该在教授编程之前,先教授正则表达式: “知道[正则表达式]可能意味着用 3 步解决一个问题,而不是用 3000 步。如果你是一个技术怪侠,别忘了你用几次击键就能解决的问题,其他人需要数天的烦琐工作才能解决,而且他们容易犯错。” 1 (摘自《Python编程快速上手—让繁琐工作自动化》)\n2、不用正则表达式来查找文本模式 假设你希望在字符串中查找电话号码。你知道模式:3 个数字,一个短横线,3 个数字,一个短横线,再是 4 个数字。例如:415-555-4242。 假定我们用一个名为 isPhoneNumber()的函数,来检查字符串是否匹配模式,它 返回 True 或 False。\ndef isPhoneNumber(text): if len(text) != 12: return False for i in range(0, 3): if not text[i].isdecimal(): return False if text[3] != \u0026#39;-\u0026#39;: return False for i in range(4, 7): if not text[i].","title":"正则表达式"},{"content":"温度预测 1. 观察耶拿天气数据集的数据 import os fname = \u0026#39;./data/jena_climate_2009_2016.csv\u0026#39; f = open(fname) data = f.read() f.close() lines = data.split(\u0026#39;\\n\u0026#39;) header = lines[0].split(\u0026#39;,\u0026#39;) lines = lines[1:] print(header) print(len(lines)) ['\u0026quot;Date Time\u0026quot;', '\u0026quot;p (mbar)\u0026quot;', '\u0026quot;T (degC)\u0026quot;', '\u0026quot;Tpot (K)\u0026quot;', '\u0026quot;Tdew (degC)\u0026quot;', '\u0026quot;rh (%)\u0026quot;', '\u0026quot;VPmax (mbar)\u0026quot;', '\u0026quot;VPact (mbar)\u0026quot;', '\u0026quot;VPdef (mbar)\u0026quot;', '\u0026quot;sh (g/kg)\u0026quot;', '\u0026quot;H2OC (mmol/mol)\u0026quot;', '\u0026quot;rho (g/m**3)\u0026quot;', '\u0026quot;wv (m/s)\u0026quot;', '\u0026quot;max. wv (m/s)\u0026quot;', '\u0026quot;wd (deg)\u0026quot;'] 420551  # 解析数据 import numpy as np float_data = np.zeros((len(lines), len(header) - 1)) for i, line in enumerate(lines): values = [float(x) for x in line.split(\u0026#39;,\u0026#39;)[1:]] float_data[i, :] = values # 绘制温度时间序列 from matplotlib import pyplot as plt temp = float_data[:, 1] # 温度（摄氏度） plt.plot(range(len(temp)), temp) plt.show() plt.plot(range(1440), temp[:1440]) [\u0026lt;matplotlib.lines.Line2D at 0x7f596ff56978\u0026gt;]  2. 准备数据 问题描述：一个时间步是10分钟，没steps个时间步采样一次数据，给定过去lookback个时间步之内的数据，能否预测delay个时间步之后的温度？用到的参数如下:\nlookback = 720 # 给定过去5天内的观测数据\nsteps = 6 # 观测数据的采样频率是每小时一个数据点\ndelay = 144 # 目标是未来24小时之后的数据\n数据标准化\nmean = float_data[:200000].mean(axis=0) float_data -= mean std = float_data[:200000].std(axis=0) float_data /= std 下面这个生成器以当前的浮点数数组作为输入，并从最近的数据中生成数据批量，同时生成未来的目标温度。sample是输入数据的一个批量，targets是对应的目标温度数组\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6): \u0026#39;\u0026#39;\u0026#39; data:浮点数据组成的原始数组 lookback: 输入数据应该包括过去多少个时间步 delay：目标应该在未来多少个时间后 min_index 和 max_index: data数组中的索引，用于界定需要抽取那些时间步。 shuffle: 是打乱样本，还是按顺序抽取样本 batch_size:每个批量的样本数 step:数据采样的周期。 \u0026#39;\u0026#39;\u0026#39; if max_index is None: max_index = len(data) - delay - 1 i = min_index + lookback while 1: if shuffle: rows = np.random.randint( min_index + lookback, max_index, size=batch_size) else: if i + batch_size \u0026gt;= max_index: i = min_index + lookback rows = np.arange(i, min(i + batch_size, max_index)) i += len(rows) samples = np.zeros((len(rows), lookback // step, data.shape[-1])) targets = np.zeros((len(rows),)) for j, row in enumerate(rows): indices = range(rows[j] - lookback, rows[j], step) samples[j] = data[indices] targets[j] = data[rows[j] + delay][1] yield samples, targets 准备训练生成器、验证生成器和测试生成器\nlookback = 1440 step = 6 delay = 144 batch_size = 128 train_gen = generator(float_data, lookback=lookback, delay=delay, min_index=0, max_index=200000, shuffle=True, step=step, batch_size=batch_size) val_gen = generator(float_data, lookback=lookback, delay=delay, min_index=2000001, max_index=3000000, step=step, batch_size=batch_size) test_gen = generator(float_data, lookback=lookback, delay=delay, min_index=3000001, max_index=None, step=step, batch_size=batch_size) val_steps = (300000 - 200001 - lookback) // batch_size # 为了查看整个验证集，需要从val_gen中抽取多少次 test_steps = (len(float_data) - 300001 - lookback) // batch_size # 为了查看整个测试集，需要从test_gen中抽取多少次 3. 一种基本的机器学习方法 在尝试机器学习方法之前，建立一个基于常识的基准方法是很有用的；同样，在开始研究复杂且计算代价很高的模型（如RNN）之前，尝试使用简单且计算代价低的机器学习模型也是很有用的，比如小型的密集连接网络。这可以保证进一步增加问题的复杂度是合理的，并且会带来真正的好处\n# 训练并评估一个密集连接模型 from keras.models import Sequential from keras import layers from keras.optimizers import RMSprop model = Sequential() model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1]))) model.add(layers.Dense(32, activation=\u0026#39;relu\u0026#39;)) model.add(layers.Dense(1)) model.compile(optimizer=RMSprop(), loss=\u0026#39;mae\u0026#39;) history = model.fit_generator(train_gen, steps_per_epoch=500, epochs=20, validation_data=val_gen, validation_steps=val_steps) ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/%E6%B8%A9%E5%BA%A6%E9%A2%84%E6%B5%8B/","summary":"温度预测 1. 观察耶拿天气数据集的数据 import os fname = \u0026#39;./data/jena_climate_2009_2016.csv\u0026#39; f = open(fname) data = f.read() f.close() lines = data.split(\u0026#39;\\n\u0026#39;) header = lines[0].split(\u0026#39;,\u0026#39;) lines = lines[1:] print(header) print(len(lines)) ['\u0026quot;Date Time\u0026quot;', '\u0026quot;p (mbar)\u0026quot;', '\u0026quot;T (degC)\u0026quot;', '\u0026quot;Tpot (K)\u0026quot;', '\u0026quot;Tdew (degC)\u0026quot;', '\u0026quot;rh (%)\u0026quot;', '\u0026quot;VPmax (mbar)\u0026quot;', '\u0026quot;VPact (mbar)\u0026quot;', '\u0026quot;VPdef (mbar)\u0026quot;', '\u0026quot;sh (g/kg)\u0026quot;', '\u0026quot;H2OC (mmol/mol)\u0026quot;', '\u0026quot;rho (g/m**3)\u0026quot;', '\u0026quot;wv (m/s)\u0026quot;', '\u0026quot;max. wv (m/s)\u0026quot;', '\u0026quot;wd (deg)\u0026quot;'] 420551  # 解析数据 import numpy as np float_data = np.zeros((len(lines), len(header) - 1)) for i, line in enumerate(lines): values = [float(x) for x in line.","title":"温度预测"},{"content":"理解LSTM层与GRU层  SimpleRNN的问题在于，在时刻t，理论上来说，它应该能够记住许多时间部之前见过的各种信息，但实际上它是不可能学到这种长期依赖的。其原因在于“梯度消失”问题，这一效应类似于在层数较多的非循环网络中观察到的效应，随着层数的增加，网络最终变得无法训练。\n 1. LSTM层 LSTM层是SimpleRNN的一种变体，它增加了一种携带信息跨越多个时间步的方法。假设有一条传送带，其运行方向平行于你所处理的序列。序列中的信息可以在任意位置跳上传送带，然后被传送到更晚的时间步，并在需要时原封不动地跳回来。这实际上就是LSTM的原理：它保存信息以便后面使用，从而防止较早期的信号在处理过程中逐渐消失。\n1.1 Keras 中一个LSTM的例子 # 准备数据 from keras.datasets import imdb from keras.preprocessing import sequence max_features = 10000 maxlen = 500 batch_size = 32 print(\u0026#39;Loading data...\u0026#39;) (input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features) print(len(input_train), \u0026#39;train_sequences\u0026#39;) print(len(input_test), \u0026#39;test sequences\u0026#39;) print(\u0026#39;Pad sequences (samples x time)\u0026#39;) input_train = sequence.pad_sequences(input_train, maxlen=maxlen) input_test = sequence.pad_sequences(input_test, maxlen=maxlen) print(\u0026#39;input_train shape: \u0026#39;, input_train.shape) print(\u0026#39;input_test shape:\u0026#39;, input_test.shape) Loading data... 25000 train_sequences 25000 test sequences Pad sequences (samples x time) input_train shape: (25000, 500) input_test shape: (25000, 500)  # 使用Keras中的LSTM层 from keras.layers import LSTM, Dense from keras.models import Sequential from keras.layers import Embedding, SimpleRNN max_features = 10000 model = Sequential() model.add(Embedding(max_features, 32)) model.add(LSTM(32)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2) /usr/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. \u0026quot;Converting sparse IndexedSlices to a dense Tensor of unknown shape. \u0026quot; Train on 20000 samples, validate on 5000 samples Epoch 1/10 20000/20000 [==============================] - 487s 24ms/step - loss: 0.5123 - acc: 0.7542 - val_loss: 0.4508 - val_acc: 0.7902 Epoch 2/10 20000/20000 [==============================] - 489s 24ms/step - loss: 0.2947 - acc: 0.8866 - val_loss: 0.2914 - val_acc: 0.8816 Epoch 3/10 20000/20000 [==============================] - 482s 24ms/step - loss: 0.2368 - acc: 0.9094 - val_loss: 0.3113 - val_acc: 0.8738 Epoch 4/10 20000/20000 [==============================] - 530s 26ms/step - loss: 0.2033 - acc: 0.9258 - val_loss: 0.3399 - val_acc: 0.8534 Epoch 5/10 20000/20000 [==============================] - 480s 24ms/step - loss: 0.1735 - acc: 0.9384 - val_loss: 0.5543 - val_acc: 0.8110 Epoch 6/10 20000/20000 [==============================] - 465s 23ms/step - loss: 0.1608 - acc: 0.9438 - val_loss: 0.2991 - val_acc: 0.8796 Epoch 7/10 20000/20000 [==============================] - 454s 23ms/step - loss: 0.1406 - acc: 0.9494 - val_loss: 0.3217 - val_acc: 0.8870 Epoch 8/10 20000/20000 [==============================] - 448s 22ms/step - loss: 0.1361 - acc: 0.9531 - val_loss: 0.6040 - val_acc: 0.8540 Epoch 9/10 20000/20000 [==============================] - 449s 22ms/step - loss: 0.1270 - acc: 0.9553 - val_loss: 0.4197 - val_acc: 0.8782 Epoch 10/10 20000/20000 [==============================] - 448s 22ms/step - loss: 0.1165 - acc: 0.9596 - val_loss: 0.3421 - val_acc: 0.8856  ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/lstm/","summary":"理解LSTM层与GRU层  SimpleRNN的问题在于，在时刻t，理论上来说，它应该能够记住许多时间部之前见过的各种信息，但实际上它是不可能学到这种长期依赖的。其原因在于“梯度消失”问题，这一效应类似于在层数较多的非循环网络中观察到的效应，随着层数的增加，网络最终变得无法训练。\n 1. LSTM层 LSTM层是SimpleRNN的一种变体，它增加了一种携带信息跨越多个时间步的方法。假设有一条传送带，其运行方向平行于你所处理的序列。序列中的信息可以在任意位置跳上传送带，然后被传送到更晚的时间步，并在需要时原封不动地跳回来。这实际上就是LSTM的原理：它保存信息以便后面使用，从而防止较早期的信号在处理过程中逐渐消失。\n1.1 Keras 中一个LSTM的例子 # 准备数据 from keras.datasets import imdb from keras.preprocessing import sequence max_features = 10000 maxlen = 500 batch_size = 32 print(\u0026#39;Loading data...\u0026#39;) (input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features) print(len(input_train), \u0026#39;train_sequences\u0026#39;) print(len(input_test), \u0026#39;test sequences\u0026#39;) print(\u0026#39;Pad sequences (samples x time)\u0026#39;) input_train = sequence.pad_sequences(input_train, maxlen=maxlen) input_test = sequence.pad_sequences(input_test, maxlen=maxlen) print(\u0026#39;input_train shape: \u0026#39;, input_train.shape) print(\u0026#39;input_test shape:\u0026#39;, input_test.shape) Loading data... 25000 train_sequences 25000 test sequences Pad sequences (samples x time) input_train shape: (25000, 500) input_test shape: (25000, 500)  # 使用Keras中的LSTM层 from keras.","title":"理解LSTM层与GRU层"},{"content":"理解循环神经网络 1. 简单的循环神经网络 RNN以渐进的方式处理信息，同时保存一个关于所处理内容的内部模型，这个模型是根据过去的信息构建的，并随着新信息的进入而不断更新。\nRNN处理序列的方式是：遍历所有序列元素，并保存一个状态（State），其中包含与已查看内容相关的信息。\nRNN的伪代码：\nstate_t = 0 for input_t in input_sequence: output_t = f(input_t, state_t) state_t = output_t 可以给出具体的函数f,从输入和状态到输出的变换，其参数包括两个矩阵（W和U）和一个偏置向量。它类似于前馈网络中密集连接层所做的变换。\nstate_t = 0 for input_t in input_sequence: output_t = activation(dot(W, input_t) + dot(U, state_t) + b) state_t = output_t # 简单RNN的numpy实现 import numpy as np timesteps = 100 # 输入序列的时间步数 input_features = 32 # 输入特征空间的维度 output_features = 64 # 输出特征空间的维度 inputs = np.random.random((timesteps, input_features)) # 输入数据：随机噪声，仅作为示例 state_t = np.zeros((output_features,)) # 初试状态：全零向量 # 创建随机的权重矩阵 W = np.random.random((output_features, input_features)) U = np.random.random((output_features, output_features)) b = np.random.random((output_features, )) successive_outputs = [] for input_t in inputs: # input_t是形状为（input_features, )的向量 output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b) # 由输入和当前状态计算得到当前输出 successive_outputs.append(output_t) # 将这个输出保存到一个列表中 state_t = output_t # 更新网络的状态，用于下一个时间步 # 最终输出是一个形状为（timesteps, output_features）的二维张量 final_output_sequence = np.stack(successive_outputs, axis=0) final_output_sequence array([[0.99999999, 0.99999995, 0.99999999, ..., 0.99999999, 0.9999997 , 0.99999992], [1. , 1. , 1. , ..., 1. , 1. , 1. ], [1. , 1. , 1. , ..., 1. , 1. , 1. ], ..., [1. , 1. , 1. , ..., 1. , 1. , 1. ], [1. , 1. , 1. , ..., 1. , 1. , 1. ], [1. , 1. , 1. , ..., 1. , 1. , 1. ]])  2. Keras 中的循环层 上面numpy的简单实现，对应一个实际的Keras层，即SimpleRNN层。\nfrom keras.layers import SimpleRNN 与Keras中所有的循环层一样，SimpleRNN可以在两种不同的模式下运行：一种是返回每个时间步连续输出的完整序列，即形状为（batch_size, timesteps, output_features）的三维张量；另一种是只返回每个输入系列的最终输出，即形状为(batch_size, output_featrues)的二维张量。这个两种模式由return_sequences这个构造函数参数来控制。\nfrom keras.models import Sequential from keras.layers import Embedding, SimpleRNN model = Sequential() model.add(Embedding(10000, 32)) model.add(SimpleRNN(32)) #` 只返回最后一个时间步的输出 model.summary() Model: \u0026quot;sequential_2\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_2 (Embedding) (None, None, 32) 320000 _________________________________________________________________ simple_rnn_2 (SimpleRNN) (None, 32) 2080 ================================================================= Total params: 322,080 Trainable params: 322,080 Non-trainable params: 0 _________________________________________________________________  from keras.models import Sequential from keras.layers import Embedding, SimpleRNN model = Sequential() model.add(Embedding(10000, 32)) model.add(SimpleRNN(32, return_sequences=True)) # 返回完整的状态序列 model.summary() Model: \u0026quot;sequential_3\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_3 (Embedding) (None, None, 32) 320000 _________________________________________________________________ simple_rnn_3 (SimpleRNN) (None, 32) 2080 ================================================================= Total params: 322,080 Trainable params: 322,080 Non-trainable params: 0 _________________________________________________________________  为了提高网络的表示能力，将多个循环层逐个堆叠有时也是很有用的。在这种情况下，你需要让所有中间层都返回完整的输出序列。\nmodel = Sequential() model.add(Embedding(10000, 32)) model.add(SimpleRNN(32, return_sequences=True)) model.add(SimpleRNN(32, return_sequences=True)) model.add(SimpleRNN(32, return_sequences=True)) model.add(SimpleRNN(32)) model.summary() Model: \u0026quot;sequential_4\u0026quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_4 (Embedding) (None, None, 32) 320000 _________________________________________________________________ simple_rnn_4 (SimpleRNN) (None, None, 32) 2080 _________________________________________________________________ simple_rnn_5 (SimpleRNN) (None, None, 32) 2080 _________________________________________________________________ simple_rnn_6 (SimpleRNN) (None, None, 32) 2080 _________________________________________________________________ simple_rnn_7 (SimpleRNN) (None, 32) 2080 ================================================================= Total params: 328,320 Trainable params: 328,320 Non-trainable params: 0 _________________________________________________________________  3. 将RNN应用于IMDB数据集 3.1 准备数据 from keras.datasets import imdb from keras.preprocessing import sequence max_features = 10000 maxlen = 500 batch_size = 32 print(\u0026#39;Loading data...\u0026#39;) (input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features) print(len(input_train), \u0026#39;train_sequences\u0026#39;) print(len(input_test), \u0026#39;test sequences\u0026#39;) print(\u0026#39;Pad sequences (samples x time)\u0026#39;) input_train = sequence.pad_sequences(input_train, maxlen=maxlen) input_test = sequence.pad_sequences(input_test, maxlen=maxlen) print(\u0026#39;input_train shape: \u0026#39;, input_train.shape) print(\u0026#39;input_test shape:\u0026#39;, input_test.shape) Loading data... 25000 train_sequences 25000 test sequences Pad sequences (samples x time) input_train shape: (25000, 500) input_test shape: (25000, 500)  # 用Embedding层和SimpleRNN层来训练模型 from keras.layers import Dense model = Sequential() model.add(Embedding(max_features, 32)) model.add(SimpleRNN(32)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) model.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) history = model.fit(input_train, y_train, epochs=10, batch_size=10, validation_split=0.2) /usr/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. \u0026quot;Converting sparse IndexedSlices to a dense Tensor of unknown shape. \u0026quot; Train on 20000 samples, validate on 5000 samples Epoch 1/10 20000/20000 [==============================] - 4783s 239ms/step - loss: 0.5510 - acc: 0.7012 - val_loss: 0.4217 - val_acc: 0.8162 Epoch 2/10 1080/20000 [\u0026gt;.............................] - ETA: 1:14:34 - loss: 0.3898 - acc: 0.8250 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) \u0026lt;ipython-input-10-a41d37bef560\u0026gt; in \u0026lt;module\u0026gt; 8 9 model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc']) ---\u0026gt; 10 history = model.fit(input_train, y_train, epochs=10, batch_size=10, validation_split=0.2) /usr/lib/python3.7/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs) 1237 steps_per_epoch=steps_per_epoch, 1238 validation_steps=validation_steps, -\u0026gt; 1239 validation_freq=validation_freq) 1240 1241 def evaluate(self, /usr/lib/python3.7/site-packages/keras/engine/training_arrays.py in fit_loop(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq) 194 ins_batch[i] = ins_batch[i].toarray() 195 --\u0026gt; 196 outs = fit_function(ins_batch) 197 outs = to_list(outs) 198 for l, o in zip(out_labels, outs): /usr/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs) 3215 value = math_ops.cast(value, tensor.dtype) 3216 converted_inputs.append(value) -\u0026gt; 3217 outputs = self._graph_fn(*converted_inputs) 3218 return nest.pack_sequence_as(self._outputs_structure, 3219 [x.numpy() for x in outputs]) /usr/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs) 556 raise TypeError(\u0026quot;Keyword arguments {} unknown. Expected {}.\u0026quot;.format( 557 list(kwargs.keys()), list(self._arg_keywords))) --\u0026gt; 558 return self._call_flat(args) 559 560 def _filtered_call(self, args, kwargs): /usr/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args) 625 # Only need to override the gradient in graph mode and when we have outputs. 626 if context.executing_eagerly() or not self.outputs: --\u0026gt; 627 outputs = self._inference_function.call(ctx, args) 628 else: 629 self._register_gradient() /usr/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args) 413 attrs=(\u0026quot;executor_type\u0026quot;, executor_type, 414 \u0026quot;config_proto\u0026quot;, config), --\u0026gt; 415 ctx=ctx) 416 # Replace empty list with None 417 outputs = outputs or None /usr/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 58 tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name, 59 op_name, inputs, attrs, ---\u0026gt; 60 num_outputs) 61 except core._NotOkStatusException as e: 62 if name is not None: KeyboardInterrupt:  import matplotlib.pyplot as plt acc = history.history[\u0026#39;acc\u0026#39;] val_acc = history.history[\u0026#39;val_acc\u0026#39;] loss = history.history[\u0026#39;loss\u0026#39;] val_loss = history.history[\u0026#39;val_loss\u0026#39;] epochs = range(1, len(acc)+1) plt.plot(epochs, acc, \u0026#39;bo\u0026#39;, label=\u0026#39;Training acc\u0026#39;) plt.plot(epochs, val, \u0026#39;b\u0026#39;, label=\u0026#39;Validation acc\u0026#39;) plt.title(\u0026#34;Trainging and validation accuracy\u0026#34;) plt.legend() plt.figure() plt.plot(epochs, loss, \u0026#39;bo\u0026#39;, label=\u0026#39;Training loss\u0026#39;) plt.plot(epochs, val_loss, \u0026#39;b\u0026#39;, label=\u0026#39;Validation loss\u0026#39;) plt.title(\u0026#39;Training and validation loss\u0026#39;) plt.legend() plt.show() ","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/%E7%90%86%E8%A7%A3rnn/","summary":"理解循环神经网络 1. 简单的循环神经网络 RNN以渐进的方式处理信息，同时保存一个关于所处理内容的内部模型，这个模型是根据过去的信息构建的，并随着新信息的进入而不断更新。\nRNN处理序列的方式是：遍历所有序列元素，并保存一个状态（State），其中包含与已查看内容相关的信息。\nRNN的伪代码：\nstate_t = 0 for input_t in input_sequence: output_t = f(input_t, state_t) state_t = output_t 可以给出具体的函数f,从输入和状态到输出的变换，其参数包括两个矩阵（W和U）和一个偏置向量。它类似于前馈网络中密集连接层所做的变换。\nstate_t = 0 for input_t in input_sequence: output_t = activation(dot(W, input_t) + dot(U, state_t) + b) state_t = output_t # 简单RNN的numpy实现 import numpy as np timesteps = 100 # 输入序列的时间步数 input_features = 32 # 输入特征空间的维度 output_features = 64 # 输出特征空间的维度 inputs = np.random.random((timesteps, input_features)) # 输入数据：随机噪声，仅作为示例 state_t = np.zeros((output_features,)) # 初试状态：全零向量 # 创建随机的权重矩阵 W = np.","title":"理解循环神经网络"},{"content":"类型断言 类型断言是一个使用在接口上的操作，语法上看起来像是x.(T)，因此被称为断言类型，这里x是接口，T是类型。一个类型断言检查它操作对象的动态类型是否和断言的类型匹配。\n这里有两种可能：\n 如果断言的类型T是一个具体类型\n类型断言检查x的动态类型是否和T相同。如果检查成功了，类型断言的结果是x的动态值，即T。换句话说，具体类型的类型断言从它的操作对象中获得具体的值。如果检查失败，接下来这个操作会panic。  import ( \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; ) var w io.Writer w = os.Stdout f := w.(*os.File) // 类型检查成功了，所以f的值为os.Stdout f == os.Stdout // true fmt.Printf(\u0026#34;%p\u0026#34;, f)  输出\n 0xc0004560c0\n c := w.(*bytes.Buffer) // 类型检查失败  输出\n interface conversion: \u0026lt;io.Writer\u0026gt; is \u0026lt;*os.File\u0026gt;, not \u0026lt;*bytes.Buffer\u0026gt; 断言的类型T是一个接口类型  t, ok := i.(T) 如果i是类型T（实现了T接口），即检查成功了，那么t将是i的原值，ok为true；如果检查失败了，t将为T类型的零值，ok为false，并且不引发panic。\n对一个接口类型的类型断言改变了类型的表述方式，改变了可以获取的方法集合（通常更大）， 但是它保护了接口值内部的动态类型和值的部分。\nvar w io Wirter w = os.Stdout rw := w.(io.ReadWriter) w = new(ByterCounter) rw = w.(io.ReadWriter)  输出\n repl.go:1:10: expected \u0026lsquo;;\u0026rsquo;, found \u0026lsquo;IDENT\u0026rsquo; Wirter如果断言操作的对象是一个nil接口值，那么不论被断言的是什么类型，断言都会失败。\n对一个更少限制性的接口类型（更少的方法集合）做断言，因为它表现的就像赋值操作一样，除了对于nil接口值的情况。\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/go%E4%B8%AD%E7%9A%84%E7%B1%BB%E5%9E%8B%E6%96%AD%E8%A8%80-unc/","summary":"类型断言 类型断言是一个使用在接口上的操作，语法上看起来像是x.(T)，因此被称为断言类型，这里x是接口，T是类型。一个类型断言检查它操作对象的动态类型是否和断言的类型匹配。\n这里有两种可能：\n 如果断言的类型T是一个具体类型\n类型断言检查x的动态类型是否和T相同。如果检查成功了，类型断言的结果是x的动态值，即T。换句话说，具体类型的类型断言从它的操作对象中获得具体的值。如果检查失败，接下来这个操作会panic。  import ( \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; ) var w io.Writer w = os.Stdout f := w.(*os.File) // 类型检查成功了，所以f的值为os.Stdout f == os.Stdout // true fmt.Printf(\u0026#34;%p\u0026#34;, f)  输出\n 0xc0004560c0\n c := w.(*bytes.Buffer) // 类型检查失败  输出\n interface conversion: \u0026lt;io.Writer\u0026gt; is \u0026lt;*os.File\u0026gt;, not \u0026lt;*bytes.Buffer\u0026gt; 断言的类型T是一个接口类型  t, ok := i.(T) 如果i是类型T（实现了T接口），即检查成功了，那么t将是i的原值，ok为true；如果检查失败了，t将为T类型的零值，ok为false，并且不引发panic。\n对一个接口类型的类型断言改变了类型的表述方式，改变了可以获取的方法集合（通常更大）， 但是它保护了接口值内部的动态类型和值的部分。\nvar w io Wirter w = os.Stdout rw := w.","title":"类型断言"},{"content":"网页静态文件找不到 在19-2-18的开始，突然发现网页的静态文件找不到了 在将static目录移动到app目录内之后，发现网页可以正常显示。\n 原来static目录是和app目录一个层级\n 针对此问题的思考 STATIC_URL = \u0026lsquo;/static/\u0026rsquo; 注意此处是url，即对于静态文件的定位，这是必要的前提配置 STATIC_URL的定义制定了静态资源的url，具体指各个app下的static目录  STATIC_ROOT = os.path.join(BASE_DIR, \u0026ldquo;static\u0026rdquo;) STATIC_ROOT是总的static目录，主要用于在运行 collectstatic命令时存储所有的静态文件  STATICFILES_DIRS = [os.path.join(BASE_DIR, \u0026ldquo;static\u0026rdquo;), \u0026lsquo;mysite/static\u0026rsquo;,]  STATICFILES_DIRS是一个列表，存放各个app的static目录及公共的static目录  ​\n官网配置   确保django.contrib.staticfiles包含在您的 INSTALLED_APPS。\n  在您的设置文件中，定义STATIC_URL，例如：\n  STATIC_URL = \u0026#39;/static/\u0026#39; 在模板中，使用static模板标记使用已配置的相对路径构建URL STATICFILES_STORAGE。  {% load static %} \u0026lt;img src=\u0026#34;{% static \u0026#34;my_app/example.jpg\u0026#34; %}\u0026#34; alt=\u0026#34;My image\u0026#34;/\u0026gt; 将静态文件存储static在应用程序中调用的文件夹中。例如my_app/static/my_app/example.jpg。  对于模板中的{% load static%} 当在模板中使用过load static之后，再次使用static时，将会使用STATICFILES_FINDERS寻找静态文件，其默认值为：\n[ \u0026#39;django.contrib.staticfiles.finders.FileSystemFinder\u0026#39;, \u0026#39;django.contrib.staticfiles.finders.AppDirectoriesFinder\u0026#39;, ] 对此的解释是，将会从STATICFILES_DIRS的目录中以及每个app下的static目录中寻找静态文件。\n在url后加入+ static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\n（要在setting定义 MEDIA_URL MEDIA_ROOT）\n","permalink":"http://yangchnet.github.io/Dessert/posts/django/%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6%E6%89%BE%E4%B8%8D%E5%88%B0/","summary":"网页静态文件找不到 在19-2-18的开始，突然发现网页的静态文件找不到了 在将static目录移动到app目录内之后，发现网页可以正常显示。\n 原来static目录是和app目录一个层级\n 针对此问题的思考 STATIC_URL = \u0026lsquo;/static/\u0026rsquo; 注意此处是url，即对于静态文件的定位，这是必要的前提配置 STATIC_URL的定义制定了静态资源的url，具体指各个app下的static目录  STATIC_ROOT = os.path.join(BASE_DIR, \u0026ldquo;static\u0026rdquo;) STATIC_ROOT是总的static目录，主要用于在运行 collectstatic命令时存储所有的静态文件  STATICFILES_DIRS = [os.path.join(BASE_DIR, \u0026ldquo;static\u0026rdquo;), \u0026lsquo;mysite/static\u0026rsquo;,]  STATICFILES_DIRS是一个列表，存放各个app的static目录及公共的static目录  ​\n官网配置   确保django.contrib.staticfiles包含在您的 INSTALLED_APPS。\n  在您的设置文件中，定义STATIC_URL，例如：\n  STATIC_URL = \u0026#39;/static/\u0026#39; 在模板中，使用static模板标记使用已配置的相对路径构建URL STATICFILES_STORAGE。  {% load static %} \u0026lt;img src=\u0026#34;{% static \u0026#34;my_app/example.jpg\u0026#34; %}\u0026#34; alt=\u0026#34;My image\u0026#34;/\u0026gt; 将静态文件存储static在应用程序中调用的文件夹中。例如my_app/static/my_app/example.jpg。  对于模板中的{% load static%} 当在模板中使用过load static之后，再次使用static时，将会使用STATICFILES_FINDERS寻找静态文件，其默认值为：\n[ \u0026#39;django.contrib.staticfiles.finders.FileSystemFinder\u0026#39;, \u0026#39;django.contrib.staticfiles.finders.AppDirectoriesFinder\u0026#39;, ] 对此的解释是，将会从STATICFILES_DIRS的目录中以及每个app下的static目录中寻找静态文件。\n在url后加入+ static(settings.MEDIA_URL, document_root=settings.","title":"网页静态文件找不到"},{"content":"记住账号密码 git config --global credential.helper store 然后再运行一遍git pull或git push就可以了\n使用密钥验证(推荐) 参考主机上设置两个git账号\n","permalink":"http://yangchnet.github.io/Dessert/posts/git/git%E6%AF%8F%E6%AC%A1%E9%83%BD%E8%A6%81%E8%BE%93%E5%85%A5%E5%AF%86%E7%A0%81/","summary":"记住账号密码 git config --global credential.helper store 然后再运行一遍git pull或git push就可以了\n使用密钥验证(推荐) 参考主机上设置两个git账号","title":"解决github每次push都要输入密码"},{"content":"鸭子类型 1. 什么是鸭子类型 只要走起路来像鸭子，叫起来像鸭子，就可以认为是鸭子。这就是鸭子类型\n2. 鸭子类型有什么作用？ 对应于golang中的接口的概念，一个接口定义了一组操作，这组操作可以看做是鸭子的走路，叫。也就是说，只要任何类型满足了这组方法，那么就可以看做是鸭子\u0026ndash;即满足了这个接口，可以看做是这个接口类型。\n","permalink":"http://yangchnet.github.io/Dessert/posts/golang/%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B/","summary":"鸭子类型 1. 什么是鸭子类型 只要走起路来像鸭子，叫起来像鸭子，就可以认为是鸭子。这就是鸭子类型\n2. 鸭子类型有什么作用？ 对应于golang中的接口的概念，一个接口定义了一组操作，这组操作可以看做是鸭子的走路，叫。也就是说，只要任何类型满足了这组方法，那么就可以看做是鸭子\u0026ndash;即满足了这个接口，可以看做是这个接口类型。","title":"鸭子类型"},{"content":"","permalink":"http://yangchnet.github.io/Dessert/posts/dlml/%E6%9C%AA%E5%91%BD%E5%90%8D/","summary":"","title":""},{"content":" Revise in 2022-3-18\n defer用法 defer用来延迟对某个语句的调用，常用于处理成对的操作，如打开、关闭、连接、断开连接，加锁、释放锁。通过defer语句，无论函数逻辑多复杂，都能保证在任何代码执行路径下，资源被释放。defer应该直接跟在请求资源的语句后。\ndefer语句将函数的调用push到一个列表中，当外层函数返回时，会执行保存的函数列表\n举个例子，这个程序打开两个文件并将一个文件的内容复制到另一个文件的函数\nfunc CopyFile(dstName, srcName string) (written int64, err error) { src, err := os.Open(srcName) if err != nil { return } dst, err := os.Create(dstName) if err != nil { return } written, err = io.Copy(dst, src) dst.Close() src.Close() return } 这个函数似乎可以正常工作，但其实存在一个bug，如果对os.Create的调用失败，该函数将返回但却不关闭源文件，通过在第二个return语句中调用src.Close可以解决这个问题。但是如果函数更加复杂，问题可能不会那么容易被发现和解决。通过使用defer语句，可以确保始终关闭文件。\nfunc CopyFile(dstName, srcName string) (written int64, err error) { src, err := os.Open(srcName) if err != nil { return } defer src.Close() dst, err := os.Create(dstName) if err != nil { return } defer dst.Close() return io.Copy(dst, src) } Defer语句使我们可以考虑在打开每个文件后立即将其关闭，从而确保无论函数中有return语句多少，文件都将被关闭。\ndefer 的特性 1. defer函数的参数是在defer函数声明时的参数 被defer调用的函数是在其调用被推入调用堆栈之前被估值的。\nfunc main() { var f = func() { fmt.Println(false) } defer f() f = func() { fmt.Println(true) } }  false 一个延迟调用的实参是在此调用被推入延迟调用堆栈之前估值的。\nimport \u0026#34;fmt\u0026#34; func a() { i := 0 defer fmt.Println(i) // 这里的i，值为0,那么defer调用时，i也为0  i++ return } a()  0 如果传入函数的实参为指针，那么对指针指向的值进行的更改会被defer显示出来。例如：\ntype User struct { Name string Passwd string } func main() { u := \u0026amp;User{ Name: \u0026#34;tom\u0026#34;, } defer f(u) u.Name = \u0026#34;Alice\u0026#34; } func f(u *User) { fmt.Println(u.Name) }  Alice 2. defer函数的执行顺序与声明顺序相反，类似于栈 func b() { for i := 0; i \u0026lt; 4; i++ { defer fmt.Println(i) } } b()  3 2 1 0 当遇到panic时，会先按照 defer 的后进先出的顺序执行，最后才会执行panic\nfunc main() { defer_call() } func defer_call() { defer func() { fmt.Println(\u0026#34;打印前\u0026#34;) }() defer func() { fmt.Println(\u0026#34;打印中\u0026#34;) }() defer func() { fmt.Println(\u0026#34;打印后\u0026#34;) }() panic(\u0026#34;触发异常\u0026#34;) } 打印后 打印中 打印前 panic: 触发异常 3. defer函数可以读取函数的返回值 func c() (i int) { defer func() { i++ }() return 1 } c()  2 defer语句中的函数会在return语句更新返回值变量后再执行，又因为在函数中定义的匿名函数可以访问该函数包括返回值变量在内的所有变量，所以，对匿名函数采用defer机制，可以使其访问函数的返回值。\n被延迟执行的匿名函数甚至可以修改函数返回给调用者的返回值\nfunc triple(x int) ( result int){ defer func() {result += x}() return x*2 } fmt.Println(triple(4))  12 4. defer调用的属主实参 方法的属主实参是在被调用时被推入延迟调用堆栈的\ntype T int func (t T) M(n int) T { print(t) return t } func main() { var t T defer t.M(1).M(2) t.M(3).M(4) }  1342 5. 循环体中的defer语句 在循环语句中的defer语句要特别注意，因为只有在函数执行完毕后，这些被延迟的函数才会执行。 下面的代码将会导致系统的文件描述符耗尽，因为在所有文件都被处理之前， 没有文件会被关闭。\nfo _,filename := range filenames{ f, err := os.Open(filename) if err != nil { return err } defer f.close() descriptors //... } 一种解决办法是将循环体中的defer语句移至另外一个函数，在每次循环时，调用这个函数。\nfor _, filename := { if err := doFile(filename); err != nil { return err } } func doFile(filename string) error{ f, err := os.Open(filename) if err != nil { return err } defer f.Close() //...process } ","permalink":"http://yangchnet.github.io/Dessert/posts/golang/defer%E7%94%A8%E6%B3%95/","summary":"Revise in 2022-3-18\n defer用法 defer用来延迟对某个语句的调用，常用于处理成对的操作，如打开、关闭、连接、断开连接，加锁、释放锁。通过defer语句，无论函数逻辑多复杂，都能保证在任何代码执行路径下，资源被释放。defer应该直接跟在请求资源的语句后。\ndefer语句将函数的调用push到一个列表中，当外层函数返回时，会执行保存的函数列表\n举个例子，这个程序打开两个文件并将一个文件的内容复制到另一个文件的函数\nfunc CopyFile(dstName, srcName string) (written int64, err error) { src, err := os.Open(srcName) if err != nil { return } dst, err := os.Create(dstName) if err != nil { return } written, err = io.Copy(dst, src) dst.Close() src.Close() return } 这个函数似乎可以正常工作，但其实存在一个bug，如果对os.Create的调用失败，该函数将返回但却不关闭源文件，通过在第二个return语句中调用src.Close可以解决这个问题。但是如果函数更加复杂，问题可能不会那么容易被发现和解决。通过使用defer语句，可以确保始终关闭文件。\nfunc CopyFile(dstName, srcName string) (written int64, err error) { src, err := os.Open(srcName) if err != nil { return } defer src.","title":"defer用法"}]