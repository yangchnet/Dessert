<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>拥抱大模型（五）：Memory，让大模型拥有记忆 | Linote</title><meta name=keywords content="memory"><meta name=description content="要让大模型有记忆也很简单，你只需要把之前的对话传递给他就行，在ConversationChain中，就使用了这一技巧：
from langchain import OpenAI from langchain.chains import ConversationChain # 初始化大语言模型 llm = OpenAI( temperature=0.5, model_name=&#34;text-davinci-003&#34; ) # 初始化对话链 conv_chain = ConversationChain(llm=llm) # 打印对话的模板 print(conv_chain.prompt.template) 来看下模板：
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI: 可以看到，在模板里又一个history字段，这里存储了之前对话的上下文信息。当有了 {history} 参数，以及 Human 和 AI 这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理。 langchain中提供的记忆机制 ConversationBufferMemory 在 LangChain 中，通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。下面，我们就在对话链中引入 ConversationBufferMemory。"><meta name=author content="李昌"><link rel=canonical href=http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%94memory%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86/><link crossorigin=anonymous href=/Dessert/assets/css/stylesheet.min.7e145c6c051b0f6645e8d84c6faed7fed1214bbe82c223c2c19815bee6ee8403.css integrity="sha256-fhRcbAUbD2ZF6NhMb67X/tEhS76CwiPCwZgVvubuhAM=" rel="preload stylesheet" as=style><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Fira+Mono&display=swap" rel=stylesheet><script defer crossorigin=anonymous src=/Dessert/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://raw.githubusercontent.com/lich-Img/blogImg/master/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://raw.githubusercontent.com/lich-Img/blogImg/master/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://raw.githubusercontent.com/lich-Img/blogImg/master/favicon32.ico><link rel=apple-touch-icon href=http://yangchnet.github.io/Dessert/apple-touch-icon.png><link rel=mask-icon href=http://yangchnet.github.io/Dessert/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.81.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="拥抱大模型（五）：Memory，让大模型拥有记忆"><meta property="og:description" content="要让大模型有记忆也很简单，你只需要把之前的对话传递给他就行，在ConversationChain中，就使用了这一技巧：
from langchain import OpenAI from langchain.chains import ConversationChain # 初始化大语言模型 llm = OpenAI( temperature=0.5, model_name=&#34;text-davinci-003&#34; ) # 初始化对话链 conv_chain = ConversationChain(llm=llm) # 打印对话的模板 print(conv_chain.prompt.template) 来看下模板：
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI: 可以看到，在模板里又一个history字段，这里存储了之前对话的上下文信息。当有了 {history} 参数，以及 Human 和 AI 这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理。 langchain中提供的记忆机制 ConversationBufferMemory 在 LangChain 中，通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。下面，我们就在对话链中引入 ConversationBufferMemory。"><meta property="og:type" content="article"><meta property="og:url" content="http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%94memory%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86/"><meta property="og:image" content="http://yangchnet.github.io/Dessert/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-04T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-04T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://yangchnet.github.io/Dessert/papermod-cover.png"><meta name=twitter:title content="拥抱大模型（五）：Memory，让大模型拥有记忆"><meta name=twitter:description content="要让大模型有记忆也很简单，你只需要把之前的对话传递给他就行，在ConversationChain中，就使用了这一技巧：
from langchain import OpenAI from langchain.chains import ConversationChain # 初始化大语言模型 llm = OpenAI( temperature=0.5, model_name=&#34;text-davinci-003&#34; ) # 初始化对话链 conv_chain = ConversationChain(llm=llm) # 打印对话的模板 print(conv_chain.prompt.template) 来看下模板：
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI: 可以看到，在模板里又一个history字段，这里存储了之前对话的上下文信息。当有了 {history} 参数，以及 Human 和 AI 这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理。 langchain中提供的记忆机制 ConversationBufferMemory 在 LangChain 中，通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。下面，我们就在对话链中引入 ConversationBufferMemory。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"http://yangchnet.github.io/Dessert/posts/"},{"@type":"ListItem","position":3,"name":"拥抱大模型（五）：Memory，让大模型拥有记忆","item":"http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%94memory%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"拥抱大模型（五）：Memory，让大模型拥有记忆","name":"拥抱大模型（五）：Memory，让大模型拥有记忆","description":"要让大模型有记忆也很简单，你只需要把之前的对话传递给他就行，在ConversationChain中，就使用了这一技巧：\nfrom langchain import OpenAI from langchain.chains import ConversationChain # 初始化大语言模型 llm = OpenAI( temperature=0.5, model_name=\u0026quot;text-davinci-003\u0026quot; ) # 初始化对话链 conv_chain = ConversationChain(llm=llm) # 打印对话的模板 print(conv_chain.prompt.template) 来看下模板：\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI: 可以看到，在模板里又一个history字段，这里存储了之前对话的上下文信息。当有了 {history} 参数，以及 Human 和 AI 这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理。 langchain中提供的记忆机制 ConversationBufferMemory 在 LangChain 中，通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。下面，我们就在对话链中引入 ConversationBufferMemory。","keywords":["memory"],"articleBody":"要让大模型有记忆也很简单，你只需要把之前的对话传递给他就行，在ConversationChain中，就使用了这一技巧：\nfrom langchain import OpenAI from langchain.chains import ConversationChain # 初始化大语言模型 llm = OpenAI( temperature=0.5, model_name=\"text-davinci-003\" ) # 初始化对话链 conv_chain = ConversationChain(llm=llm) # 打印对话的模板 print(conv_chain.prompt.template) 来看下模板：\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. Current conversation: {history} Human: {input} AI: 可以看到，在模板里又一个history字段，这里存储了之前对话的上下文信息。当有了 {history} 参数，以及 Human 和 AI 这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理。 langchain中提供的记忆机制 ConversationBufferMemory 在 LangChain 中，通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。下面，我们就在对话链中引入 ConversationBufferMemory。\nfrom langchain import OpenAI from langchain.chains import ConversationChain from langchain.chains.conversation.memory import ConversationBufferMemory # 初始化大语言模型 llm = OpenAI( temperature=0.5, model_name=\"text-davinci-003\") # 初始化对话链 conversation = ConversationChain( llm=llm, memory=ConversationBufferMemory() ) # 第一天的对话 # 回合1 conversation(\"我姐姐明天要过生日，我需要一束生日花束。\") print(\"第一次对话后的记忆:\", conversation.memory.buffer) 输出：\n第一次对话后的记忆: Human: 我姐姐明天要过生日，我需要一束生日花束。 AI: 哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。 在下一轮对话中，这些记忆会作为一部分传入提示。\n# 回合2 conversation(\"她喜欢粉色玫瑰，颜色是粉色的。\") print(\"第二次对话后的记忆:\", conversation.memory.buffer) 有了记忆机制，LLM 能够了解之前的对话内容，这样简单直接地存储所有内容为 LLM 提供了最大量的信息，但是新输入中也包含了更多的 Token（所有的聊天历史记录），这意味着响应时间变慢和更高的成本。\n为了节省token，langchain给我们提供了一系列的机制 ConversationBufferWindowMemory缓冲记忆 这个Memory的实现原理类似滑动窗口，它只记录了一定数量的过去互动\nfrom langchain import OpenAI from langchain.chains import ConversationChain from langchain.chains.conversation.memory import ConversationBufferWindowMemory # 创建大语言模型实例 llm = OpenAI( temperature=0.5, model_name=\"text-davinci-003\") # 初始化对话链 conversation = ConversationChain( llm=llm, memory=ConversationBufferWindowMemory(k=1) ) # 第一天的对话 # 回合1 result = conversation(\"我姐姐明天要过生日，我需要一束生日花束。\") print(result) # {'input': '我姐姐明天要过生日，我需要一束生日花束。', 'history': '', 'response': ' 哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？'} # 回合2 result = conversation(\"她喜欢粉色玫瑰，颜色是粉色的。\") # print(\"\\n第二次对话后的记忆:\\n\", conversation.memory.buffer) print(result) # {'input': '她喜欢粉色玫瑰，颜色是粉色的。', 'history': 'Human: 我姐姐明天要过生日，我需要一束生日花束。\\nAI: 哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？', 'response': ' 好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？'} # 第二天的对话 # 回合3 result = conversation(\"我又来了，还记得我昨天为什么要来买花吗？\") print(result) # {'input': '我又来了，还记得我昨天为什么要来买花吗？', 'history': 'Human: 她喜欢粉色玫瑰，颜色是粉色的。\\nAI: 好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？', 'response': ' 当然记得，你昨天来买花是为了给你喜欢的人送一束粉色玫瑰花束，表达你对TA的爱意。'} 这里设置k=1，也就是只会记住与 AI 之间的最新的互动，即只保留上一次的人类回应和 AI 的回应。\n这里第二次还能记住第一次的互动，但第三次显然没记住，回答错误。\n这种方法不适合记住遥远的互动，但它非常擅长限制使用的 Token 数量。如果只需要记住最近的互动，缓冲窗口记忆是一个很好的选择。但是，如果需要混合远期和近期的互动信息，则还有其他选择。 ConversationSummaryMemory对话总结记忆 ConversationSummaryMemory（对话总结记忆）的思路就是将对话历史进行汇总，然后再传递给 {history} 参数。这种方法旨在通过对之前的对话进行汇总来避免过度使用 Token。ConversationSummaryMemory 有这么几个核心特点。\n 汇总对话：此方法不是保存整个对话历史，而是每次新的互动发生时对其进行汇总，然后将其添加到之前所有互动的“运行汇总”中。 使用 LLM 进行汇总：该汇总功能由另一个 LLM 驱动，这意味着对话的汇总实际上是由 AI 自己进行的。 适合长对话：对于长对话，此方法的优势尤为明显。虽然最初使用的 Token 数量较多，但随着对话的进展，汇总方法的增长速度会减慢。与此同时，常规的缓冲内存模型会继续线性增长。  使用方法：\nfrom langchain.chains.conversation.memory import ConversationSummaryMemory # 初始化对话链 conversation = ConversationChain( llm=llm, memory=ConversationSummaryMemory(llm=llm) ) ConversationSummaryMemory使用的“history”不再是之前人类和 AI 对话的简单复制粘贴，而是经过了总结和整理之后的一个综述信息。\nConversationSummaryMemory 的优点是对于长对话，可以减少使用的 Token 数量，因此可以记录更多轮的对话信息，使用起来也直观易懂。不过，它的缺点是，对于较短的对话，可能会导致更高的 Token 使用。另外，对话历史的记忆完全依赖于中间汇总 LLM 的能力，还需要为汇总 LLM 使用 Token，这增加了成本，且并不限制对话长度。\n通过对话历史的汇总来优化和管理 Token 的使用，ConversationSummaryMemory 为那些预期会有多轮的、长时间对话的场景提供了一种很好的方法。然而，这种方法仍然受到 Token 数量的限制。在一段时间后，我们仍然会超过大模型的上下文窗口限制。\nConversationSummaryBufferMemory 对话总结缓冲记忆 它是一种混合记忆模型，结合了上述各种记忆机制，包括 ConversationSummaryMemory 和 ConversationBufferWindowMemory 的特点。这种模型旨在在对话中总结早期的互动，同时尽量保留最近互动中的原始内容。它是通过 max_token_limit 这个参数做到这一点的。当最新的对话文字长度在 300 字之内的时候，LangChain 会记忆原始对话内容；当对话文字超出了这个参数的长度，那么模型就会把所有超过预设长度的内容进行总结，以节省 Token 数量。\nfrom langchain.chains.conversation.memory import ConversationSummaryBufferMemory # 初始化对话链 conversation = ConversationChain( llm=llm, memory=ConversationSummaryBufferMemory( llm=llm, max_token_limit=300)) ConversationSummaryBufferMemory 的优势是通过总结可以回忆起较早的互动，而且有缓冲区确保我们不会错过最近的互动信息。当然，对于较短的对话，ConversationSummaryBufferMemory 也会增加 Token 数量。总体来说，ConversationSummaryBufferMemory 为我们提供了大量的灵活性。它是我们迄今为止的唯一记忆类型，可以回忆起较早的互动并完整地存储最近的互动。在节省 Token 数量方面，ConversationSummaryBufferMemory 与其他方法相比，也具有竞争力。 四种记忆机制的总结 当对话轮次逐渐增加时，各种记忆机制对 Token 的消耗数量： ","wordCount":"323","inLanguage":"en","datePublished":"2024-02-04T00:00:00Z","dateModified":"2024-02-04T00:00:00Z","author":{"@type":"Person","name":"李昌"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%94memory%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8B%A5%E6%9C%89%E8%AE%B0%E5%BF%86/"},"publisher":{"@type":"Organization","name":"Linote","logo":{"@type":"ImageObject","url":"https://raw.githubusercontent.com/lich-Img/blogImg/master/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><header class=header><nav class=nav><div class=logo><a href=http://yangchnet.github.io/Dessert accesskey=h title="Linote (Alt + H)">Linote</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=http://yangchnet.github.io/Dessert/archives/ title=存档><span>存档</span></a></li><li><a href=http://yangchnet.github.io/Dessert/categories/ title=分类><span>分类</span></a></li><li><a href=http://yangchnet.github.io/Dessert/search/ title=搜索><span>搜索</span></a></li><li><a href=http://yangchnet.github.io/Dessert/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://yangchnet.github.io/Dessert>Home</a>&nbsp;»&nbsp;<a href=http://yangchnet.github.io/Dessert/posts/>Posts</a></div><h1 class=post-title>拥抱大模型（五）：Memory，让大模型拥有记忆</h1><div class=post-meta><span title="2024-02-04 00:00:00 +0000 UTC">February 4, 2024</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;李昌</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#langchain%e4%b8%ad%e6%8f%90%e4%be%9b%e7%9a%84%e8%ae%b0%e5%bf%86%e6%9c%ba%e5%88%b6 aria-label=langchain中提供的记忆机制>langchain中提供的记忆机制</a><ul><li><a href=#conversationbuffermemory aria-label=ConversationBufferMemory>ConversationBufferMemory</a></li><li><a href=#conversationbufferwindowmemory%e7%bc%93%e5%86%b2%e8%ae%b0%e5%bf%86 aria-label=ConversationBufferWindowMemory缓冲记忆>ConversationBufferWindowMemory缓冲记忆</a></li><li><a href=#conversationsummarymemory%e5%af%b9%e8%af%9d%e6%80%bb%e7%bb%93%e8%ae%b0%e5%bf%86 aria-label=ConversationSummaryMemory对话总结记忆>ConversationSummaryMemory对话总结记忆</a></li><li><a href=#conversationsummarybuffermemory-%e5%af%b9%e8%af%9d%e6%80%bb%e7%bb%93%e7%bc%93%e5%86%b2%e8%ae%b0%e5%bf%86 aria-label="ConversationSummaryBufferMemory 对话总结缓冲记忆">ConversationSummaryBufferMemory 对话总结缓冲记忆</a></li><li><a href=#%e5%9b%9b%e7%a7%8d%e8%ae%b0%e5%bf%86%e6%9c%ba%e5%88%b6%e7%9a%84%e6%80%bb%e7%bb%93 aria-label=四种记忆机制的总结>四种记忆机制的总结</a></li></ul></li></ul></div></details></div><div class=post-content><p>要让大模型有记忆也很简单，你只需要把之前的对话传递给他就行，在ConversationChain中，就使用了这一技巧：</p><pre><code>from langchain import OpenAI
from langchain.chains import ConversationChain

# 初始化大语言模型
llm = OpenAI(
    temperature=0.5,
    model_name=&quot;text-davinci-003&quot;
)

# 初始化对话链
conv_chain = ConversationChain(llm=llm)

# 打印对话的模板
print(conv_chain.prompt.template)
</code></pre><p>来看下模板：</p><pre><code>The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
{history}
Human: {input}
AI:
</code></pre><p>可以看到，在模板里又一个history字段，这里存储了之前对话的上下文信息。当有了 {history} 参数，以及 Human 和 AI 这两个前缀，我们就能够把历史对话信息存储在提示模板中，并作为新的提示内容在新一轮的对话过程中传递给模型。—— 这就是记忆机制的原理。</p><h1 id=langchain中提供的记忆机制>langchain中提供的记忆机制<a hidden class=anchor aria-hidden=true href=#langchain中提供的记忆机制>#</a></h1><p></p><h2 id=conversationbuffermemory>ConversationBufferMemory<a hidden class=anchor aria-hidden=true href=#conversationbuffermemory>#</a></h2><p>在 LangChain 中，通过 ConversationBufferMemory（缓冲记忆）可以实现最简单的记忆机制。下面，我们就在对话链中引入 ConversationBufferMemory。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> langchain <span style=color:#f92672>import</span> OpenAI
<span style=color:#f92672>from</span> langchain.chains <span style=color:#f92672>import</span> ConversationChain
<span style=color:#f92672>from</span> langchain.chains.conversation.memory <span style=color:#f92672>import</span> ConversationBufferMemory

<span style=color:#75715e># 初始化大语言模型</span>
llm <span style=color:#f92672>=</span> OpenAI(
    temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>,
    model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;text-davinci-003&#34;</span>)

<span style=color:#75715e># 初始化对话链</span>
conversation <span style=color:#f92672>=</span> ConversationChain(
    llm<span style=color:#f92672>=</span>llm,
    memory<span style=color:#f92672>=</span>ConversationBufferMemory()
)

<span style=color:#75715e># 第一天的对话</span>
<span style=color:#75715e># 回合1</span>
conversation(<span style=color:#e6db74>&#34;我姐姐明天要过生日，我需要一束生日花束。&#34;</span>)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;第一次对话后的记忆:&#34;</span>, conversation<span style=color:#f92672>.</span>memory<span style=color:#f92672>.</span>buffer)
</code></pre></div><p>输出：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#960050;background-color:#1e0010>第一次对话后的记忆</span>: 
Human: <span style=color:#960050;background-color:#1e0010>我姐姐明天要过生日，我需要一束生日花束。</span>
AI:  <span style=color:#960050;background-color:#1e0010>哦，你姐姐明天要过生日，那太棒了！我可以帮你推荐一些生日花束，你想要什么样的？我知道有很多种，比如玫瑰、康乃馨、郁金香等等。</span>
</code></pre></div><p>在下一轮对话中，这些记忆会作为一部分传入提示。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#75715e># 回合2</span>
conversation(<span style=color:#e6db74>&#34;她喜欢粉色玫瑰，颜色是粉色的。&#34;</span>)
<span style=color:#66d9ef>print</span>(<span style=color:#e6db74>&#34;第二次对话后的记忆:&#34;</span>, conversation<span style=color:#f92672>.</span>memory<span style=color:#f92672>.</span>buffer)
</code></pre></div><p>有了记忆机制，LLM 能够了解之前的对话内容，这样简单直接地存储所有内容为 LLM 提供了最大量的信息，但是新输入中也包含了更多的 Token（所有的聊天历史记录），这意味着响应时间变慢和更高的成本。</p><p>为了节省token，langchain给我们提供了一系列的机制</p><h2 id=conversationbufferwindowmemory缓冲记忆>ConversationBufferWindowMemory缓冲记忆<a hidden class=anchor aria-hidden=true href=#conversationbufferwindowmemory缓冲记忆>#</a></h2><p>这个Memory的实现原理类似滑动窗口，它只记录了一定数量的过去互动</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> langchain <span style=color:#f92672>import</span> OpenAI
<span style=color:#f92672>from</span> langchain.chains <span style=color:#f92672>import</span> ConversationChain
<span style=color:#f92672>from</span> langchain.chains.conversation.memory <span style=color:#f92672>import</span> ConversationBufferWindowMemory

<span style=color:#75715e># 创建大语言模型实例</span>
llm <span style=color:#f92672>=</span> OpenAI(
    temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>,
    model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;text-davinci-003&#34;</span>)

<span style=color:#75715e># 初始化对话链</span>
conversation <span style=color:#f92672>=</span> ConversationChain(
    llm<span style=color:#f92672>=</span>llm,
    memory<span style=color:#f92672>=</span>ConversationBufferWindowMemory(k<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
)

<span style=color:#75715e># 第一天的对话</span>
<span style=color:#75715e># 回合1</span>
result <span style=color:#f92672>=</span> conversation(<span style=color:#e6db74>&#34;我姐姐明天要过生日，我需要一束生日花束。&#34;</span>)
<span style=color:#66d9ef>print</span>(result) <span style=color:#75715e># {&#39;input&#39;: &#39;我姐姐明天要过生日，我需要一束生日花束。&#39;, &#39;history&#39;: &#39;&#39;, &#39;response&#39;: &#39; 哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？&#39;}</span>
<span style=color:#75715e># 回合2</span>
result <span style=color:#f92672>=</span> conversation(<span style=color:#e6db74>&#34;她喜欢粉色玫瑰，颜色是粉色的。&#34;</span>)
<span style=color:#75715e># print(&#34;\n第二次对话后的记忆:\n&#34;, conversation.memory.buffer)</span>
<span style=color:#66d9ef>print</span>(result) <span style=color:#75715e># {&#39;input&#39;: &#39;她喜欢粉色玫瑰，颜色是粉色的。&#39;, &#39;history&#39;: &#39;Human: 我姐姐明天要过生日，我需要一束生日花束。\nAI: 哦，你姐姐明天要过生日！那太棒了！你想要一束什么样的花束呢？有很多种类可以选择，比如玫瑰花束、康乃馨花束、郁金香花束等等，你有什么喜欢的吗？&#39;, &#39;response&#39;: &#39; 好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？&#39;}</span>

<span style=color:#75715e># 第二天的对话</span>
<span style=color:#75715e># 回合3</span>
result <span style=color:#f92672>=</span> conversation(<span style=color:#e6db74>&#34;我又来了，还记得我昨天为什么要来买花吗？&#34;</span>)
<span style=color:#66d9ef>print</span>(result) <span style=color:#75715e># {&#39;input&#39;: &#39;我又来了，还记得我昨天为什么要来买花吗？&#39;, &#39;history&#39;: &#39;Human: 她喜欢粉色玫瑰，颜色是粉色的。\nAI: 好的，那粉色玫瑰花束怎么样？我可以帮你找到一束非常漂亮的粉色玫瑰花束，你觉得怎么样？&#39;, &#39;response&#39;: &#39; 当然记得，你昨天来买花是为了给你喜欢的人送一束粉色玫瑰花束，表达你对TA的爱意。&#39;}</span>
</code></pre></div><p>这里设置k=1，也就是只会记住与 AI 之间的最新的互动，即只保留上一次的人类回应和 AI 的回应。</p><p>这里第二次还能记住第一次的互动，但第三次显然没记住，回答错误。</p><p>这种方法不适合记住遥远的互动，但它非常擅长限制使用的 Token 数量。如果只需要记住最近的互动，缓冲窗口记忆是一个很好的选择。但是，如果需要混合远期和近期的互动信息，则还有其他选择。</p><h2 id=conversationsummarymemory对话总结记忆>ConversationSummaryMemory对话总结记忆<a hidden class=anchor aria-hidden=true href=#conversationsummarymemory对话总结记忆>#</a></h2><p>ConversationSummaryMemory（对话总结记忆）的思路就是将对话历史进行汇总，然后再传递给 {history} 参数。这种方法旨在通过对之前的对话进行汇总来避免过度使用 Token。ConversationSummaryMemory 有这么几个核心特点。</p><ul><li>汇总对话：此方法不是保存整个对话历史，而是每次新的互动发生时对其进行汇总，然后将其添加到之前所有互动的“运行汇总”中。</li><li>使用 LLM 进行汇总：该汇总功能由另一个 LLM 驱动，这意味着对话的汇总实际上是由 AI 自己进行的。</li><li>适合长对话：对于长对话，此方法的优势尤为明显。虽然最初使用的 Token 数量较多，但随着对话的进展，汇总方法的增长速度会减慢。与此同时，常规的缓冲内存模型会继续线性增长。</li></ul><p>使用方法：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> langchain.chains.conversation.memory <span style=color:#f92672>import</span> ConversationSummaryMemory

<span style=color:#75715e># 初始化对话链</span>
conversation <span style=color:#f92672>=</span> ConversationChain(
    llm<span style=color:#f92672>=</span>llm,
    memory<span style=color:#f92672>=</span>ConversationSummaryMemory(llm<span style=color:#f92672>=</span>llm)
)
</code></pre></div><p>ConversationSummaryMemory使用的“history”不再是之前人类和 AI 对话的简单复制粘贴，而是经过了总结和整理之后的一个综述信息。</p><p>ConversationSummaryMemory 的优点是对于长对话，可以减少使用的 Token 数量，因此可以记录更多轮的对话信息，使用起来也直观易懂。不过，它的缺点是，对于较短的对话，可能会导致更高的 Token 使用。另外，对话历史的记忆完全依赖于中间汇总 LLM 的能力，还需要为汇总 LLM 使用 Token，这增加了成本，且并不限制对话长度。</p><p>通过对话历史的汇总来优化和管理 Token 的使用，ConversationSummaryMemory 为那些预期会有多轮的、长时间对话的场景提供了一种很好的方法。然而，这种方法仍然受到 Token 数量的限制。在一段时间后，我们仍然会超过大模型的上下文窗口限制。</p><p></p><h2 id=conversationsummarybuffermemory-对话总结缓冲记忆>ConversationSummaryBufferMemory 对话总结缓冲记忆<a hidden class=anchor aria-hidden=true href=#conversationsummarybuffermemory-对话总结缓冲记忆>#</a></h2><p>它是一种混合记忆模型，结合了上述各种记忆机制，包括 ConversationSummaryMemory 和 ConversationBufferWindowMemory 的特点。这种模型旨在在对话中总结早期的互动，同时尽量保留最近互动中的原始内容。它是通过 max_token_limit 这个参数做到这一点的。当最新的对话文字长度在 300 字之内的时候，LangChain 会记忆原始对话内容；当对话文字超出了这个参数的长度，那么模型就会把所有超过预设长度的内容进行总结，以节省 Token 数量。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> langchain.chains.conversation.memory <span style=color:#f92672>import</span> ConversationSummaryBufferMemory

<span style=color:#75715e># 初始化对话链</span>
conversation <span style=color:#f92672>=</span> ConversationChain(
    llm<span style=color:#f92672>=</span>llm,
    memory<span style=color:#f92672>=</span>ConversationSummaryBufferMemory(
        llm<span style=color:#f92672>=</span>llm,
        max_token_limit<span style=color:#f92672>=</span><span style=color:#ae81ff>300</span>))
</code></pre></div><p>ConversationSummaryBufferMemory 的优势是通过总结可以回忆起较早的互动，而且有缓冲区确保我们不会错过最近的互动信息。当然，对于较短的对话，ConversationSummaryBufferMemory 也会增加 Token 数量。总体来说，ConversationSummaryBufferMemory 为我们提供了大量的灵活性。它是我们迄今为止的唯一记忆类型，可以回忆起较早的互动并完整地存储最近的互动。在节省 Token 数量方面，ConversationSummaryBufferMemory 与其他方法相比，也具有竞争力。</p><h2 id=四种记忆机制的总结>四种记忆机制的总结<a hidden class=anchor aria-hidden=true href=#四种记忆机制的总结>#</a></h2><p><img loading=lazy src=https://raw.githubusercontent.com/lich-Img/blogImg/master/img/20241220172609.png alt=20241220172609>
当对话轮次逐渐增加时，各种记忆机制对 Token 的消耗数量：
<img loading=lazy src=https://raw.githubusercontent.com/lich-Img/blogImg/master/img/20241220172630.png alt=20241220172630></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://yangchnet.github.io/Dessert/tags/memory/>memory</a></li></ul><nav class=paginav><a class=prev href=http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BA%8Cprompt%E7%BB%99%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9C%89%E7%94%A8%E7%9A%84%E6%8F%90%E7%A4%BA/><span class=title>« Prev Page</span><br><span>拥抱大模型（二）：Prompt，给大模型有用的提示</span></a>
<a class=next href=http://yangchnet.github.io/Dessert/posts/llm/%E6%8B%A5%E6%8A%B1%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%9Bchain%E5%83%8F%E4%BA%BA%E7%B1%BB%E4%B8%80%E6%A0%B7%E6%80%9D%E8%80%83/><span class=title>Next Page »</span><br><span>拥抱大模型（四）：Chain，像人类一样思考</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://yangchnet.github.io/Dessert>Linote</a></span>
<script src=https://utteranc.es/client.js repo=yangchnet/Dessert issue-term=pathname theme=github-light crossorigin=anonymous async></script><span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>